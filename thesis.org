* Config :ignore:
#+latex_class: mimosis
#+begin_src emacs-lisp :exports none  :results none
(unless (boundp 'org-latex-classes)
  (setq org-latex-classes nil))
(add-to-list 'org-latex-classes
             '("memoir"
               "\\documentclass{memoir}
    [NO-DEFAULT-PACKAGES]
    [PACKAGES]
    [EXTRA]
    \\newcommand{\\mboxparagraph}[1]{\\paragraph{#1}\\mbox{}\\\\}
    \\newcommand{\\mboxsubparagraph}[1]{\\subparagraph{#1}\\mbox{}\\\\}"
               ("\\chapter{%s}" . "\\chapter*{%s}")
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\mboxparagraph{%s}" . "\\mboxparagraph*{%s}")
               ("\\mboxsubparagraph{%s}" . "\\mboxsubparagraph*{%s}")))
(add-to-list 'org-latex-classes
             '("mimosis"
               "\\documentclass{mimosis-class/mimosis}
  [NO-DEFAULT-PACKAGES]
  [PACKAGES]
  [EXTRA]"
               ("\\chapter{%s}" . "\\addchap{%s}")
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
#+end_src
# #+EXPORT_FILE_NAME: ./tmp/thesis.pdf
** Org Mode Export Options :noexport:
#+EXCLUDE_TAGS: journal noexport
#+OPTIONS: title:nil toc:nil date:nil author:nil H:6

** Macros :ignore:
# #+MACRO: acronym #+latex_header: \newacronym[description={$1}]{$2}{$2}{$3}
#+MACRO: glossaryentry #+latex_header: \newglossaryentry{$1}{name={$2},description={$3},sort={$4}}
#+MACRO: acronym #+latex_header: \newacronym{$1}{$2}{$3}
# #+MACRO: newline @@latex:\hspace{0pt}\\@@ @@html:<br>@@
# #+MACRO: fourstar @@latex:\bigskip{\centering\color{BrickRed}\FourStar\par}\bigskip@@
# #+MACRO: clearpage @@latex:\clearpage@@@@odt:<text:p text:style-name="PageBreak"/>@@

** LaTeX Export Headers and Options :noexport:
*** Packages :ignore:
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{amsmath,amssymb,amsfonts}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{todonotes}
*** Font Awesome icons
#+LATEX_HEADER: \usepackage{fontawesome}
*** Maths cancel
#+LATEX_HEADER: \usepackage[makeroom]{cancel}
*** Footnotes
#+LATEX_HEADER: \usepackage{footnote}
*** Tensor indexing (pre subscripts)
#+LATEX_HEADER: \usepackage{tensor}

*** Epigraph (chapter quotes)
#+LATEX_HEADER: \usepackage{epigraph}
*** Grey box for block quotes
#+LATEX_HEADER: \usepackage[most]{tcolorbox}
#+LATEX_HEADER: \definecolor{block-gray}{gray}{0.85}
#+LATEX_HEADER: \newtcolorbox{myquote}{colback=block-gray,boxrule=0pt,boxsep=0pt,breakable}
# #+LATEX_HEADER: \newtcolorbox{myquote}{colback=block-gray,grow to right by=-10mm,grow to left by=-10mm, boxrule=0pt,boxsep=0pt,breakable}
*** Acronym and Glossary :ignore:
#+latex_header: \usepackage[acronym]{glossaries}
#+latex_header: \makeglossaries

*** Equation Definitions

#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \newcommand{\defeq}{\vcentcolon=}

*** Create a Definition theorem
#+LATEX_HEADER: \newtheorem{definition}{Definition}[section]
#+LATEX_HEADER: \newtheorem{theorem}{Theorem}[section]
#+LATEX_HEADER: \newtheorem{lemma}{Lemma}[section]
*** Floating images configuration

By default,  if a figure consumes 60% of the page it will get its own float-page. To change that we have to adjust the value of the floatpagefraction derivative.
#+latex_header: \renewcommand{\floatpagefraction}{.8}%

See more information [[https://tex.stackexchange.com/questions/68516/avoid-that-figure-gets-its-own-page][here]].

*** Hyperref
Self-explanatory.
#+latex_header: \usepackage[colorlinks=true, citecolor=BrickRed, linkcolor=BrickRed, urlcolor=BrickRed]{hyperref}

*** Bookmarks

The bookmark package implements a new bookmark (outline) organisation for package hyperref. This lets us change the "tree-navigation" associated with the generated pdf and constrain the menu only to H:2.
#+latex_header: \usepackage{bookmark}
#+latex_header: \bookmarksetup{depth=2}

*** BBding

Symbols such as diamond suit, which can be used for aesthetically separating paragraphs, could be added with the package =fdsymbol=. I'll use bbding which offers the more visually appealing =\FourStar=. I took this idea from seeing the thesis of the mimosis package author.
#+latex_header: \usepackage{bbding}

*** CS Quotes
The [[https://ctan.org/pkg/csquotes][csquotes]] package offers context sensitive quotation facilities, improving the typesetting of inline quotes.

Already imported by mimosis class.
# #+latex_header: \usepackage{csquotes}

To enclose quote environments with quotes from csquotes, see [[https://tex.stackexchange.com/questions/365231/enclose-a-custom-quote-environment-in-quotes-from-csquotes][the following TeX SE thread]].

#+latex_header: \def\signed #1{{\leavevmode\unskip\nobreak\hfil\penalty50\hskip1em
#+latex_header:   \hbox{}\nobreak\hfill #1%
#+latex_header:   \parfillskip=0pt \finalhyphendemerits=0 \endgraf}}

#+latex_header: \newsavebox\mybox
#+latex_header: \newenvironment{aquote}[1]
#+latex_header: {\savebox\mybox{#1}\begin{quote}\openautoquote\hspace*{-.7ex}}
#+latex_header:    {\unskip\closeautoquote\vspace*{1mm}\signed{\usebox\mybox}\end{quote}}

And then use quotes as:
#+begin_example
# The options derivative adds text after the environment. We use it to add the author.
#+ATTR_LATEX: :options {\cite{Frahm1994}}
#+begin_aquote
/Current (fMRI) applications often rely on "effects" or "statistically significant differences", rather than on a proper analysis of the relationship between neuronal activity, haemodynamic consequences, and MRI physics./
#+end_aquote
#+end_example

Note that org-ref links won't work here because the attr latex will be pasted as-is in the .tex file.

*** Date Time

The date time package allows us to specify a "formatted" date object, which will print different formats according to the current locale & language. I use this in my title page.
#+latex_header: \usepackage[level]{datetime}

*** Setup bibliography
General configuration.
# #+latex_header: \usepackage[autocite=plain, backend=biber, doi=true, url=true, hyperref=true,uniquename=false, maxbibnames=99, maxcitenames=2, sortcites=true, style=authoryear-comp]{biblatex}
# #+LATEX_HEADER: \usepackage[citestyle=authoryear-comp, maxcitenames=2, maxbibnames=99, doi=false, isbn=false, eprint=false, backend=bibtex, hyperref=true, url=false, natbib=true, style=authoryear-comp]{biblatex}
#+LATEX_HEADER: \usepackage[citestyle=authoryear-comp, maxcitenames=2, maxbibnames=99, doi=false, isbn=false, eprint=false, backend=bibtex, hyperref=true, url=false, natbib=true, style=authoryear-comp]{biblatex}
# #+LATEX_HEADER: \addbibresource{~/Dropbox/org/ref/mendeley/library.bib}
#+LATEX_HEADER: \addbibresource{~/Dropbox/org/ref/zotero-library.bib}

Improvements provided with the Mimosis class.
# #+latex_header: \input{bibliography-mimosis}

# And fix the andothers to show et al in English as well:
# #+latex_header: \DefineBibliographyStrings{english}{andothers={\textit{et\, al\adddot}}} 
# #+latex_header:\DefineBibliographyStrings{english}{and={\textit{and}}}


Remove ISSN, DOI and URL to shorten the bibliography.
#+latex_header: \AtEveryBibitem{%
#+latex_header:   \clearfield{urlyear}
#+latex_header:   \clearfield{urlmonth}
#+latex_header:   \clearfield{note}
#+latex_header:  \clearfield{issn} % Remove issn
#+latex_header:  \clearfield{doi} % Remove doi
#+latex_header: \ifentrytype{online}{}{% Remove url except for @online
#+latex_header:   \clearfield{url}
#+latex_header: }
#+latex_header: }

And increase the spacing between the entries, as per default they are too small.
#+latex_header: \setlength\bibitemsep{1.1\itemsep}

Also reduce the font-size
#+latex_header: \renewcommand*{\bibfont}{\footnotesize}

*** Improve chapter font colors and font size
The following commands make chapter numbers BrickRed, which look like the Donders color.
#+latex_header: \makeatletter
#+latex_header: \renewcommand*{\chapterformat}{  \mbox{\chapappifchapterprefix{\nobreakspace}{\color{BrickRed}\fontsize{40}{45}\selectfont\thechapter}\autodot\enskip}}
#+latex_header: \renewcommand\@seccntformat[1]{\color{BrickRed} {\csname the#1\endcsname}\hspace{0.3em}}
#+latex_header: \makeatother

*** Setspace for controlling line spacing

Already imported when using mimosis.
# #+latex_header: \usepackage{setspace}
#+latex_header: \setstretch{1.25} 

*** Parskip

Fine tuning of spacing between paragraphs. See [[https://tex.stackexchange.com/questions/161254/smaller-parskip-than-half-for-koma-script][thread here]].

#+latex_header: \setparsizes{0em}{0.1\baselineskip plus .1\baselineskip}{1em plus 1fil}

*** Table of Contents improvements

# TOC only the chapters, not their content.
# #+latex_header: \setcounter{tocdepth}{1}
#+latex_header: \setcounter{tocdepth}{2}

*** Possible Equation improvements

Make the equation numbers follow the chapter, not the whole thesis.
#+latex_header: \numberwithin{equation}{chapter}

*** TikZ and bayesnet for graphical models
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{bayesnet}

*** Used in papers but not thesis
# #+LATEX_HEADER: \usepackage[format=plain,labelfont={bf},textfont=it]{caption} % make captions italic
# #+LATEX_HEADER: \usepackage[margin=0pt,font=small,labelfont=bf,labelsep=endash]{caption}
# #+LATEX_HEADER: \usepackage{xcolor}
# #+LATEX_HEADER: \usepackage{textcomp}
# #+LATEX_HEADER: \usepackage{algorithmic}

*** Notes in margins
# #+LATEX_HEADER: \usepackage{geometry}
#+LATEX_HEADER: \setlength{\marginparwidth}{3cm}
# #+LATEX_HEADER: \xdef\marginnotetextwidth{\textwidth}
#+LATEX_HEADER: \usepackage{marginnote}
# #+LATEX_HEADER: \renewcommand*{\marginfont}{\footnotesize}
# #+LATEX_HEADER: \newcommand{\parmarginnote}[1]{\hspace{\z@}\marginnote{#1}\ignorespaces}
#+LATEX_HEADER: \newcommand{\parmarginnote}[1]{\marginnote{#1}}
** Text Variables :noexport:
# #+latex_header: \newcommand{\ThesisTitle}{{Probabilistic Inference for Control in Multimodal Dynamical Systems}}
#+latex_header: \newcommand{\ThesisTitle}{{Data Efficient Learning for Control in Multimodal Dynamical Systems}}
#+latex_header: \newcommand{\ThesisSubTitle}{Synergising Bayesian Inference and Riemannian Geometry for Control}
#+latex_header: \newcommand{\FormattedThesisDefenseDate}{\mbox{\formatdate{1}{1}{2100}}}
#+latex_header: \newcommand{\FormattedAuthorDateOfBirth}{\mbox{\formatdate{1}{1}{2000}}}
#+latex_header: \newcommand{\FormattedThesisDefenseTime}{\mbox{10:00}}
#+latex_header: \newcommand{\AuthorShortName}{\mbox{Aidan Scannell}}
#+latex_header: \newcommand{\AuthorFullName}{\mbox{Aidan J. Scannell}}
#+latex_header: \newcommand{\ThesisISBN}{\mbox{}}

** Math Variables :noexport:
#+LATEX_HEADER: \DeclareMathOperator{\R}{\mathbb{R}}
#+LATEX_HEADER: \DeclareMathOperator{\E}{\mathbb{E}}
#+LATEX_HEADER: \DeclareMathOperator{\V}{\mathbb{V}}
#+LATEX_HEADER: \DeclareMathOperator{\K}{\mathbf{K}}

*** Num Data / Mode / State Dimension / Control Dimension (k, d, t/n)
#+LATEX_HEADER: \newcommand{\numData}{\ensuremath{t}}
# #+LATEX_HEADER: \newcommand{\numData}{\ensuremath{n}}
#+LATEX_HEADER: \newcommand{\numEpisodes}{\ensuremath{e}}
#+LATEX_HEADER: \newcommand{\numTimesteps}{\ensuremath{t}}
#+LATEX_HEADER: \newcommand{\numInd}{\ensuremath{m}}
#+LATEX_HEADER: \newcommand{\stateDim}{\ensuremath{d}}
#+LATEX_HEADER: \newcommand{\controlDim}{\ensuremath{f}}
#+LATEX_HEADER: \newcommand{\modeInd}{\ensuremath{k}}
#+LATEX_HEADER: \newcommand{\modeDesInd}{\ensuremath{\text{des}}}
#+LATEX_HEADER: \newcommand{\testInd}{\ensuremath{*}}
#+LATEX_HEADER: \newcommand{\NumData}{\ensuremath{\MakeUppercase{\numData}}}
#+LATEX_HEADER: \newcommand{\NumInd}{\ensuremath{\MakeUppercase{\numInd}}}
#+LATEX_HEADER: \newcommand{\StateDim}{\ensuremath{\MakeUppercase{\stateDim}}}
#+LATEX_HEADER: \newcommand{\ControlDim}{\ensuremath{\MakeUppercase{\controlDim}}}
#+LATEX_HEADER: \newcommand{\ModeInd}{\ensuremath{\MakeUppercase{\modeInd}}}
#+LATEX_HEADER: \newcommand{\NumEpisodes}{\MakeUppercase{\numEpisodes}}
#+LATEX_HEADER: \newcommand{\NumTimesteps}{\MakeUppercase{\numTimesteps}}

# Macros for single/all data notation
#+LATEX_HEADER: \newcommand{\singleData}[1]{\ensuremath{#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\allData}[1]{\ensuremath{\MakeUppercase{#1}}}
# #+LATEX_HEADER: \newcommand{\singleData}[1]{\ensuremath{#1_{\numData}}}
# #+LATEX_HEADER: \newcommand{\allData}[1]{\ensuremath{#1}}
# #+LATEX_HEADER: \newcommand{\allData}[1]{\ensuremath{#1_{1:\NumData}}}

# Macros for data dimensions
# #+LATEX_HEADER: \newcommand{\singleDataDim}[1]{\ensuremath{#1_{\stateDim, \numData}}}
#+LATEX_HEADER: \newcommand{\singleDataDim}[1]{\ensuremath{_{\stateDim}#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\singleDim}[1]{\ensuremath{#1_{\stateDim}}}
# #+LATEX_HEADER: \newcommand{\singleDim}[1]{\ensuremath{_{\stateDim}#1}}
# #+LATEX_HEADER: \newcommand{\singleDimi}[2]{\ensuremath{\tensor*[_{#2}]{#1}{}}}
#+LATEX_HEADER: \newcommand{\singleDim}[1]{\ensuremath{\singleDimi{#1}{\stateDim}}}

# Macros for mode k notation
# #+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{#1^{(\modeInd)}}}
# #+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{#1^{\modeInd}}}
# #+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{\tensor*[^{\modeInd}]{#1}{}}}
#+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{#1_{\modeInd}}}
#+LATEX_HEADER: \newcommand{\modeDes}[1]{\ensuremath{#1^{\modeDesInd}}}

#+LATEX_HEADER: \newcommand{\singleDimiMode}[2]{\ensuremath{\tensor*[_#2^\modeInd]{#1}{}}}
#+LATEX_HEADER: \newcommand{\singleDimMode}[1]{\ensuremath{\singleDimiMode{#1}{\stateDim}}}
#+LATEX_HEADER: \newcommand{\singleDimModeData}[1]{\ensuremath{\tensor*[_\stateDim^\modeInd]{#1}{_\numData}}}

*** Data set
# Dataset/inputs/outputs
#+LATEX_HEADER: \newcommand{\state}{\ensuremath{\mathbf{x}}}
#+LATEX_HEADER: \newcommand{\control}{\ensuremath{\mathbf{u}}}

# #+LATEX_HEADER: \newcommand{\x}{\ensuremath{\mathbf{x}}}
# #+LATEX_HEADER: \newcommand{\y}{\ensuremath{\mathbf{y}}}
#+LATEX_HEADER: \newcommand{\x}{\ensuremath{\hat{\state}}}
#+LATEX_HEADER: \newcommand{\y}{\ensuremath{\Delta\state}}
#+LATEX_HEADER: \newcommand{\dataset}{\ensuremath{\mathcal{D}}}

# Single/all input/output notation
# #+LATEX_HEADER: \newcommand{\singleInput}{\ensuremath{\singleData{\x}}}
#+LATEX_HEADER: \newcommand{\singleInput}{\ensuremath{\x_{\numData-1}}}
#+LATEX_HEADER: \newcommand{\singleOutput}{\ensuremath{\singleData{\y}}}
#+LATEX_HEADER: \newcommand{\allInput}{\ensuremath{\allData{\x}}}
#+LATEX_HEADER: \newcommand{\allOutput}{\ensuremath{\allData{\y}}}

# Single/all state/control notation
#+LATEX_HEADER: \newcommand{\singleState}{\ensuremath{\state_{\numData-1}}}
#+LATEX_HEADER: \newcommand{\singleControl}{\ensuremath{\control_{\numData-1}}}
#+LATEX_HEADER: \newcommand{\allState}{\ensuremath{\allData{\state}}}
#+LATEX_HEADER: \newcommand{\allControl}{\ensuremath{\allData{\control}}}

*** Noise Vars
#+LATEX_HEADER: \newcommand{\noiseVar}{\ensuremath{\sigma}}
#+LATEX_HEADER: \newcommand{\noiseVarK}{\ensuremath{\mode{\noiseVar}}}
#+LATEX_HEADER: \newcommand{\noiseVarOneK}{\ensuremath{\singleDimiMode{\noiseVar}{1}}}
#+LATEX_HEADER: \newcommand{\noiseVarDK}{\ensuremath{\singleDimiMode{\noiseVar}{\StateDim}}}
#+LATEX_HEADER: \newcommand{\noiseVardK}{\ensuremath{\singleDimMode{\noiseVar}}}
# #+LATEX_HEADER: \newcommand{\noiseVarOneK}{\ensuremath{\noiseVarK_{1}}}
# #+LATEX_HEADER: \newcommand{\noiseVarDK}{\ensuremath{\noiseVarK_{\StateDim}}}
# #+LATEX_HEADER: \newcommand{\noiseVardK}{\ensuremath{\noiseVarK_{\stateDim}}}
# #+LATEX_HEADER: \newcommand{\noiseVardK2}{\ensuremath{\left(\noiseVardK\right)^2}}

*** Mode Indicator Variable
#+LATEX_HEADER: \newcommand{\modeVar}{\ensuremath{\alpha}}
#+LATEX_HEADER: \newcommand{\modeVarn}{\ensuremath{\singleData{\modeVar}}}
#+LATEX_HEADER: \newcommand{\ModeVar}{\ensuremath{\bm{\modeVar}}}
# #+LATEX_HEADER: \newcommand{\ModeVar}{\ensuremath{\allData{\bm{\modeVar}}}}
#+LATEX_HEADER: \newcommand{\modeVarK}{\ensuremath{\modeVarn=\modeInd}}
# #+LATEX_HEADER: \newcommand{\ModeVarK}{\ensuremath{\mode{\bm{\modeVar}}}}
#+LATEX_HEADER: \newcommand{\ModeVarK}{\ensuremath{\ModeVar_{\modeInd}}}

*** Tensor Indexing
# Experts indexing
#+LATEX_HEADER: \newcommand{\nkd}[1]{\ensuremath{#1_{\numData,\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\nkD}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\NkD}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\nKD}[1]{\ensuremath{#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\Nkd}[1]{\ensuremath{#1_{:,\modeInd,\stateDim}}}

# Gating function indexing
#+LATEX_HEADER: \newcommand{\nk}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\Nk}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\nK}[1]{\ensuremath{#1_{\numData}}}

# Experts Inducing indexing
#+LATEX_HEADER: \newcommand{\mkd}[1]{\ensuremath{#1_{\numData,\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\mkD}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\MkD}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\mKD}[1]{\ensuremath{#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\Mkd}[1]{\ensuremath{#1_{:,\modeInd,\stateDim}}}

# Gating Inducing indexing
#+LATEX_HEADER: \newcommand{\mk}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\Mk}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\mK}[1]{\ensuremath{#1_{\numData}}}

# Desired Mode Gating indexing
#+LATEX_HEADER: \newcommand{\MDes}[1]{\ensuremath{#1_{:, k^*}}}

*** Gating Network New
# Function notation
#+LATEX_HEADER: \newcommand{\gatingFunc}{\ensuremath{h}}
#+LATEX_HEADER: \newcommand{\hk}{\ensuremath{\mode{\gatingFunc}}}

# Single data notation
#+LATEX_HEADER: \newcommand{\hkn}{\ensuremath{\nk{\gatingFunc}}}
#+LATEX_HEADER: \newcommand{\hn}{\ensuremath{\nK{\mathbf{\gatingFunc}}}}

# All inputs set/vector/tensor notation
#+LATEX_HEADER: \newcommand{\GatingFunc}{\ensuremath{\mathbf{\gatingFunc}}}
#+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\MakeUppercase\GatingFunc}}}}
#+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\Nk{\GatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\allData{\GatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\allData{\mode{\GatingFunc}}}}

*** Experts New
# Function notation
#+LATEX_HEADER: \newcommand{\latentFunc}{\ensuremath{f}}
#+LATEX_HEADER: \newcommand{\LatentFunc}{\ensuremath{\mathbf{\latentFunc}}}
#+LATEX_HEADER: \newcommand{\fkd}{\ensuremath{\latentFunc_{\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\fk}{\ensuremath{\mathbf{\latentFunc}_{\modeInd}}}
# #+LATEX_HEADER: \newcommand{\fk}{\ensuremath{\latentFunc_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\f}{\ensuremath{\mathbf{f}}}

# Vector/Matrix/Tensor notation
#+LATEX_HEADER: \newcommand{\F}{\ensuremath{\MakeUppercase{\mathbf{\latentFunc}}}}
# #+LATEX_HEADER: \newcommand{\Fnkd}{\ensuremath{\latentFunc_{\numData, \modeInd, \stateDim}}}
# #+LATEX_HEADER: \newcommand{\Fnk}{\ensuremath{\mathbf{\latentFunc}_{\numData, \modeInd}}}
# #+LATEX_HEADER: \newcommand{\Fk}{\ensuremath{\F_{:,\modeInd}}}
# #+LATEX_HEADER: \newcommand{\Fn}{\ensuremath{\F_{\numData}}}
#+LATEX_HEADER: \newcommand{\Fnkd}{\ensuremath{\nkd{\latentFunc}}}
#+LATEX_HEADER: \newcommand{\Fnk}{\ensuremath{\nkD{\mathbf{\latentFunc}}}}
#+LATEX_HEADER: \newcommand{\Fk}{\ensuremath{\NkD{\F}}}
#+LATEX_HEADER: \newcommand{\Fn}{\ensuremath{\nKD{\F}}}
#+LATEX_HEADER: \newcommand{\F}{\ensuremath{\F}}

# #+LATEX_HEADER: \newcommand{\Fdk}{\ensuremath{\mathbf{\latentFunc}_{:,\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\Nkd{\mathbf{\latentFunc}}}}

# Single input notation
#+LATEX_HEADER: \newcommand{\fn}{\ensuremath{\Fn}}
#+LATEX_HEADER: \newcommand{\fkn}{\ensuremath{\Fnk}}
#+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\Fnkd}}

# All inputs set/vector/tensor notation
# #+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\Fdk}}

*** Params
#+LATEX_HEADER: \newcommand{\gatingParams}{\ensuremath{\bm\phi}}
#+LATEX_HEADER: \newcommand{\expertParams}{\ensuremath{\bm\theta}}
#+LATEX_HEADER: \newcommand{\gatingParamsK}{\ensuremath{\mode{\bm\phi}}}
#+LATEX_HEADER: \newcommand{\expertParamsK}{\ensuremath{\mode{\bm\theta}}}
*** Sparse GPs
**** Experts
***** Variables
#+LATEX_HEADER: \newcommand{\uf}{\ensuremath{u}}
#+LATEX_HEADER: \newcommand{\uFkd}{\ensuremath{\Mkd{\mathbf{\uf}}}}
#+LATEX_HEADER: \newcommand{\uFk}{\ensuremath{\MkD{\MakeUppercase{\mathbf{\uf}}}}}
#+LATEX_HEADER: \newcommand{\uF}{\ensuremath{\MakeUppercase{\mathbf{\uf}}}}

***** Inputs
# #+LATEX_HEADER: \newcommand{\zf}{\ensuremath{\bm{\zeta}}}
# #+LATEX_HEADER: \newcommand{\zFkd}{\ensuremath{\Mkd{\zf}}}
# #+LATEX_HEADER: \newcommand{\zFk}{\ensuremath{\MkD{\zf}}}
# #+LATEX_HEADER: \newcommand{\zF}{\ensuremath{\MKD{\zf}}}
#+LATEX_HEADER: \newcommand{\zf}{\ensuremath{\mathbf{Z}}}
#+LATEX_HEADER: \newcommand{\zFkd}{\ensuremath{\Mkd{\zf}}}
#+LATEX_HEADER: \newcommand{\zFk}{\ensuremath{\MkD{\zf}}}
#+LATEX_HEADER: \newcommand{\zF}{\ensuremath{\MKD{\zf}}}

**** Gating
***** Variables
#+LATEX_HEADER: \newcommand{\uh}{\ensuremath{U}}
#+LATEX_HEADER: \newcommand{\uHk}{\ensuremath{\Mk{\hat{\mathbf{\uh}}}}}
#+LATEX_HEADER: \newcommand{\uH}{\ensuremath{\hat{\MakeUppercase{\mathbf{\uh}}}}}

#+LATEX_HEADER: \newcommand{\hu}{\ensuremath{\uh}}
#+LATEX_HEADER: \newcommand{\Hku}{\ensuremath{\uHk}}
#+LATEX_HEADER: \newcommand{\Hu}{\ensuremath{\uH}}

***** Inputs
# #+LATEX_HEADER: \newcommand{\zh}{\ensuremath{\bm{\xi}}}
# #+LATEX_HEADER: \newcommand{\zHk}{\ensuremath{\Mk{\zh}}}
# #+LATEX_HEADER: \newcommand{\zH}{\ensuremath{\MK{\zh}}}
#+LATEX_HEADER: \newcommand{\zh}{\ensuremath{\hat{\mathbf{Z}}}}
#+LATEX_HEADER: \newcommand{\zHk}{\ensuremath{\Mk{\zh}}}
#+LATEX_HEADER: \newcommand{\zH}{\ensuremath{\MK{\zh}}}

# #+LATEX_HEADER: \newcommand{\zHDes}{\ensuremath{\zH_{:, k^*}}}
#+LATEX_HEADER: \newcommand{\zHDes}{\ensuremath{\MDes{\zH}}}

**** Misc
#+LATEX_HEADER: \newcommand{\Z}{\ensuremath{\mathbf{Z}}}
**** Old
# Sparse GP macro
# #+LATEX_HEADER: \newcommand{\inducing}[1]{\ensuremath{\hat{#1}}}

# #+LATEX_HEADER: \newcommand{\fu}{\ensuremath{\inducing{\latentFunc}}}
# #+LATEX_HEADER: \newcommand{\Fu}{\ensuremath{\inducing{\mathbf{\latentFunc}}}}
# #+LATEX_HEADER: \newcommand{\Fku}{\ensuremath{\mode{\inducing{\mathbf{\latentFunc}}}}}
# #+LATEX_HEADER: \newcommand{\Fkdu}{\ensuremath{\singleDim{\Fku}}}
# #+LATEX_HEADER: \newcommand{\hu}{\ensuremath{\inducing{\gatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hu}{\ensuremath{\inducing{\mathbf{\gatingFunc}}}}
# #+LATEX_HEADER: \newcommand{\Hku}{\ensuremath{\mode{\inducing{\mathbf{\gatingFunc}}}}}

# #+LATEX_HEADER: \newcommand{\Zfk}{\ensuremath{\mode{\mathbf{Z}}_{\latentFunc}}}
# #+LATEX_HEADER: \newcommand{\Zfk}{\ensuremath{\mode{\bm{\zeta}}}}
# #+LATEX_HEADER: \newcommand{\Zf}{\ensuremath{\mathbf{Z}}}
# #+LATEX_HEADER: \newcommand{\Zf}{\ensuremath{\mathbf{Z}_{\latentFunc}}}

# #+LATEX_HEADER: \newcommand{\Zhk}{\ensuremath{\mode{\mathbf{Z}}_{\gatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Zh}{\ensuremath{\bm{\xi}}}
# #+LATEX_HEADER: \newcommand{\Zhk}{\ensuremath{\mode{\Zh}}}

# #+LATEX_HEADER: \newcommand{\ZhDes}{\ensuremath{\modeDes{\zH}}}

*** Continuous
#+LATEX_HEADER: \newcommand{\derivative}[1]{\ensuremath{\dot{#1}}}
#+LATEX_HEADER: \newcommand{\stateDerivative}{\ensuremath{\derivative{\state}}}
# #+LATEX_HEADER: \newcommand{\stateDerivative}{\ensuremath{\dot{\mathbf{x}}}}

*** Prob Dists New
#+LATEX_HEADER: \newcommand{\pFkd}{\ensuremath{p\left(\Fkd \right)}}
*** Prob Dists
#+LATEX_HEADER: \newcommand{\pFkd}{\ensuremath{p\left(\Fkd \mid \allInput \right)}}
#+LATEX_HEADER: \newcommand{\pFk}{\ensuremath{p\left(\Fk \mid \allInput, \expertParams\right)}}

#+LATEX_HEADER: \newcommand{\pF}{\ensuremath{p\left(\F \mid \allInput, \expertParams\right)}}
#+LATEX_HEADER: \newcommand{\pfk}{\ensuremath{p\left(\fk \mid \allInput, \expertParamsK \right)}}
#+LATEX_HEADER: \newcommand{\pfknd}{\ensuremath{p\left(\fknd \mid \allInput\right)}}

#+LATEX_HEADER: \newcommand{\pFkGivenUk}{\ensuremath{p\left(\Fk \mid \uFk \right)}}
# #+LATEX_HEADER: \newcommand{\pYkGivenUk}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pYkGivenFku}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \uFk \right)}}

#+LATEX_HEADER: \newcommand{\qF}{\ensuremath{q\left(\F \right)}}
#+LATEX_HEADER: \newcommand{\qFu}{\ensuremath{q\left(\uF \right)}}
#+LATEX_HEADER: \newcommand{\qFku}{\ensuremath{q\left(\uFk \right)}}
#+LATEX_HEADER: \newcommand{\pFku}{\ensuremath{p\left(\uFk \mid \zFk \right)}}
#+LATEX_HEADER: \newcommand{\pFkuGivenX}{\ensuremath{p\left(\uFk \mid \zFk \right)}}
#+LATEX_HEADER: \newcommand{\pFuGivenX}{\ensuremath{p\left(\uF \mid \zF \right)}}
#+LATEX_HEADER: \newcommand{\qFk}{\ensuremath{q\left(\Fk \right)}}
#+LATEX_HEADER: \newcommand{\qfk}{\ensuremath{q\left(\fk \right)}}
#+LATEX_HEADER: \newcommand{\qfkn}{\ensuremath{q\left(\fkn \right)}}
#+LATEX_HEADER: \newcommand{\qfn}{\ensuremath{q\left(\fn \right)}}
#+LATEX_HEADER: \newcommand{\pFkGivenFku}{\ensuremath{p\left(\Fk \mid \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pfkGivenFku}{\ensuremath{p\left(\fkn \mid \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenFku}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pYGivenUX}{\ensuremath{p\left(\allOutput \mid \uF, \allInput \right)}}
#+LATEX_HEADER: \newcommand{\pYGivenU}{\ensuremath{p\left(\allOutput \mid \uF \right)}}


#+LATEX_HEADER: \newcommand{\pY}{\ensuremath{p\left(\allOutput \right)}}
# #+LATEX_HEADER: \newcommand{\pykGivenfk}{\ensuremath{p\left(\singleOutputK \mid \fkn \right)}}
# #+LATEX_HEADER: \newcommand{\pYkGivenFk}{\ensuremath{p\left(\allOutputK \mid \Fk \right)}}
# #+LATEX_HEADER: \newcommand{\pYkGivenX}{\ensuremath{p(\allOutputK \mid \allInput)}}
#+LATEX_HEADER: \newcommand{\pykGivenx}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenxNegF}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \singleInput, \neg\Fk \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenfk}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \fkn \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenfkd}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \fknd \right)}}
#+LATEX_HEADER: \newcommand{\pYkGivenFk}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \Fk \right)}}
#+LATEX_HEADER: \newcommand{\pYkGivenX}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \allInput \right)}}
#+LATEX_HEADER: \newcommand{\pYGivenX}{\ensuremath{p\left(\allOutput \mid \allInput \right)}}

**** Gating network
#+LATEX_HEADER: \newcommand{\PrA}{\ensuremath{\Pr\left(\ModeVarK \right)}}
#+LATEX_HEADER: \newcommand{\Pra}{\ensuremath{\Pr\left(\modeVarK \right)}}
#+LATEX_HEADER: \newcommand{\PaGivenhx}{\ensuremath{P\left(\modeVarn \mid \hn, \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\PraGivenx}{\ensuremath{\Pr\left(\modeVarn \mid \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\PraGivenhx}{\ensuremath{\Pr\left(\modeVarK \mid \hn, \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\PraGivenxNegH}{\ensuremath{\Pr\left(\modeVarK \mid \singleInput, \neg\Hall \right)}}
#+LATEX_HEADER: \newcommand{\PrAGivenX}{\ensuremath{\Pr\left(\ModeVarK \mid \allInput \right)}}

#+LATEX_HEADER: \newcommand{\pHGivenX}{\ensuremath{p\left(\Hall \mid \allInput\right)}}
#+LATEX_HEADER: \newcommand{\pHkGivenX}{\ensuremath{p\left(\Hk \mid \allInput\right)}}

*** Kernels
# #+LATEX_HEADER: \newcommand{\Kkxx}{\mode{\mathbf{K}}_{\allInput\allInput}}
#+LATEX_HEADER: \newcommand{\Kkxx}{\mode{\mathbf{K}}_{d, \allInput\allInput}}

# TO derivative kernels
#+LATEX_HEADER: \newcommand{\ddK}{\ensuremath{\partial^2\K_{**}}}
#+LATEX_HEADER: \newcommand{\dK}{\ensuremath{\partial\K_{*}}}
#+LATEX_HEADER: \newcommand{\Kxx}{\ensuremath{\K_{}}}
#+LATEX_HEADER: \newcommand{\iKxx}{\ensuremath{\Kxx^{-1}}}

#+LATEX_HEADER: \newcommand{\dKz}{\ensuremath{\partial\K_{*\zH}}}
#+LATEX_HEADER: \newcommand{\Kzz}{\ensuremath{\K_{\zH\zH}}}
#+LATEX_HEADER: \newcommand{\iKzz}{\ensuremath{\Kzz^{-1}}}
*** Desired Mode
# Function notation
#+LATEX_HEADER: \newcommand{\HDes}{\ensuremath{\MDes{\GatingFunc}}}
#+LATEX_HEADER: \newcommand{\uHDes}{\ensuremath{\MDes{\uH}}}

# Inducing points
#+LATEX_HEADER: \newcommand{\pDes}{\ensuremath{p\left( \uHDes \mid \zHDes \right)}}
#+LATEX_HEADER: \newcommand{\qDes}{\ensuremath{q\left( \uHDes \right)}}
#+LATEX_HEADER: \newcommand{\mDes}{\ensuremath{\MDes{\mathbf{m}}}}
#+LATEX_HEADER: \newcommand{\SDes}{\ensuremath{\MDes{\mathbf{S}}}}

*** Jacobian
# Single data notation
#+LATEX_HEADER: \newcommand{\singleTest}[1]{\ensuremath{#1_{\testInd}}}
#+LATEX_HEADER: \newcommand{\testInput}{\ensuremath{\singleTest{\state}}}

# Jacobian notation
#+LATEX_HEADER: \newcommand{\Jac}{\ensuremath{\mathbf{J}}}
#+LATEX_HEADER: \newcommand{\testJac}{\ensuremath{\singleTest{\Jac}}}
#+LATEX_HEADER: \newcommand{\muJac}{\ensuremath{\mu_{\Jac}}}
#+LATEX_HEADER: \newcommand{\covJac}{\ensuremath{\Sigma_{\Jac}}}

*** Old
**** Gating Network Old
# # Function notation
# #+LATEX_HEADER: \newcommand{\gatingFunc}{\ensuremath{h}}
# #+LATEX_HEADER: \newcommand{\hk}{\ensuremath{\mode{\gatingFunc}}}

# # Single data notation
# #+LATEX_HEADER: \newcommand{\hkn}{\ensuremath{\singleData{\hk}}}
# #+LATEX_HEADER: \newcommand{\hn}{\ensuremath{\singleData{\mathbf{\gatingFunc}}}}

# # All inputs set/vector/tensor notation
# #+LATEX_HEADER: \newcommand{\GatingFunc}{\ensuremath{\mathbf{\gatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\GatingFunc}}
# #+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\mode{\GatingFunc}}}
# # #+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\allData{\GatingFunc}}}
# # #+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\allData{\mode{\GatingFunc}}}}
**** Desired Mode Old
# #+LATEX_HEADER: \newcommand{\HDes}{\ensuremath{\modeDes{\GatingFunc}}}
# #+LATEX_HEADER: \newcommand{\HuDes}{\ensuremath{\modeDes{\Hu}}}
# #+LATEX_HEADER: \newcommand{\mDes}{\ensuremath{\modeDes{\mathbf{m}}}}
# #+LATEX_HEADER: \newcommand{\SDes}{\ensuremath{\modeDes{\mathbf{S}}}}

**** Experts Old
# # Function notation
# #+LATEX_HEADER: \newcommand{\latentFunc}{\ensuremath{f}}
# #+LATEX_HEADER: \newcommand{\f}{\ensuremath{f}}
# #+LATEX_HEADER: \newcommand{\fk}{\ensuremath{\mode{\latentFunc}}}
# # #+LATEX_HEADER: \newcommand{\fkd}{\ensuremath{\singleDim{\fk}}}
# #+LATEX_HEADER: \newcommand{\fkd}{\ensuremath{\singleDimMode{\f}}}

# # Single input notation
# #+LATEX_HEADER: \newcommand{\fn}{\ensuremath{\singleData{\mathbf{\latentFunc}}}}
# #+LATEX_HEADER: \newcommand{\fkn}{\ensuremath{\singleData{\mode{\mathbf{\latentFunc}}}}}
# # #+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\singleDim{\singleData{\fk}}}}
# # #+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\singleDimMode{\singleData{\f}}}}
# #+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\singleDimModeData{\f}}}

# # All inputs set/vector/tensor notation
# # #+LATEX_HEADER: \newcommand{\F}{\ensuremath{\allData{\mathbf{\f}}}}
# #+LATEX_HEADER: \newcommand{\F}{\ensuremath{\mathbf{\f}}}
# #+LATEX_HEADER: \newcommand{\Fk}{\ensuremath{\mode{\F}}}
# # #+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\singleDim{\Fk}}}
# #+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\singleDimMode{\F}}}

#+LATEX_HEADER: \newcommand{\allOutputK}{\ensuremath{\mode{\allOutput}}}
#+LATEX_HEADER: \newcommand{\singleOutputK}{\ensuremath{\mode{\singleOutput}}}

** Acronyms :noexport:
{{{glossaryentry(LaTeX,\LaTeX,A document preparation system,LaTeX)}}}
{{{acronym(mogpe,MoGPE,Mixtures of Gaussian Process Experts)}}}
{{{acronym(gp,GP,Gaussian process)}}}
{{{acronym(mdp,MDP,Markov decision process)}}}
{{{glossaryentry(Real Numbers,$\real$,The set of Real numbers,Real Numbers)}}}
* Frontmatter :ignore:
#+BEGIN_EXPORT latex
\frontmatter
#+END_EXPORT
** Title Page :ignore:noexport:

  #+BEGIN_EXPORT latex
  \begin{titlepage}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % First page: Thesis Title and Author Name
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    % Uncomment when adding the background figure to the cover.
    \BgThispage

    \cleardoublepage
    \pagestyle{empty}

    \begin{center}
      \null\vfill
      {\huge{\bfseries \ThesisTitle}\par}
      \vspace{\stretch{0.5}}
      {\large \ThesisSubTitle \par}
      \vspace{\stretch{2}}
      \vspace{\baselineskip}
      {\large By \AuthorFullName\par}
      \vspace{\stretch{2}}
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \includegraphics[scale=0.8]{./logos/bristolcrest_colour}\\
      \vspace{6mm}
      {\large Faculty of Engineering\\
      \textsc{University of Bristol}}\\
      \vspace{11mm}
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \begin{minipage}{10cm}
        A dissertation submitted to the University of Bristol in accordance with the requirements of the degree of \textsc{Doctor of Philosophy} in the Faculty of Engineering.
      \end{minipage}\\
      # \vspace{\baselineskip}
      # \vspace{\stretch{1}}
      \vspace{\baselineskip}
      \vspace{\stretch{1}}
      \noindent
      \begin{tabular}{@{}l@{\hspace{22pt}}ll}
        \textbf{Supervisors}:          & Prof.\ Arthur Richards\\
                                       & Dr.\ Carl Henrik Ek\\
      \end{tabular}
      \vspace{\stretch{1}}
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \vfill
    \end{center}

    \cleardoublepage
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % End Titlepage
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{titlepage}
  #+END_EXPORT

** Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:
#+BEGIN_EXPORT latex
\begin{SingleSpace}
%\initial{R}einforcement learning and data-driven control have seen significant advances over the last decade,
%especially in simulated environments.
%Real world systems are often highly nonlinear, exhibit stochasticity and multimodalities,
%are expensive to run (slow, energy intensive, subject to wear and tear) and
%must be controlled subject to constraints (for safety, efficiency, etc).

%From robotics, to industrial processing, to finanace, learning-based approaches to control
%help alleviate the dependence on domain exerts for system identification and controller design.
This dissertation is concerned with \textbf{learning} and \textbf{control}
in unknown, (or partially unknown), multimodal dynamical systems.
It is motivated by controlling a dynamical system from an initial state (in a desired dynamics mode),
to a target state, whilst remaining in the desired dynamics mode.
It is motivated by controlling quadrotors with unoperatable dynamics modes that are
induced via spatially varying turbulence i.e. the goal is to fly the quadrotor from an initial state in the desired (operatable) dynamics mode,
to a target state, whilst remaining in the desired dynamics mode.

%This dissertation is concerned with \textbf{learning} and \textbf{control}
%in unknown, (or partially unknown), multimodal dynamical systems.
%It is motivated by controlling a quadrotor with unoperatable dynamics modes that are
%induced via spatially varying turbulence.
%The operatable mode corresponds to regions of the state space subject to \textbf{low turbulence}, and the
%unoperatable mode(s) corresponds to regions subject to \textbf{high turbulence}.
%The goal is to fly the quadrotor from an initial state in the desired (operatable) dynamics mode,
%to a target state, whilst remaining in the desired dynamics mode.

This work first considers learning a representation of the transition dynamics assuming
access to a historial data set of state transitions.
In particular, it seeks to synergise model learning and control, by
inferring latent structure (in the model learning) that can be exploited by control algorithms.


This work constructs a discrete-time representation of the multimodal transition dynamics
as a probabilistic model based on Gaussian processes.
The model resembles the Mixture of Gaussian Process Experts model with a gating network based on Gaussian processes.
Motivated by data efficient learning and well calibarated uncertainty estimates, learning in the model
is performed via Bayesian inference.
A novel variational lower bound based on sparse approximations provides scalability
via stochastic gradient methods.


When controlling a real world system, it is desirable
to perform decision-making under uncertainty; both the uncertainty assocaited with our belief of the system,
and the uncertainty inherent in the system (stochasticity).
In a risk-averse setting, it is desirable to avoid entering regions of a dynamics model with high \textit{epistemic
uncertainty}, or high \textit{aleatoric uncertainty}.
This is because the state-control trajectory cannot be predicted confidently, and thus,
constraints may be violated.
Conversely, in an explorative setting,
if the \textit{epistemic uncertainty} has been quantified, then it can be used to guide exploration into
regions of the dynamics that have not previously been observed.
This experience can then be used to update the model and reduce the associated \textit{epistemic uncertainty}.

Probabilistic modelling and Bayesian inference are a promosing
avenue for learning dynamics models to be used for controlling real world systems.
They provide a principled approach to modelling both the \textit{epistemic uncertainty} assocaited with the model,
and the \textit{aleatoric uncertainty} inherent to the system.



This work is interested in techniques for driving an unknown system from an initial state
in a desired dynamics mode, to a target state, whilst remaining in a desired dynamics mode
(which is believed to be operatable).



Modelling a multimodal parameter as unimodal, or incorrectly specifying how a multimodal
parameter switches between modes, will have a detrimental impact on controller performance.
In some cases, it may even lead to catastrophic system failure.




(e.g. stability, poor performance, efficiency)

Traditional control techniques rely on dynamics models derived from first-principles based on physics,
which are often subject to model misspecification e.g. incorrectly specified parameters or
models (modelling a nonlinear system to be linear, or a multimodal system to be unimodal).

Real world systems are often highly nonlinear, exhibit stochasticity and multimodalities,
are expensive to run (slow, energy intensive, subject to wear and tear) and
must be controlled subject to constraints (for safety, efficiency, etc).

Dynamical systems are also subject to \textit{aleatoric uncertainty},
the type of uncertainty that is inherent to the system and cannot be reduced,
for example stochasticity (i.e. process noise) and observation noise.


This constitues a type of uncertainty known as \textit{epistemic uncertainty}, which can be reduced in the limit
of infinite data.
Dynamical systems are also subject to \textit{aleatoric uncertainty},
the type of uncertainty that is inherent to the system and cannot be reduced,
for example stochasticity (i.e. process noise) and observation noise.


This dissertation is motivated by controlling multimodal dynamical systems, which are
either unknown, or partially unknown.
Traditional control techniques rely on dynamics models derived from first-principles based on physics,
which are often subject to model misspecification (e.g. incorrectly specified parameters).
This constitues a type of uncertainty known as \textit{epistemic uncertainty}, which can be reduced in the limit
of infinite data.
Dynamical systems are also subject to \textit{aleatoric uncertainty},
the type of uncertainty that is inherent to the system and cannot be reduced,
for example stochasticity (i.e. process noise) and observation noise.

Modelling a multimodal parameter as unimodal, or incorrectly specifying how a multimodal
parameter switches between modes, will have a detrimental impact on controller performance.
In some cases, it may even lead to catastrophic system failure.

Learning dynamics models via probabilistic modelling and Bayesian inference is a promosing
avenue for controlling real world systems.
It provides a principled approach to modelling both \textit{epistemic uncertainty}, and \textit{aleatoric uncertainty}
(uncertainty inherent in the system that cannot be reduced).

Correctly quantified uncertainty is crucial for intelligent decision-making.

When synthesising controllers, it is desirable to account for epistemic uncertainty.
In a risk-averse setting, it is desirable to avoid entering regions of a learned dynamics model with high \textit{epistemic
uncertainty}.
This is because the state-control trajectory cannot be predict confidently, and thus,
constraints may be violated.
Conversely, in an explorative setting,
if the \textit{epistemic uncertainty} has been quantified, then it can be used to guide exploration into
regions of the dynamics that have not previously been observed.
This experience can then be used to update the model and reduce the associated \textit{epistemic uncertainty}.

% Traditional control techniques rely on dynamics models derived from first-principles, which
% are often subject to model misspecification (e.g. incorrectly specified parameters).
% This is a type of uncertainty known as \textit{epistemic uncertainty}.
% It is desirable to account for epistemic uncertainty,
% as well as the uncertainty inherent in the system, known as \textit{aleatoric uncertainty}
% (arising from the inherent stochasticity of the system) when synthesising controllers.

Traditional control techniques rely on dynamics models derived from first-principles, which
are often subject to model misspecification (e.g. incorrectly specified parameters).
This is a type of uncertainty known as \textit{epistemic uncertainty}.
When synthesising controllers, it is desirable to account for epistemic uncertainty.
In a risk-averse setting, it is desirable to avoid entering regions of a learned dynamics model with high \textit{epistemic
uncertainty}.
This is because the state-control trajectory cannot be predict confidently, and thus,
constraints may be violated.
Conversely, in an explorative setting,
if the \textit{epistemic uncertainty} has been quantified, then it can be used to guide exploration into
regions of the dynamics that have not previously been observed.
This experience can then be used to update the model and reduce the associated \textit{epistemic uncertainty}.

Learning dynamics models via probabilistic modelling and Bayesian inference is a promosing
approach for controlling real world systems;
as it provides a principled approach to modelling both epistemic and aleatoric uncertainty.
Correctly quantified uncertainty is crucial for intelligent decision-making.

This dissertation is motivated by controlling dynamical systems that exhibit multimodalities, which are
either unknown, or partially unknown.
Incorrectly specifying a multimodal parameter as unimodal, or incorrectly specifying how a multimodal
parameter switches between modes will have a detrimental impact on controller performance.
In some cases, it may even lead to catastrophic failure.

% Learning dynamics models for control (in either the model-based reinforcement learning or model-based planning settings)


Model-based approaches offer a principled approach for handling constraints

Model-based approaches offer a principled approach for modelling both


This may be
where some of the dynamics modes

stochasticity of
and design controllers to account for
model misspecifications, stochasticity of



uncertainty in model parameters


Constraints
data-efficiency
target exploration
uncertainty
stochasticity



dynamics model learning
integration of learned dynamics models planning


unknown or partially unknown transition dynamics
The promise of learning-based control  address model misspecification

When leveraging learned dynamics models for control



which must be correctly accounted
for when learning from observations
\end{SingleSpace}
#+END_EXPORT

** Declaration :noexport:
:PROPERTIES:
:UNNUMBERED: t
:END:
#+BEGIN_EXPORT latex
\begin{SingleSpace}
\begin{quote}
\initial{I} declare that the work in this dissertation was carried out in accordance with the requirements of  the University's Regulations and Code of Practice for Research Degree Programmes and that it  has not been submitted for any other academic award. Except where indicated by specific  reference in the text, the work is the candidate's own work. Work done in collaboration with, or with the assistance of, others, is indicated as such. Any views expressed in the dissertation are those of the author.

\vspace{1.5cm}
\noindent
\hspace{-0.75cm}\textsc{SIGNED: .................................................... DATE: ..........................................}
\end{quote}
\end{SingleSpace}
#+END_EXPORT

** Acknowledgements :noexport:
:PROPERTIES:
:UNNUMBERED: t
:END:
#+BEGIN_EXPORT latex
\begin{SingleSpace}
\initial{H}ere goes the dedication.
\end{SingleSpace}
#+END_EXPORT
* TOC and Mainmatter :ignore:
#+BEGIN_EXPORT latex
\tableofcontents
% This ensures that the subsequent sections are being included as root
% items in the bookmark structure of your PDF reader.
\begingroup
    \let\clearpage\relax
    \glsaddall
    \printglossary[type=\acronymtype]
    \newpage
    \printglossary
\endgroup
\printindex

\mainmatter
#+END_EXPORT

* Testing Maths Variables :noexport:
** Tables :ignore:
#+CAPTION: Variables
| Name                    | Symbol     | Equation                                                   |
|-------------------------+------------+------------------------------------------------------------|
| State                   | $\state$   | $\R^{\StateDim}$                                           |
| Control                 | $\control$ | $\R^{\ControlDim}$                                         |
| Time                    | $t$        | $\R$                                                       |
| State-action input      | $\x$       | $(\state, \control) \in \R^{\StateDim \times \ControlDim}$ |
| State difference        | $\y$       | $\state_{t} - \state_{t-1} \in \R^{\StateDim}$             |
| Mode indicator variable | $\modeVar$ | $\{1,\ldots,\ModeInd\}$                                    |
|                         |            |                                                            |

#+CAPTION: Variables at single data points
| Name                    | Symbol           | Equation                                                          |
|-------------------------+------------------+-------------------------------------------------------------------|
| State                   | $\singleState$   | $\R^{\StateDim}$                                                  |
| Control                 | $\singleControl$ | $\R^{\ControlDim}$                                                |
| State-Action input      | $\singleInput$   | $(\singleState, \singleControl) \in \R^{\StateDim + \ControlDim}$ |
| State Difference        | $\singleOutput$  | $\R^{\StateDim}$                                                  |
| Mode indicator variable | $\modeVarn$      | $\{1,\ldots,\ModeInd\}$                                           |

#+CAPTION: Variables at all data points
| Name                    | Symbol        | Equation                                                                      |
|-------------------------+---------------+-------------------------------------------------------------------------------|
| State                   | $\allState$   | $\R^{\NumData \times \StateDim}$                                              |
| Control                 | $\allControl$ | $\R^{\NumData \times \ControlDim}$                                            |
| State-Action input      | $\allInput$   | $(\allState, \allControl) \in \R^{\NumData \times (\StateDim + \ControlDim)}$ |
| State Difference        | $\allOutput$  | $\R^{\NumData \times \StateDim}$                                              |
| Mode indicator variable | $\ModeVarK$   | $\{\singleData{\modeVar}=k\}_{\numData=1}^{\NumData}$                         |

#+CAPTION: Gating network notation
|                | Name                                   | Symbol        | Equation                                                                         |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| Function       | Gating function k                      | $\hk$         | $\hk : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R$                    |
|                | Gating function                        | $\gatingFunc$ | $\gatingFunc : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R^{\ModeInd}$ |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| $\singleInput$ | Gating function k at $\singleInput$    | $\hkn$        | $\hk(\singleInput) \in \R$                                                       |
|                | Gating function at $\singleInput$      | $\hn$         | $\gatingFunc(\singleInput) \in \R^{\ModeInd}$                                    |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| $\allInput$    | Gating function k                      | $\Hk$         | $\hk(\allInput) \in \R^{\NumData}$                                               |
|                | Gating functions                       | $\Hall$       | $\gatingFunc(\allInput) \in \R^{\NumData \times \ModeInd}$                       |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| $\zH$          | Inducing variables - gating function k | $\uHk$        | $\hk(\zHk) \in \R^{\NumInd}$                                                     |
|                | Inducing variables - gating functions | $\uH$         | $\h(\zH) \in \R^{\NumInd \times \ModeInd}$                                       |


#+CAPTION: Transition dynamics function notation
|                | Name                                    | Symbol  | Equation                                                                                 |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Dimension d of mode k                   | $\fkd$  | $\fkd : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R$                           |
| Function       | Mode k                                  | $\fk$   | $\fk : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R^{\StateDim}$                |
|                | All modes function                                          | $\f$    | $\f : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R^{\ModeInd \times \StateDim}$ |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Dimension d mode k                      | $\fknd$ | $\fkd(\singleInput) \in \R$                                                              |
| $\singleInput$ | Mode k                                  | $\fkn$  | $\fk(\singleInput) \in \R^{\StateDim}$                                                   |
|                | All modes                               | $\fn$   | $\f(\singleInput) \in \R^{\ModeInd \times \StateDim}$                                    |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Dimension d mode k                      | $\Fkd$  | $\fkd(\allInput) \in \R^{\NumData}$                                                      |
| $\allInput$    | Mode k                                  | $\Fk$   | $\fk(\allInput) \in \R^{\NumData \times \StateDim}$                                      |
|                | All modes                               | $\F$    | $\f(\allInput) \in \R^{\NumData \times \ModeInd \times \StateDim}$                       |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Inducing variables - dimension d mode k | $\uFkd$ | $\fkd(\zFkd) \in \R^{\NumInd}$                                                           |
| $\zF$          | Inducing variables - mode k             | $\uFk$  | $\fk(\zFk) \in \R^{\NumInd \times \StateDim}$                                            |
|                | Inducing variables - all modes          | $\uF$   | $\f(\zF) \in \R^{\NumInd \times \ModeInd \times \StateDim}$                              |
** Experts
GP prior over each output dimension $d$ for each dynamics mode $k$, 
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior-single-dim}
p\left(\Fkd \mid \allInput \right) &= \mathcal{N}\left( \Fkd \mid \singleDimMode{\mu}(\allInput), \singleDimMode{k}(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
Assume each output dimension is independent,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior}
\pFk &= \prod_{\stateDim=1}^{\StateDim} \pFkd
\end{align}
#+END_EXPORT
Assume each dynamics mode $k$ is independent,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-prior}
\pF &= \prod_{k=1}^{K} \pFk
\end{align}
#+END_EXPORT
The process noise in each mode is modelled as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-likelihood}
\pYkGivenFk = \prod_{\numData=1}^{\NumData} \pykGivenfk &= \prod_{\numData=1}^{\NumData} \mathcal{N}\left( \singleOutput \mid \fkn, \text{diag}\left[ \left(\noiseVarOneK\right)^{2}, \ldots, \left( \noiseVarDK \right)^{2} \right]  \right)
\end{align}
#+END_EXPORT
where $\noiseVardK$ represents the noise variance associated with the $d$^{\text{th}} dimension of the $k$^{\text{th}} mode.

Each expert is then given by marginalising its associated latent function values,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert}
\pYkGivenX = \int  \pYkGivenFk \pFk \text{d} \Fk
\end{align}
#+END_EXPORT

The dynamics modes are combined via a distribution over the mode indicator variable $\modeVar$.
The resulting marginal likelihood is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-prior}
\pYGivenX = \sum_{\modeInd=1}^{\ModeInd} \Pr(\ModeVarK) \pYkGivenX
\end{align}
#+END_EXPORT

** Mixture of Experts
Mixture model marginal likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixture-marginal-likelihood}
\pYGivenX = \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \Pr(\modeVarK) p(\singleOutput \mid \modeVarn=\modeInd, \singleInput)
\end{align}
#+END_EXPORT
Mixture of experts marginal likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-moe-marginal-likelihood}
\pYGivenX &= \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \PraGivenx \pykGivenx
\end{align}
#+END_EXPORT

** Gating Network
# TODO: gating function depends on spatial component of state only
This work is interested in transition dynamics where the governing mode varies over the input domain

This work specifies a probability mass function over the mode indicator variable that is governed by a set of input-dependent
latent functions. These model how the transition dynamics switch between modes over the input domain.
In the literature they are commonly referred to as gating functions.
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mode-indicator-dist}
\PaGivenhx = \prod_{\modeInd=1}^{\ModeInd} \PraGivenhx^{[\modeVarn = \modeInd]},
\end{align}
#+END_EXPORT
The probabilities $\Pr(\modeVarn=\modeInd \mid \hn )$ are obtained by normalising the outputs of all the gating functions, e.g.
$\text{softmax}(\hn)$.
Following a Bayesian formulation independent GP priors are placed on each of the gating functions,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-funcs-prior}
\pHGivenX = \prod_{\modeInd=1}^{\ModeInd} \pHkGivenX
= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \Hk \mid \mode{\mu}(\allInput), \mode{k}(\allInput, \allInput) \right).
\end{align}
#+END_EXPORT
Each GP models the epistemic uncertainty associated with its gating function.
The probabilities $\PraGivenx$ associated with the probability mass function over
the mode indicator variable are then obtained by marginalising the
latent gating functions,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-indicator-mult}
\PraGivenxNegH
&= \int \text{softmax}_k(\hn) p(\hn \mid \singleInput, \neg\Hall) \text{d} \mathbf{h}_t.
\end{align}
#+END_EXPORT
This equation integrates out the uncertainty associated with the gating functions.
High variance in the gating function GPs tends the distribution over the mode indicator variable
to a uniform distribution.

** Marginal Likelihood
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-expert}
\pykGivenxNegF = \pyk
\end{align*}
#+END_EXPORT

Our marginal likelihood can be written with the same factorisation as the \acrshort{moe}
marginal likelihood in Equation ref:eq-moe-marginal-likelihood,

#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood}
\pYGivenX &= \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \underset{\text{Mixing probability}}{\PraGivenxNegH} \underset{\text{Dynamics mode } k}{\pykGivenxNegF}
\end{align}
#+END_EXPORT
The 
$\PraGivenxNegH$ contains $\ModeInd$ GP conditionals with complexity

$\pykGivenxNegF$ contains a GP conditional with complexity 

** Inference

* Introduction
** intro :ignore:
# *Dynamical Systems*
# The modern world is pervaded with dynamical systems that we seek to control to achieve a desired behaviour.
# Examples include autonomous vehicles, aircraft, robotic manipulators, financial markets and energy management systems.
# In the last decade, learning-based control citep:hewingLearningBased2020,sutton2018reinforcement has become
# a popular paradigm for controlling dynamical systems.
# This can be accounted to significant improvements in sensing and computational capabilities as well as
# recent successes in machine learning.

# with the /hope/ of addressing some of the shortcomings associated with pure control theoretic approaches?
# Perhaps with the /hope/ of addressing some of the shortcomings associated with pure control theoretic approaches.
# This can be accounted to recent successes in machine learning and the hope of
# and significant improvements in sensing and computational capabilities.

# Modern artificial intelligence seeks solutions that allow machines to /understand/ and /learn/.
# In the field of machine learning, these challenges are often solved using probabilistic models.
# In this setting, /understanding/ is achieved by separating signal from noise and removing redundancies in
# complex and noisy data.
# Leveraging learned models for control requires the models to extrapolate beyond training observations.
# That is, they must enable the machine to /reason/ about previously unseen inputs.
# In the field of machine learning, /understanding/ is achieved by separating signal from noise and removing redundancies in
# complex and noisy data.
# Leveraging learned models for control requires the models to extrapolate beyond training observations.
# That is, they must be able to /reason/ about previously unseen inputs.



# 1. Learning the system dynamics: model based control strategies rely on suitable and sufficiently accurate model
#    representations of the system dynamics. A promising approach is to learn /unknown/, or /partially unknown/
#    dynamics from observations. This enables control in previously uncontrollable systems, and can improve control
#    by learning (and accounting) for any model errors.
# 2. Learning the controller design:

# (coming from both the reinforcement learning
# cite:sutton2018reinforcement and control theory \todo{cite control theory book?} communities),


In the last decade, learning-based control citep:hewingLearningBased2020,sutton2018reinforcement has become
a popular paradigm for controlling dynamical systems.
This can be accounted to significant improvements in sensing and computational capabilities, as well as
recent successes in machine learning.
This growing interest in learning and data-driven control
has emphasised the importance for real-world considerations.
In contrast to simulation, the control of physical systems has real-world consequences.
For example,
components may get damaged, the system may damage its environment, or the system may catastrophically fail.
As such, any control strategy deployed in the real-world should ensure the safety of itself and
its surrounding environment.

\parmarginnote{multimodal systems}
Many dynamical systems exhibit multimodalities (in their transition dynamics), where some of the dynamics modes
may be /unsafe/.
These multimodalities may be due to spatially varying model parameters, for example,
process noise terms modelling aircraft turbulence, or friction coefficients.
In these systems, it is desirable to remain in a specific dynamics mode that is known to be operatable.
Perhaps the other modes are hard to control due to stability, or the controlling the mode switch itself is hard.
In this setting, safety can be viewed as remaining in the desired dynamics mode.
It may also be desirable to remain in a desired mode for /efficiency/ or /performance/ reasons,
however, this dissertation is motivated through the lens of safety.
# modelling the interaction between ground vehicle's tyres and different surface types.

Safe control in systems where the environment is /known a priori/ has been well studied by the control and formal
methods communities.
However, safe control when the environment is /not known a priori/ has been less well studied.
Recent work by cite:berkenkampSafe2019 attempts to address safe learning-based control in the
reinforcement learning setting, where the environment is /not known a priori/.
It is important to note that the definition of safety varies,
and should be defined on a system by system basis;
cite:berkenkampSafe2019 define safety in terms of closed-loop
stability but it is also common to define safety in terms of constraint satisfaction (on the states and controls).
# They define safety in terms of stability and exploit Lyapanov functions to construct a safe RL framework.
# Other approaches

Model-based control comprises a powerful set of techniques for finding controls of constrained dynamical
systems, given a transition dynamics model describing the evolution of the controlled system.
It is commonly used for controlling aircraft, robotic manipulators, and walking
robots cite:vonstrykDirect1992,bettsSurvey1998,gargUnified2010.
One caveat is that it requires a relatively accurate mathematical model of the system.
Traditionally, these mathematical models are built using first principles based on physics.
However, accurately modelling the underlying transition dynamics can be challenging and
lead to the introduction of model errors.
Incorrectly specifying these model parameters (and their associated uncertainty)
can have a detrimental impact on controller performance
and is an active area of research in the robust
and stochastic optimal control communities cite:freemanRobust1996,stengelStochastic1986.
# For example, model parameters might be,
# 1) hard to specify accurately, e.g. friction coefficients associated with surfaces,
# 2) assumed constant when in reality they vary spatially, e.g. process noise terms modelling the effect of
#    turbulence on aircraft,
# 3) assumed constant when in reality they vary with time, e.g. mass reducing due to fuel consumption.

The difficulties associated with constructing mathematical representations of dynamical systems
can be overcome by learning from observations cite:ljungSystem1999.
However, learning dynamics models for control introduces other difficulties.
For example, it is important to know
where the model cannot predict confidently due to a lack of training observations.
This concept is known as /epistemic uncertainty/ and is reduced in the limit of infinite data.
In a risk-averse setting, it is desirable to avoid entering regions of a learned dynamics model with high /epistemic
uncertainty/.
This is because it cannot predict the state-control trajectory confidently, and thus, if the safety
constraints are satisfied.
Conversely, in an explorative setting,
if the /epistemic uncertainty/ has been quantified, then it can be used to guide exploration into
regions of the dynamics that have not previously been observed.
This experience can then be used to update the model and reduce its associated /epistemic uncertainty/.
# it is
# desirable to enter regions of the dynamics that have not previously been observed as this experience can be used
# reduce the epistemic uncertainty in the learned dynamics.
# in order to collect data.
# The quantified epistemic uncertainty can be used to guide exploration, and
# in turn, reduce the epistemic uncertainty.

# It is also important to accurately model /aleatoric uncertainty/ (unknowns that differ each time
# an experiment is run).
# For example, uncertainty arising from our observations of the system (observation noise) and uncertainty associated
# with the stochasticity of the transition dynamics (process noise).
# In the field of machine learning, these challenges are often solved using probabilistic models.
# In the probabilistic framework, understanding is achieved by separating signal from noise and removing redundancies in
# complex and noisy data.
# Probabilistic models provide a principled approach to modelling and disentangling these different sources
# of uncertainty when learning from observations.


This dissertation is interested in safe learning-based control in multimodal dynamical systems with /unknown/, or
/partially unknown/, transition dynamics, where safety is governed by the underlying dynamics modes.
It is assumed that complete knowledge of how the transition dynamics switch between modes is also /not known a priori/.
In essence, the safety constraints are /not known a priori/ and should be learned from observations.
Therefore, this dissertation is interested in jointly inferring the /constraints/ alongside the underlying dynamics modes,
through repeated interactions with the system.

This dissertation first addresses learning representations of multimodal dynamical systems that learn structure

The chapters in this dissertation break the problem down into smaller and more manageable pieces of work.
In Chapter ref:chap-dynamics access to a historical data set of state transitions is assumed.
This alleviates


** ignore

cite:deisenrothPILCO2011,schneiderExploiting1996.

For example, cite:deisenrothPILCO2011,cutlerEfficient2015,panProbabilistic2014 use Gaussian processes (GPs) to learn
transition dynamics from observations.

GPs lend themselves to data-efficient learning through the selection of informative priors, and
when used in a Bayesian setting offer well calibrated uncertainty estimates.
Methods for learning probabilistic multimodal transition dynamics have also been proposed:
cite:Mckinnon used a mixture of GP experts method,
cite:Moerland studied the used of deep generative models and
cite:Kaiser2020a proposed a Bayesian model that learns independent
dynamics modes whilst maintaining a
probabilistic belief over which mode is responsible for predicting at a given input location.


In some systems it is desirable to remain in a dynamics mode that is known to be safe whilst avoiding
regions of the dynamics where specific parameters
Each mode in a dynamics

*Real World* Controlling systems in the real-world (especially physical systems)
can have /real/ and /undesirable/ consequences.
- Safety
  - /do not know/ the environment /a priori/
  - model bias
  - cannot reason about safety of control
- Data-efficiency


Model errors - epistemic/aleatoric uncertainty




Remain in a desired dynamics mode/regime.

*Motivation* For example, consider controlling an autonomous vehicle to navigate
to a desired location in an environment subject to different road surfaces (asphalt, loose gravel, grass)
operating in an
environment subject to spatially friction coefficients

*Motivation* For example, consider controlling an autonomous air vehicle operating in an
environment subject to regions of high turbulence, i.e. spatially varying  process noise.
Process noise and observation noise are the constituent sources of aleatoric uncertainty; uncertainties that are inherent in a system and cannot be reduced.
They pose a significant issue for controlling such systems as we cannot model turbulence and thus account
for it in control algorithms.
# Unlike epistemic uncertainty that can be reduced by collecting more data.

** Contributions
This dissertation explores learning
- Chapter ref:chap-dynamics: details an approach to learning the underlying dynamics modes (and
  how they're separated) in multimodal dynamical systems.
  The approach formulates a probabilistic representation of the transition dynamics resembling a Mixture of
  Gaussian Process Experts model. It then performs approximate Bayesian inference via a novel variational lower
  bound that principally handles uncertainty and provides scalability via stochastic gradient methods.
  The method is tested on a real-world quadcopter data set and two data sets obtained from simulated environments ...
- Chapter ref:chap-traj-opt: introduces a trajectory optimisation technique that finds trajectories that attempt
  to remain in a desired dynamics mode, and in regions of the learned dynamics that have been observed, so can be
  predicted confidently. It exploits the latent geometry and well-calibrated (epistemic) uncertainty estimates
  inferred by the probabilistic model from Chapter ref:chap-dynamics.
- Chapter ref:chap-control:

** Associated Publications
fullcite:scannellTrajectory2021

** Motivation
This paper is interested in controlling a DJI Tello quadcopter in an environment
with spatially varying turbulence induced by a fan at the side of the room, shown
in Fig. ref:fig-problem-statement.
It is hard to know the exact transition dynamics due to complex and uncertain
interactions between the quadcopter and the fan.
The system's transition dynamics resemble a mixture of two modes: a turbulent mode in front of
the fan and a non-turbulent mode everywhere else.
When planning a trajectory from start state $\mathbf{x}_0$ to desired state $\mathbf{x}_f$
it is preferred to avoid entering the turbulent mode, as it
results in poor performance and sometimes even system failure.

#+BEGIN_EXPORT latex
\begin{figure}[!t]
\centering
  \includegraphics[width=0.9\columnwidth]{images/quadcopter_bimodal_domain.pdf}
  \caption{
This work seeks to velocity control a DJI Tello quadcopter in an indoor environment
subject to two modes of operation characterised by process noise (turbulence).
A high turbulence mode is induced by placing a desktop fan at the right side of the room.
Data from four trajectories following a single 2D $\mathbf{x}=(x,y)$ target trajectory captures the variability
(process noise) in the dynamics.
Our goal is to find trajectories between $\mathbf{x}_0$ and $\mathbf{x}_f$ that either prioritise
remaining in the non-turbulent mode or prioritise avoiding regions of the learned dynamics
with high epistemic uncertainty due to lack of training observations.}
\label{fig-problem-statement}
\end{figure}
#+END_EXPORT

** Introduction traj opt paper
Many physical systems operate under switching dynamics modes due to
changing environmental or internal conditions.
Examples include: robotic grasping where objects with different
properties have to be manipulated, robotic locomotion in environments with varying surface types
and the control of aircraft in environments subject to different levels of turbulence.
When controlling these systems, it may be preferred to find trajectories that remain
in a single dynamics mode.
This paper is interested in controlling a DJI Tello quadcopter in an environment
with spatially varying turbulence induced by a fan at the side of the room, shown
in Fig. ref:fig-problem-statement.
It is hard to know the exact transition dynamics due to complex and uncertain
interactions between the quadcopter and the fan.
The system's transition dynamics resemble a mixture of two modes: a turbulent mode in front of
the fan and a non-turbulent mode everywhere else.
When planning a trajectory from start state $\mathbf{x}_0$ to desired state $\mathbf{x}_f$
it is preferred to avoid entering the turbulent mode, as it
results in poor performance and sometimes even system failure.

#+BEGIN_EXPORT latex
\begin{figure}[!t]
\centering
  \includegraphics[width=0.9\columnwidth]{images/quadcopter_bimodal_domain.pdf}
  \caption{
This work seeks to velocity control a DJI Tello quadcopter in an indoor environment
subject to two modes of operation characterised by process noise (turbulence).
A high turbulence mode is induced by placing a desktop fan at the right side of the room.
Data from four trajectories following a single 2D $\mathbf{x}=(x,y)$ target trajectory captures the variability
(process noise) in the dynamics.
Our goal is to find trajectories between $\mathbf{x}_0$ and $\mathbf{x}_f$ that either prioritise
remaining in the non-turbulent mode or prioritise avoiding regions of the learned dynamics
with high epistemic uncertainty due to lack of training observations.}
\label{fig-problem-statement}
\end{figure}
#+END_EXPORT

Trajectory optimisation comprises a powerful set of techniques for finding open-loop controls of dynamical
systems such that an objective function is minimised whilst satisfying a set of
constraints.
It is commonly used for controlling aircraft, robotic manipulators, and walking
robots cite:VonStryk1992,Betts1998,Garg2010.
One caveat to trajectory optimisation is that it requires a relatively accurate mathematical model of
the system.
Traditionally, these mathematical models are built using first principles based on physics.
However, accurately modelling the underlying transition dynamics can be challenging and
lead to the introduction of model errors.
For example, both observation and process noise
are inherent in many real-world systems and can be hard to model
due to both spatial and temporal variations.
Incorrectly accounting for this uncertainty can have a detrimental impact on controller performance
and is an active area of research in the robust
and stochastic optimal control communities cite:FreemanRandyA.2009,Stengel1988.

The difficulties associated with constructing mathematical models can be overcome by learning from
observations cite:Ljung1997.
However, learning dynamics models for control introduces other difficulties.
For example, it is important to know
where the model cannot predict confidently due to a lack of training observations.
This concept is known as epistemic uncertainty and is reduced in the limit of infinite data.
Probabilistic models have been used to account for epistemic uncertainty and also
provide a principled approach to modelling stochasticity i.e. aleatoric uncertainty
cite:Schneider1996,Deisenroth2011.
For example, cite:Cutler,Deisenroth2011,Pan2014 use Gaussian processes (GPs) to learn
transition dynamics.
GPs lend themselves to data-efficient learning through the selection of informative priors, and
when used in a Bayesian setting offer well calibrated uncertainty estimates.
Methods for learning probabilistic multimodal transition dynamics have also been proposed:
cite:Mckinnon used a mixture of GP experts method,
cite:Moerland studied the used of deep generative models and
cite:Kaiser2020a proposed a Bayesian model that learns independent
dynamics modes whilst maintaining a
probabilistic belief over which mode is responsible for predicting at a given input location.

There has also been work developing control algorithms exploiting learned multimodal transition dynamics
cite:Herzallah2020.
However, our work differs as it seeks to find trajectories that
remain in a single dynamics mode
whilst avoiding regions of the transition dynamics that cannot be predicted confidently.
To the best of our knowledge, there is no previous work addressing such trajectory optimisation
in transition dynamics models.

* Background and Related Work
** intro :ignore:
This chapter provides an overview of learning-based control and details the relevant background information
for the remainder of the dissertation.
** Dynamical Systems
Dynamical systems describe the behaviour of a system over time $t$ and
are a key component of both control theory and reinforcement learning.
At any given time a dynamical system has a state, which is
represented as a vector of real numbers $\mathbf{x}(t) \in \R^D$.
For example, the state of a 2-dimensional robotic system with state comprising of 2D Cartesian coordinates
and an orientation would be represented as $\mathbf{x}(t) = [x(t), y(t), \theta(t)]^{T}$.
The system can be controlled by applying control actions $\mathbf{u}(t) \in \R^F$ at any given time $t$.
This work considers continuous-time, continuous-state, nonlinear stochastic dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-unimodal-dynamics-cont}
\stateDerivative(t) &= f(\state(t), \control(t)) + \epsilon(t) \quad \forall t
\end{align}
#+END_EXPORT
where $f : \R^D \times \R^{F} \rightarrow \R^{D}$ represents the transition dynamics function and $\epsilon(t)$
is an i.i.d process noise term with $\E[\epsilon(t)] = 0$.
The process noise term accounts for unwanted (and, in general, unknown) disturbances of the system.
For example, it is extremely hard to model aerodynamic effects on aircraft so these could be accounted
for in the process noise term.
Throughout this thesis it is assumed that the state $\mathbf{x}$ is observed directly and is not subject to
observation noise.
This is a standard assumption in the \acrfull{mdp} framework which is commonly adopted
in the reinforcement learning literature.
\todo[inline]{Slightly weird referring to MDP literature on continuous time setting}

\todo[inline]{Should I make a bigger point out of the observation noise assumption}

The system is controlled via a policy $\mathbf{u}(t) = \pi(\mathbf{x}(t), t)$, which given the state $\mathbf{x}(t)$
and time step $t$ decides which control action $\mathbf{u}(t)$ to apply to the system.
The policy can be time-dependent and can also depend on all past information up to time step $t$.
In the time-independent case the policy is denoted $\pi(\mathbf{x}(t))$ and
the resulting closed-loop system is denoted $f_{\pi}(\mathbf{x}) = f(\mathbf{x},\pi(\mathbf{x}))$.


*** Multimodal Modelling
products of experts

**** Mixtures of Gaussian Process Experts
Tresp

infinite mixtures

DAGP

pros and cons of different gating networks

**** Our Motivations
Motivated by obtaining well-calibrated variance's throughout the model
placing priors on the gating network
** Optimal Control
*** intro :ignore:
Optimal control is a branch of mathematical optimisation that seeks to find a set of controls
over a time period $t \in [t_0, t_f]$ that optimises an objective function $\mathbf{J}_{\pi}(\x)$.
The objective function might be formulated to solve a particular task or to make the dynamical
system behave in a certain way.

Typically this objective function is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-objective}
\mathbf{J} \defeq \phi(\x(t_{f}), t_{f}) + \int^{t_{f}}_{t_{0}} L(\mathbf{x}(t), \mathbf{u}(t), t) \text{d}t,
\end{align}
#+END_EXPORT
which consists of two terms:
1. a terminal cost term $\phi : \R^{D} \times \R \rightarrow \R$,
2. an integral cost term (or Lagrangian) $L : \R^{D} \times \R^{F} \times \R \rightarrow \R$.

*** Trajectory Optimisation
Trajectory optimisation seeks to find the state and control trajectories
for times $t \in [t_0, t_f]$ that
minimise some cost function $g$ whilst satisfying constraints $c$
and boundary conditions.
The trajectory optimisation problem is given by,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-objective}
\min_{\mathbf{u}(t)} &\int_{t_0}^{t_f} g(\mathbf{x}(t), \mathbf{u}(t)) \text{d}t \quad \forall t \\
\text{s.t. }&\text{Eq. \ref{eq-unimodal-dynamics-cont}} \\
&c(\mathbf{x}(t)) \leq 0 \quad \forall t \\
&\mathbf{x}(t_0) = \mathbf{x}_0  \quad \mathbf{x}(t_f) = \mathbf{x}_f \numberthis
\end{align*}
#+END_EXPORT
*** Model Predictive Control
*** Reinforcement Learning
** Probabilistic Modelling
*** Epistemic Uncertainty

If we have used observations of a system to train a predictive model then we can only be confident in predictions near our observations.
As we extrapolate away from the observations we can no longer be certain and this is known as
epistemic uncertainty.
It can be reduced by collecting more data and retraining a model.
This is shown in Figure [[ref:epistemic]].
# #+NAME: epistemic
# #+ATTR_LATEX: :width 1.\textwidth :placement [h] :center nil
# #+caption: Plot demonstrating the concept of epistemic uncertainty. We can be confident in the learnt function close to our observations but as we exptrapolate away from them we become uncertain what the function should look like. If we have a notion of epistemic uncertainty in a MBRL algorithm we can use it to encourage the agent to visit these areas and collect data, which in turn will reduce the epistemic uncertainty.
# [[file:images/limited_data.pdf]]

*** Aleatoric Uncertainty

As mentioned previously, aleatoric uncertainty consists of process noise and observation noise; uncertainties that are inherent in a system and cannot be reduced.
# #+NAME: bimodal-dataset
# #+ATTR_LATEX: :width 1.\textwidth :placement [h] :center nil
# #+caption: An artificial 1D dataset with two levels of process noise.
# [[file:images/dataset.pdf]]
Figure [[ref:bimodal-dataset]] shows an artificial 1D dataset that demonstrates the concept of aleatoric uncertainty.
In our work we generally assume that there is no observation noise and therefore the aleatoric uncertainty only consists of process noise.

*** Variational Inference
*** Bayesian Inference

Suppose we have some observations $\mathbf{Y}$ generated from a family of models $p(\mathbf{Y}, \mathbf{\Theta})$, where $\mathbf{\Theta}$ represents the unknown random variables that the model depends on.

**** Maximum Likelihood
In maximum likelihood we seek to find the best model by maximising the likelihood $p(\mathbf{Y} | \mathbf{\Theta})$. We obtain a point estimate for the "best" variables $\mathbf{\Theta}$. The likelihood function is higher for more complex model structures, leading to overfitting.

**** Type II MLE/MAP
Bayesian methods overcome overfitting by treating the model parameters as random variables and averaging over the likelihood for different settings of the parameters. They achieve this by maximising the logarithm of the marginal likelihood (or evidence) $p(\mathbf{Y})$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq:type-2-ml}
	\text{log}\ p(\mathbf{Y}) &= \text{log} \int p(\mathbf{Y}, \mathbf{\Theta}) \text{d}\mathbf{\Theta} = \text{log} \int p(\mathbf{Y} | \mathbf{\Theta}) p(\mathbf{\Theta}) \text{d}\mathbf{\Theta}.
\end{align}
#+END_EXPORT
This is advantageous as we now take a weighted average over all possible settings of $\mathbf{\Theta}$ and we obtain the posterior $p(\mathbf{\Theta} | \mathbf{Y})$ for the unknown variables, as opposed to just a point estimate as in maximum likelihood. This provides automatic Occam's razor, penalising complex models and preventing overfitting.

The integral in Eq. ref:eq:type-2-ml is often always tractable so approximations

**** Variational Inference
\todo{seek to convert inference into an optimisation}

Variational approaches seek to lower bound the marginal likelihood (or evidence) $p(\mathbf{Y})$ using a functional that depends on variational parameters $q(\mathbf{\Theta})$.

#+BEGIN_EXPORT latex
\begin{align}
	\text{log}\ p(\mathbf{Y}) &= \text{log} \int p(\mathbf{Y}, \mathbf{\Theta}) \text{d}\mathbf{\Theta} = \text{log} \int p(\mathbf{Y} | \mathbf{\Theta}) p(\mathbf{\Theta}) \text{d}\mathbf{\Theta}
	\\
	&= \text{log} \int \frac{q(\mathbf{\Theta})}{q(\mathbf{\Theta})} p(\mathbf{Y}, \mathbf{\Theta}) \text{d}\mathbf{\Theta}
	\\
	&= \text{log} \int q(\mathbf{\Theta}) \frac{p(\mathbf{Y}, \mathbf{\Theta})}{q(\mathbf{\Theta})} \text{d}\mathbf{\Theta}
	\\
	&= \text{log} \E_{q(\mathbf{\Theta})} \bigg[ \frac{p(\mathbf{Y}, \mathbf{\Theta})}{q(\mathbf{\Theta})} \bigg]
	\\
	&\geq \E_{q(\mathbf{\Theta})} \bigg[ \text{log} \frac{p(\mathbf{Y}, \mathbf{\Theta})}{q(\mathbf{\Theta})} \bigg]
\end{align}
#+END_EXPORT

The bound becomes exact if $q(\mathbf{\Theta}) = p(\mathbf{\Theta} | \mathbf{Y})$. The true posterior is intractable but the variational posterior $q(\mathbf{\Theta})$ must be constrained (so that it is tractable). It is common to factorise $q(\mathbf{\Theta})$ with respect to groups of elements belonging to parameter set $\mathbf{\Theta}$, this is known as mean field approximation.

*** Gaussian Processes
This section introduces Gaussian processes and the sparse approximations that are used throughout this work.
In particular, it walks through the lower bounds (used to perform variational inference) that are the building blocks
of the inference in this thesis.

#+BEGIN_EXPORT latex
\begin{definition}[Gaussian process]
A Gaussian process (GP) is a collection of random variables, any finite number of which
have a joint Gaussian distribution \cite{rasmussenGaussian2006}.
\end{definition}
#+END_EXPORT

More intuitively, a Gaussian process is a distribution over functions $f : \R^{D_f} \rightarrow \R$
\marginpar{mean and covariance functions}
fully defined by a mean function $\mu(\cdot)$ and a covariance function $k(\cdot, \cdot)$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-mean-cov}
\mu(\mathbf{x}) &= \E[f(\mathbf{x})], \\
k(\mathbf{x}, \mathbf{x}') &= \E[(f(\mathbf{x}) -\mu(\mathbf{x})) (f(\mathbf{x}') - \mu(\mathbf{x}'))],
\end{align}
#+END_EXPORT
Importantly, for a given set of training inputs from the
\marginpar{Gaussian process prior}
functions domain $\mathbf{X} = \{ \mathbf{x}_1, \ldots, \mathbf{x}_N \}$ the associated function values
$\mathbf{f} = \{f(\mathbf{x}_1), \ldots, f(\mathbf{x}_N) \}$
are jointly Gaussian. The prior distribution over $\mathbf{f}$ is then given by,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-prior}
p(\mathbf{f} \mid \mathbf{X}) = \mathcal{N}(\mathbf{f} \mid \mu(\mathbf{X}), k(\mathbf{X}, \mathbf{X})).
\end{equation}
#+END_EXPORT
By definition, the training observations $\mathbf{f}$ are jointly Gaussian with any un-observed function value
\marginpar{jointly Gaussian}
$f_*$ at a new test input $\mathbf{x}_*$,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-spare-gp-joint}
\left[\begin{array}{c}
      \mathbf{f} \\
      f^{*}
\end{array}\right]
\sim\ &\mathcal{N}\left(
      \bm{0} ,
\left[\begin{array}{cc}
      k(\mathbf{X}, \mathbf{X}) & k(\mathbf{X},\mathbf{x}_{*}) \\
      k(\mathbf{x}_{*},\mathbf{X}) & k(\mathbf{x}_{*},\mathbf{x}_{*})
 \end{array}\right]\right).
\end{align*}
#+END_EXPORT
The distribution over the test function value $f_*$ (i.e. to make a prediction)
\marginpar{noise-free predictions}
can therefore easily be obtained by conditioning on the prior observations.
This is easily obtained using the properties of multivariate normal distributions to give
the predictive conditional distribution,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-prediction}
p(f_{*} \mid \mathbf{x}_*, \mathbf{f}, \mathbf{X}) &= \mathcal{N}(f_* \mid \mu, \sigma^2)  \\
\mu &= \mu(\mathbf{x}_*) + k(\mathbf{x}_*, \mathbf{X}) k(\mathbf{X}, \mathbf{X})^{-1} (\mathbf{f} - \mu(\mathbf{X})) \nonumber\\
\sigma^2 &= k(\mathbf{x}_*, \mathbf{x}_*) - k(\mathbf{x}_*, \mathbf{X}) k(\mathbf{X}, \mathbf{X})^{-1} k(\mathbf{X}, \mathbf{x}_*). \nonumber
\end{align}
#+END_EXPORT
The conditional in Eq. ref:eq-gp-prediction is computationally expensive due to conditioning
on all of the training data $\mathbf{X}, \mathbf{f}$.
The computational complexity of Gaussian process methods scale with $\mathcal{O}(N^{3})$
due to the inversion of the covariance matrix $k(\mathbf{X},\mathbf{X})$.
Many approximation schemes have been proposed to reduce the computational complexity, see
cite:quinonero-candelaUnifying2005 for a review.

\todo{introduce noisy outputs and Gaussian likelihood}



**** Sparse Gaussian Processes for Regression label:sec-sparse-gps
This work focuses on inducing point methods cite:snelsonSparse2005a, where the latent variables
are augmented with inducing input-output pairs known as inducing "inputs" $\Z$, and inducing
"variables" $\uF = f(\Z)$.
Introducing a set of $M$ inducing points from the same GP prior
$p(\uF \mid \Z) &= \mathcal{N}(\hat{\mathbf{f}} \mid \mu(\Z), k(\Z, \Z))$
can reduce the computational cost
if $M < N$.
Note that these inducing variables $\uF$ are jointly gaussian with the latent function values $\F$,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-spare-gp-joint}
p(\mathbf{f}, \uF \mid \mathbf{X}) =\ &\mathcal{N}\left(
\left[\begin{array}{c}
      \mathbf{f} \\
      \uF
\end{array}\right]
      \mid \bm{0} ,
\left[\begin{array}{cc}
      \mathbf{K}_{nn} & \mathbf{K}_{nm} \\
      \mathbf{K}_{nm}^{T} & \mathbf{K}_{mm}
 \end{array}\right]\right).
\end{align*}
#+END_EXPORT
where $\K_{mm}$ is the covariance function evaluated between all inducing inputs $\Z$, and
$\K_{nm}$ is the covariance function evaluated between the data inputs $\mathbf{X}$ and the inducing inputs $\Z$.
This joint distribution can be re-written using the properties of multivariate normal distributions as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-joint-cond}
p(\mathbf{f}, \uF \mid \mathbf{X}) &= p(\mathbf{f} \mid \uF) p(\uF \mid \mathbf{X}) \\
&= \mathcal{N}(\mathbf{f} \mid \mathbf{K}_{nm} \mathbf{K}_{mm}^{-1} \uF, \mathbf{K}_{nn} - \mathbf{Q}_{nn})
\mathcal{N}(\uF \mid \mathbf{0}, \mathbf{K}_{mm})
\end{align}
#+END_EXPORT
where $\mathbf{Q}_{nn} = \mathbf{K}_{nm}\mathbf{K}_{mm}^{-1}\mathbf{K}_{nm}^{T}$.
The full joint distribution is then given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-full-joint}
p(\mathbf{y}, \mathbf{f}, \uF \mid \mathbf{X}) &=  p(\mathbf{y} \mid \mathbf{f}) p(\mathbf{f} \mid \uF) p(\uF \mid \mathbf{X})
\end{align}
#+END_EXPORT
Computationally efficient inference is obtained by approximating the integration over $\mathbf{f}$.
\marginpar{FITC}
The popular Fully Independent Training Conditional (FITC) method (in the case of a Gaussian likelihood)
is obtained via the factorisation: $p(\mathbf{y} \mid \uF) \approx \prod^{N}_{n=1} p(y_{n} \mid \uF)$.
To obtain a variational approximation Jensen's inequality is first used to bound this conditional,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-bound-cond}
\text{log}p(\mathbf{y}\mid\uF) \geq \E_{p(\F \mid \uF)} \left[ \text{log} p(\mathbf{y} \mid \F) \right] := \mathcal{L}_{1}.
\end{align}
#+END_EXPORT
This is the standard bound shown in cite:hensmanGaussian2013 ($\mathcal{L}_{1}$ defined in Eq. 1).
In cite:titsiasVariational2009 this bound is then substituted into the marginal likelihood
to obtain a tractable lower bound,
\marginpar{Titsias' bound}
#+BEGIN_EXPORT latex
\begin{align}
\text{log} p(\mathbf{y} \mid \mathbf{X})
&= \text{log} \E_{p(\uF \mid \mathbf{X})} \left[ p(\mathbf{y} \mid \uF) \right] \\
&\geq \text{log} \E_{p(\uF \mid \mathbf{X})} \left[ \text{exp}(\mathcal{L}_{1}) \right] := \mathcal{L}_{\text{titsias}} \label{eq-titsias-bound-2} \\
&= \text{log}\mathcal{N}(\mathbf{y} \mid \mathbf{0}, \mathbf{Q}_{nn} + \sigma^{2} \mathbf{I})
- \frac{1}{2\sigma^2}\text{tr}(\K_{nn} - \mathbf{Q}_{nn}) \label{eq-titsias-bound},
\end{align}
#+END_EXPORT
where $\sigma^2$ is the noise variance associated with the Gaussian likelihood.
The bound in Eq. ref:eq-titsias-bound becomes exact (i.e. recovers the true log marginal likelihood)
when the inducing inputs $\Z$ equal the
data inputs $\mathbf{X}$ as $\K_{nm}=\K_{mm}=\K_{nn}$ so $\mathbf{Q}_{nn}=\K_{nn}$.

cite:hensmanGaussian2013 noted that this bound has complexity $O(NM^{2})$ so
introduced additional variational parameters to achieve a more computationally scalable bound
with complexity $O(M^{3})$.
Instead of collapsing the inducing points in Eq. ref:eq-titsias-bound
they explicitly represent them as a variational distribution,
#+BEGIN_EXPORT latex
\begin{align} \label{eq--inducing-dist}
q(\uF) = \mathcal{N}\left(\uF \mid \mathbf{m}, \mathbf{S} \right),
\end{align}
#+END_EXPORT
which they use to lower bound the marginal likelihood $p(\mathbf{y} \mid \mathbf{X})$ by
\marginpar{Hensman's bound}
further bounding $\mathcal{L}_{\text{titsias}}$,
#+BEGIN_EXPORT latex
\begin{align}
\mathcal{L}_{\text{titsias}} &= \text{log} \E_{p(\uF \mid \mathbf{X})} \left[ \text{exp}(\mathcal{L}_{1}) \right] \\
&\geq \E_{q(\uF)} \left[ \mathcal{L}_{1} \right]
- \text{KL}\left[q(\uF) \mid\mid p(\uF \mid \mathbf{X})\right] \\
&\geq \E_{q(\F)} \left[ p(\mathbf{y} \mid \F) \right]
- \text{KL}\left[q(\uF) \mid\mid p(\uF \mid \mathbf{X})\right] := \mathcal{L}_{\text{hensman}}, \label{eq-hensman-bound}
\end{align}
#+END_EXPORT
where $q(\F) = \int p(\F \mid \uF) q(\uF) \text{d} \uF$.
This is equivalent to $\mathcal{L}_{3}$ defined in Eq. 4 cite:hensmanGaussian2013.
If the likelihood factorizes across data $p(\mathbf{y} \mid \F) = \prod_{n=1}^{N} p(y_{n} \mid f_{n})$
only the marginals of $q(\F)$ are needed to calculate Eq. ref:eq-hensman-bound,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-bound-3-fact}
\mathcal{L}_{\text{hensman}}
= \sum_{n=1}^{N}\E_{q(f_{n})} \left[ \text{log} p(y_{n} \mid f_{n}) \right]
- \text{KL}\left[q(\uF) \mid\mid p(\uF \mid \mathbf{X}) \right].
\end{align}
#+END_EXPORT
Importantly, this bound is written as a sum over input-output pairs which has induced the necessary conditions
to preform stochastic gradient methods on $q(\uF)$.


**** Sparse Gaussian Processes :noexport:
A GP cite:edwardrasmussenGaussian2010 is a distribution over functions $f : \R^{D_f} \rightarrow \R$
fully defined by a mean function $\mu(\cdot)$ and a covariance function $k(\cdot, \cdot)$.
For a given set of inputs  from the
functions domain $\mathbf{X} = \{ \mathbf{x}_1, \ldots, \mathbf{x}_N \}$ the associated function values
$\mathbf{f} = \{f(\mathbf{x}_1), \ldots, f(\mathbf{x}_N) \}$
are jointly Gaussian,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-prior}
p(\mathbf{f} \mid \mathbf{X}) = \mathcal{N}(\mathbf{f} \mid \bm\mu_{\mathbf{X}}, \mathbf{K}_{\mathbf{X}, \mathbf{X}})
\end{equation}
#+END_EXPORT
where $\bm\mu_{\mathbf{X}}= \mu(\mathbf{X})$ is the mean vector, and
$\mathbf{K}_{\mathbf{X},\mathbf{X}} = k(\mathbf{X}, \mathbf{X})$ is the covariance function evaluated
between the inputs $\mathbf{X}$.
In this work, the squared exponential covariance function with Automatic Relevance Determination
is used for all GPs.
The distribution over the function value $f_*$ at a new input $\mathbf{x}_*$ (i.e. to make a prediction)
is given by the conditional,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-gp-prediction}
p(f_{*} \mid \mathbf{x}_*, \mathbf{f}, \mathbf{X}) &= \mathcal{N}(f_* \mid \mu, \sigma^2) \numberthis \\
\mu &= \mu_{\mathbf{x}_*} + \mathbf{k}_{\mathbf{x}_*, \mathbf{X}} \mathbf{K}_{\mathbf{X}, \mathbf{X}}^{-1} (\mathbf{f} - \bm\mu_{\mathbf{X}}) \\
\sigma^2 &= k_{\mathbf{x}_*, \mathbf{x}_*} - \mathbf{k}_{\mathbf{x}_*, \mathbf{X}} \mathbf{K}_{\mathbf{X}, \mathbf{X}}^{-1} \mathbf{k}_{\mathbf{X}, \mathbf{x}_*}.
\end{align*}
#+END_EXPORT
The conditional in Eq. ref:eq-gp-prediction is computationally expensive due to conditioning
on all of the training data $\mathbf{X}, \mathbf{f}$.
Introducing a set of $M$ inducing points from the same GP prior can reduce the computational cost
if $M < N$.
The inducing inputs are denoted $\bm\xi$ and outputs as
$\hat{\mathbf{f}} = f(\bm\xi)$.
The inducing variables
$\hat{\mathbf{f}}$ are jointly Gaussian with the latent function values $\mathbf{f}$.
The GP predictive distribution can be approximated by conditioning on this
smaller set of inducing variables,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-prediction}
p({f}_{*} \mid \mathbf{x}_*, \mathbf{f}, \mathbf{X}) &\approx
p({f}_* \mid \mathbf{x}_*, \hat{\mathbf{f}}, \bm\xi) \\
p(\hat{\mathbf{f}}) &= \mathcal{N}(\hat{\mathbf{f}} \mid \mathbf{m}, \mathbf{S}).
\end{align}
#+END_EXPORT
The approximation becomes exact when the
inducing variables $\hat{\mathbf{f}}$ are a sufficient statistic
for the latent function values $\mathbf{f}$ cite:Titsias2009.
# The predicted latent function values are then mutually independent given the inducing variables.
# A central assumption is that given enough well
# placed inducing variables $\hat{\mathbf{f}}$ they are a
# sufficient statistic for the latent function values $\mathbf{f}$.

*** Mixtures of Gaussian Process Experts
Gaussian processes (GPs) are the state-of-the art approach for Bayesian nonparametric regression.
However, they suffer from two important limitations.
Firstly, the covariance function is commonly assumed to be stationary
due to the challenge of parameterising them to be non-stationary.
This limits their modelling flexibility.
For example, if the function has a discontinuity due to different underlying lengthscales
in different parts of the input space then a stationary covariance function will not be adequate.
Similarly, if the observations are subject to different noise variances in different regions
of the input space then conventional homoscedastic regression will not suffice.
Secondly, GPs cannot model multimodal predictive distributions,
i.e. where there are multiple
regions of high probability mass with regions of smaller probability mass in between.
GP regression with a Gaussian likelihood models a Gaussian predictive distribution
that is not capable of modelling such multimodal distributions.
Using any likelihood but a Gaussian requires approximate inference techniques which
are usually accompanied by increased computational cost.
# A motivating factor for adopting GP methods is their associated uncertainty quantification.
# #+begin_export latex
# \hl{Standard GP regression (with a Gaussian likelihood) models a Gaussian predictive distribution
# that is not capable of modelling such multimodal distributions.
# Using any likelihood but a Gaussian also results in inference no longer being
# analytically tractable. These likelihoods require approximate inference techniques which
# are usually accompanied by computational issues.}
# #+end_export
# We can think of the first limitation as fitting the predictive mean to the observations
# and the second with correctly quantifying uncertainty in the predictive posterior.

Mixture models have been proposed that
for a set of input $\mathbf{x}$ and output $\mathbf{y}$ observations
can model a multimodal distribution over the output
$p(\mathbf{y} | \mathbf{x}) = \sum_{k=1}^K \pi_k\ p(\mathbf{y} | \alpha=k, \mathbf{x})$.
The predictive distribution $p(\mathbf{y} | \mathbf{x})$ consists of
$k$ mixture components $p(\mathbf{y} | \alpha=k, \mathbf{x})$ that are weighted according to
the mixing coefficients $\pi_k$.
The mixture components may take the form of any distribution, for
example, Bernoulli or Gaussian.
Mixture models assume that each observation belongs to one of the components and then try to infer the
distribution of each component separately.
The capability of these models can be further extended by allowing the mixing coefficients themselves
to be a function of the input variable $x$.
This was introduced by cite:Jacobs1991 in the mixture of experts (ME) model where the mixing coefficients
are known as gating functions $\pi_k(\mathbf{x}) = p(\alpha=k | \mathbf{x})$
(and collectively the gating network).
The individual component densities $p(\mathbf{y} | \alpha=k, \mathbf{x})$ are then referred to as experts
because at different regions in the input space different components are responsible for predicting.
The gating network is governed by an expert indicator variable $\alpha_n \in \{1, ..., K\}$
that assigns each observation to one of the $K$ experts.
# are given by $\pi_k(\mathbf{x}) = p(\alpha=k | \mathbf{x})$ and
# referred to as gating functions (or collectively the gating network).
# This results in the predictive distribution
# $p(\mathbf{y} | \mathbf{x}) = \sum_{k=1}^K \pi_k(\mathbf{x}) p(\mathbf{y} | \alpha=k, \mathbf{x})$.
# The role of the gating network is to indicate which expert is most likely responsible for generating
# the data in a given region of the input space.

Modelling the experts as GPs gives rise to a class of powerful models known as
mixture of GP experts (MoGPE).
These models address both of the limitations that we discussed.
They can model multimodal predictive distributions as they model a mixture of distributions
over the outputs, usually a Gaussian mixture in the regression setting (cite:Tresp,Rasmussen2002).
They are able to model non-stationary functions as
each expert can learn different lengthscales, noise variances etc and the gating
network can turn each one "on" and "off" in different regions of the input space.
Many approaches to MoGPE have been proposed and in general the key differences between them are
the formulation of the gating network
and their inference algorithms (and associated approximations) cite:Yuksel2012.
The original MoGPE work by cite:Tresp proposed a gating network as a softmax of $K$ GPs
(bottom left plot in Figure [[ref:gating_network_comparison]]).
An EM inference scheme is proposed, which assuming there are $N$ observations requires
$\mathcal{O}(3KN^3)$ computations per iteration.
# The original MoGPE work by cite:Tresp proposed a gating network resembling a
# GP classification model (bottom left plot in Figure [[ref:gating_network_comparison]]).
# An EM inference scheme is proposed, requiring $\mathcal{O}(3KN^3)$ computations per iteration,
# assuming that there are $N$ observations.

Although this gating function divides up the input space cite:Rasmussen2002 argue that
data not assigned to a GP expert will lead to bias near the boundaries.
Instead cite:Rasmussen2002 formulate the gating network in terms of conditional
distributions on the expert indicator variable, on which they place an input-dependent
Dirichlet process prior.
cite:Meeds2005 proposed an alternative infinite MoGPE similar to cite:Rasmussen2002 except
that they specify a full generative model over the input and output space $p(\mathbf{y}, \mathbf{x})$
as opposed to just a
conditional model $p(\mathbf{y} | \mathbf{x})$.
Each experts inputs are now Gaussian distributed
(middle top plot of Figure [[ref:gating_network_comparison]]) giving rise to the gating
network in the middle bottom plot of Figure [[ref:gating_network_comparison]].
This gating network is not capable of turning an expert "on" in multiple regions
of the input space. It will instead generate a new cluster and assign it a new expert.
These approaches reduce the computational burden by associating each expert with a
subset of the observations but rely on Markov Chain Monte Carlo (MCMC) inference schemes.
# #+NAME: gating_network_comparison
# #+ATTR_LATEX: :width 0.7\textwidth :placement [h] :center t
# #+caption: Comparison of different gating networks (one per column).
# #+caption: The bottom plots show the expert mixing probabilities $p(\alpha=k|x)$
# #+caption: and the top plots show any associated distributions over the inputs.
# #+caption: The left and right regions (shaded green) are subsets of the domain where expert
# #+caption: one has generated the observations (observations are not shown) and the middle region (shaded blue) where expert two has.
# #+caption: The bottom left plot shows the mixing probabilities of a gating network based on GPs.
# #+caption: The middle top plot shows three separate Gaussian pdfs representing clustering of the inputs
# #+caption: (we have introduced a cluster indicator variable $z$).
# #+caption: The middle bottom plot shows the associated expert mixing probabilities (assuming $z=\alpha$).
# #+caption: In the right top plot we have assumed that there are only two experts (with $z\neq \alpha$) and
# #+caption: marginalised the cluster indicator variable from the plot to the left - leading to
# #+caption: a Gaussian mixture over the inputs.
# #+caption: The plot below shows the resulting mixing probabilities.
# [[file:images/gating-network-comparison-2by3-cropped.pdf]]
cite:Yuan proposed a MoGPE with a variational inference scheme which is much
faster than using MCMC.
They assume that the expert indicators $\alpha$ are generated by a Dirichlet distribution and
that given the expert indicator the inputs follow a Gaussian mixture model (GMM), i.e. they
have introduced a cluster indicator variable $z$.
This contrasts earlier approaches that let the expert indicator act as the input cluster
indicator.
This is illustrated in the right hand plots of Figure [[ref:gating_network_comparison]] which
show two Gaussian mixtures over the inputs (one for each expert) and
the resulting expert mixing probabilities.
Introducing the separate cluster indicator variable has given rise to a
gating network that can turn a single expert "on" in multiple regions of the input space.
However, it does not provide a handle for encoding prior information like the GP based gating network.
# This resembles two separate mixture models, one over the inputs and one over the outputs.

We note that mixture models are inherently unidentifiable as
different combinations of mixture components
and mixing coefficients can generate the same predictive distributions.
The gating network can be considered a handle for encoding prior knowledge that can be used to constrain
the set of admissible functions.
This can improve identifiability and lead to learned representations that reflect
our understanding of the system. The simplest case being reordering the experts.
Figure ref:gating_network_comparison provides a visual comparison of
different gating network formulations,
providing intuition for how they can differ in restricting the set of admissible functions.

# Modelling the relationship between the  input space and the expert indicator variable (gating network)

# with GPs enables
# us to efficiently capture the dependencies through the choice of GP prior.
# Modelling the gating network with GPs enables
# us to efficiently capture the dependencies through the choice of mean and covariance functions.
Modelling the gating network with GPs enables us
to encode informative prior knowledge through the choice of mean and covariance functions.
For example, adopting a squared exponential covariance function would encode prior belief that
the mixing probabilities should vary smoothly across the input space.
If modelling a dynamical system with oscillatory behaviour (e.g. an engine)
a periodic kernel could be adopted.
Prior knowledge of the mixing probability values can be encoded through
the choice of mean function.
We may also be interested in exploiting techniques from differential geometry
e.g. finding probabilistic geodesics (cite:Tosi2014) on the gating network.
Again, our choice of covariance function can be used to encode how differentiable
the gating network should be.
Learning better representations will also improve the models
ability to extrapolate as it will be more true to the underlying system.
# Again, our choice of covariance function can encode how differentiable
# our gating network should be.

# We may also be interested in harnessing the structure learnt by the gating network,
# for example, finding probabilistic geodesics (cite:Tosi2014).
# This would require a differential covariance function to be used.
# Learning better representations will also improve the models
# ability to extrapolate as it will be more true to the underlying system.
# For example, if one wanted to exploit techniques from differential geometry (e.g. finding geodesics)
# they could utilise a differentiable covariance function.
# For example, if one wanted to exploit techniques from differential geometry (e.g. finding geodesics)
# they could utilise a differentiable covariance function.

# The extension from the middle plot of Figure ref:gating_network_comparison
# to modelling a GMM over the inputs (bottom plot) leads to a
# more powerful gating network from a modelling perspective - as it can turn a single expert "on"
# Introducing the separate cluster indicator variable (middle plot of
# Figure ref:gating_network_comparison to bottom plot) gives rise to a
# gating network that can turn a single expert "on" in multiple regions of the input space.
# However, it does not provide a handle for encoding prior information like a GP based gating network.
# The formulation of the gating network is commonly motivated by improving
# the computational complexity of the inference scheme.
# As such, we consider our method (GP based gating network) as trading in the computational
# benefits of gating networks based on GMMs
# (bottom plots of Figure ref:gating_network_comparison)
# for the ability to improve identifiability through informative gating network priors.
# \todo{Insert reference to periodic gating function in results once it's added.}


We can also consider the implications of the mentioned gating networks from an inference perspective.
It is known that developing inference algorithms for propagating uncertain (Gaussian) inputs
through GPs is itself a challenging problem (see cite:Ustyuzhaninov2019).
Therefore, developing a scalable inference algorithm to propagate inputs that are distributed according to
a Gaussian mixture will likely lead to valuable information loss.
Theoretically the approaches by cite:Rasmussen2002,Meeds2005 are able to achieve very
accurate results but their inference relies on MCMC sampling methods,
which can be slow to converge.
# Previous work that formulated a gating network based on GPs cite:Tresp used an EM inference scheme
# which decouples the learning of the gating network and the experts.
# The main contributions of this paper are twofold,
# \todo{What is a true MoE model???}
# 1. We re-formulate a gating network based on GPs to a) improve indentifiability and b) achieve a true mixture of experts model, i.e. not split the observations between experts,
# 2. We derive a variational lower bound which improves the complexity issues associated with inference when adopting a GP based gating network.
#    - Motivated by learning representations of dynamical systems with two regimes we instantiate
#      the model with two experts as it is  a special case where the gating network can be calculated
#      in closed form.
#      We seek to learn an operatable mode with one expert and explain away the un-operatable mode
#      with the other. With this representation the gating network indicates which regions of the
#      input space correspond to the operatable mode and provides a convenient space for planning.
# The GMM formulation of the gating network is motivated by improving
# the computational complexity of the inference scheme.
# We consider our approach as trading in the computational benefits of a GMM gating network
# for the ability to improve identifiability by placing informative GP priors on the gating network.
We consider our approach as trading in the computational benefits that can be obtained through
the formulation of the gating network
for the ability to improve identifiability with informative GP priors.
The main contributions of this paper are twofold. Firstly, we re-formulate a gating network
based on GPs to improve indentifiability.
Secondly, we derive a variational lower bound
which improves on the complexity issues associated with inference when adopting such a gating network.
Motivated by learning representations of dynamical systems with two regimes we instantiate
the model with two experts as it is  a special case where the gating network can be calculated
in closed form.
This is because we seek to learn an operatable mode with one expert and explain away the un-operatable mode
with the other. This results
in the gating network indicating which regions of the
input space are operatable, providing a convenient space for planning.
# This is because we seek to learn an operatable mode with one expert and explain away the un-operatable mode
# with the other.
# The gating network then indicates which regions of the
# input space are operatable, providing a convenient space for planning.
# with the other. With this representation the gating network indicates which regions of the
# input space correspond to the operatable mode which provides a convenient space for planning.

The remainder of the paper is organised as follows. We first introduce our model
in Section ref:sec-modelling
and then derive our variational lower
bound in Section ref:sec-variational-approximation.
In Section ref:sec-model-validation we test our method on an artificial data set where we show
the benefits of adopting informative covariance functions in the gating network.
We then test our model on the motorcycle data set and compare it to a sparse variational GP.
# The remainder of the paper is organised as follows. First we introduce our generative model
# in Section ref:sec-modelling where we compare our marginal likelihood to the literature.
# We then derive our variational lower
# bound in Section ref:sec-variational-approximation and detail how we optimise it and make
# predictions.
# In Section ref:sec-model-validation we test our model on an artificial data set where we show
# the benefits of adopting informative covariance functions in the gating network GP.
# We then test our model on the motorcycle data set and compare its performance
# to a sparse variational GP.



** Old                                                            :noexport:
In cite:NhatAnhNguyen2018 they highlight that the previously mentioned approaches are limited as
each expert is trained using only the local data
assigned to it. This means that they do not take into account the global information, i.e. the
correlations between clusters.
The experts are therefore likely to overfit to their local training data.
cite:NhatAnhNguyen2018 propose a model based on both global and local sparse GPs to overcome this
and also employ stochastic optimisation providing greater scalability.
They reduce the computational complexity in their bound through the variational
approximation on the indicator variable as well as the sparse GP framework cite:Hensman2013.

Our approach does not lead to over fitting as each GP is trained using all of the observations.
However, each of our experts has a set of inducing inputs that are unique to the expert which
greatly improves computational complexity (at training and prediction).
These inducing inputs enable stochastic variational inference in our model which also significantly speeds
up inference.

In contrast, our main motivation is learning good representations of the underlying functions
(both the experts and the gating network).
Mixtures of GP experts models are inherently unidentifiable as
different combinations of experts
and gating networks can generate the same distributions over the observations.
As such, we trade in the computational benefits of the gating networks used by much
of the literature for the modelling advantages offered by a gating network based on GPs.
This provides a handle for encoding prior knowledge that can be used to constrain
the set of admissible functions that we wish to learn within.
This in turn improves identifiability and results in learned representations that are more true
to our understanding of the system.
It is worth noting that we do derive a variational lower bound with competitive complexity.
To the best of our knowledge the complexity of our variational lower bound is
state of the art for \gls{mgp} models with GP based gating networks.




Before comparing our work to the literature we first provide some intuition for the modelling
assumptions behind different gating networks in Figure [[ref:gating_network_comparison]].
We denote the inputs $x$, expert indicator variable $\alpha$ and the cluster indicator variable $z$.
The expert mixing probabilities are given by $P(\alpha=k|x)$ and the distributions over the inputs
are given by $p(x | \alpha=k, z=c)$.
\todo{dist or density?}
The top plot is a gating
network based on GPs and importantly we can see that it has been able to turn expert 1 "on" in
multiple regions of the domain and is not limited to a single local subset of the domain.
Alternatively, many approaches utilise some notion of clustering the inputs locally and the middle right plot
shows three Gaussian pdfs representing this.
It is common to assign each cluster to a single expert,
i.e. let the cluster indicator equal the expert indicator ($k=c$).
This leads to improved computational complexity by dividing up the domain and decomposing the
covariance matrix into a set of smaller matrices.
However, this leads to fitting separate experts to the left and right regions (shaded green) which may be
contrary to our knowledge of the generation of observations.
We apply Bayes' rule to recover the expert mixing probabilities $P(\alpha=k |x)$ shown in the
middle left plot (so that we can compare it to the GP based gating network above).

One can maintain separate expert and cluster indicator variables (as seen in cite:Yuan) which
leads to each experts inputs being modelled as a mixture of Gaussians. This is shown in the bottom
right plot where all we have done is take the middle right plot and simply marginalising the
cluster indicator variable.
The distribution over the inputs associated with expert
one $p(x | \alpha=1)$ (dashed green) has high *cluster* mixing
probabilities for the clusters in the left and right (green) regions and a low *cluster* mixing
probability in the middle (blue) region.
This leads to a gating network that is also able to turn expert 1 "on" in multiple regions of the domain
like the GP based gating network.

This is good but we now consider the added modelling benefit of a GP based gating network.
\todo[inline]{insert text/example about encoding prior info and making hyperparameters not trainable etc}

We can also consider the implications from an inference perspective.
It is known that developing inference algorithms for propagating uncertain (Gaussian) inputs
through GPs is itself an unsolved problem (see cite:Ustyuzhaninov).
Therefore developing an inference algorithm to propagate inputs that are distributed according to
a Gaussian mixture model will likely lead to valuable information loss.
This is a purely intuitive statement and we have not performed any experiments.

# Stochastic variational inference enables GP models to scale to larger data sets so we would like
# to develop such an algorithm.

Hopefully this provides some intuition about the different assumptions that can be encoded (or not)
by different gating network formulations.
We now highlight XXX of the factors that might make a GP based gating network desirable.

1. Handle for encoding prior information,
   - Constrain the set of admissible functions.
2. Information loss,
   - Propagating Gaussian inputs through GPs is an active area of research cite:Ustyuzhaninov and we
     argue that any approximations required to propagate inputs that are distributed according to
     a Gaussian mixture model will likely lose valuable information.

\todo[inline]{We can encode more info into a GP prior that into a GMM?}
\todo[inline]{Can GMMs extrapolate away from data??}
\todo[inline]{With GP gating function we can ensure it is a Riemannian manifold i.e. differentiable. Is this the case with GMMs?}


\todo{Yuan uses VI and changed gating network structure}
cite:Yuan proposed a mixture of GP experts with a variational inference scheme which is much
faster than using MCMC.
They assume that the expert indicators are generated by a Dirichlet distribution and
that given the expert indicator the inputs follow a Gaussian mixture model, i.e. they
have introduced a cluster indicator variable.
This is in contrast to earlier approaches that let the expert indicator act as the input cluster
indicator.
Intuitively we think that this gives rise to a gating network similar
to the bottom plot of Figure [[ref:gating_network_comparison]].
However, the expert indicator does not depend on the inputs, rather the inputs depend on the cluster
indicator which depends on the expert indicator.
\todo{However, the "gating network" is no longer input dependent, unless a similar modification to cite:Rasmussen2002 is made to the DP.}

In cite:NhatAnhNguyen2018 they highlight that these approaches are limited as
each expert is trained using only the local data
assigned to it. This means that they do not take into account the global information, i.e. the
correlations between clusters.
The experts are therefore likely to overfit to their local training data.
cite:NhatAnhNguyen2018 propose a model based on both global and local sparse GPs to overcome this
and also employ stochastic optimisation providing greater scalability.
They reduce the computational complexity in their bound through the variational
approximation on the indicator variable as well as the sparse GP framework cite:Hensman2013.
Our approach does not lead to over fitting as each GP is trained using all of the observations.
However, each of our experts has a set of inducing inputs that are unique to the expert which
greatly improves computational complexity (at training and prediction).
These inducing inputs enable stochastic variational inference in our model which also significantly speeds
up inference.

Our gating function only requires a single GP as we instantiate our model with only two experts.
We exploit the probit link function (as opposed to the softmax function used by cite:Tresp)
because it leads to analytic mixing probabilities
and factorisation across the data given $M$ global inducing points.
We do not introduce a variational distribution over the indicator variable as we are able
to analytically marginalise it in our evidence lower bound without a significant computational burden.


Our model is similar to cite:Tresp but overcomes the limitations highlighted
by cite:Rasmussen2002.
That is, we simultaneously improve the models computational complexity and
the data assignment at prediction time.
Each of our experts predicts on the basis of a set of inducing inputs that are unique to the expert.
These inducing inputs enable stochastic variational inference in our model which significantly speeds
up inference.


** Ignore                                                         :noexport:

The \glspl{mgp} literature commonly highlights two main issues that they
seek to address.
Firstly, \glspl{gp} cannot handle multi-modality, i.e. they assume that the observations
were generated by a single underlying function.
Secondly, they suffer from both time and memory complexity issues
arising from the calculation and storage of the inverse covariance matrix.
Given a data set with $N$ observations, training has time and memory complexity of $\mathcal{O}(N^3)$
and $\mathcal{O}(N^2)$ respectively.
Many approaches to a \gls{mgp} method have been proposed which attempt
to address these issues cite:Tresp,Rasmussen2002,Meeds2005,Yuan,NhatAnhNguyen2018.
However, most previous work appears to be motivated by addressing the complexity issues
associated with \glspl{gp}.
That is, they are interested in decomposing the $N \times N$ covariance matrix into a set of smaller
matrices.
In contrast, our main motivation is learning good representations of the underlying functions
(both the experts and the gating network).
Mixtures of GP experts models are inherently unidentifiable as
different combinations of experts
and gating networks can generate the same distributions over the observations.
As such, we trade in the computational benefits of the gating networks used by much
of the literature for the modelling advantages offered by a gating network based on GPs.
This provides a handle for encoding prior knowledge that can be used to constrain
the set of admissible functions that we wish to learn within.
This in turn improves identifiability and results in learned representations that are more true
to our understanding of the system.
It is worth noting that we do derive a variational lower bound with competitive complexity.
To the best of our knowledge the complexity of our variational lower bound is
state of the art for \gls{mgp} models with GP based gating networks.

In cite:NhatAnhNguyen2018 they exploit a global GP to coarsely model the
entire data set as well as local GP experts to overcome this limitation.
Our approach does not lead to over fitting as each GP is trained using all of the observations.
We exploit the factorization achieved via the sparse variational GP framework to
couple the learning of the gating network and the experts.
Our bound achieves this with little extra computational burden.

A significant amount of recent work builds on cite:Rasmussen2002 with the
computational benefits as a key driving factor.
However, we highlight that there is also a modelling assumption that distinguishes these approaches.
The gating network of cite:Tresp is able to model spatial structure.
That is, it is capable of turning each expert "on" and "off" in different regions of the input space
i.e. expert $k$ can be turned on in multiple regions.
This contrasts placing a GMM on the inputs and associating each expert with the observations
associated with specific components.
This approach improves the computational complexity but is not capable of capturing spatial structure.
For example, if expert $k$ is responsible for generating the data in multiple areas of the
input space then a GP based gating network will be capable of capturing this.

\todo[inline]{what do we mean by spatial structure??? Essentially we have GP compared to GMM. Do Gaussian mixture models capture spatial structure??}


We do not directly assign each observation to a single expert but instead assume that
it is generated as a mixture of the experts.

cite:Nguyen2014 also provide fast variational inference based on the sparse GP framework.
Their model also "splits" the dataset such that each expert acts only on the
observations assigned to it.
Each observation is assigned to an expert based on its proximity to the experts centroids.


A limitation of these approaches is that each expert is trained using only the local data
assigned to it. This means that they do not take into account the global information, i.e. the
correlations between clusters.
The experts are therefore likely to overfit to their local training data.

Our approach is similar to cite:Yuan in the sense that we break the dependency among the training
outputs enabling variational inference.
However, we achieve this through the sparse variational GP framework seen in cite:Hensman2013.
We actually induce the necessary conditions for stochastic variational inference as
we obtain factorisation over data points given a set of $M$ global inducing points.

cite:NhatAnhNguyen2018 propose a model based on both global and local sparse GPs
and also employ stochastic optimisation providing greater scalability.
They reduce the computational complexity in their bound through the variational
approximation on the indicator variable as well as the sparse GP framework cite:Hensman2013.
We do not introduce a variational distribution over the indicator variable as we are able
to analytically marginalise it in our evidence lower bound without a significant computational burden.
\todo[inline]{Variational dists allow us to condider the unc in a variable and perform full Bayesian inference. What is the significance of what we have done? I think that ideally we would analytically marginalise all our latent variables so what we have done is very nice.}
cite:NhatAnhNguyen2018 use a GMM gating network which and achieve complexity $\mathcal{O}(NM^2K)$.
We want spatial structure so use GP based gating network which leads to $\mathcal{O}(NM^3K)$.


The gating network enforces that each component of the GP mixture is localized.
In cite:Lazaro-Gredilla2011 they formulate a mixture without the use of a gating function
which enables a given
location in the input space to be associated with multiple outputs. This could be used for modelling
multiple objects in a tracking system and is known as the data association problem.
We are not interested in the data association problem but instead wish to encode spatial structure
through the use of a gating function.
We introduce our model from a generative model perspective which provides insight
into how we can easily switch between the data association setting and a mixture of GPs by
simply marginalising the indicator variable.

Our gating function only requires a single GP as we instantiate our model with only two experts.
We exploit the probit link function (as opposed to the softmax function used by cite:Tresp)
because it leads to analytic mixing probabilities
and factorisation across the data given $M$ global inducing points.


Our model is similar to cite:Tresp but overcomes the limitations highlighted
by cite:Rasmussen2002.
That is, we simultaneously improve the models computational complexity and
the data assignment at prediction time.
Each of our experts predicts on the basis of a set of inducing inputs that are unique to the expert.
These inducing inputs enable stochastic variational inference in our model which significantly speeds
up inference.

** Concepts of Riemannian Geometry label:sec-riemannian-geometry
This section introduces the concepts of Riemannian geometry that are used to formulate
a \textit{geometric} objective function for
the trajectory optimisation presented in Chapter ref:chap-traj-opt.
The aim of the \textit{geometric} objective function is to find shortest trajectories
(known as geodesics) on a Riemannian manifold.
This section starts by defining Riemannian manifolds and then details how lengths can
be calculated on these manifolds.

The Euclidean norm provides an intuitive notion for the length of a vector in the Euclidean space.
Just as the dot product is the inner product of the Euclidean space it is possible to
define inner products for Riemannian spaces (also known as Riemannian manifolds).
Remembering that a norm can be expressed for any space endowed with an inner product
provides us with the tools to for calculating lengths on manifolds.

*** Riemannian Manifolds
Intuitively Riemannian manifolds are smoothly curved spaces with an inner product.
Formally, cite:carmoRiemannian1992 define them as smooth manifolds equipped with a Riemannian metric.
#+BEGIN_EXPORT latex
\begin{definition}[Riemannian Metric]
A Riemannian metric $\mathbf{G}$ on a
manifold $\mathcal{M}$ is a symmetric and positive definite matrix which defines
a smoothly varying inner product
$\langle \dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \rangle_{\mathbf{x}} = \dot{\mathbf{x}}_a^T \mathbf{G}(\mathbf{x}) \dot{\mathbf{x}}_b$
in the tangent space $T_{\mathbf{x}}\mathcal{M}$, for each point $\mathbf{x} \in \mathcal{M}$ and
$\dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \in T_{\mathbf{x}}\mathcal{M}$.
\end{definition}
\todo{comment on how I have assumed tangent space is velocity etc}
#+END_EXPORT
Riemannian manifolds are often represented as charts; a parameter space for the
curved surface. An example of a chart is the spherical coordinate system that is
used to describe a sphere.
The chart is often a flat space and the curvature of the manifold arises
through smooth changes in the metric.
Measurements on the surface can thus be computed in the chart locally and
integrated to give global measures.
On a Riemannian manifold $\mathcal{M}$ the length of a trajectory (curve) $\bar{\mathbf{x}}$
is given by the norm of the tangent vector along the trajectory,
#+BEGIN_EXPORT latex
\todo{define $\bar{\mathbf{x}}$}
\begin{align} \label{eq-manifold-length}
\text{Length}(\bar{\mathbf{x}})
&=\int_{t_0}^{t_f}\left\|\dot{\mathbf{x}}(t)\right\|_{\mathbf{G}(\mathbf{x}(t))} \mathrm{d}t \\
&=\int_{t_0}^{t_f}\sqrt{\dot{\mathbf{x}}(t)^T \mathbf{G}(\mathbf{x}(t)) \dot{\mathbf{x}}(t) } \mathrm{d} t,
\end{align}
#+END_EXPORT
where $\mathbf{G}(\mathbf{x}(t))$ is the metric tensor at $\mathbf{x}(t)$.
Throughout this thesis we use Newton's notation to denote differentiation with respect to time.
\todo{Make this point about Newton's notation at earliest use of it.}

*** Lengths on Riemannian Manifolds
With this method for calculating lengths on Riemannian manifolds
let us now turn to the definition of geodesics, i.e. the notion of shortest paths on Riemannian manifolds.
Formally, a geodesic is defined as follows.
#+BEGIN_EXPORT latex
\begin{definition}[Geodesic]
Given two points $\mathbf{x}_0, \mathbf{x}_f \in
\mathcal{M}$, a Geodesic is a length minimising trajectory (curve)
$\bar{\mathbf{x}}_g$ connecting the points such that,
\begin{align} \label{eq-geodesic-conds}
  \bar{\mathbf{x}}_{g}=\arg \min_{\bar{\mathbf{x}}} \operatorname{Length}(\bar{\mathbf{x}}), \quad \bar{\mathbf{x}}(t_0)=\mathbf{x}_{0}, \bar{\mathbf{x}}(t_f)=\mathbf{x}_{f}.
\end{align}
\end{definition}
#+END_EXPORT
An important observation from cite:carmoRiemannian1992 that we exploit in
Chapter ref:chap-traj-opt, is that geodesics satisfy a continuous-time $2^{\text{nd}}$ order ODE,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-2ode}
 \ddot{\mathbf{x}}(t)
&= y(t, \mathbf{x}, \dot{\mathbf{x}}) \\
&=-\frac{1}{2} \mathbf{G}^{-1}(\mathbf{x}(t))\left[\frac{\partial \operatorname{vec}[\mathbf{G}(\mathbf{x}(t))]}{\partial \mathbf{x}(t)}\right]^{T}\left(\dot{\mathbf{x}}(t) \otimes \dot{\mathbf{x}}(t)\right),
\end{align}
\normalsize
#+END_EXPORT
where $\operatorname{vec}[\mathbf{G}(\mathbf{x}(t)])$ stacks the columns of $\mathbf{G}(\mathbf{x}(t))$
and $\otimes$ denotes the Kronecker product.
The implication of Eqs. ref:eq-geodesic-conds and ref:eq-2ode is that trajectories that are solutions
to the $2^{\text{nd}}$ order ODE in Eq. ref:eq-2ode implicitly minimise the objective,
#+BEGIN_EXPORT latex
\todo{define $\bar{\mathbf{x}}$}
\begin{align} \label{eq-length-objective}
J &= \min \int_{t_0}^{t_f}\left\|\dot{\mathbf{x}}(t)\right\|_{\mathbf{G}(\mathbf{x}(t))} \mathrm{d}t
\end{align}
#+END_EXPORT


# Computing geodesics involves finding a solution to Eq. ref:eq-2ode
# with $\mathbf{x}(t_0) = \mathbf{x}_1$ and $\mathbf{x}(t_f) = \mathbf{x}_2$.
# This is a boundary value problem with a smooth solution so it can be solved
# using any direct trajectory optimisation framework and can therefore
# incorporate state and action constraints.
# #+BEGIN_EXPORT latex
# \todo[inline]{Hmm, not sure this is true, read up - It only involves discretizing the geodesic curve and not
# the feature space and as this is always 1-dimensional the approach scales
# to higher dimensional feature spaces.}
# #+END_EXPORT


# We can formulate this as an initial value problem and use techniques such as the
# shooting method to determine the correct inintial velocity and from this the
# geodesic path.
# This is advantageous as it only involves discretizing the geodesic curve and not
# the the feature space. This is always 1-dimensional and thus the approach scales
# to highgher dimensional feature spaces.

* Probabilistic Inference for Multimodal Dynamical Systems label:chap-dynamics
#+begin_export latex
\epigraph{All models are wrong, but some are useful.}{\textit{George Box}}
#+end_export
** Intro :ignore:
This chapter introduces the class of continuous-time multimodal dynamical systems
that this work considers and then details an approach to performing Bayesian inference in such models.
Throughout this chapter it is assumed that pairs of input $\x$ and output $\y$ observations
have previously been sampled from the system at a constant frequency,
(i.e. with a fixed time-step) to give a data set $\dataset$.
Based on this assumption, this chapter constructs a discrete-time representation of the system's transition dynamics
and then formulates it as a probabilistic model based on GPs.
This thesis is interested in multimodal systems where the dynamics modes vary over the
input (state, control) domain, such that the mode switching can by modelled
by input-dependent functions.
This set of functions governing the mode switching is commonly referred to as a gating network.
This work is motivated by data-efficient learning and specifically focuses on gating networks
where prior knowledge of the system can be encoded into the model via informative priors.
As a result, the probabilistic model constructed in this chapter resembles a \acrfull{mogpe}
with a \acrshort{gp}-based gating network.
This work derives a novel variational lower bound based on sparse GPs that
enables the model to be trained with stochastic gradient methods.
The work in this chapter is implemented in GPflow/TensorFlow and is available on GitHub
cite:matthewsGPflow2017,GPflow2017,tensorflow2015-whitepaper[fn:1:Code accompanying this Chapter can be found @ [[https://github.com/aidanscannell/mogpe][\icon{\faGithub}/aidanscannell/mogpe]].].

** Problem Statement
This chapter considers the transition dynamics in Equation ref:eq-unimodal-dynamics-cont
extended to multimodal systems, given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-multimodal-dynamics-cont}
\dot{\mathbf{x}}(t) &= \f(\mathbf{x}(t), \mathbf{u}(t)) + \epsilon(t) \\
&= \fk(\mathbf{x}(t), \mathbf{u}(t)) + \mode{\epsilon}(t) \quad \text{if} \quad \alpha(t)=\modeInd
\end{align}
#+END_EXPORT

with states $\state \in \R^\StateDim$ and controls $\control \in \R^\ControlDim$.
One of $\ModeInd$ dynamics modes $\{\fk \}_{\modeInd=1}^\ModeInd$ and associated noise models
$\mode{\epsilon}(t) \sim \mathcal{N}\left(0, \text{diag}\left[ \left(\noiseVarOneK\right)^{2}, \ldots, \left( \noiseVarDK \right)^{2} \right]  \right)$
are selected by a switching (or gating) variable
$\alpha(t) \in \{1,\ldots,\ModeInd\}$.

The work in this chapter assumes access to historical data comprising state transitions
from $\NumEpisodes$ trajectories (episodes) sampled with a fixed time step $\Delta t=t_*$ resulting in
$\NumTimesteps$ time steps per episode.
This gives a data set with $N = \NumEpisodes \NumTimesteps$ elements.
We combine the independent episodes to get the data set
$\mathcal{D} = \left\{ \left\{ (\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t} \right\}_{\numTimesteps=1}^{\NumTimesteps} \right\}_{\numEpisodes=1}^{\NumEpisodes}$.
We abuse notation and consider the time indexing notation.

# We combine the independent episodes to get the data set
# $\mathcal{D} = \left\{(\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t}\right\}_{t=1}^{T}$
# where we have abused the time indexing notation.

This work learns a discrete-time representation of Eq. ref:eq-multimodal-dynamics-cont,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-multimodal-dynamics-disc}
\singleOutput = \fk (\singleState, &\singleControl ; \Delta t = t_*) + \mode{\epsilon_{t-1}}
\quad \text{if} \quad \modeVarK,
\end{align}
#+END_EXPORT
where $\state_t \in \R^D$ and $\control_t \in \R^F$ are the states and controls
at time $t$ respectively, and $\alpha_t \in \{1, \dotsc, \ModeInd\}$ is a mode indicator variable that
indicates one of $\ModeInd$ dynamics modes at time $t$.

A time series of observations from time $a$ to time $b$ (inclusive)
is denoted by $\mathbf{x}_{a:b}$ (analogously for other variables).
A single input is denoted as
$\singleInput = (\state_{t-1}, \control_{t-1})$, all inputs are denoted as
$\allInput$, and the set of all outputs as $\allOutput$.
The $\stateDim^{\text{th}}$ dimension of the $\modeInd^{\text{th}}$ mode's latent transition dynamics
function $\fk$, evaluated at $\singleInput$,
is denoted $\fknd = \fkd (\singleInput)$,
for all dimensions as $\fkn$ and at all data points as $\Fk$.

** Probabilistic Modelling
#+BEGIN_EXPORT latex
\renewcommand{\mode}[1]{\ensuremath{#1_{\modeInd}}}
\renewcommand{\modei}[2]{\ensuremath{#1_{#2}}}
\renewcommand{\singleOutput}{\ensuremath{\Delta x_{\numData}}}
\renewcommand{\allOutput}{\ensuremath{\Delta\mathbf{\state}}}
\newcommand{\singleModeVar}{\ensuremath{\singleData{\modeVar}}}
\newcommand{\allModeVar}{\ensuremath{\bm{\modeVar}}}
\newcommand{\singleModeVarK}{\ensuremath{\singleModeVar = \modeInd}}
\newcommand{\allModeVarK}{\ensuremath{\{\singleModeVarK\}_{\numData=1}^\NumData}}

\newcommand{\expertPrior}{\ensuremath{p\left(\mode{f}(\allInput) \right)}}
\newcommand{\expertsPrior}{\ensuremath{p\left(\LatentFunc(\allInput) \right)}}
\newcommand{\expertMeanFunc}{\ensuremath{\mode{\mu}}}
\newcommand{\expertCovFunc}{\ensuremath{\mode{k}}}
\newcommand{\expertLikelihood}{\ensuremath{p\left(\allOutput \mid \mode{f}(\allInput)\right)}}
\newcommand{\singleExpertLikelihood}{\ensuremath{p(\singleOutput \mid \mode{f}(\singleInput))}}
\newcommand{\expertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVarK, \allInput \right)}}
\newcommand{\singleExpertPosterior}{\ensuremath{p\left(\singleOutput \mid \singleModeVarK, \allInput \right)}}
% \newcommand{\expertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVarK \right)}}

\newcommand{\gatingPrior}{\ensuremath{p\left(\GatingFunc(\allInput ) \right)}}
\newcommand{\gatingMeanFunc}{\ensuremath{\mode{\hat{\mu}}}}
\newcommand{\gatingCovFunc}{\ensuremath{\mode{\hat{k}}}}
\newcommand{\singleGatingLikelihood}{\ensuremath{\Pr\left(\singleModeVarK \mid \GatingFunc(\singleInput) \right)}}
\newcommand{\gatingLikelihood}{\ensuremath{P\left(\singleModeVar \mid \GatingFunc(\singleInput) \right)}}
%\newcommand{\gatingPosterior}{\ensuremath{\Pr\left( \singleModeVar \mid \singleInput \right)}}
\newcommand{\gatingPosterior}{\ensuremath{\Pr\left( \allModeVarK \mid \allInput \right)}}
\newcommand{\singleGatingPosterior}{\ensuremath{\Pr\left( \singleModeVarK \mid \singleInput, \gatingParams \right)}}
\newcommand{\evidence}{\ensuremath{p\left(\allOutput \mid \allInput \right)}}

\newcommand{\moeExpertPosterior}{\ensuremath{p\left(\singleOutput \mid \singleModeVarK, \singleInput, \expertParamsK \right)}}
\newcommand{\moeGatingPosterior}{\ensuremath{\Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right)}}
\newcommand{\moeEvidence}{\ensuremath{p\left(\allOutput \mid \allInput, \expertParams, \gatingParams \right)}}
\newcommand{\singleMoeEvidence}{\ensuremath{p\left(\singleOutput \mid \singleInput, \expertParams, \gatingParams \right)}}

\newcommand{\npmoeExpertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVar, \allInput, \expertParams \right)}}
\newcommand{\npmoeGatingPosterior}{\ensuremath{\Pr\left(\allModeVar \mid \allInput, \gatingParams \right)}}

\newcommand{\moeLikelihood}{\ensuremath{p\left(\allOutput \mid \LatentFunc(\allInput), \GatingFunc (\allInput) \right)}}
\newcommand{\singleMoeLikelihood}{\ensuremath{p\left(\singleOutput \mid \mode{\latentFunc}(\allInput), \GatingFunc (\allInput) \right)}}
#+END_EXPORT
*** Intro :ignore:
In this section I will first introduce mixture models as they are a natural choice for modelling
multimodal systems.
After highlighting the inherent identifiability issues associated with mixture models, I will introduced the Mixture of
Experts (MoE) model, an extension to mixture models where
the mixture weight (aka mixing probability) depends on the input variable.
I will then detail how it breaks down for nonparametric experts
and introduce the infinite Mixture of Gaussian Process Experts (MoGPE) model from the literature,
which is motivated by the computational issues associated with inference and prediction.
Although our model suffers from the complexity issues highlighted by cite:rasmussenInfinite2001, our variational
inference scheme significantly improves scalability of inference and prediction in our model, whilst providing
attractive mechanisms for improving identifiablity.

*Mixture Models* Given an input $\singleInput$ and an output $\singleOutput$, mixture models
model a mixture of distributions over the output,
\marginpar{mixture models}
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-mixture-model}
\singleMoeEvidence = \sum_{\modeInd=1}^\ModeInd
\underbrace{\Pr(\singleModeVarK \mid \gatingParams)}_{\text{mixing probability}}
\underbrace{p(\singleOutput \mid \modeVarK, \singleInput, \expertParamsK)}_{\text{component } k}.
\end{equation}
#+END_EXPORT
where $\gatingParams$ and $\expertParams$ represent model parameters
and $\singleModeVar$ is a discrete indicator variable assigning observations to components.
The predictive distribution $\singleMoeEvidence$ consists of
$K$ mixture components ${p(\singleOutput \mid \modeVarK, \singleInput, \expertParamsK)}$ that are weighted according to
the mixing probabilities $\Pr(\modeVarK \mid \gatingParams)$.
Mixture models are inherently unidentifiable as different combinations of mixture components
and mixing probabilities can generate the same distributions over the output.

*Mixture of Experts (MoE)* cite:jacobsAdaptive1991 is an extension where the mixing probabilities
depend on the input variable $\moeGatingPosterior$, and are
collectively referred to as the *gating network*.
The individual component densities $\moeExpertPosterior$ are then referred to as *experts*,
as at different regions in the input space, different components are responsible for predicting.
Given $\ModeInd$ experts $\moeExpertPosterior$, each with parameters $\expertParamsK$,
the MoE marginal likelihood is given by,
\marginpar{mixture of experts}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixture-marginal-likelihood}
\moeEvidence = \prod_{\numData=1}^\NumData \sum_{\modeInd=1}^{\ModeInd}
\underbrace{\moeGatingPosterior}_{\text{gating network}}
\underbrace{\moeExpertPosterior}_{\text{expert } k},
\end{align}
#+END_EXPORT
where $\singleModeVar$ is the expert indicator variable assigning observations to experts.
The probability distribution over the expert indicator variable is referred to as the gating network and
indicates which expert governs the model at a given input location.

*Nonparametric Mixtures of Experts* As highlighted in cite:rasmussenInfinite2001,
the traditional MoE marginal likelihood does not apply when the
experts are nonparametric.
This is because the model assumes that the observations are i.i.d. given the model parameters,
which is contrary to GP models, which model the dependencies in the joint distribution, given the
hyperparameters.
cite:rasmussenInfinite2001 point out that there is a joint distribution corresponding to every possible
combination of assignments (of observations to experts).
The marginal likelihood is then a sum over exponentially many assignments,
\marginpar{mixture of nonparametric experts}
#+BEGIN_EXPORT latex
\begin{align}  \label{eq-np-moe-marginal-likelihood-assign}
\moeEvidence &= \sum_{\allModeVar} \npmoeGatingPosterior \npmoeExpertPosterior \\
&= \sum_{\allModeVar} \npmoeGatingPosterior 
\left[ \prod_{\modeInd=1}^\ModeInd p\left(\{\singleOutput : \singleModeVarK \} \mid \{\singleInput : \singleModeVarK \}, \expertParamsK \right) \right],  \nonumbedr
\end{align}
#+END_EXPORT
where $\allModeVar = \{\modeVar_1, \ldots, \modeVar_\NumData \}$ represents a set of assignments for all observations.
This distribution factors into the product over experts, where each expert models the joint Gaussian distribution
over the observations assigned to it.

Given a factorised likelihood for each expert (e.g. Gaussian),
an alternative approach is to let each expert predict on the basis of all observations
and leave the gating network to soft assign the observations.
The marginal likelihood of this model is then given by,
#+BEGIN_EXPORT latex
\marginpar{marginal likelihood}
\begin{align} \label{eq-np-moe-marginal-likelihood}
\moeEvidence &=
\prod_{\numData=1}^\NumData \sum_{\modeInd=1}^\ModeInd
\underbrace{\Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right)}_{\text{gating network}}
\underbrace{\E_{\expertPrior} \left[ \singleExpertLikelihood \right]}_{\text{expert } k},
\end{align}
#+END_EXPORT
where each expert follows the standard Gaussian likelihood model,
\marginpar{expert's Gaussian likelihood}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-likelihood}
\singleOutput &= \mode{f}(\singleInput) + \epsilon_{k}, \quad \epsilon_{k} \sim \mathcal{N}(0, \mode{\noiseVar}^2), \\
\singleExpertLikelihood &= \mathcal{N}\left( \singleOutput \mid \mode{f}(\singleInput), \mode{\noiseVar}^2 \right),
\end{align}
#+END_EXPORT
with $\mode{f}$ and $\mode{\noiseVar}^2$ representing the latent function and the noise variance associated
with the $\modeInd$^{\text{th}} expert.
Independent GP priors are placed on each of the expert's latent functions,
\marginpar{expert's GP prior}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior}
\expertPrior &= \mathcal{N}\left( \mode{f}(\allInput) \mid \expertMeanFunc(\allInput), \expertCovFunc(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
where $\expertMeanFunc(\cdot)$ and $\expertCovFunc(\cdot, \cdot)$ represent the mean and
covariance functions associated with the $\modeInd^{\text{th}}$ expert respectively.
This leads to each expert resembling a standard GP regression model with a Gaussian likelihood.
For notational conciseness the dependence on the inputs $\allInput$ and hyperparameters $\expertParamsK$
is dropped for each GP prior,
i.e. $\expertPrior = p\left( \mode{f}(\allInput) \mid \allInput, \expertParamsK \right)$.
\todo{add reference to section number}
Importantly, this model retains the uncertainty in the assignment of observations to experts
and will lead to each expert not overfitting to the observations assigned to it.
This approach can be see as trading in the computational benefits of assigning observations to experts
(Equation ref:eq-np-moe-marginal-likelihood-assign) in favour
of directly capturing the correlations between all observations.

*** Identifiable Mixtures of Gaussian Processes for Control
# The gating network models how the dynamics switch between modes over the input space and is of particular importance
# in this work.
# The gating network is of particular importance in this work as it models how the dynamics switch
# between modes over the input space.
The gating network models how the dynamics switch between modes over the input space.
It can be seen as a handle for encoding prior knowledge that can be used to constrain the set of admissible functions.
It is of particular interest in this work as it
can improve identifiability and lead to learned representations that better reflect
our understanding of the system. The simplest case being reordering the experts.

As highlighted previously, the gating network can be formulated to hard assign observations to experts with the
goal of improving the computational problems associated with inference and prediction in MoGPE models.
This work chooses to sacrifice these computational benefits in favour of constructing a gating network that can,
1) Improve *identifiability* in MoGPE models,
   - Selecting informative mean and covariance functions for each gating function GP prior can ensure that each expert corresponds to the underlying dynamics mode that it was intended to model. For example, if the dynamics oscillate between modes over the input space then a periodic kernel could be selected. Learning representations that are more true to the underlying system will also enhance the models ability to extrapolate from training observations.
2) Infer *informative geometric structure* that can be exploited for *control*,
   - The goal of Chapter ref:chap-traj-opt is to construct a control technique that attempts to find trajectories that remain in a single dynamics mode and avoid regions of the dynamics that have not been observed so cannot be predicted confidently. The GP-based gating network infers informative geometric structure regarding how the dynamics switch between modes over the input space whilst providing a principled approach to modelling the epistemic uncertainty associated with the gating functions. This makes the gating network a convenient latent space to project the control problem onto.
     \todo{Add more - why does this need to be GP and not just GMM??}

With these motivations, this work adopts a GP-based gating network, as seen in the original
MoGPE model cite:trespMixtures2000a.
Our marginal likelihood is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood}
\evidence &=
\prod_{\numData=1}^\NumData \sum_{\modeInd=1}^\ModeInd
\underbrace{\E_{\gatingPrior} \left[ \singleGatingLikelihood \right]}_{\text{gating network}}
\underbrace{\E_{\expertPrior} \left[ \singleExpertLikelihood \right]}_{\text{expert } k}.
\end{align}
#+END_EXPORT
where the gating network resembles a Gaussian process
classification model, with a factorised classification likelihood $\singleGatingLikelihood$ dependent on
input dependent functions
$\GatingFunc = \{\mode{\gatingFunc} : \R^{\StateDim \times \ControlDim} \rightarrow \R \}_{\modeInd=1}^\ModeInd$,
known as gating functions.

The probability mass function over the expert indicator variable is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-prob-mass}
\gatingLikelihood = \prod_{\modeInd=1}^\ModeInd \left(\singleGatingLikelihood\right)^{[\singleModeVarK]}
\end{align}
#+END_EXPORT
where $[\singleModeVarK]$ denotes the Iverson bracket.
The probabilities of this Categorical distribution $\singleGatingLikelihood$
are governed by a classification likelihood (e.g. Bernoulli, Softmax).

*Softmax ($\ModeInd>2$)* In the general case, that is, when there are more than two experts,
$\ModeInd > 2$, the gating network's likelihood is defined as the Softmax function,
\marginpar{softmax}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-softmax}
\singleGatingLikelihood = \text{softmax}_{\modeInd}\left(\GatingFunc(\singleInput)\right) = \frac{\text{exp}\left(\mode{\gatingFunc}(\singleInput)\right)}{\sum_{j=1}^{\ModeInd} \text{exp}\left(\gatingFunc_j(\singleInput) \right)}.
\end{align}
#+END_EXPORT
Each gating function $\mode{\gatingFunc}(\cdot)$ describes how its corresponding mode's mixing
probability varies over the input space.
Modelling the gating network with input-dependent functions enables
informative prior knowledge to be encoded through the placement of GP priors on each gating function.
For example, if modelling a dynamical system with oscillatory behaviour (e.g. an engine)
a periodic kernel could be adopted.
Independent GP priors are placed on each gating function, giving the gating network prior,
\marginpar{GP priors}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-prior}
\gatingPrior = \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \mode{\gatingFunc}(\allInput) \mid \gatingMeanFunc(\allInput), \gatingCovFunc(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
where $\gatingMeanFunc(\cdot)$ and $\gatingCovFunc(\cdot,\cdot)$ are the mean and covariance functions
associated with the $\modeInd^\text{th}$ gating function.
Each mode's mixing probability $\singleGatingPosterior$ is then obtained by marginalising
*all* of the gating functions,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-posterior}
\singleGatingPosterior
&= \E_{\gatingPrior}\left[ \singleGatingLikelihood \right].
\end{align}
#+END_EXPORT
In the general case where $\singleGatingLikelihood$ uses the softmax function
(Eq. ref:eq-softmax) this integral is intractable, so we approximate it using Monte carlo quadrature.

*Bernoulli ($\ModeInd=2$)* Instantiating the model with two experts, $\singleModeVar \in \{1, 2\}$, is a special case
where only a single gating function is needed.
\marginpar{two experts}
This is because the output of a function $\gatingFunc(\singleInput)$ can be mapped through a sigmoid
function $\text{sig} : \R \rightarrow [0, 1]$ and interpreted as a probability,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sigmoid}
\Pr(\singleModeVar=1 \mid \modei{\gatingFunc}{1}(\singleInput)) = \text{sig}(\modei{\gatingFunc}{1}(\singleInput)).
\end{align}
#+END_EXPORT
If this sigmoid function satisfies the point symmetry condition then
we know that
$\Pr(\singleModeVar=2 \mid \modei{\gatingFunc}{1}(\singleInput)) = 1 - \Pr(\singleModeVar=1 \mid \modei{\gatingFunc}{1}(\singleInput))$.
This is neat because it only requires a single gating function and no normalisation term needs to be calculated.

In the two expert case where $\singleGatingLikelihood$ uses a sigmoid function (Eq. ref:eq-sigmoid),
the integral in Eq. ref:eq-gating-posterior becomes analytic if the sigmoid function is selected
to be the Gaussian cumulative distribution function
$\Phi(\gatingFunc(\cdot)) = \int^{\gatingFunc(\cdot)}_{-\infty} \mathcal{N}(\tau | 0, 1) \text{d} \tau$.
The integral is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixing-probabilities-analytic}
\Pr(\modeVarn=1 \mid \singleInput, \gatingParams) &=
 \int \Phi\left(\gatingFunc(\singleInput)\right) \mathcal{N}\left(\gatingFunc(\singleInput) \mid \mu_h, \sigma^2_h \right) \text{d} \gatingFunc(\singleInput) \\
&= \Phi \left(\frac{\mu_{h}}{\sqrt{1 + \sigma^2_{h} }}\right).
\end{align}
#+END_EXPORT
*** graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\allInput)$};
     % \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\hk$};


      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[const, below=of f] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      \node[latent, right=of a, yshift=1.4cm, xshift=-0.4cm] (h) {$\mode{\gatingFunc}(\allInput)$};
      \node[const, right=of h, xshift=-0.4cm] (phik) {$\gatingParamsK$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    %}
  \caption{Graphical model of the transition dynamics where the state difference $\singleOutput$
is generated by pushing the state and control $\singleInput$ through the latent process.}
\label{fig-graphical-model}
\end{figure}
#+END_EXPORT


# #+BEGIN_EXPORT latex
# \begin{figure}[t]
#   \centering
#   \resizebox{0.8\columnwidth}{!}{
#     \begin{tikzpicture}[
#       pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
#       post/.style={->,shorten >=0.4pt,>=stealth',semithick}
#       ]
#       \node[const] (x) {$\hat{\mathbf{x}}_{t-1}$};
#       \node[latent, left=of x, yshift=-1.4cm] (f) {$\mathbf{f}^{(k)}_t$};
#       %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
#       \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {${h}^{(k)}_t$};

#       \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\hat{\mathbf{f}}^{(k)}$};
#       \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\hat{\mathbf{h}}^{(k)}$};
#       \node[const, left=of uk, xshift=0.4cm] (zk) {$\bm\xi^{(k)}_f$};
#       \node[const, right=of uh, xshift=-0.4cm] (zh) {$\bm\xi^{(k)}_h$};

#       \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\bm\theta^{(k)}$};
#       \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\bm\phi^{(k)}$};

#       \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\sigma^{(k)}$};

#       \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\Delta\mathbf{x}_t$};
#       %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
#       \node[latent, right=of y, xshift=-0.4cm] (a) {${\alpha}_t$};

#       %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

#       \factor[above=of a] {h-a} {left:Cat} {h} {a};

#       \draw[post] (a)--(y);
#       \draw[post] (x)-|(f);
#       %\draw[post] (f)--(yk);
#       \draw[post] (f)--(y);
#       %\draw[post] (yk)--(y);
#       %\draw[post] (h)--(a);
#       \draw[post] (x)-|(h);
#       \draw[post] (uk)--(f);
#       \draw[post] (uh)--(h);
#       \draw[post] (zk)--(uk);
#       \draw[post] (zh)--(uh);
#       \draw[post] (thetak)--(f);
#       \draw[post] (phik)--(h);
#       \draw[post] (sigmak)|-(y);

#       \plate {} {(x) (y) (a) (f) (h)} {$T$};
#       %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
#       \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$K$};
#       \plate {} {(zh) (uh) (h) (phik)} {$K$};
#     \end{tikzpicture}
#     }
#   \caption{Graphical model of the transition dynamics
# where the state difference $\Delta\mathbf{x}_{t}$
# is generated by pushing the state and control $\hat{\mathbf{x}}_{t-1}$
# through the latent process.}
# %\label{fig:graphical_model}
# \end{figure}
# #+END_EXPORT

*** Generative Model
The marginal likelihood can be written to clearly show the factorised likelihood (mixture of Gaussians)
and the expectation over the latent variables,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood-factorised}
\evidence &=
\E_{\gatingPrior \expertsPrior} \left[
\prod_{\numData=1}^\NumData \sum_{\modeInd=1}^\ModeInd
\singleGatingLikelihood \singleExpertLikelihood \right],
\end{align}
#+END_EXPORT
where $\expertsPrior = \prod_{\modeInd=1}^\ModeInd \expertPrior$ is the experts prior and
$\gatingPrior$ is the gating network prior.
Fig. [[ref:fig-graphical-model]]
shows the graphical model where the $\ModeInd$ latent gating functions $\GatingFunc$
are evaluated and normalised to obtain the mixing probabilities
$\singleGatingLikelihood$.
The mode indicator variable $\singleModeVar$ is then sampled from the Categorical distribution
governed by these probabilities.
The indicated mode's latent function $\mode{f}$ and process noise $\noiseVarK$ are
then evaluated to generate the state difference $\singleOutput$.

This model makes single-step probabilistic predictions,
where the predictive distribution over the state difference $\Delta \mathbf{x}_{t}$ is
given by a mixture of Gaussians.
This provides the model flexibility to model multimodal transition dynamics $f$ as mixtures of $K$ experts.
In the two expert case the marginal likelihood is given as an analytic mixture of two Gaussians
and has complexity $\mathcal{O}(\ModeInd\NumData^4)$ to evaluate.
The general case with more than two experts has complexity $\mathcal{O}(\ModeInd^2\NumData^4)$.

For ease of notation and understanding only a single output dimension has been considred,
although in most scenarios the state dimension will be greater than $1$.
The extension to multiple output dimensions is fairly straight forward.
\todo{Can I just say it's left out because trivial?}
*** Gating Network :noexport:
The gating network governs how the dynamics switch between modes.
This work is interested in spatially varying modes so
formulates an input dependent Categorical distribution over $\alpha_t$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-prob-mass}
P\left(\alpha_t \mid \mathbf{h}_t\right) = \prod_{k=1}^K \left(\Pr(\alpha_t=k \mid \mathbf{h}_t)\right)^{[\alpha_t = k]},
\end{align}
#+END_EXPORT
where $[\alpha_t=k]$ denotes the Iverson bracket.
It should be noted that other gating networks have been proposed that provide computational benefits
for inference. However, these gating networks often lack a handle for encoding domain knowledge
that can be used to restrict the set of possible

\todo[inline]{insert gating network references}
The probabilities of this Categorical distribution $\Pr(\alpha_t=k \mid \mathbf{h}_t)$
are obtained by evaluating $K$ latent
gating functions $\{h^{(k)}\}_{k=1}^K$ and normalising their output.
Each gating function evaluated at $\hat{\mathbf{x}}_{t-1}$ is denoted
as $h^{(k)}_t = h^{(k)}(\hat{\mathbf{x}}_{t-1})$
and at all observations ${h}^{(k)}_{1:T}$.
The set of all gating functions evaluated at $\hat{\mathbf{x}}_{t-1}$ is denoted
as $\mathbf{h}_t$ and at all observations as $\mathbf{h}_{1:T}$.
Each gating function $h^{(k)}$ describes how its corresponding mode's mixing
probability varies over the input space.


This work is interested in finding trajectories that can avoid areas of the transition dynamics model
that cannot be predicted confidently.
Placing independent sparse GP priors on each gating function
provides a principled approach to
modelling the epistemic uncertainty associated with each gating function.
The gating function's posterior covariance is a quantitative value that can be exploited by the
trajectory optimisation.
# GPs also provide data-efficient and interpretable learning through the selection of informative mean
# and covariance functions.
# For example, if the transition dynamics modes are subject to oscillatory mixing then a periodic
# covariance function could be selected.

Each gating function's inducing inputs are denoted
$\bm\xi_h^{(k)}$ and outputs as $\hat{\mathbf{h}}^{(k)}$.
For all gating functions, they are collected as $\bm\xi_h$ and $\hat{\mathbf{h}}$ respectively.
The probability that the $t^{\text{th}}$ observation is generated by mode $k$
given the inducing inputs is obtained by marginalising $\mathbf{h}_t$,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha=\modeInd \mid \mathbf{h}(\cdot) )
&= \E_{p(\mathbf{h}(\cdot))} \left[ \Pr(\alpha_t=k \mid \mathbf{h}(\cdot)) \right],
\end{align*}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha=\modeInd \mid \mathbf{h}(\cdot) )
&= \E_{p(\mathbf{h}(\cdot))} \left[ \text{softmax}_k(\mathbf{h}(\cdot)) \right],
\end{align*}
#+END_EXPORT
#+BEGIN_EXPORT latex
% \small
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha_t=k \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}} )
&= \int \text{softmax}_k(\mathbf{h}_t) p(\mathbf{h}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}) \text{d} \mathbf{h}_t,
\end{align*}
#+END_EXPORT
where $p(\mathbf{h}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}) = \prod_{k=1}^K p\left({h}^{(k)}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}^{(k)}\right)$
is the $K$ independent sparse GP priors on the gating functions.

**** Two Experts
Instantiating the model with two experts is a special case where only a single gating function is needed.
The output of a function can be mapped through a sigmoid
function $\text{sig} : \R \rightarrow [0, 1]$ and interpreted as
a probability $\Pr(\alpha=1 \mid \gatingFunc(\cdot))$.
If this sigmoid function satisfies the point symmetry condition then
we know that $\Pr(\alpha=2 \mid \gatingFunc(\cdot)) = 1 - \Pr(\alpha =1 \mid \gatingFunc(\cdot))$.

The distribution over $\gatingFunc(\cdot)$

We note that $p({h}_n^{(k)} \mid \mathbf{h}_{\neg n}, \mathbf{X})$
is a GP conditional and denote its mean $\mu_h$ and variance $\sigma^2_h$.
Choosing the sigmoid as the Gaussian cdf
$\Phi(h_n) = \int^{h_n}_{-\infty} \mathcal{N}(\tau | 0, 1) \text{d} \tau$
leads to,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixing-probabilities-analytic}
\Pr(\alpha_n=1 | \mathbf{X}) &=
 \int \Phi({h}_n) \mathcal{N}(h_n \mid \mu_h, \sigma^2_h) \text{d} {h}_n
= \Phi \left(\frac{\mu_{h}}{\sqrt{1 + \sigma^2_{h} }}\right).
\end{align}
#+END_EXPORT
Given this gating network our marginal likelihood is now an analytic mixture of two Gaussians.

can analytically marginalise.

** Bayesian Model Old 2 :noexport:
*** Intro :ignore:
The model is built upon GP priors on each of the transition dynamics functions $\fk$
with independent GPs placed on each output (state) dimension $d$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior-single-dim}
p\left(\Fkd \mid \allInput \right) &= \mathcal{N}\left( \Fkd \mid \singleDimMode{\mu}(\allInput), \singleDimMode{k}(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
where $\singleDimMode{\mu}(\cdot)$ and $\singleDimMode{k}(\cdot, \cdot)$ represent the mean and
covariance functions associated with the $d^{\text{th}}$ dimension of the $k^{\text{th}}$ dynamics mode respectively.
Each mode's output dimensions are assumed independent and are given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior}
\pFk &= \prod_{\stateDim=1}^{\StateDim} \pFkd.
\end{align}
#+END_EXPORT
The process noise in each mode is assumed to be factorised across both observations and output (state) dimensions.
For each mode it is modelled as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-likelihood}
\pYkGivenFk
&= \prod_{\numData=1}^{\NumData} \prod_{\stateDim=1}^{\StateDim} \pykGivenfkd \\
&= \prod_{\numData=1}^{\NumData} \prod_{\stateDim=1}^{\StateDim} \mathcal{N}\left( \singleOutput \mid \fknd, \text{diag}\left[ \left(\noiseVarOneK\right)^{2}, \ldots, \left( \noiseVarDK \right)^{2} \right]  \right)
\end{align}
#+END_EXPORT
where $\noiseVardK$ represents the noise variance associated with the $d$^{\text{th}} dimension of the $k$^{\text{th}} mode.
Each mode is then obtained by marginalising its associated latent function values,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert}
\pYkGivenX = \int  \pYkGivenFk \pFk \text{d} \Fk,
\end{align}
#+END_EXPORT
akin to GP regression.
Each dynamics mode $k$ is assumed to be independent,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-prior}
\pF &= \prod_{k=1}^{K} \pFk.
\end{align}
#+END_EXPORT
At a given input location the mode governing the dynamics is indicated by the discrete mode indicator
variable $\modeVar$.
Following a mixture model formulation, we construct a discrete probability distribution over the mode
indicator variable $\PrA$.
The resulting marginal likelihood is then given by marginalising the mode indicator variable,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixture-marginal-likelihood}
\pYGivenX = \sum_{\modeInd=1}^{\ModeInd} \PrA \pYkGivenX.
\end{align}
#+END_EXPORT
The distribution over the discrete mode indicator variable $\PrA$ is referred to as the gating network.
It governs how the dynamics switch between modes.
# This work is interested in spatially varying modes and formulates a
# which will now be detailed.
# formulates an input dependent Categorical distribution over $\alpha_t$,

*** Gating Network
Mixture models are inherently unidentifiable as different combinations of mixture components
and mixing coefficients can generate the same predictive distributions.
The gating network can be seen as a handle for encoding prior knowledge that can be used to constrain
\marginpar{identifiability}
the set of admissible functions.
This can improve identifiability and lead to learned representations that better reflect
our understanding of the system. The simplest case being reordering the experts.
This motivates Mixtures of Experts models citep:jacobsAdaptive1991, where the distribution over the mode
indicator variable is formulated to be input-dependent. It's probability mass function is then given by,
#+BEGIN_EXPORT latex
%\begin{align} \label{eq-prob-mass}
%P\left(\modeVar \mid \x \right) = \prod_{k=1}^K \left(\Pr(\modeVar= \modeInd \mid \x) \right)^{[\modeVar=\modeInd]}
%\end{align}
\begin{align} \label{eq-prob-mass}
P\left(\alpha \mid \mathbf{h}(\cdot) \right) = \prod_{k=1}^K \left(\Pr(\alpha=k \mid \mathbf{h}(\cdot))\right)^{[\alpha = \modeInd]}
\end{align}
#+END_EXPORT
where $[\modeVar=\modeInd]$ denotes the Iverson bracket.
The probabilities of this Categorical distribution $\Pr(\modeVar=\modeInd \mid \GatingFunc(\cdot))$
are obtained by evaluating $K$ latent
gating functions $\GatingFunc(\cdot)=\{\mode{\gatingFunc}(\cdot)\}_{\modeInd=1}^\ModeInd$ and normalising their output.
Each gating function $\mode{\gatingFunc}(\cdot)$ describes how its corresponding mode's mixing
probability varies over the input space.
\marginpar{softmax}
In the general case, the gating network is defined by the softmax function,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-softmax}
\Pr\left(\modeVar=\modeInd \mid \GatingFunc(\cdot)\right) = \text{softmax}_{\modeInd}\left(\GatingFunc(\cdot)\right) = \frac{\text{exp}\left(\hk(\cdot)\right)}{\sum_{\modeInd=1}^{\ModeInd} \hk(\cdot)}.
\end{align}
#+END_EXPORT
However, instantiating the model with two experts is a special case where only a single gating function is needed.
\marginpar{two experts}
The output of a function $\gatingFunc(\cdot)$ can be mapped through a sigmoid
function $\text{sig} : \R \rightarrow [0, 1]$ and interpreted as
a probability,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sigmoid}
\Pr(\alpha=1 \mid \gatingFunc(\cdot)) = \text{sig}(\gatingFunc(\cdot)).
\end{align}
#+END_EXPORT
If this sigmoid function satisfies the point symmetry condition then
we know that $\Pr(\alpha=2 \mid \gatingFunc(\cdot)) = 1 - \Pr(\alpha =1 \mid \gatingFunc(\cdot))$.
This is neat because it only requires a single gating function and no normalisation term needs to be calculated.

Modelling the gating network with input-dependent functions enables
informative prior knowledge to be encoded through the placement of GP priors.
\marginpar{GP priors}
For example, if modelling a dynamical system with oscillatory behaviour (e.g. an engine)
a periodic kernel could be adopted.
GPs also lend themselves to data-efficient learning whilst providing a principled
approach to modelling the epistemic uncertainty associated with the gating functions.
Learning better representations improves the models
ability to extrapolate as it will be more true to the underlying system.

We place independent GP priors on each gating function,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-prior}
\Hk \sim \mathcal{N}\left( \mu_{h,k}(\allInput), k_{h,k}(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
We place independent GP priors on each gating function, giving the distribution over all gating functions as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-prior}
p\left(\GatingFunc(\allInput)\right) &= \prod_{\modeInd=1}^{\ModeInd} \pHkGivenX = \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mu_{h,k}(\allInput), k_{h,k}(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-prior}
\pHGivenX &= \prod_{\modeInd=1}^{\ModeInd} \pHkGivenX = \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mu_{h,k}(\allInput), k_{h,k}(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
where $\mu_{h,k}(\cdot)$ and $k_{h,k}(\cdot,\cdot)$ are the mean and covariance functions
associated with the $k^\text{th}$ gating function.

Each mode's mixing probability $\Pr(\modeVarK \mid \singleInput)$ is then obtained by marginalising
*all* of the gating functions,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-indicator-mult}
\Pr(\modeVarK \mid \singleInput )
&= \E_{p(\mathbf{h}(\singleInput))} \left[ \Pr(\modeVarK \mid \mathbf{h}(\singleInput)) \right].
\end{align*}
#+END_EXPORT
In the general case where $\Pr(\modeVar=\modeInd \mid \mathbf{h}(\cdot))$ uses the softmax function
(Eq. ref:eq-softmax) this integral is intractable, so we approximate it using monte carlo
quadrature.
However, in the two expert case (Eq. ref:eq-sigmoid), selecting the sigmoid function to be the Gaussian cumulative distribution
function
$\Phi(\gatingFunc(\cdot)) = \int^{\gatingFunc(\cdot)}_{-\infty} \mathcal{N}(\tau | 0, 1) \text{d} \tau$
leads to analytic integration,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixing-probabilities-analytic}
\Pr(\modeVarn=1 \mid \singleInput) &=
 \int \Phi\left(\gatingFunc(\singleInput)\right) \mathcal{N}\left(\gatingFunc(\singleInput) \mid \mu_h, \sigma^2_h \right) \text{d} \gatingFunc(\singleInput) \\
&= \Phi \left(\frac{\mu_{h}}{\sqrt{1 + \sigma^2_{h} }}\right).
\end{align}
#+END_EXPORT
In the two expert case the marginal likelihood is given as an analytic mixture of two Gaussians.
\marginpar{marginal likelihood}

*** Generative Model
This model makes single-step probabilistic predictions,
where the predictive distribution over the state difference $\Delta \mathbf{x}_{t}$ is
given by a mixture of Gaussians.
This provides the model flexibility to model multimodal transition dynamics $f$ as mixtures of $K$ modes.
With this formulation, the marginal likelihood can be rewritten as,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-marginal-likelihood}
\pYGivenX &= \sum_{\modeInd=1}^\ModeInd \PrAGivenX \pYkGivenX \\
&= \sum_{\modeInd=1}^\ModeInd \PraGivenx \pykGivenx \\
\end{align*}
#+END_EXPORT
#+BEGIN_EXPORT latex
\small
\begin{align*} \label{eq-marginal-likelihood}
p(\Delta\mathbf{x}_{1:T} | \hat{\mathbf{x}}_{1:T}) =
&\prod_{t=1}^T  \sum_{k=1}^K \Bigg(
\underbrace{\left\langle \Pr\left(\alpha_t = k | \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}\right) \right\rangle_{p\left(\hat{\mathbf{h}} \mid \bm\xi_h\right)}}_{\text{Mixing Probability}}  \\
&\underbrace{\left\langle p\left(\Delta\mathbf{x}_t | \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{f}}^{(k)}\right) \right\rangle_{p\left(\hat{\mathbf{f}}^{(k)} \mid \bm\xi^{(k)}_f\right)}}_{\text{Dynamics mode } k} \Bigg), \numberthis
\end{align*}
\normalsize
#+END_EXPORT
where $\left\langle \cdot \right\rangle_{p(x)}$ denotes an expectation under $p(x)$.
Fig. [[ref:fig-graphical-model]]
shows the graphical model where the $K$ latent gating functions $h^{(k)}$
are evaluated and normalised to obtain the mixing probabilities
$\Pr(\alpha_t=k \mid \hat{\mathbf{x}}_{t-1})$.
The mode indicator variable $\alpha_t$ is then sampled from a Categorical distribution
governed by these probabilities.
The indicated mode's latent function $f^{(k)}$ and process noise $\sigma^{(k)}$ are
then evaluated to generate the state difference $\Delta\mathbf{x}_{t}$.

*** graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\Fk$};
     % \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\hk$};


      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[const, below=of f] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      \node[latent, right=of a, yshift=1.4cm, xshift=-0.4cm] (h) {$\Hk$};
      \node[const, right=of h, xshift=-0.4cm] (phik) {$\gatingParamsK$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    %}
  \caption{Graphical model of the transition dynamics where the state difference $\singleOutput$
is generated by pushing the state and control $\singleInput$ through the latent process.}
\label{fig-graphical-model}
\end{figure}
#+END_EXPORT


# #+BEGIN_EXPORT latex
# \begin{figure}[t]
#   \centering
#   \resizebox{0.8\columnwidth}{!}{
#     \begin{tikzpicture}[
#       pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
#       post/.style={->,shorten >=0.4pt,>=stealth',semithick}
#       ]
#       \node[const] (x) {$\hat{\mathbf{x}}_{t-1}$};
#       \node[latent, left=of x, yshift=-1.4cm] (f) {$\mathbf{f}^{(k)}_t$};
#       %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
#       \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {${h}^{(k)}_t$};

#       \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\hat{\mathbf{f}}^{(k)}$};
#       \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\hat{\mathbf{h}}^{(k)}$};
#       \node[const, left=of uk, xshift=0.4cm] (zk) {$\bm\xi^{(k)}_f$};
#       \node[const, right=of uh, xshift=-0.4cm] (zh) {$\bm\xi^{(k)}_h$};

#       \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\bm\theta^{(k)}$};
#       \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\bm\phi^{(k)}$};

#       \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\sigma^{(k)}$};

#       \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\Delta\mathbf{x}_t$};
#       %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
#       \node[latent, right=of y, xshift=-0.4cm] (a) {${\alpha}_t$};

#       %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

#       \factor[above=of a] {h-a} {left:Cat} {h} {a};

#       \draw[post] (a)--(y);
#       \draw[post] (x)-|(f);
#       %\draw[post] (f)--(yk);
#       \draw[post] (f)--(y);
#       %\draw[post] (yk)--(y);
#       %\draw[post] (h)--(a);
#       \draw[post] (x)-|(h);
#       \draw[post] (uk)--(f);
#       \draw[post] (uh)--(h);
#       \draw[post] (zk)--(uk);
#       \draw[post] (zh)--(uh);
#       \draw[post] (thetak)--(f);
#       \draw[post] (phik)--(h);
#       \draw[post] (sigmak)|-(y);

#       \plate {} {(x) (y) (a) (f) (h)} {$T$};
#       %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
#       \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$K$};
#       \plate {} {(zh) (uh) (h) (phik)} {$K$};
#     \end{tikzpicture}
#     }
#   \caption{Graphical model of the transition dynamics
# where the state difference $\Delta\mathbf{x}_{t}$
# is generated by pushing the state and control $\hat{\mathbf{x}}_{t-1}$
# through the latent process.}
# %\label{fig:graphical_model}
# \end{figure}
# #+END_EXPORT



*** Gating Network :noexport:
The gating network governs how the dynamics switch between modes.
This work is interested in spatially varying modes so
formulates an input dependent Categorical distribution over $\alpha_t$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-prob-mass}
P\left(\alpha_t \mid \mathbf{h}_t\right) = \prod_{k=1}^K \left(\Pr(\alpha_t=k \mid \mathbf{h}_t)\right)^{[\alpha_t = k]},
\end{align}
#+END_EXPORT
where $[\alpha_t=k]$ denotes the Iverson bracket.
It should be noted that other gating networks have been proposed that provide computational benefits
for inference. However, these gating networks often lack a handle for encoding domain knowledge
that can be used to restrict the set of possible

\todo[inline]{insert gating network references}
The probabilities of this Categorical distribution $\Pr(\alpha_t=k \mid \mathbf{h}_t)$
are obtained by evaluating $K$ latent
gating functions $\{h^{(k)}\}_{k=1}^K$ and normalising their output.
Each gating function evaluated at $\hat{\mathbf{x}}_{t-1}$ is denoted
as $h^{(k)}_t = h^{(k)}(\hat{\mathbf{x}}_{t-1})$
and at all observations ${h}^{(k)}_{1:T}$.
The set of all gating functions evaluated at $\hat{\mathbf{x}}_{t-1}$ is denoted
as $\mathbf{h}_t$ and at all observations as $\mathbf{h}_{1:T}$.
Each gating function $h^{(k)}$ describes how its corresponding mode's mixing
probability varies over the input space.


This work is interested in finding trajectories that can avoid areas of the transition dynamics model
that cannot be predicted confidently.
Placing independent sparse GP priors on each gating function
provides a principled approach to
modelling the epistemic uncertainty associated with each gating function.
The gating function's posterior covariance is a quantitative value that can be exploited by the
trajectory optimisation.
# GPs also provide data-efficient and interpretable learning through the selection of informative mean
# and covariance functions.
# For example, if the transition dynamics modes are subject to oscillatory mixing then a periodic
# covariance function could be selected.

Each gating function's inducing inputs are denoted
$\bm\xi_h^{(k)}$ and outputs as $\hat{\mathbf{h}}^{(k)}$.
For all gating functions, they are collected as $\bm\xi_h$ and $\hat{\mathbf{h}}$ respectively.
The probability that the $t^{\text{th}}$ observation is generated by mode $k$
given the inducing inputs is obtained by marginalising $\mathbf{h}_t$,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha=\modeInd \mid \mathbf{h}(\cdot) )
&= \E_{p(\mathbf{h}(\cdot))} \left[ \Pr(\alpha_t=k \mid \mathbf{h}(\cdot)) \right],
\end{align*}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha=\modeInd \mid \mathbf{h}(\cdot) )
&= \E_{p(\mathbf{h}(\cdot))} \left[ \text{softmax}_k(\mathbf{h}(\cdot)) \right],
\end{align*}
#+END_EXPORT
#+BEGIN_EXPORT latex
% \small
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha_t=k \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}} )
&= \int \text{softmax}_k(\mathbf{h}_t) p(\mathbf{h}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}) \text{d} \mathbf{h}_t,
\end{align*}
#+END_EXPORT
where $p(\mathbf{h}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}) = \prod_{k=1}^K p\left({h}^{(k)}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}^{(k)}\right)$
is the $K$ independent sparse GP priors on the gating functions.

**** Two Experts
Instantiating the model with two experts is a special case where only a single gating function is needed.
The output of a function can be mapped through a sigmoid
function $\text{sig} : \R \rightarrow [0, 1]$ and interpreted as
a probability $\Pr(\alpha=1 \mid \gatingFunc(\cdot))$.
If this sigmoid function satisfies the point symmetry condition then
we know that $\Pr(\alpha=2 \mid \gatingFunc(\cdot)) = 1 - \Pr(\alpha =1 \mid \gatingFunc(\cdot))$.

The distribution over $\gatingFunc(\cdot)$

We note that $p({h}_n^{(k)} \mid \mathbf{h}_{\neg n}, \mathbf{X})$
is a GP conditional and denote its mean $\mu_h$ and variance $\sigma^2_h$.
Choosing the sigmoid as the Gaussian cdf
$\Phi(h_n) = \int^{h_n}_{-\infty} \mathcal{N}(\tau | 0, 1) \text{d} \tau$
leads to,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixing-probabilities-analytic}
\Pr(\alpha_n=1 | \mathbf{X}) &=
 \int \Phi({h}_n) \mathcal{N}(h_n \mid \mu_h, \sigma^2_h) \text{d} {h}_n
= \Phi \left(\frac{\mu_{h}}{\sqrt{1 + \sigma^2_{h} }}\right).
\end{align}
#+END_EXPORT
Given this gating network our marginal likelihood is now an analytic mixture of two Gaussians.

can analytically marginalise.

** Bayesian Model Old :noexport:
The model is built upon sparse GP priors on each of the transition dynamics functions $f^{(k)}$
with independent GPs placed on each state dimension $d$,

#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-prior}
p(\Fk \mid \allInput, \uFk) = \prod_{\numData = 1}^{\dataInd} p(\Fkn \mid \singleInput, \uFk)
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-dynamics-prior}
p\left(\mathbf{f}^{(k)}_{1:T} \mid \hat{\mathbf{x}}_{1:T}, \hat{\mathbf{f}}^{(k)} \right) =
&\prod_{t=1}^T
p\left(\mathbf{f}^{(k)}_{t} \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{f}}^{(k)} \right) \numberthis \\
&\prod_{t=1}^T \prod_{d=1}^D
p\left(f^{(k)}_{t,d} \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{f}}^{(k)}_{d} \right)
\end{align*}
#+END_EXPORT
where $p\left(f^{(k)}_{t,d} \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{f}}^{(k)}_{d} \right)$
is a sparse GP conditional (Eq. ref:eq-sparse-gp-prediction).
For notational conciseness, the dependency on the inducing inputs $\bm\xi^{(k)}_f$ is dropped
throughout.
The $M$ inducing inputs and outputs associated with the $d^{\text{th}}$ dimension
of the $k^{\text{th}}$ mode's latent function $f^{(k)}$ are denoted as
$\bm\xi_{f,d}^{(k)}$ and
$\hat{\mathbf{f}}^{(k)}_d$ respectively.
They are collected as
$\bm\xi^{(k)}_f$ and $\hat{\mathbf{f}}^{(k)}$ for all output dimensions
and as $\bm\xi_f$ and $\hat{\mathbf{f}}$ for all modes.
The process noise in each mode is modelled as,
#+BEGIN_EXPORT latex
\small
\begin{align*} \label{eq-likelihood}
p\left(\Delta\mathbf{x}_{t} \mid \mathbf{f}^{(k)}_{t}\right)
= \mathcal{N}\left(\Delta\mathbf{x}_{t} \mid \mathbf{f}_t^{(k)}, \text{diag}\left(\left(\sigma^{(k)}_{1}\right)^2, \ldots, \left(\sigma^{(k)}_D\right)^2\right) \right),
\end{align*}
\normalsize
#+END_EXPORT
where $\left(\sigma^{(k)}_{d}\right)^2$ represents the noise variance associated
with the $d^{\text{th}}$ dimension of the $k^{\text{th}}$ mode.
# #+BEGIN_EXPORT latex
# \small
# \begin{gathered} \label{eq-likelihood}
# p\left(\Delta\mathbf{x}_{t} \mid \mathbf{f}^{(k)}_{t}\right)
# = \mathcal{N}\left(\Delta\mathbf{x}_{t} \mid \mathbf{f}_t^{(k)}, \text{diag}\left(\left(\sigma^{(k)}_{1}\right)^2, \ldots, \left(\sigma^{(k)}_D\right)^2\right) \right),
# \end{gathered}
# \normalsize
# #+END_EXPORT

# The dynamics modes given the inducing variables,
# #+BEGIN_EXPORT latex
# \small
# \begin{align*} \label{eq-dynamics-mode}
# p(\Delta\mathbf{x}_{1:T} \mid \hat{\mathbf{x}}_{1:T}, \hat{\mathbf{f}}^{(k)}) =
# \prod_{t=1}^T \int
# &p\left(\Delta\mathbf{x}_t \mid \mathbf{f}^{(k)}_t\right)
# p\left(\mathbf{f}^{(k)}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{f}}^{(k)} \right)
# \text{d} \mathbf{f}^{(k)}_t
# \end{align*}
# \normalsize
# #+END_EXPORT

# The dynamics modes are combined by the gating network to obtain,
# #+BEGIN_EXPORT latex
# \small
# \begin{align*} \label{eq-marginal-likelihood}
# p(\Delta\mathbf{x}_{1:T} \mid \hat{\mathbf{x}}_{1:T}, \hat{\mathbf{f}}) =
# \prod_{t=1}^T \sum_{k=1}^K
# &\Pr(\alpha_t = k \mid \hat{\mathbf{x}}_{t-1})
# p(\Delta\mathbf{x}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{f}}^{(k)})
# %&\int p\left(\Delta\mathbf{x}_t \mid \hat{\mathbf{f}}^{(k)}\right)
# %p\left(\hat{\mathbf{f}}^{(k)} \mid \hat{\mathbf{x}}_{t-1}\right) \text{d} \hat{\mathbf{f}}^{(k)}
# %&p(\Delta\mathbf{x}_t \mid \alpha_t=k, \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{f}}^{(k)}). \numberthis \\
# \end{align*}
# \normalsize
# #+END_EXPORT

** Inference [[label:sec-inference]]
#+begin_export latex
\epigraph{Nature laughs at the difficulties of integration.}{\textit{Pierre-Simon Laplace}}
#+end_export
#+BEGIN_EXPORT latex
\newcommand{\kernel}{\ensuremath{k}}
\newcommand{\expertKernel}{\ensuremath{\mode{\kernel}}}
\newcommand{\gatingKernel}{\ensuremath{\mode{\hat{\kernel}}}}

\newcommand{\numInducing}{\ensuremath{m}}
\newcommand{\NumInducing}{\ensuremath{\MakeUppercase{\numInducing}}}
%\newcommand{\inducingInput}{\ensuremath{\mathbf{Z}}}
\newcommand{\inducingInput}{\ensuremath{\bm{\zeta}}}
\newcommand{\inducingOutput}{\ensuremath{\mathbf{u}}}

%\newcommand{\expertInducingInput}{\ensuremath{\mode{\inducingInput}}}
%\newcommand{\expertsInducingInput}{\ensuremath{\inducingInput}}}
\newcommand{\expertInducingInput}{\ensuremath{\mode{\bm{\zeta}}}}
\newcommand{\expertsInducingInput}{\ensuremath{\bm{\zeta}}}
\newcommand{\expertInducingOutput}{\ensuremath{\mode{\inducingOutput}}}
\newcommand{\expertsInducingOutput}{\ensuremath{\MakeUppercase{\inducingOutput}}}

%\newcommand{\gatingInducingInput}{\ensuremath{\hat{\inducingInput}}}
\newcommand{\gatingInducingInput}{\ensuremath{\bm{\xi}}}
\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\hat{\inducingOutput}}}}
\newcommand{\gatingsInducingOutput}{\ensuremath{\hat{\MakeUppercase{\inducingOutput}}}}

%\newcommand{\expertInducingPrior}{\ensuremath{p(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput)}}
\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput)}}
%\newcommand{\expertInducingVariational}{\ensuremath{q(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\expertInducingVariational}{\ensuremath{q(\expertInducingOutput)}}
\newcommand{\expertsInducingVariational}{\ensuremath{q(\expertsInducingOutput)}}
\newcommand{\expertsVariational}{\ensuremath{q(\LatentFunc_\numData)}}
%\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \expertInducingOutput)}}
%\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \expertInducingOutput)}}


%\newcommand{\gatingInducingPrior}{\ensuremath{p(\GatingFunc(\gatingInducingInput))}}
\newcommand{\gatingInducingPrior}{\ensuremath{p(\gatingInducingOutput)}}
\newcommand{\gatingsInducingPrior}{\ensuremath{p(\gatingsInducingOutput)}}
%\newcommand{\gatingInducingVariational}{\ensuremath{q(\GatingFunc(\gatingInducingInput))}}
\newcommand{\gatingInducingVariational}{\ensuremath{q(\gatingInducingOutput)}}
\newcommand{\gatingsInducingVariational}{\ensuremath{q(\gatingsInducingOutput)}}
\newcommand{\gatingsVariational}{\ensuremath{q(\GatingFunc_\numData)}}
\newcommand{\singleGatingGivenInducing}{\ensuremath{p(\singleModeVarK \mid \gatingsInducingOutput)}}
\newcommand{\singleLatentGatingGivenInducing}{\ensuremath{p(\GatingFunc(\singleInput) \mid \gatingsInducingOutput)}}

\newcommand{\expertKL}{\ensuremath{\text{KL}\left( \expertInducingVariational \mid\mid \expertInducingPrior \right)}}
\newcommand{\expertsKL}{\ensuremath{\sum_{\modeInd=1}^\ModeInd\text{KL}\left( \expertInducingVariational \mid\mid \expertInducingPrior \right)}}
\newcommand{\gatingKL}{\ensuremath{\text{KL}\left( \gatingInducingVariational \mid\mid \gatingInducingPrior \right)}}
\newcommand{\gatingsKL}{\ensuremath{\sum_{\modeInd=1}^\ModeInd \text{KL}\left( \gatingInducingVariational \mid\mid \gatingInducingPrior \right)}}
#+END_EXPORT
*** intro :ignore:
The marginal likelihood in Eq. ref:eq-marginal-likelihood-factorised is extremely expensive
to evaluate $\mathcal{O}(\ModeInd^2 \NumData^{4})$
\todo{add marginal likelihood's correct complexity}
and is also intractable due to the marginalisation of $\mathbf{h}$ in the gating network
(except for the two expert case).
For these reasons, this work derives a variational approximation based on inducing variables that provides scalability
by utilising stochastic gradient-based optimisation.

Following the approach by cite:titsiasVariational2009, the probability space is augmented
with a set of $\NumInducing$ inducing variables for each GP.
However, instead of collapsing these inducing variables they are explicitly
represented as variational distributions and used to lower bound citep:hensmanGaussian2013
and then further bound citep:hensmanScalable2015 the marginal likelihood.

*Inducing Variables* As this approach essentially parameterises a nonparametric model
it is interesting to pause here and consider the implications of defining inducing inputs in different ways.
For example, what are the implications of having shared or separate inducing inputs
for the gating network GPs, for the expert GPs and for combinations of the experts and gating functions?

1. *Separate inducing inputs* for each *expert* GP, i.e. $\expertInducingOutput=\mode{\latentFunc}(\expertInducingInput)$,
   - Introducing separate inducing inputs for each expert's GP can be seen as loosely "partitioning"
     the observations between experts,
   - Less inducing inputs needed for each expert,
   - Achieves data partitioning behaviour like other MoGPE methods.
2. *Shared inducing inputs* for the *gating network* GPs, i.e. $\gatingInducingOutput=\mode{\gatingFunc}(\gatingInducingInput)$,
   - Partitioning of the data set is not desirable for the gating function GPs as each gating function
     should depend on all of training observations,
   - For this reason the inducing inputs should be shared between each gating function GP.
\newpage

*** Augmented probability space :ignore:
*Augmented Probability Space* Following the approach by cite:titsiasVariational2009 and cite:hensmanGaussian2013, which is
detailed in Section ref:sec-sparse-gps Eqs. ref:eq-titsias-bound - ref:eq-hensman-bound,
the probability space is first augmented with a set of $\NumInducing$ inducing variables from each GP prior,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-inducing-prior}
\expertsInducingPrior
%= \prod_{\modeInd=1}^\ModeInd \expertInducingPrior
&= \prod_{\modeInd=1}^\ModeInd \expertInducingPrior
= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \expertInducingOutput) \mid
\expertMeanFunc(\expertInducingInput),
\expertCovFunc(\expertInducingInput, \expertInducingInput) \right) \\
\gatingsInducingPrior
&= \prod_{\modeInd=1}^\ModeInd \gatingInducingPrior
= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \gatingInducingOutput) \mid
\gatingMeanFunc(\gatingInducingInput),
\gatingCovFunc(\gatingInducingInput, \gatingInducingInput) \right).
\end{align}
#+END_EXPORT
where for notational conciseness the dependence on the inducing inputs
$(\gatingInducingInput, \expertInducingInput, \expertsInducingInput)$
has been dropped. The augmented marginal likelihood is then given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-augmented-marginal-likelihood}
\evidence &= \E_{\gatingsInducingPrior \expertsInducingPrior} \left[
\prod_{\numData=1}^\NumData \sum_{\modeInd=1}^{\ModeInd} \singleGatingGivenInducing \singleExpertGivenInducing \right],
\end{align}
#+END_EXPORT
where the joint distribution over the data is captured by the inducing variables
$(\gatingsInducingOutput, \expertsInducingOutput)$.
Figure ref:fig-graphical-model-sparse shows the graphical model of the augmented joint probability model.
The conditional distributions $\singleGatingGivenInducing$ and
$\singleExpertGivenInducing$ follow from standard sparse GP methodologies,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\singleExpertGivenInducing =& \E_{\singleLatentExpertGivenInducing} \left[ \singleExpertLikelihood \right] \\
\singleGatingGivenInducing =& \E_{\singleLatentGatingGivenInducing} \left[ \singleGatingLikelihood \right] \\
\singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
\expertKernel(\singleInput, \expertInducingInput) \expertKernel^{-1}(\expertInducingInput, \expertInducingInput) \expertInducingOutput,
\tilde{\mode{\K}}
\right), \\
\singleLatentGatingGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
\gatingKernel(\singleInput, \gatingInducingInput) \gatingKernel^{-1}(\gatingInducingInput, \gatingInducingInput) \gatingInducingOutput,
\tilde{\mode{\K}}
\right),
\end{align}
#+END_EXPORT
where $\K_mm =$ ...

sufficient statistic assumption

In the limit where $M=N$ and  $\expertInducing=\allInput$ this conditional

This minimises the KL divergence and ensures that $\expertsInducingInputs$ are distributed amongst the training
inputs $\allInput$ such that ....

*** sparse graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\singleInput)$};
      %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
      \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\mode{\gatingFunc}(\singleInput)$};

      \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\expertInducingOutput$};
      \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\gatingInducingOutput$};
      \node[const, left=of uk, xshift=0.4cm] (zk) {$\expertInducingInput$};
      \node[const, right=of uh, xshift=-0.4cm] (zh) {$\gatingInducingInput$};

      \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\expertParamsK$};
      \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\gatingParamsK$};

      \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (uk)--(f);
      \draw[post] (uh)--(h);
      \draw[post] (zk)--(uk);
      \draw[post] (zh)--(uh);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a) (f) (h)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(uh) (h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    }
  \caption{Graphical model of the transition dynamics where the state difference $\singleOutput$
is generated by pushing the state and control $\singleInput$ through the latent process.}
\label{fig-graphical-model-sparse}
\end{figure}
#+END_EXPORT



*** Lower bound 1 :ignore:
\newline

*Lower Bound 1* Instead of collapsing the inducing variables here citep:titsiasVariational2009
they can be explicitly represented as variational distributions citep:hensmanGaussian2013,
and used to obtain a variational lower bound on Eq. ref:eq-marginal-likelihood-factorised,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-lower-bound-tight}
\text{log} \evidence &\geq  \sum_{\numData=1}^\NumData \E_{\gatingsInducingVariational \expertsInducingVariational}
\left[ \text{log} \left( \sum_{\modeInd=1}^\ModeInd
\singleGatingGivenInducing \singleExpertGivenInducing \right) \right] \\
&- \gatingsKL \\
&- \expertsKL := \mathcal{L}_1
\end{align}
#+END_EXPORT
From Eq. ref:eq-lower-bound-tight it is clear that the optimal distribution for each of the variational distributions
is Guassian, so we parameterise them as such,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-variational-inducing-dist}
\expertsInducingVariational &= \prod_{\modeInd=1}^{\ModeInd} \expertInducingVariational
= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left(\expertInducingOutput \mid \mode{\mathbf{m}}, \mode{\mathbf{S}} \right) \\
\gatingsInducingVariational &= \prod_{\modeInd=1}^{\ModeInd} \gatingInducingVariational
= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left(\gatingInducingOutput \mid \mode{\hat{\mathbf{m}}}, \mode{\hat{\mathbf{S}}} \right).
\end{align}
#+END_EXPORT
This bound meets the necessary conditions to perform stochastic gradient methods on
$\expertsInducingVariational$ and $\gatingsInducingVariational$ as the variational expectation (first term)
is written as a sum over input-output pairs.
However, this expectation cannot be calculated in closed form and
must be approximated.
The joint distribution over the inducing variables for each GP
$\qFku$ is an $\NumInducing$ dimensional multivariate normal distribution so the
integral being approximated is $\NumInducing$ dimensional.
\todo{add more on why we don't want M dimensional integral}

*** Lower bound 2 :ignore:
\newline

*Lower Bound 2* Following cite:hensmanScalable2015 this bound $\mathcal{L}_1$
(Eq. ref:eq-lower-bound-1) can be further bounded to remove the $\NumInducing$ dimensional integrals.
Applying Jensen's inequality to the conditional probability $\singleGatingGivenInducing \singleExpertGivenInducing$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-further-bound}
\singleGatingGivenInducing \singleExpertGivenInducing \geq
\E_{\singleLatentExpertGivenInducing \singleLatentGatingGivenInducing}
\left[ \text{log} \left( \singleGatingLikelihood \singleExpertLikelihood \right ) \right]
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-lower-bound-further}
\mathcal{L}_{1}
\geq \sum_{\numData=1}^\NumData &\E_{\expertsInducingVariational \gatingsInducingVariational}
\left[ \E_{\singleLatentGatingGivenInducing \singleLatentExpertGivenInducing} \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \right] \nonumber \\
&- \gatingsKL - \expertsKL \\
= \sum_{\numData=1}^\NumData &\E_{\gatingsVariational \expertsVariational}
\left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \nonumber \\
&- \gatingsKL - \expertsKL := \mathcal{L}_{2},
\end{align}
#+END_EXPORT
where $\gatingsVariational$ and $\expertsVariational$ represents the variational posteriors given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-variational-posteriors}
\gatingsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{\gatingInducingVariational} \left[ \singleLatentGatingGivenInducing \right] \\
\expertsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{ \expertInducingVariational} \left[ \singleLatentExpertGivenInducing \right].
\end{align}
#+END_EXPORT
As each GP's inducing variables are normally distributed the functional form of the
variational posteriors are given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq--variational-posteriors-functional}
\gatingsVariational &= \prod_{\modeInd=1}^\ModeInd \mathcal{N} \left( \mode{\gatingFunc}(\singleInput) \mid
\mode{\hat{\mathbf{A}}} \mode{\hat{\mathbf{m}}},
\gatingKernel(\singleInput, \singleInput)
+ (\mode{\hat{\mathbf{S}}} - \gatingKernel(\gatingInducingInput,\gatingInducingInput))
\mode{\hat{\mathbf{A}}}^T
\right) \\
\expertsVariational &= \prod_{\modeInd=1}^\ModeInd \mathcal{N} \left( \mode{\latentFunc}(\singleInput) \mid
\mode{\mathbf{A}} \mode{\mathbf{m}},
\expertKernel(\singleInput, \singleInput)
+ (\mode{\mathbf{S}} - \expertKernel(\expertInducingInput,\expertInducingInput))
\mode{\mathbf{A}}^T
\right),
\end{align}
#+END_EXPORT
where
$\mode{\mathbf{A}} = \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}$ and
$\mode{\hat{\mathbf{A}}} = \gatingKernel(\singleInput, \gatingInducingInput)\left(\expertKernel(\gatingInducingInput, \gatingInducingInput)\right)^{-1}$.
Importantly, the variational posteriors $\gatingsVariational$ and $\expertsVariational$
analytically marginalise $\gatingsInducingVariational$ and $\expertsInducingVariational$
(with Gaussian convolutions) removing the undesirable approximate $M$ dimensional integrals from
Eq. ref:eq-lower-bound-tight. The variational expectation in Eq. ref:eq-lower-bound-further requires approximation
but now only requires one dimensional integrals of the log-likelihood to be approximated.
We approximate the integrals with Gibbs sampling and
in practice only use single samples because the added stochasticity helps the optimisation.

The bound in Eq. ref:eq-experts-bound-4-fact meets the necessary conditions to perform stochastic
gradient methods on $\qFu$ as the sum of $\NumData$ terms corresponds to input-output pairs.
The inducing inputs $\{\Z\}_{\modeInd=1}^{\ModeInd}$, kernel hyperparameters and noise variances are treated as
variational hyperparameters and are optimised using stochastic gradient descent alongside the variational
parameters.
Note that our augmented model captures the dependencies in the joint distribution of the data through the
inducing variables, but as $M \ll \NumData$ these have a much lower computational burden.
Given a batch of $\NumData_b$ observations this bound has complexity
$\mathcal{O}\left( \NumData_{b} \ModeInd M^{3} \right)$ to evaluate.
It is worth noting that in contrast to other MoGPE models, our model does not partition the data set,
i.e. each of our expert depends on **all** of the training inputs.
However, after augmenting each expert with inducing points,
the augmented model has the flexibility to /partition/ our inducing points.
As such, when selecting the number of inducing points $\NumInducing$ for each GP, one could partition $\NumData$
and select the number of inducing points based off of the number of data points believed to be
associated with each expert.
\todo{citation for selecting number of inducing points}

*** sparse graphical model old :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\fkn$};
      %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
      \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\hkn$};

      \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\uFk$};
      \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\Hku$};
      \node[const, left=of uk, xshift=0.4cm] (zk) {$\zFk$};
      \node[const, right=of uh, xshift=-0.4cm] (zh) {$\zHk$};

      \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\expertParamsK$};
      \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\gatingParamsK$};

      \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (uk)--(f);
      \draw[post] (uh)--(h);
      \draw[post] (zk)--(uk);
      \draw[post] (zh)--(uh);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a) (f) (h)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(zh) (uh) (h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    }
  \caption{Graphical model of the transition dynamics where the state difference $\singleOutput$
is generated by pushing the state and control $\singleInput$ through the latent process.}
\label{fig-graphical-model-sparse}
\end{figure}
#+END_EXPORT



#+BEGIN_EXPORT latex
\todo[inline]{
\begin{itemize}
\item We want $\pFGivenFu$ outside the log (and not just $\qFu$) so we can analytically integrate $\qFu$ (like in Eq. \ref{eq-experts-var-dist})??
\item In the ICRA paper I had only $\qFu$ outside the log so we had to approximate the M-dimensional integral over $\qFu$ with samples.
\end{itemize}
}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo[inline]{
This is the bound I had coded up previously,
\begin{align} \label{eq-expert-bound-old}
\mathcal{L}_{\text{old}} =
&\sum_{\numData}^{\NumData} E_{\prod_{\modeInd=1}^{\ModeInd} \qFku} \text{log} \sum_{k=1}^{K} \PrA \left[ \pykGivenFku \right] \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right]
\end{align}
This bound is valid but it is nicer to move the expectations over $\pFk$ outside the log, like in Eq. \ref{eq-experts-bound-4-fact}.
}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo[inline]{
This is another bound I implemented,
\begin{align} \label{eq-expert-bound-old}
\mathcal{L}_{\text{old}} =
&\sum_{\numData}^{\NumData} \text{log} \sum_{k=1}^{K} \PrA \E_{\prod_{\modeInd=1}^{\ModeInd} \qFku} \left[ \pykGivenFku \right] \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right]
\end{align}
\begin{itemize}
\item THIS IS WRONG as the expectation over $\qFku$ should be outside the log.
\item which is why I used to approximate it with single samples....
\end{itemize}
}
#+END_EXPORT

\todo[inline]{Need to extend bound for the gating network}

** Inference_old [[label:sec-inference]]
#+begin_export latex
\epigraph{Nature laughs at the difficulties of integration.}{\textit{Pierre-Simon Laplace}}
#+end_export
#+BEGIN_EXPORT latex
\renewcommand{\modei}[2]{\ensuremath{#1_{#2}}}
\renewcommand{\singleOutput}{\ensuremath{\Delta x_{\numData}}}
\renewcommand{\allOutput}{\ensuremath{\Delta\mathbf{\state}}}
\newcommand{\expertInducingInput}{\ensuremath{\singleData{\modeVar}}}
\newcommand{\expertInducingOutput}{\ensuremath{\singleData{\modeVar}}}
#+END_EXPORT
*** intro :ignore:
The marginal likelihood in Eq. ref:eq-marginal-likelihood-factorised is extremely expensive
to evaluate $\mathcal{O}(\ModeInd^2 \NumData^{4})$
\todo{add marginal likelihood's correct complexity}
and is also intractable due to the marginalisation of $\mathbf{h}$ in the gating network
(except for the two expert case).
For these reasons, this work derives a variational approximation based on inducing variables that provides scalability
by utilising stochastic gradient-based optimisation.

Following the approach by cite:titsiasVariational2009, the probability space is augmented
with a set of $M$ inducing variables for each GP.
However, instead of collapsing these inducing variables they are explicitly
represented as variational distributions and used to lower bound citep:hensmanGaussian2013
and then further bound citep:hensmanScalable2015 the marginal likelihood.



Let us first consider lower bounding the MoGPE model for the general gating network case
i.e. without specifying a particular gating network. The marginal likelihood of this model is given by,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-general-moe-marginal-likelihood}
\text{log} \pYGivenX &= \text{log} \sum_{k=1}^{K} \PrA \E_{\pFk} \left[ \pYkGivenFk \right],
\end{align*}
#+END_EXPORT
Following the approach by cite:titsiasVariational2009 and cite:hensmanGaussian2013, which is
detailed in Section ref:sec-sparse-gps Eqs. ref:eq-titsias-bound - ref:eq-hensman-bound,
the probability space is first augmented with a set of $M$ inducing variables from each GP prior,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-experts-inducing-dist}
\pFuGivenX = \prod_{\modeInd=1}^{\ModeInd} \pFkuGivenX = \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \uFk \mid \mode{\mu}(\zFk), \mode{k}(\zFk, \zFk) \right).
\end{align*}
#+END_EXPORT
The augmented log marginal likelihood is then given by,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-general-moe-marginal-likelihood}
\text{log} \pYGivenX &= \text{log} \sum_{k=1}^{K} \PrA \E_{\prod_{\modeInd=1}^{\ModeInd} \pFkuGivenX} \left[ \E_{\pFkGivenFku} \left[ \pYkGivenFk \right] \right] .
%\text{log} \pYGivenX &= \text{log} \sum_{k=1}^{K} \PrA \prod_{\modeInd=1}^{\ModeInd} \pYkGivenFk \pFkGivenFku \pFkuGivenX.
\end{align*}
#+END_EXPORT
For notational conciseness the dependency on the inputs has been dropped.
Note that we have assumed each expert's inducing variables are independent and augmented each expert with
the product over all experts inducing variables.
This will enable us to move the expectation over the inducing variables outside the sum over the indicator variable.
\todo[inline]{no need to say this...}

We start by considering the augmented log marginal likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-marginal-likelihood}
\text{log} \pYGivenX &= \text{log} \sum_{k=1}^{K} \PrA \E_{\prod_{\modeInd=1}^{\ModeInd} \pFku} \left[ \pYkGivenFku \right].
\end{align}
#+END_EXPORT
and moving the integrals over the inducing variables outside of the sum over the indicator variable,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-marginal-likelihood-fact}
\text{log} \pYGivenX = \text{log} \E_{\prod_{\modeInd=1}^{\ModeInd} \pFku} \left[ \sum_{k=1}^{K} \PrA  \pYkGivenFku \right],
\end{align}
\todo{Don't think I need to mention Fubini-Tonelli theorem here}
#+END_EXPORT
Instead of collapsing the inducing variables here citep:titsiasVariational2009
they can be explicitly represented as variational distributions citep:hensmanGaussian2013,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-var-experts-inducing-dist}
\qFu = \prod_{k=1}^{K} \qFku = \prod_{k=1}^{K} \mathcal{N}\left(\uFk \mid \mode{\mathbf{m}}, \mode{\mathbf{S}} \right)
\end{align}
#+END_EXPORT
These variational distributions can then be used to obtain a lower
bound on Eq. ref:eq-expert-marginal-likelihood-fact,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-bound-2}
\text{log} \pYGivenX
\geq &\E_{\prod_{\modeInd=1}^{\ModeInd}\qFku} \left[ \text{log} \sum_{k=1}^{K} \PrA  \pYkGivenFku \right] \nonumber \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right] := \mathcal{L}_{1}.
\end{align}
#+END_EXPORT
This bound meets the necessary conditions to perform stochastic gradient methods on $\qFu$ as the first term
can be written as a sum over input-output pairs,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-bound-2-fact}
\mathcal{L}_{1} =
&\sum_{\numData=1}^{\NumData} \E_{\prod_{\modeInd=1}^{\ModeInd}\qFku} \left[ \text{log} \sum_{k=1}^{K} \Pra  \pykGivenFku \right] \nonumber \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right].
\end{align}
#+END_EXPORT
However, the expectation over the inducing variables cannot be calculated in closed form and
must be approximated. The joint distribution over the inducing variables for each GP $\qFku$ is $M$ dimensional and as
a result the integral is $M$ dimensional.
Following cite:hensmanScalable2015 this bound (Eq. ref:eq-expert-bound-2-fact) can be further bounded by
moving the expectations over each expert's conditional of its latent function values given its inducing variables
$\pFkGivenFku$
outside the log, i.e. applying Jensen's inequality to the conditional probability $\pYkGivenFku$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-bound-3}
\mathcal{L}_{1}
\geq &\E_{\prod_{k=1}^{K}\qFku} \left[ \E_{\pFkGivenFku} \left[ \text{log} \sum_{k=1}^{K} \PrA \pYkGivenFk \right] \right] \nonumber \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right] := \mathcal{L}_{\text{experts}},
\end{align}
#+END_EXPORT
Denoting the variational posterior
$\qF := \prod_{k=1}^{K} \qFk = \prod_{k=1}^{K} \int \pFkGivenFku \qFku \text{d} \uFk$ enables us
to rewrite Eq. ref:eq-expert-bound-3 as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-bound-4}
\mathcal{L}_{\text{experts}}
&= \E_{\qF} \left[ \text{log} \sum_{k=1}^{K} \PrA \pYkGivenFk \right] - \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right]
\end{align}
#+END_EXPORT
As each experts inducing variables distribution $\qFku$ is Gaussian the functional form of the variational posterior is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-var-dist}
\qF = \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \Fk \mid \mode{\mathbf{A}} \mode{\mathbf{m}}, \mode{\K}_{\numData \numData } + (\mode{\mathbf{S}} - \mode{\K}_{mm}) \left(\mode{\mathbf{A}}\right)^{T} \right),
\end{align}
#+END_EXPORT
where $\mode{\mathbf{A}} = \mode{\K}_{\numData m}\left(\mode{\K}_{mm}\right)^{-1}$.
The model's likelihood is a mixture of Gaussians and factorizes
across data as $\prod_{\numData=1}^{\NumData} \Pra \pykGivenfk$. $\mathcal{L}_{\text{experts}}$ can thus be
written as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-bound-4-fact}
\mathcal{L}_{\text{experts}} =
&\sum_{\numData=1}^{\NumData}\E_{\qfn} \left[ \text{log} \sum_{k=1}^{K} \Pra \pykGivenfk \right] \nonumber \\
&- \sum_{\modeInd=1}^{\ModeInd} \text{KL}\left[\qFku \mid\mid \pFku\right].
\end{align}
#+END_EXPORT
Importantly, the variational posterior $\qfn$ analytically marginalises each $\qFku$
(with Gaussian convolutions) removing the undesirable approximate $M$ dimensional integrals from
Eq. ref:eq-expert-bound-2-fact. The expectation in Eq. ref:eq-experts-bound-4-fact still requires approximation
but now only requires one dimensional integrals of the log-likelihood to be approximated.
We approximate the integrals with Gibbs sampling and
in practice only use single samples because the added stochasticity helps the optimisation.

The bound in Eq. ref:eq-experts-bound-4-fact meets the necessary conditions to perform stochastic
gradient methods on $\qFu$ as the sum of $\NumData$ terms corresponds to input-output pairs.
The inducing inputs $\{\Z\}_{\modeInd=1}^{\ModeInd}$, kernel hyperparameters and noise variances are treated as
variational hyperparameters and are optimised using stochastic gradient descent alongside the variational
parameters.
Note that our augmented model captures the dependencies in the joint distribution of the data through the
inducing variables, but as $M \ll \NumData$ these have a much lower computational burden.
Given a batch of $\NumData_b$ observations this bound has complexity
$\mathcal{O}\left( \NumData_{b} \ModeInd M^{3} \right)$ to evaluate.
It is worth noting that in contrast to other MoGPE models, our model does not partition the data set,
i.e. each of our expert depends on **all** of the training inputs.
However, after augmenting each expert with inducing points,
the augmented model has the flexibility to /partition/ our inducing points.
As such, when selecting the number of inducing points $\NumInducing$ for each GP, one could partition $\NumData$
and select the number of inducing points based off of the number of data points believed to be
associated with each expert.
\todo{citation for selecting number of inducing points}

*** sparse graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\fkn$};
      %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
      \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\hkn$};

      \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\uFk$};
      \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\Hku$};
      \node[const, left=of uk, xshift=0.4cm] (zk) {$\zFk$};
      \node[const, right=of uh, xshift=-0.4cm] (zh) {$\zHk$};

      \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\expertParamsK$};
      \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\gatingParamsK$};

      \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (uk)--(f);
      \draw[post] (uh)--(h);
      \draw[post] (zk)--(uk);
      \draw[post] (zh)--(uh);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a) (f) (h)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(zh) (uh) (h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    }
  \caption{Graphical model of the transition dynamics where the state difference $\singleOutput$
is generated by pushing the state and control $\singleInput$ through the latent process.}
\label{fig-graphical-model-sparse}
\end{figure}
#+END_EXPORT



#+BEGIN_EXPORT latex
\todo[inline]{
\begin{itemize}
\item We want $\pFGivenFu$ outside the log (and not just $\qFu$) so we can analytically integrate $\qFu$ (like in Eq. \ref{eq-experts-var-dist})??
\item In the ICRA paper I had only $\qFu$ outside the log so we had to approximate the M-dimensional integral over $\qFu$ with samples.
\end{itemize}
}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo[inline]{
This is the bound I had coded up previously,
\begin{align} \label{eq-expert-bound-old}
\mathcal{L}_{\text{old}} =
&\sum_{\numData}^{\NumData} E_{\prod_{\modeInd=1}^{\ModeInd} \qFku} \text{log} \sum_{k=1}^{K} \PrA \left[ \pykGivenFku \right] \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right]
\end{align}
This bound is valid but it is nicer to move the expectations over $\pFk$ outside the log, like in Eq. \ref{eq-experts-bound-4-fact}.
}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo[inline]{
This is another bound I implemented,
\begin{align} \label{eq-expert-bound-old}
\mathcal{L}_{\text{old}} =
&\sum_{\numData}^{\NumData} \text{log} \sum_{k=1}^{K} \PrA \E_{\prod_{\modeInd=1}^{\ModeInd} \qFku} \left[ \pykGivenFku \right] \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right]
\end{align}
\begin{itemize}
\item THIS IS WRONG as the expectation over $\qFku$ should be outside the log.
\item which is why I used to approximate it with single samples....
\end{itemize}
}
#+END_EXPORT

\todo[inline]{Need to extend bound for the gating network}

** Results
\todo{Add results for mcycle, quadcopter-sim and quadcopter and more??}
The model was trained on data collected from the velocity-controlled quadcopter experiment.
The controls were kept constant during data collection to reduce the dynamics model to
$\Delta\mathbf{x}_t = f(\mathbf{x}_{t-1})$.
The trajectory optimisation then exploits differential flatness cite:rossPseudospectral2004 to
recover the velocity controls.
The model was trained with $K=2$ dynamics modes,
and a subset of the observations were withheld during training to test the model's ability to model
epistemic uncertainty.
Fig. ref:fig-problem-statement shows mode 1's mixing probability over the domain
which has clearly learned two dynamics modes characterised by process noise.
Fig. ref:fig-svgp_2d_traj shows the predictive mean (left) and variance (right) of the gating
function associated with dynamics mode 1 ($h^{(1)}$).
The mean is high where the model believes mode 1 is responsible for predicting, low where
it believes another mode is responsible, and zero where it is uncertain.
The variance (right) has also clearly captured information regrading the
epistemic uncertainty, i.e. where there are no observations.

** Conclusion

using a novel variational lower bound that
principally handles uncertainty and provides scalability
via stochastic gradient-based optimisation.

The method is evaluated on a real-world quadcopter example
that shows the transition dynamics model can successfully learn a factorised representation
of the underlying dynamics modes.
* Trajectory Optimisation in Desired Modes via Latent Geometry label:chap-traj-opt
** Intro :ignore:
This chapter presents the second stage of a two-stage method to
perform trajectory optimisation in multimodal dynamical systems with unknown nonlinear stochastic
transition dynamics.
The method aims to find trajectories that remain in a preferred dynamics mode
where possible and in regions of the transition dynamics that
have been observed and can be predicted confidently.
The first stage leverages the Mixture of Gaussian Process Experts (MoGPE) method from Chapter ref:chap-dynamics to learn
a predictive dynamics model from historical data.
Importantly, this model learns gating functions that indicate the probability of being in a particular
dynamics mode at a given state location.
In the second stage, presented in this chapter,
the geometry and GP posterior covariance associated with these latent gating functions
are used to formulate an objective function aimed at finding trajectories that,

- Goal 1 :: remain in a preferred dynamics mode $k^*$ where possible, label:to-goal-mode
- Goal 2 :: avoid regions of the learned dynamics with high epistemic uncertainty, i.e. that cannot be predicted confidently (due to limited training observations). label:to-goal-unc
This chapter formulates such an objective function and then presents a method that finds trajectories that
implicitly minimise it by exploiting concepts of Riemannian geometry.
More specifically, the trajectory optimisation is projected onto a continuous-time
ODE whose solutions implicitly minimise the objective function.

Section ref:sec-problem-statement formally states our problem and
Section ref:sec-traj-opt recaps concepts from Riemannian
geometry before detailing how they are extended to probabilistic geometries
and
used to project the trajectory optimisation onto the latent geodesic ODE.
It then details how this latent geodesic ODE is solved using direct collocation.
Section ref:sec-results gives results of the method tested on a real-world
velocity-controlled quadcopter example.

** Problem Statement label:sec-problem-statement

If one believes that the modes only vary across a subset of the inputs,
for example, if the modes vary spatially and do not depend on the controls,
then it can easily be modified to model this.

The trajectory optimisation in this chapter aims to find trajectories that satisfy
two goals,
- Goal 1 :: remain in a preferred dynamics mode $k^*$ where possible, label:to-goal-mode
- Goal 2 :: avoid regions of the learned dynamics with high epistemic uncertainty, i.e. that cannot be predicted confidently (due to limited training observations). label:to-goal-unc

Formally, trajectory optimisation seeks to find the state and control trajectories
for times $t \in [t_0, t_f]$ that minimise some cost function $g$ whilst satisfying constraints $c$
and boundary conditions.
The trajectory optimisation problem is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-traj-opt}
\min_{\mathbf{u}(t)} &\int_{t_0}^{t_f} g(\mathbf{x}(t), \mathbf{u}(t)) \text{d}t \quad \forall t \nonumber \\
\text{s.t. }&\text{Eq. \ref{eq-unimodal-dynamics-cont}} \nonumber \\
&c(\mathbf{x}(t)) \leq 0 \quad \forall t \nonumber \\
&\mathbf{x}(t_0) = \mathbf{x}_0  \quad \mathbf{x}(t_f) = \mathbf{x}_f
\end{align}
#+END_EXPORT
The standard approach to solving such a problem is to construct an objective function that encodes
our two goals.
We follow this approach and construct an objective function based on probabilistic Riemannian geometry.
However, instead of minimising this objective function subject to our systems transition dynamics,
we exploit the observation detailed in Section ref:sec-riemannian-geometry Eq. ref:eq-length-objective.
That is, trajectories that minimise our objective function are actually solutions
to the $2^{\text{nd}}$ order ODE in Eq. ref:eq-2ode;
an ODE whose solutions are geodesics (i.e. shortest paths) on the Riemannian manifold endowed with the metric
$\mathbf{G}$.
With this observation we project the trajectory optimisation onto this ODE
and deploy a simple objective function.
Our goals are thus encoded into the Riemannian metric tensor $\mathbf{G}$ that
appears in Eq ref:eq-2ode.
Let us now provide an intuitive and thorough walk-through of the approach detailed above.

** Geometric Objective Function
To help provide intuition let us first formulate an objective function with two terms,
one to address each of our goals,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-cost-trajectory}
J&= \int_{t_0}^{t_f}  g_{\text{mode}}(\mathbf{x}(t), \mathbf{u}(t)) + g_{\text{epistemic}}(\mathbf{x}(t), \mathbf{u}(t)) \text{d}t,
\end{align}
#+END_EXPORT
where the $g_{\text{mode}}$ term favours remaining in dynamics mode $k^*$
and the $g_{\text{epistemic}}$ term favours trajectories that avoid regions of the
dynamics with high epistemic uncertainty.
We will now instantiate Eq. ref:eq-cost-trajectory following two different approaches that use
information learned by our transition dynamics model.
The first objective is simple to construct and is intuitively the "go to" objective function for our problem.
The second objective is more involved and requires the concepts of Riemannian geometry
from Section ref:sec-riemannian-geometry to be extended to probabilistic geometries.

*** A Simple Cost Function
Remember that the gating network indicates the probability of being in a particular dynamics mode at a given
time step and that the Bayesian formulation of the dynamics model lends itself to well-calibrated uncertainty estimates.
The objective function can therefore be instantiated as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-cost-trajectory-learned}
J &= \int_{t_0}^{t_f} \underbrace{- \Pr(\alpha(t)=k^*)}_{g_{\text{mode}}}
+ \lambda \underbrace{\V_{\text{epistemic}}\left[\latentFunc(\mathbf{x}(t), \mathbf{u}(t))\right]}_{g_{\text{epistemic}}}  \text{d}t,
\end{align}
#+END_EXPORT
where the variance term $\V_{\text{epistemic}}$ is due to the epistemic uncertainty arising from
learning $f$ from observations and $\lambda$ is a user-tuneable parameter.
This objective function is fairly intuitive,
we want to maximise the probability of being in our desired mode and we want to minimise
the amount of variance (due to epistemic uncertainty) introduced over the trajectory.

\todo{should probably test this and comment on why we don't use it...}

*** A Geometric Cost Function
The geometry of the latent gating functions can also be used to formulate a cost term
that favours trajectories remaining in a preferred dynamics mode.
The $g_{\text{mode}}$ term in Eq. ref:eq-cost-trajectory can be expressed as finding shortest
paths on the desired mode's gating function.
Intuitively, the length of a trajectory from $\mathbf{x}_0$ to $\mathbf{x}_f$ on the
manifold given by the desired mode's gating function
increases when it passes over the contours; analogous to climbing a hill.
Given appropriate scaling of the gating function, shortest trajectories between two locations are
those that attempt to follow the contours, i.e. remain in a single mode by not climbing up or down any hills.
This formulation is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-geodesic-objective}
J = \int_{t_0}^{t_f}\left\|\dot{\mathbf{x}}(t)\right\|_{\mathbf{G}(\mathbf{x}(t))} \mathrm{d}t,
\end{align}
#+END_EXPORT
where $\mathbf{G}(\mathbf{x}(t))$ represents a Riemannian metric tensor evaluated at $\mathbf{x}(t)$,
encoding the desired cost, e.g. a Riemannian metric tensor associated with the desired
mode's gating function.
Given that $\mathbf{G}(\mathbf{x}(t))$ represents a Riemannian metric tensor then
Eq. ref:eq-geodesic-objective is a method for calculating the length of a trajectory on the Riemannian
manifold endowed with the metric $\mathbf{G}$.
Formulating $\mathbf{G}$ as the Riemannian metric tensor for the manifold parameterised by the
desired mode's gating function $\gatingFuncKs$ encodes the $g_{\text{mode}}$ term.
However, the objective in Eq ref:eq-geodesic-objective has not addressed the
$g_{\text{epistemic}}$ term in Eq. ref:eq-cost-trajectory.

We can address this by first observing that our metric tensor is parameterised
by the desired mode's gating function. This will induce
a probability distribution over the metric tensor $\mathbf{G}$.
This will become apparent in Section ref:sec-prob-geo when we detail how the metric tensor
is calculated but let us assume it is the case for now.
With this observation, the $g_{\text{epistemic}}$ term can be encoded into the cost in Eq. ref:eq-geodesic-objective,
by extending it to probabilistic geometries, which we detail in the next section.

**** Extension to Probabilistic Geometries label:sec-prob-geo
With the observation that our metric tensor is probabilistic,
the objective in Eq. ref:eq-geodesic-objective should be rewritten to consist of
expectations over the metric tensor,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-geodesic-objective-expectation}
J =  \int_{t_0}^{t_f} \E_{p\left(\mathbf{G}(\mathbf{x}(t))\right)} \left[
\left\|\dot{\mathbf{x}}(t)\right\|_{\mathbf{G}(\mathbf{x}(t))} \right]
\mathrm{d}t.
\end{align}
#+END_EXPORT
Let us now follow cite:tosiMetrics2014 and formulate a metric tensor that captures the variance in the manifold
via a probability distribution.
Motivated by obtaining a probability distribution over the metric tensor, they
introduce the following Riemannian metric,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-metric-tensor}
  \langle \dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \rangle_{\mathbf{x}} = \dot{\mathbf{x}}_a^T \Jac^T \Jac \dot{\mathbf{x}}_b =
  \dot{\mathbf{x}}_a^T \mathbf{G}(\mathbf{x}) \dot{\mathbf{x}}_b
\end{align}
#+END_EXPORT
where $\Jac=\frac{\partial h}{\partial \mathbf{x}} \in \R^{P \times \StateDim}$ denotes the
Jacobian of $h : \R^{\StateDim} \rightarrow \R^{P}$.
Each gating function's output is one-dimensional in our case, i.e. $P=1$.
The differential operator is linear so the derivative of a GP is also a GP.
Consequently, the Jacobian $\testJac = \Jac(\testInput)$ of
the function evaluated at a new input $\testInput$
is jointly Gaussian with the function evaluated at the training inputs $\allInput$.
For the desired mode's gating function the joint distribution takes the form,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-joint-jacobian-dist}
p(\testJac, \HDes \mid \testInput, \allInput) &=
 \mathcal{N}\left(
\left[\begin{array}{c}
        \testJac \\
        \HDes
      \end{array}\right] \mid
\left[\begin{array}{c}
        \bm{0} \\
        \modeDes{\bm\mu}
      \end{array}\right], \left[\begin{array}{cc}
                              \ddK & \dK^{T} \\
                              \dK & \K
                          \end{array}\right]\right) \\
\Kxx &= \modeDes{k}\left( \allInput, \allInput \right) \in \R^{\NumData \times \NumData} \\
\dK &= \frac{\partial \modeDes{k}\left(\testInput, \allInput\right)}{\partial \testInput} \in \R^{\StateDim \times \NumData} \\
\ddK &= \frac{\partial^2 \modeDes{k}\left(\testInput, \testInput \right)}{\partial \testInput \partial \testInput} \in \R^{\StateDim \times \StateDim}
\end{align}
\todo{state dim or state and control dim??}
#+END_EXPORT
where $\modeDes{k}$ is the covariance function associated with the desired mode's gating function.
Note that we assume the derivative of the desired mode's mean function with respect to an input is zero,
i.e. $\frac{\partial \modeDes{\mu}}{\partial \testInput} = 0$.
As such, we can easily obtain the conditional distribution over $\testJac$ using
the properties of multivariate normal distributions,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-predictive-jacobian-dist}
p\left(\testJac | \HDes, \testInput, \allInput \right)
&= \mathcal{N}\left(\testJac \mid \muJac, \covJac \right) \\
&= \mathcal{N}\left(\testJac \mid \dK \iKxx \left( \HDes - \modeDes{\bm\mu} \right), \ddK - \dK \iKxx \dK^{T} \right)
\end{align}
#+END_EXPORT
which is a $\StateDim$ -dimensional Gaussian distribution.
Importantly, the metric tensor in Eq. ref:eq-metric-tensor is the outer product of two normally
distributed random variables so the resulting random variable $\mathbf{G}$ follows
a non-central Wishart distribution citep:andersonNonCentral1946,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-metric-dist}
  p(\mathbf{G} \mid \testInput, \allInput)
  =\mathcal{W}_{\StateDim}\left(P, \covJac, \mathbb{E}\left[\Jac^{T}\right] \mathbb{E}[\Jac]\right),
\end{align}
#+END_EXPORT
where $P$ is the number of degrees of freedom (always one in our case).
#+BEGIN_EXPORT latex
\begin{myquote}
\subsubsection*{Sparse Gaussian Process Jacobian}
Before detailing our approach to minimising Eq. \ref{eq-geodesic-objective-expectation}
we note that the distribution over the Jacobian is different for GP models that
exploit sparse approximations. We detail the extension here.

Sparse approximations are based on hallucinated inducing variables that are samples from the same GP prior,
\begin{equation}
p\left(\uHDes \mid \zHDes \right) &=\mathcal{N}\left(\uHDes \mid \modeDes{\mu}(\zHDes), \modeDes{k}(\zHDes, \zHDes) \right).
\end{equation}
In the sparse approximation setting it is these inducing variables that are jointly
Gaussian with the Jacobian at a new test location,
\begin{align} \label{eq-joint-jacobian-dist-sparse}
p(\testJac, \uHDes \mid \testInput, \zHDes) &=
 \mathcal{N}\left(
\left[\begin{array}{c}
        \testJac \\
        \uHDes
      \end{array}\right] \mid
\left[\begin{array}{c}
        % {0} \\
        \partial\modeDes{\hat{\bm\mu}} \\
        \modeDes{\hat{\bm\mu}}
      \end{array}\right], \left[\begin{array}{cc}
                              \ddK & \dKz^{T} \\
                              \dKz & \Kzz
                          \end{array}\right]\right) \\
\Kzz &= \modeDes{k}\left( \zHDes, \zHDes \right) \in \R^{\NumInd \times \NumInd} \\
\dKz &= \frac{\partial \modeDes{k}\left(\testInput, \zHDes \right)}{\partial \testInput} \in \R^{\StateDim \times \NumInd},
\end{align}
In Section \ref{sec-inference} we adopted a variational approach
and approximated the inducing variable's posterior with a variational distribution,
\begin{equation} \label{eq-}
p\left(\uHDes \mid \zHDes \right) \approx \qDes = \mathcal{N}(\uHDes \mid \mDes, \SDes),
\end{equation}
and treated $\mDes$ and $\SDes$ as variational parameters to learn.
In order to maintain the positive-definiteness of $\SDes$, we actually utilised the Cholesky
decomposition $\SDes = \mathbf{L}\mathbf{L}^T$ and optimised $\mathbf{L}$.
To obtain the predictive distribution over the Jacobian at a new input location $\testInput$
we must condition on the inducing variable $\uHDes$ and then integrate it out with $\qDes$,
which is a GP,
\begin{align} \label{eq-predictive-jacobian-dist-sparse}
\small
q&\left(\testJac \mid \testInput, \zHDes \right)
=\int \qDes p\left(\testJac \mid \testInput, \uHDes, \zHDes \right)
\text{d} \uHDes \\
&= \mathcal{N}\left(\testJac \mid
\dKz \Kzz^{-1} \left( \mDes - \modeDes{\hat{\bm\mu}} \right),
\ddK - \dKz \iKzz \left( \Kzz - \SDes \right) \iKzz \dKz^{T} \right). \nonumber
\normalsize
\end{align}
\todo{assumed that GP jacobian has zero mean}
Note that when $\qDes=\pDes$ Eq. \ref{eq-predictive-jacobian-dist-sparse} reduces to the original marginal
distribution $p(\testJac \mid \testInput)$.
\end{myquote}
#+END_EXPORT


**** Old :noexport:
The trajectory optimisation is projected onto a continuous-time
ODE whose solutions are geodesics on a probabilistic manifold, induced
by one of the latent gating function GPs.
Solutions to this ODE are trajectories that remain in a single dynamics mode
(where possible) and avoid regions of the dynamics that cannot be predicted confidently.
This latent geodesic ODE is solved using a Hermite-Simpson collocation method cite:kellyIntroduction2017.

The second stage,
this gating function acts as a coordinate map for a latent Riemannian manifold on which
geodesics are solutions to our trajectory optimisation problem.
Geodesics on this manifold satisfy a continuous-time second-order ODE.

A set of collocation constraints are derived that ensure trajectories are solutions to this ODE,
implicitly solving the trajectory optimisation problem.

Motivated by trajectory optimisation, this work adopts (and extends) the well-known
mixture of GP experts (MoGPE) method with a GP-based gating network to learn a time-invariant
transition dynamics model cite:trespMixtures2000a.
The trajectory optimisation exploits the geometric structure
learned by the GP-based gating network along with its well calibrated uncertainty estimates.
The trajectory optimisation is projected onto a continuous-time
ODE whose solutions are geodesics on a probabilistic manifold, induced
by one of the latent gating function GPs.
Solutions to this ODE are trajectories that remain in a single dynamics mode
(where possible) and avoid regions of the dynamics that cannot be predicted confidently.
This latent geodesic ODE is solved using a Hermite-Simpson collocation method cite:kellyIntroduction2017.

** Implicit Trajectory Optimisation
In the previous section we formulated an objective function that encodes our two goals.
As alluded to previously, we do not optimise this objective function in the usual way.
Instead, we exploit the fact that trajectories minimising Eq. ref:eq-geodesic-objective-expectation
are geodesics on $\mathcal{M}$
and must satisfy the continuous-time second-order ODE,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-2ode-stochastic}
 \ddot{\mathbf{x}}(t)
&= f_G(t, \dot{\mathbf{x}}, \mathbf{x}) \\
&=-\frac{1}{2} \mathbf{G}^{-1}(\mathbf{x}(t))\left[\frac{\partial \operatorname{vec}[\mathbf{G}(\mathbf{x}(t))]}{\partial \mathbf{x}(t)}\right]^{T}\left(\dot{\mathbf{x}}(t) \otimes \dot{\mathbf{x}}(t)\right), \nonumber
\end{align}
#+END_EXPORT
where $\operatorname{vec}[\mathbf{G}(\mathbf{x}(t)])$ stacks the columns of $\mathbf{G}(\mathbf{x}(t))$
and $\otimes$ denotes the Kronecker product.
This is a classic result of differential geometry cite:carmoRiemannian1992.
Performing trajectory optimisation in Eq. ref:eq-2ode-stochastic
is equivalent to solving the trajectory optimisation in Eq. ref:eq-traj-opt
with the objective function in Eq. ref:eq-geodesic-objective-expectation
subject to the same boundary conditions.

However, the ODE in Eq. ref:eq-2ode-stochastic contains the metric tensor $\mathbf{G}$ which is a random
variable. As a result, Eq. ref:eq-2ode-stochastic is actually a stochastic differential equation (SDE).
Solving SDEs is notoriously hard so we instead exploit the observation that
the expected metric tensor,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-metric}
  \E[\mathbf{G}] = \E[\mathbf{J}^T] \E[\mathbf{J}] + \mathbf{\Sigma}_J,
\end{align}
#+END_EXPORT
includes the covariance term $\mathbf{\Sigma}_J$.
This implies that the expected
metric is larger when the covariance in the mapping is higher.
This will lead to lengths on the manifold increasing in areas of high covariance.
This is a desirable behaviour because it encourages trajectories to
avoid regions of the learned dynamics with high epistemic uncertainty -
implementing the $g_{\text{epistemic}}$ cost term in Eq. ref:eq-cost-trajectory.

Instead of considering the SDE in Eq. ref:eq-2ode-stochastic we substitute the expected metric tensor
in place of its associated random variable $\mathbf{G}$, to obtain a deterministic ODE,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-2ode-expected}
 \ddot{\mathbf{x}}(t)
&= f_G(t, \dot{\mathbf{x}}, \mathbf{x}) \\
&=-\frac{1}{2} \E\left[\mathbf{G} \right]^{-1}(\mathbf{x}(t))\left[\frac{\partial \operatorname{vec}\left[\E\left[\mathbf{G}(\mathbf{x}(t)) \right]\right]}{\partial \mathbf{x}(t)}\right]^{T}\left(\dot{\mathbf{x}}(t) \otimes \dot{\mathbf{x}}(t)\right). \nonumber
\end{align}
#+END_EXPORT
Solutions to this ODE implicitly minimise the objective in Eq. ref:eq-geodesic-objective endowed with this
expected metric.

*** Direct Collocation
Since neither $\dot{\mathbf{x}}(t_0)$ nor $\dot{\mathbf{x}}(t_f)$ are known, Eq. ref:eq-2ode-expected cannot
be solved with simple forward or backward integration.
Instead, we transcribe the problem using the approach of differential
flatness cite:milamNew2000c,rossPseudospectral2004.
A set of outputs $\mathbf{z}(t)$ are defined such that the
states $\mathbf{x}(t)$ and controls $\mathbf{u}(t)$ can be
expressed in terms of the flat output $\mathbf{z}(t)$ and a finite number of its derivatives,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\mathbf{x}(t) &= A(\mathbf{z}(t), \dot{\mathbf{z}}(t), \ldots) \\
\mathbf{u}(t) &= B(\mathbf{z}(t), \dot{\mathbf{z}}(t), \ldots).
\end{align}
#+END_EXPORT
In the velocity-controlled quadcopter example, the flat output is the
state $\mathbf{z}(t) = \mathbf{x}(t)$
and the control is simply the state derivative
$\mathbf{u}(t) &= \dot{\mathbf{z}}(t)$.

The original trajectory optimisation problem can then be converted to finding $\mathbf{z}(t)$
for $t \in [t_0, t_f]$ subject to the boundary conditions and the dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-diff-flat-ode}
\ddot{\mathbf{x}}(\mathbf{z}(t), \dot{\mathbf{z}}(t)) &= f_G(t, \dot{\mathbf{x}}(\mathbf{z}(t), \dot{\mathbf{z}}(t)), \mathbf{x}(\mathbf{z}(t), \dot{\mathbf{z}}(t))).
\end{align}
#+END_EXPORT
Collocation methods are used to transcribe continuous-time trajectory optimisation problems into
nonlinear programs, i.e. constrained parameter optimisation cite:kellyIntroduction2017,fahrooDirect2000  .
The expected metric in Eq. ref:eq-expected-metric is substituted into
Eq. ref:eq-diff-flat-ode and solved via direct collocation.
This work implements a simple Hermite-Simpson collocation method
that enforces the state derivative predicted by the polynomials to
equal the geodesic ODE $f_G$ at a set of $I$ collocation points $\{\mathbf{z}_{i,c}\}_{i=1}^I$.
This is achieved through the collocation defects,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-defect}
\Delta_i &= \ddot{\mathbf{z}}_{i,c} - f_G(t_{i,c}, \dot{\mathbf{z}}_{i,c},\mathbf{z}_{i,c}) \numberthis
\end{align*}
#+END_EXPORT
where $\ddot{\mathbf{z}}_{i,c}$ is the $2^{\text{nd}}$
derivative w.r.t time at the $i^{\text{th}}$ collocation point predicted by the polynomials.
Eq. ref:eq-defect defines a set of contraints ensuring trajectories are
solutions to the geodesic ODE $f_G$.
The nonlinear program that this work solves uses a dummy cost and is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\min_{\mathbf{z}(t), \dot{\mathbf{z}}(t)}& \int_{t_0}^{t_f} 1 \text{d}t \\
&\text{s.t. }\text{Eqs. \ref{eq-diff-flat-ode} and \ref{eq-defect}}  \\
\mathbf{x}&\left(\mathbf{z}(t_0), \dot{\mathbf{z}}(t_0) \right) = \mathbf{x}_0 \\
\mathbf{x}&(\mathbf{z}(t_f),\dot{\mathbf{z}}(t_f)) = \mathbf{x}_f
%c&(\mathbf{x}(\mathbf{z}(t), \dot{\mathbf{z}}(t)), \mathbf{u}(\mathbf{z}(t), \dot{\mathbf{z}}(t)) \leq 0 \quad \forall t
\end{align}
#+END_EXPORT
This is solved using Sequential Least Squares Programming (SLSQP) in SciPy.

** Trajectory Optimisation label:sec-traj-opt :noexport:
This work seeks to solve the trajectory optimisation in Eq. ref:eq-objective, i.e.
find trajectories from $\mathbf{x}_0$ to $\mathbf{x}_f$ that
minimise the cost in Eq. ref:eq-cost-trajectory.
This section details our approach that
projects the trajectory optimisation onto one of the gating function's GPs,
implicitly minimising the cost function in Eq. ref:eq-cost-trajectory.

The length of a trajectory from $\mathbf{x}_0$ to $\mathbf{x}_f$ on the
manifold given by the mean (Fig. ref:fig-svgp_2d_traj left)
increases when it passes over the contours - analogous to climbing a hill.
Given appropriate scaling of the mean,
shortest trajectories (geodesics) between two locations will be those that attempt to follow contours
i.e. remain in a single mode.
Geodesics are solutions to a continuous-time second-order ODE onto which the trajectory
optimisation is projected.
This section now recaps concepts of Riemannian geometry before extending them to probabilistic geometries
and detailing how this latent ODE is solved using direct collocation.

*** A Geometric Cost Function
The cost function in Eq. ref:eq-cost-trajectory is difficult to pose, but
the $g_{\text{mode}}$ term can be expressed as finding shortest
paths on the desired mode's gating function.
This formulation is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-geodesic-objective}
\min \int_{t_0}^{t_f}\left\|\dot{\mathbf{x}}(t)\right\|_{\mathbf{G}(\mathbf{x}(t))} \mathrm{d}t
\end{align}
#+END_EXPORT
where $\mathbf{G}(\mathbf{x}(t))$ is the metric tensor at $\mathbf{x}(t)$ for a Riemannian manifold
encoding the desired cost, i.e. the desired mode's gating function.
Intuitively Riemannian manifolds are smoothly curved spaces with
an inner product.
Formally, they are smooth manifolds equipped with a Riemannian metric cite:carmoRiemannian1992,
#+BEGIN_EXPORT latex
\begin{definition}[Riemannian Metric]
A Riemannian metric $\mathbf{G}$ on a
manifold $\mathcal{M}$ is a symmetric and positive definite matrix which defines
a smoothly varying inner product
$\langle \dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \rangle_{\mathbf{x}} = \dot{\mathbf{x}}_a^T \mathbf{G}(\mathbf{x}) \dot{\mathbf{x}}_b$
in the tangent space $T_{\mathbf{x}}\mathcal{M}$, for each point $\mathbf{x} \in \mathcal{M}$ and
$\dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \in T_{\mathbf{x}}\mathcal{M}$.
\end{definition}
#+END_EXPORT


# The gating function defining the coordinate map of the Riemannian manifold is modelled as a
# sparse variational Gaussian process and is therefore probabilistic.
# This work extends the probabilistic metric tensor by cite:Tosi2014 to sparse
# variational GPs.
The Riemannian manifold in this work is actually probabilistic because its coordinate
map is modelled as a sparse variational GP.
This work follows cite:tosiMetrics2014 and uses a metric tensor that captures the variance in the manifold
by means of a probability distribution.
In particular, the expected value of this metric tensor contains a covariance term which
leads to lengths on the manifold increasing in areas of high covariance.
This is a desirable behaviour because it encourages trajectories to
avoid regions of the learned dynamics with high epistemic uncertainty -
implementing the $g_{\text{epistemic}}$ cost term in Eq. ref:eq-cost-trajectory.

Motivated by obtaining a probability distribution over the metric tensor, they
introduce the following Riemannian metric,
#+BEGIN_EXPORT latex
\begin{align}
  \langle \dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \rangle_{\mathbf{x}} = \dot{\mathbf{x}}_a^T \mathbf{J}^T \mathbf{J} \dot{\mathbf{x}}_b =
  \dot{\mathbf{x}}_a^T \mathbf{G}(\mathbf{x}) \dot{\mathbf{x}}_b
\end{align}
#+END_EXPORT
where $\mathbf{J}=\frac{\partial h}{\partial \mathbf{x}}$ denotes the Jacobian of $h$.
As the differential operator is linear, the derivative of a GP is also a GP.
For a sparse GP
the Jacobian $\mathbf{J}_*$ of the function evaluated at a new input $\mathbf{x}_*$
is jointly Gaussian with the function's associated inducing variables.
For the $k^{\text{th}}$ gating function it is given by,
#+BEGIN_EXPORT latex
\small
\begin{align*} \label{eq-joint-jacobian-dist}
p(\mathbf{J}^{(k)}_*, \hat{\mathbf{h}}^{(k)} &\mid \mathbf{x}_*, \bm\xi_h^{(k)}) = \\
& \mathcal{N}\left(
\left[\begin{array}{c}
        \mathbf{J}^{(k)}_* \\
        \hat{\mathbf{h}}^{(k)}
      \end{array}\right] \mid
\left[\begin{array}{c}
        \bm{0} \\
        \bm{\mu}_h^{(k)}
      \end{array}\right], \left[\begin{array}{cc}
                              \partial^2\mathbf{K}_{**}^{(k)} & \partial\mathbf{K}_{*h}^{(k)} \\
                            \partial\mathbf{K}_{h*}^{(k)} & \mathbf{K}_{hh}^{(k)}
                          \end{array}\right]\right),
\end{align*}
\normalsize
#+END_EXPORT
where $\K_{hh}^{(k)} = k^{(k)}\left(\bm\xi_h^{(k)}, \bm\xi_h^{(k)}\right) \in \R^{M \times M}$ is the
$k^{\text{th}}$ gating function's covariance function evaluated between its inducing inputs,
$\partial\K_{*h}^{(k)} = \frac{\partial k^{(k)}\left(\mathbf{x}_*, \bm\xi_h^{(k)}\right)}{\partial \mathbf{x}_*} \in \R^{D \times M}$ is
its partial derivative w.r.t its first input (the new input $\mathbf{x}_*$), and
$\partial^2\K_{**}^{(k)} = \frac{\partial^2 k^{(k)}\left(\mathbf{x}_*, \mathbf{x}_*\right)}{\partial \mathbf{x}_* \partial \mathbf{x}_*} \in \R^{D \times D}$
is its derivative w.r.t both inputs (which are both the new input $\mathbf{x}_*$).
Remembering that the inducing variables are actually probabilistic and modelled as a Gaussian
$q\left(\hat{\mathbf{h}}^{(k)}\right) = \mathcal{N}\left(\hat{\mathbf{h}}^{(k)} \mid \mathbf{m}_h^{(k)}, \mathbf{S}_h^{(k)}\right)$,
the predictive distribution of the Jacobian given a new input $\mathbf{x}_*$
is obtained by marginalising the inducing variables,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-predictive-jacobian-dist}
p\left(\mathbf{J}^{(k)}_* | \mathbf{x}_*, \bm\xi_h^{(k)}\right)
&=\int q\left(\hat{\mathbf{h}}^{(k)}\right) p\left(\mathbf{J}^{(k)}_* \mid \mathbf{x}_*, \hat{\mathbf{h}}^{(k)}, \bm\xi_h^{(k)}\right) \text{d} \hat{\mathbf{h}}^{(k)} \\
&= \mathcal{N}\left(\mathbf{J}^{(k)}_* \mid \bm\mu_J^{(k)}, \mathbf{\Sigma}_{J}^{(k)}\right). \numberthis
\end{align*}
#+END_EXPORT
This induces a non-central Wishart distribution over the metric tensor $\mathbf{G}$,
#+BEGIN_EXPORT latex
\begin{align}
  \mathbf{G}=\mathcal{W}_{D+F}\left(p, \boldsymbol{\Sigma}_{J}, \mathbb{E}\left[\mathbf{J}^{\top}\right] \mathbb{E}[\mathbf{J}]\right),
\end{align}
#+END_EXPORT
where $p$ is the number of degrees of freedom (always one in our case).
The expected metric tensor is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-metric}
  \E[\mathbf{G}] = \E[\mathbf{J}^T] \E[\mathbf{J}] + \mathbf{\Sigma}_J.
\end{align}
#+END_EXPORT
This expected metric tensor includes a covariance term $\mathbf{\Sigma}_J$ which implies that the
metric is larger when the covariance in the mapping is higher.
As a result, trajectories minimising Eq. ref:eq-geodesic-objective endowed with this
metric will attempt to avoid regions of the transition dynamics with high epistemic uncertainty.

# where,
# #+BEGIN_EXPORT latex
# \small
# \begin{align*} \label{eq-predictive-jacobian-mean-var}
# \bm\mu_J^{(k)} &= \partial\mathbf{K}_{*h}^{(k)} \left( \mathbf{K}_{hh}^{(k)}\right)^{-1} \mathbf{m}_h^{(k)}, \\
# \mathbf{\Sigma}_J^{(k)} &= \partial^2{\K_{**}^{(k)}} -
# \partial{\K_{*h}^{(k)}} \left(\mathbf{K}_{hh}^{(k)}\right)^{-1} \partial{\K_{h*}^{(k)}} \\
# &+ \partial{\K_{*h}^{(k)}} \left(\mathbf{K}_{hh}^{(k)}\right)^{-1} \mathbf{S}_h^{(k)} \left(\mathbf{K}_{hh}^{(k)}\right)^{-1} \partial{\K_{h*}^{(k)}}.
# \end{align*}
# \normalsize
# #+END_EXPORT

*** Latent Geodesic ODE Collocation
#+BEGIN_EXPORT latex
\begin{figure}[!t]
\centering
\includegraphics[width=0.8\columnwidth]{images/mixing_prob_vs_time.pdf}
\caption{Comparision of the intial and optimised trajectories' performance at staying in the desired mode. The plot shows mode 1's mixing probability over the trajectories for two settings of $\lambda$.}
\label{fig-mixing_prob_vs_time}
\end{figure}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{figure}[!t]
\centering
\includegraphics[width=0.8\columnwidth]{images/epistemic_var_traj.pdf}
\caption{Comparision of the trajectories' performance at avoiding
regions of the learned transition dynamics with high epistemic uncertainty for two settings of $\lambda$.
The plot shows the GP's posterior variance associated with mode 1's gating function over the
trajectories.}
\label{fig-epistemic_var_vs_time}
\end{figure}
#+END_EXPORT
Trajectories minimising Eq. ref:eq-geodesic-objective are geodesics on $\mathcal{M}$
and must satisfy the continuous-time second-order ODE cite:carmoRiemannian1992,
#+BEGIN_EXPORT latex
\small
\begin{align*} \label{eq-2ode}
 \ddot{\mathbf{x}}(t)
&= f_G(t, \dot{\mathbf{x}}, \mathbf{x}) \\
&=-\frac{1}{2} \mathbf{G}^{-1}(\mathbf{x}(t))\left[\frac{\partial \operatorname{vec}[\mathbf{G}(\mathbf{x}(t))]}{\partial \mathbf{x}(t)}\right]^{T}\left(\dot{\mathbf{x}}(t) \otimes \dot{\mathbf{x}}(t)\right), \numberthis
\end{align*}
\normalsize
#+END_EXPORT
where $\operatorname{vec}[\mathbf{G}(\mathbf{x}(t)])$ stacks the columns of $\mathbf{G}(\mathbf{x}(t))$
and $\otimes$ denotes the Kronecker product.
Performing  trajectory optimisation in Eq. ref:eq-objective with the cost function in Eq. ref:eq-cost-trajectory
is equivalent to solving the ODE in Eq. ref:eq-2ode subject to the same boundary conditions.
However, since neither $\dot{\mathbf{x}}(t_0)$ nor $\dot{\mathbf{x}}(t_f)$ are known, it cannot
be solved with simple forward or backward integration.
Instead, the problem is transcribed using the approach of differential
flatness cite:milamNew2000c,rossPseudospectral2004.
A set of outputs $\mathbf{z}(t)$ are defined such that the
states $\mathbf{x}(t)$ and controls $\mathbf{u}(t)$ can be
expressed in terms of the flat output $\mathbf{z}(t)$ and a finite number of its derivatives,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\mathbf{x}(t) &= A(\mathbf{z}(t), \dot{\mathbf{z}}(t), \ldots) \\
\mathbf{u}(t) &= B(\mathbf{z}(t), \dot{\mathbf{z}}(t), \ldots).
\end{align}
#+END_EXPORT
In the velocity-controlled quadcopter example, the flat output is the
state $\mathbf{z}(t) = \mathbf{x}(t)$
and the control is simply the state derivative
$\mathbf{u}(t) &= \dot{\mathbf{z}}(t)$.

The original trajectory optimisation problem can then be converted to finding $\mathbf{z}(t)$
for $t \in [t_0, t_f]$ subject to the boundary conditions and the dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-diff-flat-ode}
\ddot{\mathbf{x}}(\mathbf{z}(t), \dot{\mathbf{z}}(t)) &= f_G(t, \dot{\mathbf{x}}(\mathbf{z}(t), \dot{\mathbf{z}}(t)), \mathbf{x}(\mathbf{z}(t), \dot{\mathbf{z}}(t))).
\end{align}
#+END_EXPORT
Collocation methods are used to transcribe continuous-time trajectory optimisation problems into
nonlinear programs, i.e. constrained parameter optimisation cite:kellyIntroduction2017,fahrooDirect2000  .
The expected metric in Eq. ref:eq-expected-metric is substituted into
Eq. ref:eq-diff-flat-ode and solved via direct collocation.
This work implements a simple Hermite-Simpson collocation method
that enforces the state derivative predicted by the polynomials to
equal the geodesic ODE $f_G$ at a set of $I$ collocation points $\{\mathbf{z}_{i,c}\}_{i=1}^I$.
This is achieved through the collocation defects,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-defect}
\Delta_i &= \ddot{\mathbf{z}}_{i,c} - f_G(t_{i,c}, \dot{\mathbf{z}}_{i,c},\mathbf{z}_{i,c}) \numberthis
\end{align*}
#+END_EXPORT
where $\ddot{\mathbf{z}}_{i,c}$ is the $2^{\text{nd}}$
derivative w.r.t time at the $i^{\text{th}}$ collocation point predicted by the polynomials.
Eq. ref:eq-defect defines a set of contraints ensuring trajectories are
solutions to the geodesic ODE $f_G$.
The nonlinear program that this work solves uses a dummy cost and is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\min_{\mathbf{z}(t), \dot{\mathbf{z}}(t)}& \int_{t_0}^{t_f} 1 \text{d}t \\
&\text{s.t. }\text{Eqs. \ref{eq-diff-flat-ode} and \ref{eq-defect}}  \\
\mathbf{x}&\left(\mathbf{z}(t_0), \dot{\mathbf{z}}(t_0) \right) = \mathbf{x}_0 \\
\mathbf{x}&(\mathbf{z}(t_f),\dot{\mathbf{z}}(t_f)) = \mathbf{x}_f
%c&(\mathbf{x}(\mathbf{z}(t), \dot{\mathbf{z}}(t)), \mathbf{u}(\mathbf{z}(t), \dot{\mathbf{z}}(t)) \leq 0 \quad \forall t
\end{align}
#+END_EXPORT
This is solved using Sequential Least Squares Programming (SLSQP) in SciPy.
# Solutions to this nonlinear program are trajectories that
# attempt to remain in a single dynamics mode and, importantly, also
# avoid areas of the learned transition dynamics with high epistemic uncertainty.

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \min_{\mathbf{x}(t), \dot{\mathbf{x}}(t)}& \int_{t_0}^{t_f} 1 \text{d}t \\
# \text{s.t. }&\text{Eqs. \ref{eq-diff-flat-ode} and \ref{eq-defect}}  \\
# \mathbf{x}&\left(\mathbf{z}(t_0), \dot{\mathbf{z}}(t_0), \ddot{\mathbf{z}}(t_0) \right) = \mathbf{x}_0 \\
# \mathbf{x}&(\mathbf{z}(t_f),\dot{\mathbf{z}}(t_f), \ddot{\mathbf{z}}(t_f)) = \mathbf{x}_f \\
# c&(\mathbf{x}(t), \dot{\mathbf{x}}(t)) \leq 0 \quad \forall t
# \end{align}
# #+END_EXPORT


# ** Direct Collocation


# This work implements a simple Hermite-Simpson collocation method
# that enforces the state derivative predicted by the polynomials to
# equal the geodesic ODE $f_G$ at a set of collocation points.
# This is achieved via the collocation defects,
# #+BEGIN_EXPORT latex
# \begin{align*} \label{eq-defect}
# \Delta_i &= \dot{\mathbf{z}}_{i,c} - y(\mathbf{z}_{i,c}) \numberthis
# \end{align*}
# #+END_EXPORT
# which define a set of contraints ensuring trajectories are solutions to the geodesic ODE $f_G$.

# The nonlinear program that this work solves is given by Eq. ref:eq-objective
# with $g(\mathbf{x}(t), \mathbf{u}(t)) = 1$ and the collocation constraints in Eq. ref:eq-defect.


*** Concepts of Riemannian Geometry :noexport:
Let us now introduce the necessary concepts for finding shortest paths (geodesics) on Riemannian
manifolds.
This section considers continuous-time inputs denoted as $\mathbf{z}(t) = \mathbf{x}(t)$.
Intuitively Riemannian manifolds are smoothly curved spaces with
an inner product.
Formally they are smooth manifolds equipped with a Riemannian metric cite:carmoRiemannian1992,
#+BEGIN_EXPORT latex
\begin{definition}[Riemannian Metric]
A Riemannian metric $\mathbf{G}$ on a
manifold $\mathcal{M}$ is a symmetric and positive definite matrix which defines
a smoothly varying inner product
$\langle \dot{\mathbf{z}}_a, \dot{\mathbf{z}}_b \rangle_{\mathbf{z}} = \dot{\mathbf{z}}_a^T \mathbf{G}(\mathbf{z}) \dot{\mathbf{z}}_b$
in the tangent space $T_{\mathbf{z}}\mathcal{M}$, for each point $\mathbf{z} \in \mathcal{M}$ and
$\dot{\mathbf{z}}_a, \dot{\mathbf{z}}_b \in T_{\mathbf{z}}\mathcal{M}$.
\end{definition}
#+END_EXPORT
Riemannian manifolds are often represented as charts; a parameter space for the
curved surface. An example of a chart is the spherical coordinate system that is
used to describe a sphere.
The chart is often a flat space and the curvature of the manifold arises
through smooth changes in the metric.
Measurements on the surface can thus be computed in the chart locally and
integrated to give global measures.
On a Riemannian manifold $\mathcal{M}$ the length of a trajectory (curve) $\bar{\mathbf{z}}$
is given by the norm of the tangent vector along the trajectory,
#+BEGIN_EXPORT latex
\small
\begin{align*} \label{eq-length}
\text{Length}(\bar{\mathbf{z}}) &=\int_{t_0}^{t_f}\left\|\dot{\mathbf{z}}(t)\right\|_{\mathbf{G}(\mathbf{z}(t))} \mathrm{d}t
=\int_{t_0}^{t_f}\sqrt{\dot{\mathbf{z}}(t)^T \mathbf{G}(\mathbf{z}(t)) \dot{\mathbf{z}}(t) } \mathrm{d} t,
\end{align*}
\normalsize
% \begin{align} \label{eq-length}
% \text { Length }(\bar{\mathbf{x}}) &=\int_{t_0}^{t_f}\left\|\dot{\mathbf{x}}(t)\right\|_{\mathbf{G}(\mathbf{x}(t))} \mathrm{d} t \\
% &=\int_{t_0}^{t_f}\sqrt{\dot{\mathbf{x}}(t)^T \mathbf{G}(\mathbf{x}(t)) \dot{\mathbf{x}}(t) } \mathrm{d} t,
% \end{align}
#+END_EXPORT
where $\mathbf{G}(\mathbf{z}(t))$ is the metric tensor at $\mathbf{z}(t)$.
With this method for calculating lengths on manifolds the concept of a geodesic can be formally defined.
#+BEGIN_EXPORT latex
\begin{definition}[Geodesic]
Given two points $\mathbf{z}_0, \mathbf{z}_f \in
\mathcal{M}$, a Geodesic is a length minimising trajectory (curve)
$\bar{\mathbf{z}}_g$ connecting the points such that,
\begin{align}
  \bar{\mathbf{z}}_{g}=\arg \min_{\bar{\mathbf{z}}} \operatorname{Length}(\bar{\mathbf{z}}), \quad \bar{\mathbf{z}}(t_0)=\mathbf{z}_{0}, \bar{\mathbf{z}}(t_f)=\mathbf{z}_{f}.
\end{align}
\end{definition}
#+END_EXPORT
Geodesics satisfy a continuous-time $2^{\text{nd}}$ order ODE cite:carmoRiemannian1992,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-2ode}
 \ddot{\mathbf{z}}(t)
&= y(t, \mathbf{z}, \dot{\mathbf{z}}) \\
&=-\frac{1}{2} \mathbf{G}^{-1}(\mathbf{x}(t))\left[\frac{\partial \operatorname{vec}[\mathbf{G}(\mathbf{z}(t))]}{\partial \mathbf{z}(t)}\right]^{T}\left(\dot{\mathbf{z}}(t) \otimes \dot{\mathbf{z}}(t)\right),
\end{align}
\normalsize
#+END_EXPORT
where $\operatorname{vec}[\mathbf{G}(\mathbf{z}(t)])$ stacks the columns of $\mathbf{G}(\mathbf{z}(t))$
and $\otimes$ denotes the Kronecker product.
# Computing geodesics involves finding a solution to Eq. ref:eq-2ode
# with $\mathbf{x}(t_0) = \mathbf{x}_1$ and $\mathbf{x}(t_f) = \mathbf{x}_2$.
# This is a boundary value problem with a smooth solution so it can be solved
# using any direct trajectory optimisation framework and can therefore
# incorporate state and action constraints.
# #+BEGIN_EXPORT latex
# \todo[inline]{Hmm, not sure this is true, read up - It only involves discretizing the geodesic curve and not
# the feature space and as this is always 1-dimensional the approach scales
# to higher dimensional feature spaces.}
# #+END_EXPORT


# We can formulate this as an initial value problem and use techniques such as the
# shooting method to determine the correct inintial velocity and from this the
# geodesic path.
# This is advantageous as it only involves discretizing the geodesic curve and not
# the the feature space. This is always 1-dimensional and thus the approach scales
# to highgher dimensional feature spaces.

** Conclusion
This chapter has presented a method for performing trajectory optimisation in
multimodal dynamical systems with the transition dynamics modelled as a
Mixtures of Gaussian Process Experts method.
The trajectory optimisation is projected onto a probabilistic Riemannian
manifold parameterised by the gating network of the MoGPE model.
Given a start and end state, the trajectory optimisation can be tuned to find trajectories that either
prioritise remaining in a desired dynamics mode or prioritise avoiding regions of the learned transition
dynamics model with high epistemic uncertainty.

* Trajectory Optimisation in Desired Modes as Probabilistic Inference label:chap-control
cite:schreiterSafe2015 use a GP classifier to identify safe and unsafe regions when learning GP dynamics models
in an active learning setting.
** Intro :ignore:
#+BEGIN_EXPORT latex
\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{\MakeUppercase{\timeInd}}}
\newcommand{\stateTraj}{\ensuremath{\bar{\state}}}
\newcommand{\controlTraj}{\ensuremath{\bar{\control}}}
\newcommand{\nominalStateTraj}{\ensuremath{\stateTraj_*}}
\newcommand{\nominalControlTraj}{\ensuremath{\controlTraj_*}}

\newcommand{\utraj}{\ensuremath{\bar{\control}}}
\newcommand{\policies}{\ensuremath{\Pi}}
\newcommand{\policy}{\ensuremath{\pi_{\theta}}}
#+END_EXPORT
** Literature Review (iLQR/iLQG/DDP/MPPI for Trajectory Generation)
*Iterative LQR/LQG* can be used to generate trajectories for non-linear systems by iteratively approximating
the dynamics to be linear around a nominal trajectory and optimising for the controls.
iLQR works well for quadratic cost functions and can be used with any cost function by approximating the cost
function with a second order Taylor expansion. However, in this case iLQR is susceptible to converging
to terrible (local) optima if the true cost function is highly non-convex.

cite:mitrovicAdaptive2010 combine learned dynamics models with iLQG to develop an adaptive optimal feedback controller.

cite:boedeckerApproximate2014 present a real-time iLQR based on sparse GPs.

cite:rohrProbabilistic2021 propose a novel controller synthesis for
linearised GP dynamics that yields robust controllers with
respect to a probabilistic stability margin.

In cite:marcoAutomatic2016 they parameterise the weight matrices
$\stateMatrix$ and $\controlMatrix$ with parameters $\theta$
and perform BO (entropy search) on $J(\theta)$. Their method handles model mis-specification
by adapting the parameters.

*Differential Dynamic Programming* iLQR and iLQG  are variants of the classic Differential Dynamic Programming (DDP) algorithm cite:rosenbrockDifferential1972,
the main difference being that only a /first-order/ approximation of the dynamics is used, instead of /second-order/.
cite:liaoAdvantages1992 show that DDP is comparable to a full Newton step for the entire control
sequence. Therefore, care must be taken when either the Hessian isn't positive-definite or the minimum isn't close
and the quadratic model is inaccurate.
\todo{remember improvements to standard iLQR/DDP from cite:tassaSynthesis2012}

The extension of DDP to stochastic state and controls is addressed in cite:theodorouStochastic2010.

DDP under parametric uncertainty using generalised polynomial chaos cite:aoyamaReceding2021.


cite:panProbabilistic2014 present a probabilistic trajectory optimisation
framework for systems with unknown dynamics called probabilistic
differential dynamic programming (PDDP).
It extends differential dynamic programming to explicitly account
for uncertainty in the learned dynamics modelled using gaussian processes.
In contrast to typical gradient based policy search methods, PDDP does not
require a policy parameterisation as it learns a locally optimal,
time-varying control policy.
Based on the second-order local approximation of the value function,
PDDP performs Dynamic Programming around a nominal trajectory in Gaussian belief spaces.

*Path Integral*  control is a method for developing optimal controls based on stochastic
sampling of trajectories cite:theodorouGeneralized2010.
This work was originally limited to control affine systems cite:williamsModel2017
but has since been extended to general nonlinear
dynamical systems cite:williamsInformation2017,williamsInformationTheoretic2018.

** Problem Statement
Given our partially know open loop system,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-open-loop-dynamics}
\state_{t+1} = f(\state_t, \control_t)
\end{equation}
#+END_EXPORT
we would like a policy,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-}
\control_t = \pi_{\theta}(\state_t)
\end{equation}
#+END_EXPORT
with parameters $\theta$. The policy may be control theoretic exploiting the learned dynamics in
Eq. ref:eq-open-loop-dynamics or it may also be learned (from observations/learned dynamics).
Should the policy be open loop or closed loop?

*Control Theoretic Policy*
- Slow to calculate open loop trajectory if using our learned dynamics,
- Maybe faster if we approximate dynamics as linear/deterministic,
-
Closed loop control theoretic policy
is probably only possible if approximating dynamics as linear/deterministic.

*Learned Policy*
- Learn open loop policy where the information in the dynamics is distilled into the policy,
- Fast open loop trajectories,
- Use formulate a closed loop controller?
Maybe this policy finds open loop trajectories and it runs fast enough
that we can used it to formulate a closed loop controller?
Closed loop system
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-}
\state_{t+1} = f(\state_t, \pi_\theta(\state_t))
\end{equation}
#+END_EXPORT

BO acquisition function
#+BEGIN_EXPORT latex
\newcommand{\traj}{\ensuremath{\bar{\state}}}
\begin{align} \label{eq-}
\traj^* &= \text{arg} \max \alpha(\traj)
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\alpha(\traj) = \frac{\text{Length}_{\mathcal{X}}(\traj)}{\text{Length}_{\mathcal{M}}(\traj)}
\end{align}
#+END_EXPORT

** Differential Flatness of 3D Quadrotor Dynamics
#+BEGIN_EXPORT latex
\newcommand{\roll}{\ensuremath{\phi}}
\newcommand{\pitch}{\ensuremath{\theta}}
\newcommand{\yaw}{\ensuremath{\psi}}

\newcommand{\world}{\ensuremath{w}}
\newcommand{\body}{\ensuremath{b}}
\newcommand{\intermediate}{\ensuremath{c}}
\renewcommand{\frame}{\ensuremath{F}}
\newcommand{\worldFrame}{\ensuremath{\frame_\world}}
\newcommand{\bodyFrame}{\ensuremath{\frame_\body}}
\newcommand{\intermediateFrame}{\ensuremath{\frame_\intermediate}}

\newcommand{\inertiaMatrix}{\ensuremath{J}}
\newcommand{\zw}{\ensuremath{\mathbf{z}_\world}}

\newcommand{\positions}{\ensuremath{\mathbf{h}}}
\newcommand{\velocity}{\ensuremath{v}}
\newcommand{\Velocity}{\ensuremath{\mathbf{\velocity}}}

\newcommand{\rotationMatrix}{\ensuremath{\mathbf{R}}}
\newcommand{\angularVelocity}{\ensuremath{\bm\omega}}
\newcommand{\angularVelocityMatrix}{\ensuremath{\bm\Omega}}

\newcommand{\thrust}{\ensuremath{f_z}}
\newcommand{\torque}{\ensuremath{\tau}}
#+END_EXPORT
The quadrotor frames and Euler angles (roll $\roll$, pitch $\pitch$ and yaw $\yaw$) are shown
in Figure ref:fig-quadcopter-frames.
The subscripts $\world$, $\body$ and $\intermediate$ are used to denote the world
$\worldFrame$, body $\bodyFrame$ and intermediate $\intermediateFrame$ (after yaw angle rotation) frames respectively.
The the 3D nonlinear quadrotor dynamics are described with the quadcopter model from cite:wangSafe2018,zhouVector2014.
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\dot{\Velocity} &= g \zw + \frac{1}{m} \thrust \rotationMatrix \zw \\
\dot{\rotationMatrix} &= \rotationMatrix \angularVelocityMatrix \\
\dot{\angularVelocity} &= \inertiaMatrix^{-1} \torque - \inertiaMatrix^{-1} \angularVelocityMatrix \inertiaMatrix \angularVelocity \\
\dot{\positions} &= \velocity
\end{align}
#+END_EXPORT
where $g=9.81ms^{-2}$ is the acceleration due to gravity, $m$ is the mass, $\zw=[0,0,1]^T$,
$\positions=[x, y, z]^T$ is the position of the quadrotor in the world frame $\worldFrame$,
$\Velocity=[\velocity_x, \velocity_y, \velocity_z]^T$ is the
and velocity of the quadrotor in the world frame $\worldFrame$ respectively.
The angular velocity of the quadrotor in the body frame $\bodyFrame$ is denoted $\angularVelocity=[p, q ,r]^T$
and in tensor form $\angularVelocityMatrix=[[0,-p,q];[r,0,-p];[-q,p,0]]$

$\state=[x, y, z, \velocity_x, \velocity_y, \velocity_z, \roll, \pitch, \yaw, \dot{\roll}, \dot{\pitch}, \dot{\yaw}]$

$\state=[x, y, z, \velocity_x, \velocity_y, \velocity_z, \roll, \pitch, \yaw]^T$
$\state=[\ddot{x}, \ddot{y}, \ddot{z}, \dot{\roll}, \dot{\pitch}, \dot{\yaw}]^T$

\newpage
** 2D Quadrotor Dynamics
#+BEGIN_EXPORT latex
\newcommand{\roll}{\ensuremath{\phi}}
\newcommand{\pitch}{\ensuremath{\theta}}
\newcommand{\yaw}{\ensuremath{\psi}}

\newcommand{\world}{\ensuremath{w}}
\newcommand{\body}{\ensuremath{b}}
\newcommand{\intermediate}{\ensuremath{c}}
\renewcommand{\frame}{\ensuremath{F}}
\newcommand{\worldFrame}{\ensuremath{\frame_\world}}
\newcommand{\bodyFrame}{\ensuremath{\frame_\body}}
\newcommand{\intermediateFrame}{\ensuremath{\frame_\intermediate}}

\newcommand{\inertiaMatrix}{\ensuremath{J}}
\newcommand{\inertiaZ}{\ensuremath{I_{zz}}}
\newcommand{\zw}{\ensuremath{\mathbf{z}_\world}}

\newcommand{\positions}{\ensuremath{\mathbf{h}}}
\newcommand{\velocity}{\ensuremath{v}}
\newcommand{\velocityx}{\ensuremath{\dot{x}}}
\newcommand{\velocityy}{\ensuremath{\dot{y}}}
\newcommand{\Velocity}{\ensuremath{\mathbf{\velocity}}}

\newcommand{\rotationMatrix}{\ensuremath{\mathbf{R}}}
\newcommand{\angularVelocity}{\ensuremath{\bm\omega}}
\newcommand{\angularVelocityMatrix}{\ensuremath{\bm\Omega}}

\renewcommand{\thrust}{\ensuremath{f_{x}}}
\renewcommand{\Thrust}{\ensuremath{\mathbf{f_{x}}}}
\renewcommand{\torque}{\ensuremath{\tau_z}}

\newcommand{\quadrotorDynamics}{\ensuremath{f_{\text{quad}}}}
#+END_EXPORT
The quadrotor frames and Euler angles (roll $\roll$, pitch $\pitch$ and yaw $\yaw$) are shown
in Figure ref:fig-quadcopter-frames.
The subscripts $\world$, $\body$ and $\intermediate$ are used to denote the world
$\worldFrame$, body $\bodyFrame$ and intermediate $\intermediateFrame$ (after yaw angle rotation) frames respectively.
The the 3D nonlinear quadrotor dynamics are described with the quadcopter model from cite:wangSafe2018,zhouVector2014.

The 2D nonlinear quadrotor dynamics are based on the
state vector is given by $\state = [x, y, \velocityx, \velocityy, \yaw]$
where $\positions = [x, y]$ is the Euclidean coordinates relative to the world frame $\worldFrame$,
$\Velocity = [\velocityx, \velocityy]$ is the velocity in the world frame $\worldFrame$ and
$\yaw$ is the yaw angle, i.e. the angle around the $z$ axis.
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\dot{\state} &= \quadrotorDynamics(\state, \control)
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\rotationMatrix_{\body,\world} =
\begin{bmatrix}
\cos{\yaw} & -\sin(\yaw) \\
\sin{\yaw} & \cos(\yaw)
\end{bmatrix}
\end{align}
#+END_EXPORT
where,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\dot{\state}_{1:2} &= \dot{\positions} = \state_{3:4} \\
\dot{\state}_{3:4} &= \dot{\Velocity} = \frac{1}{m} \rotationMatrix \Thrust \\
\dot{\state}_{5} &= \dot{\yaw} = \frac{1}{\inertiaZ} \torque
\end{align}
#+END_EXPORT
where $\Thrust = [\thrust, 0]^T$ is the total thrust force in the quadrotors from the rotors and $\torque$ is the torque on the quadrotor
around the $z$ axis of the world frame $\worldFrame$.
The thrust and torque are realistic controls for a 2D quadrotor system and gives the
control vector \control = [\thrust, \torque].

#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\dot{\Velocity} &= g \zw + \frac{1}{m} \thrust \rotationMatrix \zw \\
\dot{\rotationMatrix} &= \rotationMatrix \angularVelocityMatrix \\
\dot{\angularVelocity} &= \inertiaMatrix^{-1} \torque - \inertiaMatrix^{-1} \angularVelocityMatrix \inertiaMatrix \angularVelocity \\
\dot{\positions} &= \velocity
\end{align}
#+END_EXPORT
where $g=9.81ms^{-2}$ is the acceleration due to gravity, $m$ is the mass, $\zw=[0,0,1]^T$,
$\positions=[x, y, z]^T$ is the position of the quadrotor in the world frame $\worldFrame$,
$\Velocity=[\velocity_x, \velocity_y, \velocity_z]^T$ is the
and velocity of the quadrotor in the world frame $\worldFrame$ respectively.
The angular velocity of the quadrotor in the body frame $\bodyFrame$ is denoted $\angularVelocity=[p, q ,r]^T$
and in tensor form $\angularVelocityMatrix=[[0,-p,q];[r,0,-p];[-q,p,0]]$

$\state=[x, y, z, \velocity_x, \velocity_y, \velocity_z, \roll, \pitch, \yaw, \dot{\roll}, \dot{\pitch}, \dot{\yaw}]$

$\state=[x, y, z, \velocity_x, \velocity_y, \velocity_z, \roll, \pitch, \yaw]^T$
$\state=[\ddot{x}, \ddot{y}, \ddot{z}, \dot{\roll}, \dot{\pitch}, \dot{\yaw}]^T$

\newpage
** Point Mass 2D Dynamics
#+BEGIN_EXPORT latex
\newcommand{\roll}{\ensuremath{\phi}}
\newcommand{\pitch}{\ensuremath{\theta}}
\newcommand{\yaw}{\ensuremath{\psi}}

\newcommand{\world}{\ensuremath{w}}
\newcommand{\body}{\ensuremath{b}}
\newcommand{\intermediate}{\ensuremath{c}}
\renewcommand{\frame}{\ensuremath{F}}
\newcommand{\worldFrame}{\ensuremath{\frame_\world}}
\newcommand{\bodyFrame}{\ensuremath{\frame_\body}}
\newcommand{\intermediateFrame}{\ensuremath{\frame_\intermediate}}

\newcommand{\inertiaMatrix}{\ensuremath{J}}
\newcommand{\inertiaZ}{\ensuremath{I_{zz}}}
\newcommand{\zw}{\ensuremath{\mathbf{z}_\world}}

\newcommand{\positions}{\ensuremath{\mathbf{h}}}
\newcommand{\velocity}{\ensuremath{v}}
\newcommand{\velocityx}{\ensuremath{\dot{x}}}
\newcommand{\velocityy}{\ensuremath{\dot{y}}}
\newcommand{\Velocity}{\ensuremath{\mathbf{\velocity}}}

\newcommand{\rotationMatrix}{\ensuremath{\mathbf{R}}}
\newcommand{\angularVelocity}{\ensuremath{\bm\omega}}
\newcommand{\angularVelocityMatrix}{\ensuremath{\bm\Omega}}

\renewcommand{\thrust}{\ensuremath{f_{x}}}
\renewcommand{\Thrust}{\ensuremath{\mathbf{f_{x}}}}
\renewcommand{\torque}{\ensuremath{\tau_z}}

\newcommand{\quadrotorDynamics}{\ensuremath{f_{\text{quad}}}}
#+END_EXPORT
cite:williamsAdvancing
cite:watsonStochastic2020
cite:watsonAdvancing2021

cite:levineVariational2013


cite:bhardwajDifferentiable2020

cite:mukadamContinuoustime2018

The dynamics of a point mass in 2D can represented with the
state vector $\state = [x, y, \velocityx, \velocityy, \yaw]$,
where $\positions = [x, y]$ denotes the 2D Euclidean coordinates relative to the world frame $\worldFrame$,
$\Velocity = [\velocityx, \velocityy]$ is the velocity in the world frame $\worldFrame$ and
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{tikzpicture}
% body frame
\draw[thick,->] (0,0) -- (1.5,1.5) node[anchor=south] {$x$};
\draw[thick,->] (0,0) -- (1.5,-1.5) node[anchor=north] {$y$};

% world frame
\draw[thick,->] (-3,-3) -- (3,-3) node[anchor=north west] {$x$};
\draw[thick,->] (-3,-3) -- (-3,3) node[anchor=south east] {$y$};
\foreach \x in {-3, -2, -1, 0,1,2,3}
   \draw (\x cm,-3) -- (\x cm,-3) node[anchor=north] {$\x$};
\foreach \y in {-3, -2, -1, 0,1,2,3}
    \draw (-3,\y cm) -- (-3,\y cm) node[anchor=east] {$\y$};
\end{tikzpicture}
\end{figure}
#+END_EXPORT


The point mass can apply a force along its $x$ axis (known as the thrust vector), which is denoted
$\Thrust = [\thrust, 0]^T$.
It can also rotate itself by applying a torque $\torque$ around its $z$ axis.
The resulting control vector for the 2D point mass is given by $\control = [\thrust, \torque]$.
The nonlinear dynamics of the system are given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\dot{\state} &=
\begin{bmatrix}
    \dot{\state}_{1} \\
    \dot{\state}_{2} \\
    \dot{\state}_{3} \\
    \dot{\state}_{4} \\
    \dot{\state}_{5}
\end{bmatrix}
=
\begin{bmatrix}
    \state_{3} \\
    \state_{4} \\
    \frac{1}{m} \thrust \cos{(\yaw)} \\
    \frac{1}{m} \thrust \sin{(\yaw)} \\
    \frac{1}{\inertiaZ} \torque
\end{bmatrix}
\end{align}
#+END_EXPORT
where $m$ is the mass and $\inertiaZ$ is the moment of inertia around the vertical $z$ axis.
Defining the rotation matrix from the body frame $\bodyFrame$ to the world frame $\worldFrame$ as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\rotationMatrix_{\body,\world} =
\begin{bmatrix}
\cos{\yaw} & -\sin(\yaw) \\
\sin{\yaw} & \cos(\yaw)
\end{bmatrix}
\end{align}
#+END_EXPORT
the dynamics can be calculated as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\dot{\state}_{1:2} &= \dot{\positions} = \state_{3:4} \\
\dot{\state}_{3:4} &= \dot{\Velocity} = \frac{1}{m} \rotationMatrix \Thrust \\
\dot{\state}_{5} &= \dot{\yaw} = \frac{1}{\inertiaZ} \torque
\end{align}
#+END_EXPORT

The unknown dynamics can be modelled by placing GP priors on the accelerations ($\dot{\dot{x}}, \dot{\dot{y}}$) and
the angular (yaw) velocity $\dot{\yaw}$,
#+BEGIN_EXPORT latex
\renewcommand{\input}{\ensuremath{\hat{\state}}}
\begin{align} \label{eq-quadrotor-dynamics}
\dot{\state} &=
\begin{bmatrix}
    \dot{\state}_{1} \\
    \dot{\state}_{2} \\
    \dot{\state}_{3} \\
    \dot{\state}_{4} \\
    \dot{\state}_{5}
\end{bmatrix}
=
\begin{bmatrix}
    \state_{3} \\
    \state_{4} \\
    \frac{1}{m} \thrust \cos{(\yaw)} + \mathcal{N} \left( \mu_3(\input),  k_3(\input, \input) \right) \\
    \frac{1}{m} \thrust \sin{(\yaw)} + \mathcal{N} \left( \mu_4(\input),  k_4(\input, \input) \right) \\
    \frac{1}{\inertiaZ} \torque + \mathcal{N} \left( \mu_5(\input),  k_5(\input, \input) \right)
\end{bmatrix}
\end{align}
#+END_EXPORT


\newpage
** Optimal Control as Probabilistic Inference
#+BEGIN_EXPORT latex
\newcommand{\stateDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\controlDomain}{\ensuremath{\mathcal{U}}}

\newcommand{\startStateDist}{\ensuremath{p(\state_{1})}}
\newcommand{\transitionDist}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)}}
\newcommand{\stateControlTraj}{\ensuremath{\bm\tau}}
\newcommand{\trajectoryDist}{\ensuremath{p(\stateControlTraj)}}
\newcommand{\controlDist}{\ensuremath{p(\control_\timeInd \mid \state_\timeInd)}}

\newcommand{\optimalVar}{\ensuremath{\mathcal{O}}}
%\newcommand{\dynamicsFunc}{\ensuremath{f_{k_*}}}
\newcommand{\dynamicsFunc}{\ensuremath{f}}
\newcommand{\costFunc}{\ensuremath{c}}
\newcommand{\monotonicFunc}{\ensuremath{g}}

\newcommand{\optimalProb}{\ensuremath{\Pr(\optimalVar_\timeInd = 1 \mid \state_\timeInd, \control_\timeInd)}}
\newcommand{\optimalDist}{\ensuremath{P(\optimalVar_\timeInd \mid \state_\timeInd, \control_\timeInd)}}

\newcommand{\marginalLikelihood}{\ensuremath{p(\optimalVar_{1:\TimeInd} = 1)}}
\newcommand{\jointDist}{\ensuremath{p(\optimalVar_{1:\TimeInd}=1, \state_{1:\TimeInd}, \control_{1:\TimeInd})}}
#+END_EXPORT
cite:toussaintRobot2009 formulate trajectory optimisation (and stochastic optimal control) as
approximate inference and derive a local approximation
where the maximum likelihood solution returns the deterministic optimal trajectory, and the local Gaussian belief
approximation is equivalent to the LQG perturbation model around the optimal trajectory.
cite:levineReinforcement2018 followed a similar approach formulating RL as probabilistic inference
and showing how different approximate inference algorithms instantiated different popular RL algorithms.


To formulate the optimal control problem as probabilistic inference we follow cite:levineReinforcement2018
and embed the control problem into a graphical model (see Figure ref:fig-basic-control-graphical-model).
The joint probability model (over a trajectory) is augmented with an
additional variable to encode the notion of cost (or reward)
over the trajectory (see Figure ref:fig-augmented-control-graphical-model).
The new variable is a Bernoulli random variable $\optimalVar_\timeInd \in \{0, 1\}$, that indicates if
time step $\timeInd$ is /optimal/ $\optimalVar_\timeInd=1$, or not /optimal/ $\optimalVar_\timeInd=0$.
The probability $\Pr(\optimalVar_\timeInd = 1 \mid \state_\timeInd, \control_\timeInd)$ of this
Bernoulli distribution can be formulated by mapping the negative cost through a monotonically increasing
function $\monotonicFunc$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\optimalProb &\propto \monotonicFunc( -\costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
We will return to the choice of $\monotonicFunc$ in Section \todo{insert ref to section}
as it leads to our algorithm resembling different well-known algorithms.

this is from cite:watsonStochastic2021
Control-as-inference techniques use a belief in optimality
to perform optimal control [1], a binary random variable
O ∈ {0, 1}, for which 1 indicates optimality. A convenient
approach for this likelihood is an exponential transform of
the cost [48]. The resulting Boltzmann distribution introduces
inverse ‘temperature’ α to scale the cost,
p(O=1|x,u) ∝ exp(−αC(x,u)). (10)
As a result, the negative log-likelihood is an affine transform
of the cost, which preserves convexity. In this work, we
consider when the distribution in (10) is the multivariate
Normal distribution, for which C is a Mahalanobis distance
from a target state z
∗
is space z ∈ R
dz via transform h,

*** graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[b]
  \centering
  \begin{subfigure}{.45\textwidth}
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);
    \end{tikzpicture}
    %}
  \caption{Graphical model with states and controls.}
\label{fig-basic-control-graphical-model}
\end{subfigure}
\begin{subfigure}{.45\textwidth}
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[obs, above=of u1] (o1) {$\optimalVar_1$};
      \node[obs, right=of o1] (o2) {$\optimalVar_2$};
      \node[obs, right=of o2] (o3) {$\optimalVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);
    \end{tikzpicture}
    %}
  \caption{Graphical model with optimality variable.}
\label{fig-augmented-control-graphical-model}
\end{subfigure}
\caption{Graphical models of control formulated as inference.}
\label{fig-control-graphical-model}
\end{figure}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{figure}[h!]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[latent, above=of u1] (o1) {$\optimalVar_1$};
      \node[latent, right=of o1] (o2) {$\optimalVar_2$};
      \node[latent, right=of o2] (o3) {$\optimalVar_3$};

      \node[latent, below=of x1] (a1) {$\modeVar_1$};
      \node[latent, right=of a1] (a2) {$\modeVar_2$};
      \node[latent, right=of a2] (a3) {$\modeVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);

      \draw[->] (x1)--(a1);
      \draw[->] (u1) to [out=-90,in=30] (a1);
      \draw[->] (x2)--(a2);
      \draw[->] (u2) to [out=-90,in=30] (a2);
      \draw[->] (x3)--(a3);
      \draw[->] (u3) to [out=-90,in=30] (a3);
    \end{tikzpicture}
    %}
\caption{Graphical model with optimality and mode indicator variables.}
\label{fig-constrained-control-graphical-model}
\end{figure}
#+END_EXPORT
*** model :ignore:
\newline
*Joint Probability Model* The marginal likelihood of this augmented model for an optimal trajectory i.e.
for $\optimalVar_\timeInd=1$ for all $\timeInd \in \{1,\ldots,\TimeInd\}$, is obtained by
marginalising the states and controls over the trajectory,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\marginalLikelihood &=
\int \int \jointDist \text{d} \state_{1:\TimeInd} \text{d} \control_{1:\TimeInd} \\
&= \int \int \startStateDist \prod_{\timeInd=1}^{\TimeInd} \optimalProb \transitionDist \controlDist
\text{d} \state_{1:\TimeInd} \text{d} \control_{1:\TimeInd}
\end{align}
#+END_EXPORT
where $\controlDist$ denotes the policy.
In the case of RL the policy is parameterised with parameters $\theta$ but in control we are usually interested in
optimising the states and controls directly.
The object of interest in this model is the distribution over the control $\control_\timeInd$
conditioned on the state $\state_\timeInd$ and all future time steps being optimal,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\control_{\timeInd} \mid \state_{\timeInd}, \optimalVar_{\timeInd:\TimeInd}=1) =
\frac{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd) \controlDist p(\state_\timeInd)}{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd) p(\state_\timeInd)}.
\end{align}
#+END_EXPORT
The distributions $p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd)$
and $p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd)$
can be obtained via recursive message passing,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd) =
\prod_{\timeInd+1}^{\TimeInd-1}
\int \int \optimalProb p(\state_\timeInd \mid \state_{\timeInd-1}, \control_{\timeInd-1}) \controlDist \text{d}\state_{\timeInd} \text{d}\control_{\timeInd}.
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\todo[inline]{Considering process noise in the dynamics separately from the epistemic (parametric) uncertainty,
Considering process noise in the dynamics separately from the epistemic (parametric) uncertainty,
\begin{align} \label{eq-}
p(\optimalVar_{1:\TimeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})
= \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \optimalDist
p(\state_{\timeInd+1} \mid \dynamicsFunc(\state_\timeInd)) p(\dynamicsFunc(\state_\timeInd) \mid \state_\timeInd, \control_\timeInd)
\controlDist
\end{align}
\begin{align} \label{eq-}
\optimalProb &= \monotonicFunc( -\costFunc(\state_\timeInd, \dynamicsFunc(\state_{\timeInd-1}), \control_\timeInd)) \\
\end{align}
}
#+END_EXPORT

*** Variational Inference
In variational inference the distribution over the latent variables
$p(\state_{1:\TimeInd}, \control_{1:\TimeInd})$
is approximated with another, potentially simpler
(e.g. factorised) distribution $q(\state_{1:\TimeInd}, \control_{1:\TimeInd})$.
In this case, the aim is to approximate the distribution over the optimal policy,
$p(\control_{1:\TimeInd} \mid \state_{1:\TimeInd}, \optimalVar_{1:\TimeInd})$.
#+BEGIN_EXPORT latex
\newcommand{\trajectoryVarDist}{\ensuremath{q(\state_{1:\TimeInd}, \control_{1:\TimeInd})}}
\newcommand{\controlTrajVarDist}{\ensuremath{q(\control_{1:\TimeInd} \mid \state_{1:\TimeInd})}}
\newcommand{\controlVarDist}{\ensuremath{q(\control_{\timeInd})}}
\begin{align} \label{eq-}
\text{log} \marginalLikelihood &= \text{log} \int \int \jointDist \text{d} \state_{1:\TimeInd} \text{d} \control_{1:\TimeInd} \\
&= \text{log} \int \int \jointDist \frac{\trajectoryVarDist}{\trajectoryVarDist}
\text{d} \state_{1:\TimeInd} \text{d} \control_{1:\TimeInd} \\
&= \text{log} \E_\trajectoryVarDist \left[ \frac{\jointDist}{\trajectoryVarDist} \right] \\
&= \text{log} \E_\trajectoryVarDist \left[
\frac{\cancel{\startStateDist} \prod_{\timeInd=1}^{\TimeInd} \optimalProb \cancel{\transitionDist} \controlDist}{\cancel{\startStateDist} \prod_{\timeInd=1}^\TimeInd \cancel{\transitionDist} \controlVarDist}
\right] \\
&\geq \E_\trajectoryVarDist \left[
\sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb - \sum_{\timeInd=1}^\TimeInd \text{log}
\frac{\controlVarDist}{\controlDist} \right] \\
&= \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb \right]
- \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^\TimeInd \text{log}\frac{\controlVarDist}{\controlDist}\right] \\
%- \sum_{\timeInd=1}^\TimeInd \text{KL}\left(\controlVarDist \mid \mid \controlDist \right)
&= \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb
- \text{log}\controlVarDist\right] \\
&= \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \costFunc(\state_\timeInd, \control_\timeInd)
- \text{log}\controlVarDist\right]
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\text{log} \marginalLikelihood &\geq
 \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb
- \text{log}\controlVarDist\right] \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \text{log} \optimalProb \right]
- \E_{\controlVarDist q(\state_{\timdInd})} \left[ \text{log}\controlVarDist \right] \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
- \sum_{\timeInd=1}^{\TimeInd} \underbrace{\E_{\controlVarDist} \left[ \text{log} \controlVarDist \right]}_{\text{-Entropy}} \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
+ \sum_{\timeInd=1}^{\TimeInd} \mathcal{H}(\control_{\timeInd})
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\trajectoryVarDist = \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \transitionDist \controlVarDist
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{myquote}
As shown in \cite{okadaVariational2020}, the choice of $\monotonicFunc$ leads to the optimisation algorithm
resembling different well-known algorithms.
For example, selecting $\monotonicFunc$ to be the exponential function,
\begin{align} \label{eq-}
\optimalProb &= \text{exp}( -\costFunc(\state_\timeInd, \control_\timeInd))
\end{align}
leads to the algorithm resembling Model Predictive Path Integral (MPPI)
\cite{williamsModel2017,williamsInformation2017}.
\end{myquote}
\todo{add detials on Cross Entropy Method (CEM) cite:chuaDeep2018 and other methods}
#+END_EXPORT

*** graphical model :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[obs, above=of u1] (o1) {$\optimalVar_1$};
      \node[obs, right=of o1] (o2) {$\optimalVar_2$};
      \node[obs, right=of o2] (o3) {$\optimalVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);
    \end{tikzpicture}
    %}
  \caption{Graphical model of control formulated as inference.}
\label{fig-control-graphical-model}
\end{figure}
#+END_EXPORT

** Safety
#+BEGIN_EXPORT latex
\newcommand{\controlCost}{\ensuremath{\mathbf{R}}}
\newcommand{\stateCost}{\ensuremath{\mathbf{Q}}}

\newcommand{\stateDynamics}{\ensuremath{\mathbf{A}}}
\newcommand{\controlDynamics}{\ensuremath{\mathbf{B}}}

\newcommand{\stateTrajDist}{\ensuremath{p(\stateTraj)}}
\newcommand{\stateDist}{\ensuremath{p(\state_{\timeInd})}}
\newcommand{\stateMean}{\ensuremath{\bm\mu}}
\newcommand{\stateCov}{\ensuremath{\bm\Sigma}}
\newcommand{\stateDiff}{\ensuremath{\Delta \state}}
\newcommand{\stateDiffMean}{\ensuremath{\bm\mu_{\stateDiff_\timeInd}}}
\newcommand{\stateDiffCov}{\ensuremath{\bm\Sigma_{\stateDiff_\timeInd}}}

\newcommand{\metricTensor}{\ensuremath{\mathbf{G}}}
\newcommand{\jac}{\ensuremath{\mathbf{J}}}
\newcommand{\jacGivenInput}{\ensuremath{p(\jac(\state_\timeInd) \mid \state_\timeInd)}}
\newcommand{\jacDist}{\ensuremath{p(\jac(\state_\timeInd))}}
\newcommand{\jacMean}{\ensuremath{\bm\mu_\jac}}
\newcommand{\jacCov}{\ensuremath{\bm\Sigma_\jac}}

\newcommand{\safeSet}{\ensuremath{\mathcal{C}}}
\newcommand{\StateDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\ControlDomain}{\ensuremath{\mathcal{U}}}

\newcommand{\constraintFunc}{\ensuremath{c}}
\newcommand{\barrierFunc}{\ensuremath{h}}

\newcommand{\gatingThreshold}{\ensuremath{\delta_\gatingFunc}}
#+END_EXPORT
Intuitively, the notion of safety in dynamical systems implies that /bad/ things do not happen.
However, what exactly are /bad/ things, i.e. what is safety?

*Stability* of an equilibrium point has been considered a property of a safety cite:berkenkampSafe2019.
Stability is a property of infinitely long trajectories.
A closed-loop system is said to be stable, if the state remains within some norm-ball around its equilibrium point for
all time steps, and is asymptotically stable if it eventually converges.
# For examples, stability can be considered  safety can be  Lyapunov functions

*Invariance* can also be see as a safety property.

*** Lyapunov Stability
\todo{add section on lyapunov stability}


*** Constraint Satisfaction
Alongside stability, it is common to require constraints on the states $\state$ and controls $\control$
of a controlled system.
For example, an autonomous system may wish to remain in a subset of it's state space where it knows its dynamics model
is valid.
The system may also be subject to constraints on the controls due to physical limitations, e.g.
how quickly a quadcopter can accelerate and turn.
Constraints on this type of safety can be encoded via inequality constraints on the states $\state$
and controls $\control$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
&\constraintFunc_\state(\state_\timeInd) \geq 0, \forall \timeInd \geq 0, \\
&\constraintFunc_\control(\control_\timeInd) \geq 0, \forall \timeInd \geq 0.
\end{align}
#+END_EXPORT
The feasible regions of these constraints can be written as sets,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\StateDomain =&\{ \state \in \R^\StateDim \mid \constraintFunc_\state(\state) \geq 0 \}, \\
\ControlDomain =&\{ \control \in \R^\ControlDim \mid \constraintFunc_\control(\control) \geq 0 \},
\end{align}
#+END_EXPORT
so the constraints can alternatively be written as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\state_\timeInd \in \StateDomain, \control_\timeInd \in \ControlDomain, \forall t \geq 0.
\end{align}
#+END_EXPORT
For a parametric policy $\policy$, the control constraints can be encoded directly into the policy by
parameterising the policy so that its range is restricted to $\ControlDomain$,
i.e. $\policy(\state_\timeInd) \in \ControlDomain$, for all $\state \in \R^\StateDomain$.
The state constraints can be enforced by ensuring that the set $\StateDomain$ is forward invariant.
#+BEGIN_EXPORT latex
\begin{definition}
Given a dynamical system $\dot{\state}(t) = f(\state(t))$ with states $\state(t) \in \StateDomain, \forall t$, then
a set $\safeSet$ is \textbf{forward invariant}, iff, for every possible initial state in the set $\state(0) \in \safeSet$,
all future states remain in the set, i.e. $\state(t) \in \safeSet, \forall t$.
The system is safe with respect to the set $\safeSet$ if the set is \textbf{forward invariant}.
\end{definition}
#+END_EXPORT
There are multiple approaches to enforcing state constraints via invariant sets.
Namely, Lyapanov functions \todo{cite lyapunov}
and control barrier functions cite:amesControl2019.
Lyapanov functions are more restrictive than control barrier functions as they provide stability guarantees
which are not a necessary condition to render $\StateDomain$ forward invariant.

*** Control Barrier Functions
In cite:nagumoUber1942 the authors derived the necessary and sufficient conditions for
set invariance in dynamical systems.
See cite:abrahamManifolds1988 for a modern proof.

*Barrier Certificates* were introduced in cite:prajnaBarrier2006,prajnaSafety2004 to formally guarantee safety
in nonlinear and hybrid systems.
The term /barrier/ is motivated by the optimisation literature where cost functions are augmented with
barrier functions to avoid undesirable regions.
#+BEGIN_EXPORT latex
\begin{theorem}[Nagumo's Theorem]
Given a dynamical system $\dot{\state} = f(\state)$ with $\state \in \R^\StateDim$ and a safe set $\safeSet$, that is the
superlevel set of a smooth function $\barrierFunc : \R^\StateDim \rightarrow \R$,
i.e. $\safeSet = \{ \state \in \R^\StateDim \mid \barrierFunc(\state) \geq 0 \}$ and
$\frac{\partial \barrierFunc}{\partial \state}(\state) \neq 0, \forall \state \in \R^D$
then Nagumo's Theorem gives the necessary and sufficient conditions for set invariance,
\begin{equation} \label{eq-}
\dot{\barrierFunc}(\state) \geq 0 \quad \forall \state \in \partial \safeSet \quad \implies \safeSet \quad \text{is invariant}
\end{equation}
where $\partial\safeSet$ denotes the boundary of the safe set, i.e. $\partial\safeSet = \{\state \in \R^\StateDim \mid \barrierFunc(\state)=  0 \}$.
\end{theorem}
#+END_EXPORT
These conditions have been rediscovered multiple times, in particular
cite:brezisCharacterization1970 and cite:bonyPrincipe1969.


*Control Barrier Certificates*
Safe sets based on control Lyapunov functions (CLF) can be overly restrictive as they render every
sub-level set

Compared with Lyapunov sublevel set based safe regions, control barrier certificates offer a more permissive
notion of safety.

In cite:wangSafe2018 they formulate safety through control barrier certificates.
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-}
\exists \control \quad \text{s.t.} \quad \dot{\barrierFunc}(\state, \control) \geq - \alpha(\barrierFunc(\state))
\implies \safeSet \quad \text{is invariant}
\end{equation}
#+END_EXPORT
where $\alpha$ is an (extended) class $\kappa$ function.


*** Stochastic Safety
*Chance Constraints* The stochastic MPC formulation provides a principled approach to handling chance constraints
by imposing a maximum probability of violation.
The chance constrained, stochastic optimal control problem is then given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-stochastic-mpc}
\min_{\policies} &\E \left[ l_\TimeInd(\state_\TimeInd, \control_\TimeInd) + \sum_{\timeInd=1}^{\TimeInd-1} l_\timeInd(\state_\timeInd, \control_\timeInd) \right] \\
\text{s.t.} \quad &\state_{\timeInd+1} = f(\state_\timeInd, \control_\timeInd) \\
&\control_\timeInd = \pi_\timeInd (\state_\timeInd) \\
&\Pr \{ \state_\timeInd \in \StateDomain_{\timeInd+1} \} \geq p_{\state} \\
&\Pr \{ \control_\timeInd \in \ControlDomain_{\timeInd+1} \} \geq p_{\control}
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{definition}[Probabilistic i-step Reachable Set] \label{def-prs}
A set $\mathcal{R}_\timeInd$ is said to be a probabilistic reachable set ($i$-step PRS) of probability level $p$ if,
\begin{equation}
\Pr(e_\timeInd \in \mathcal{R}_\timeInd \mid e_0=0) \geq p.
\end{equation}
\end{definition}
#+END_EXPORT

$\StateDomain_{\timeInd} = \{ \state_\timeInd \in \StateDomain \mid \gatingFunc(\state_t, \control_t) > 0 \}$

linear state feedback controllers,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-policy}
\pi_\timeInd = \FeedbackGain_\timeInd (\mu^x_i - \state_\timeInd) + \mu^x_i
\end{align}
#+END_EXPORT

*** Latent Constraints
The gating network contains important information regarding our constraints.
In particular, when aiming to remain in a single desired dynamics mode the corresponding gating function
should remain high.
Equivalence of the gating functions in the deterministic setting implies a uniform distribution over
the mode indicator variable.
Given a threshold $\gatingThreshold \geq 0$ the safe set associated with the gating function safety constraints
can be written as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\mathcal{X} &= \{ \state \in \R^\StateDim \mid \gatingFunc(\state) \geq \gatingThreshold \}.
\end{align}
#+END_EXPORT
Given that the gating function is modelled as a Gaussian process
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\Pr(\gatingFunc(\state, \control) \geq \gatingThreshold \mid \state, \control) =
1 - \Phi\left( \frac{\gatingThreshold - \E\left[\gatingFunc(\state, \control) \right]}{\sqrt{\V\left[\gatingFunc(\state, \control) \right]}} \right)
\geq p_\state
\end{align}
#+END_EXPORT

*** old
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\mathcal{X} &= \{ \state \in \R^\StateDim \mid \gatingFunc(\state) > 0 \} \\
\mathcal{X} &= \{ \state \in \R^\StateDim \mid \E\left[ \gatingFunc(\state) > 0 \right] \} \\
\mathcal{X} &= \{ \state \in \R^\StateDim \mid \Pr\left[ \modeVar = k \right] \} \\
\mathcal{X} &= \{ \state \in \R^\StateDim \mid \V\left[ \gatingFunc(\state) < 0.1 \right] \}
\end{align}
#+END_EXPORT
Use RKHS to get something like -
#+BEGIN_EXPORT latex
\begin{lemma}
Assume that $h$ has bounded RKHS norm $|| h' ||_k \leq B$ and that measurements are corrupted by
$\sigma$-sub-Gaussian noise. If $\beta_n^{1/2} = B + 4 \sigma \sqrt{I(y_n ; h) + 1 + \text{ln}(1/\delta)}$, then for all
$a \in \mathcal{A}$ and $n>0$ it holds jointly with probability at least $1-\delta$ that
$|h'(a) - \mu_n(a)| \leq \beta_n^{1/2} \simga_n(a)$.
\end{lemma}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{theorem}
For any $\delta \in (0,1)$, we have with probability at least $1 - \delta$ that $\gatingFunc(\state) > 0$.
\end{theorem}
#+END_EXPORT



#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\mathcal{X} &= \{ \state \in \R^\StateDim \mid \gatingFunc(\state) > 0 \}
\end{align}
#+END_EXPORT

\newpage
** Probabilistic Geometries for Quadratic Costs
*Linear Dynamics*
Approximate dynamics to be linear around a nominal trajectory
$(\nominalStateTraj, \nominalControlTraj)$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\state_{\timeInd+1} &= \dynamicsFunc(\state_{\timeInd}, \control_{\timeInd}) \\
&= \stateDynamics \state_{\timeInd} + \controlDynamics \control_{\timeInd}
\end{align}
#+END_EXPORT
where,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\stateDynamics = \frac{\partial \dynamicsFunc}{\partial \state} \quad
\controlDynamics = \frac{\partial \dynamicsFunc}{\partial \control}
\end{align}
#+END_EXPORT
Given that the mean and covariance functions are twice differentiable
then $\stateDynamics$ and $\controlDynamics$ are also GPs.

*Quadratic Cost*
A popular cost function is the quadratic cost given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
J(\traj, \utraj) = \sum_{\timeInd=1}^\TimeInd
\state_{\timeInd}^T
\stateCost
\state_{\timeInd}
+
\control_{\timeInd}^T
\controlCost
\control_{\timeInd}
\end{align}
#+END_EXPORT
where $\stateCost$ and $\controlCost$ are
user defined, positive semi-definite and positive
definite weight matrices respectively.
If the cost function is not quadratic then a quadratic approximation would be obtained with a Taylor Expansion.
Importantly, given a state trajectory where each state is
normally distributed
$\state_{\timeInd} \sim \mathcal{N}(\stateMean_{\timeInd}, \stateCov_{\timeInd})$
the expected cost for a trajectory can be calculated in closed-form,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\E_{\stateTrajDist} \left[ J(\traj, \utraj) \right]
= \sum_{\timeInd=1}^\TimeInd
\text{tr}(\stateCost \stateCov_\timeInd)
+
\stateMean_{\timeInd}^T
\stateCost
\stateMean_{\timeInd}
+
\control_{\timeInd}^T
\controlCost
\control_{\timeInd}
\end{align}
#+END_EXPORT
*Riemannian Cost Function*
Consider defining the state weight matrix $\stateCost$
as the Riemannian metric tensor $\metricTensor$ and replacing the state
\state_{\timeInd} with the state difference
(\state_{\timeInd} - \state_{\timeInd - 1}) to get the cost,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
J(\traj, \utraj) =& \sum_{\timeInd=1}^\TimeInd
(\state_{\timeInd} - \state_{\timeInd - 1})^T
\metricTensor(\state_{\timeInd})
(\state_{\timeInd} - \state_{\timeInd - 1})
+ \control_{\timeInd}^T \controlCost \control_{\timeInd} \\
=& \sum_{\timeInd=1}^\TimeInd
\stateDiff_{\timeInd}^T
\metricTensor(\state_{\timeInd})
\stateDiff_{\timeInd}
+ \control_{\timeInd}^T \controlCost \control_{\timeInd}.
\end{align}
#+END_EXPORT
This cost function is calculating the length
of the trajectory on the manifold $\manifold$ endowed with
the metric $\metricTensor$.
Note that
$(\state_{\timeInd} - \state_{\timeInd - 1})^T \metricTensor(\state_{\timeInd}) (\state_{\timeInd} - \state_{\timeInd - 1})$
is the curve energy but Soren says that minimising this is equivalent
to minimising the curve length.

Following from the sum and linear transformation rules of normally
distributed random variable the state difference
$\stateDiff_\timeInd =\state_{\timeInd} - \state_{\timeInd - 1}$
is also
normally distributed,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\stateDiff_\timeInd
\sim \mathcal{N}(\stateDiffMean, \stateDiffCov),
\end{align}
#+END_EXPORT
where
$\stateDiffMean=\stateMean_{\timeInd} - \stateMean_{\timeInd-1}$
and $\stateDiffCov = \stateCov_{\timeInd} + \stateCov_{\timeInd-1}$.
The expected cost for a trajectory requires an expectation over the Jacobian $\jacGivenInput$
as well as the state,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\E_{p(\state_\timeInd, \state_{\timeInd-1}, \jac(\state_\timeInd))} \left[ J(\stateTraj, \controlTraj) \right]
= \sum_{\timeInd=1}^\TimeInd
\E_{\stateDist}\left[ \stateDiff_\timeInd^T
\E_{\jacGivenInput}\left[ \jac(\state_\timeInd) \jac(\state_\timeInd)^T \right]
 \stateDiff_\timeInd \right]
\end{align}
#+END_EXPORT
There is no closed-form expression for this expectation due to the Jacobian's dependence on the state.
However, following a mean-field approximation  the Jacobain can be assumed to be independent of the state,
resulting in the expected cost being given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\E \left[ J(\stateTraj, \controlTraj) \right] &=
\sum_{\timeInd=1}^\TimeInd
\E_{\stateDist}\left[ \stateDiff_\timeInd^T
\E_{\jacDist}\left[ \jac(\state_\timeInd) \jac(\state_\timeInd)^T \right]
 \stateDiff_\timeInd \right] \\
&= \sum_{\timeInd=1}^\TimeInd \stateDiffMean^T (\jacMean \jacMean^T + \jacCov) \stateDiffMean
+ \text{tr}\left(
\left(\jacMean \jacMean^T + \jacCov\right)
\stateDiffCov \right)
\end{align}
#+END_EXPORT
It's a bit hacky but we could assumed the Jacobian distribution is given by marginalising the state
and moment matching,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\mathbf{J}(\state_{\timeInd})) =
%\int \mathbf{J}(\state_{\timeInd}) \stateDist \text{d}\state_{\timeInd}
\int p(\mathbf{J}(\state_{\timeInd}) \mid \state_\timeInd)
\mathcal{N}(\state_{\timeInd} \mid \stateMean_\timeInd, \stateCov_\timeInd)
\text{d}\state_{\timeInd}
\approx \mathcal{N}(\mathbf{J}(\state_{\timeInd}) \mid \jacMean, \jacCov)
\end{align}
#+END_EXPORT

** Active Learning
Utilising mutual information for active learning has been well motivated by, for example,
cite:krauseNearOptimal2008.
They highlight that mutual information may lead to a more accurate model than differential entropy.
It has also been shown that minimising the mutual information  is the same as minimising the expected posterior
uncertainty (conditional entropy) in the model cite:ertinMaximum2003.

Given two sets of random variables $\mathbf{X}$ and $\mathbf{F}$ with joint density $p(\mathbf{X}, \mathbf{F})$ the
mutual information is given by cite:coverElements2006,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
I(\mathbf{X};\mathbf{F}) = \int p(\mathbf{X},\mathbf{F}) \text{log}\frac{p(\mathbf{X},\mathbf{F})}{p(\mathbf{X})p(\mathbf{F})}
\text{d}\mathb{X} \text{d}\mathb{F},
\end{align}
#+END_EXPORT
and its well known relationship differential entropy $H(\cdot)$ is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information-entropy}
I(\mathbf{X};\mathbf{F}) =  H(\mathbf{X}) - H(\mathbf{X} \mid \mathbf{F}).
\end{align}
#+END_EXPORT

The goal here is to pick the most informative control trajectory $\controlTraj$ whilst observing $\stateTraj$.

cite:caponeLocalized2020,buisson-fenetActively2020
Active learning can be used to

An approximated mutual information exploration criterion is used

entropy
#+BEGIN_EXPORT latex
\newcommand{\gatingOutput}{\ensuremath{h(\singleInput)}}
\newcommand{\gatingTransition}{\ensuremath{p(\gatingOutput \mid \state_\timeInd, \control_\timeInd)}}
\begin{align} \label{eq-mutual-information}
\underset{\state \in \stateDomain, \control \in \controlDomain}{\text{argmax}} H(\stateTraj, \controlTraj) &=
\underset{\state \in \stateDomain, \control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd H_{\timeInd}(\state_{\timeInd}, \control_{\timeInd}) \\
&= \underset{\state \in \stateDomain, \control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd - \int \gatingTransition
\text{log} \gatingTransition \text{d} \gatingOutput
\end{align}
#+END_EXPORT
In dynamical systems an arbitrary state $\state$ cannot be sampled, so instead, the dynamics must be steered
to $\state$ through the unknown dynamics $\dynamicsFunc$ through a sequence of controls $\control$.
As highlighted by cite:buisson-fenetActively2020, information gain in dynamical systems is therefore
fundamentally different to the static problem addressed by cite:krauseNearOptimal2008.
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
\underset{\control \in \controlDomain}{\text{argmax}} H(\stateTraj, \controlTraj) &=
\underset{\control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd H_{\timeInd}(\state_{\timeInd}, \control_{\timeInd}) \\
&= \underset{\control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd - \int \gatingTransition
\text{log} \gatingTransition \text{d} \gatingOutput \\
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
\underset{\control \in \controlDomain}{\text{argmax}} H(\gatingFunc(\stateTraj, \controlTraj) \mid \controlTraj) &=
\underset{\control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd H_{\timeInd}(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \control_\timeInd)
\end{align}
#+END_EXPORT
where
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
p(\gatingFunc(\state_{1:\TimeInd}, \control_{1:\TimeInd}) \mid \control_{1:\TimeInd})&=
p(\state_0) \prod_{\timeInd=1}^{\TimeInd-1}
\int p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd} \mid \state_{\timeInd}, \control_{\timeInd})
p(\state_{\timeInd} \mid \state_{\timeInd-1}, \control_{\timeInd-1}) p(\control_{\timeInd}) \text{d}\state_\timeInd
\end{align}
#+END_EXPORT

cite:houlsbyBayesian2011 formulate Bayesian active learning for Gaussian process binary classification with a
Bernoulli likelihood.
The objective is to seek the input $\control$ that maximises the decrease in expected posterior entropy,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-posterior-entropy}
\text{arg}\underset{\control}{\max}
H(\bm\theta \mid \mathcal{D})
- \E_{\alpha \sim p(\alpha \mid \control, \mathcal{D})} \left[ H(\bm\theta \mid \alpha, \control, \mathcal{D}) \right].
\end{align}
#+END_EXPORT
The expectation requires the unseen output $\alpha$ and many works have

cite:houlsbyBayesian2011 highlight that the objective in Eq. ref:eq-expected-posterior-entropy is equivalent to
the mutual information between the unknown output and the parameters.
The objective in Eq. ref:eq-expected-posterior-entropy can then be written to compute entropies in the $\alpha$
space.
#+BEGIN_EXPORT latex
\begin{align} \label{eq--entropy-alpha-space}
\text{arg}\underset{\control}{\max}
H(\alpha \mid \control, \mathcal{D})
- \E_{\bm\theta \sim p(\bm\theta \mid\mathcal{D})} \left[ H(\alpha \mid \control, \bm\theta) \right].
\end{align}
#+END_EXPORT
Entropies are now calculated in a low dimensional output (latent) space.
In the binary classification setting Eq. ref:eq--entropy-alpha-space simply involves entropies of Bernoulli
random variables.
Intuitively, the first term $H(\alpha \mid \control, \mathcal{D})$) seeks to
find the input $\control$ where the model is marginally most uncertain about
its corresponding output $\alpha$.
The second term


The entropy of a Bernoulli random variable is given by,
#+BEGIN_EXPORT latex
\newcommand{\gatingFunc}{\ensuremath{g}}
\begin{align} \label{eq--entropy-bernoulli}
H(\alpha \mid \control, \gatingFunc) &= h(\Phi(\gatingFunc(\control))) \\
h(p) &= -p \text{log}p - (1-p) \text{log}(1-p)
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq--entropy--data}
H(\alpha \mid \control, \mathcal{D}) &\approx h \left(\int \Phi(\gatingFunc(\state, \control)) \mathcal{N}(\alpha \mid \mu_\alpha, \sigma^2_\alpha) \text{d} \alpha \right) \\
&= h \left( \Phi \left( \frac{\mu_\alpha}{\sqrt{\sigma^2_\alpha + 1}} \right) \right)
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq--entropy-model}
\E_{\gatingFunc \sim p(\gatingFunc \mid \mathcal{D})} \left[ H(\alpha \mid \control, \gatingFunc) \right]
&\approx \int h \left( \Phi(\gatingFunc(\state, \control)) \mathcal{N}(\gatingFunc(\state, \control) \mid \mu_\gatingFunc, \sigma^2_\gatingFunc) \text{d} \gatingFunc(\state, \control)
\end{align}
#+END_EXPORT
** Results
** Conclusion
* Applications
** 2D Point Mass Simulation
** 3D Quadcopter Simulation
** 2D Quadcopter Real-World
* Conclusion
* Appendix
** Multivariate Normals
** Derivatives of a Gaussian Process
* Back Matter :ignore:
** Bibliography :ignore:

#+BEGIN_EXPORT latex
% \begingroup
% \sloppy
% \setstretch{1}
% \setlength\bibitemsep{3pt}
\printbibliography
% \endgroup
#+END_EXPORT

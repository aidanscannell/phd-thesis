* Config :ignore:
#+latex_class: mimosis
#+begin_src emacs-lisp :exports none  :results none
(unless (boundp 'org-latex-classes)
  (setq org-latex-classes nil))
(add-to-list 'org-latex-classes
             '("memoir"
               "\\documentclass{memoir}
    [NO-DEFAULT-PACKAGES]
    [PACKAGES]
    [EXTRA]
    \\newcommand{\\mboxparagraph}[1]{\\paragraph{#1}\\mbox{}\\\\}
    \\newcommand{\\mboxsubparagraph}[1]{\\subparagraph{#1}\\mbox{}\\\\}"
               ("\\chapter{%s}" . "\\chapter*{%s}")
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
               ;; ("\\mboxparagraph{%s}" . "\\mboxparagraph*{%s}")
               ;; ("\\mboxsubparagraph{%s}" . "\\mboxsubparagraph*{%s}")))
(add-to-list 'org-latex-classes
             '("mimosis"
               "\\documentclass{mimosis-class/mimosis}
  [NO-DEFAULT-PACKAGES]
  [PACKAGES]
  [EXTRA]"
               ("\\chapter{%s}" . "\\addchap{%s}")
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}\\newline" . "\\paragraph*{%s}\\newline")
               ("\\subparagraph{%s}\\newline" . "\\subparagraph*{%s}\\newline")))
#+end_src
# #+EXPORT_FILE_NAME: ./tmp/thesis.pdf
** Org Mode Export Options :noexport:
#+EXCLUDE_TAGS: journal noexport
#+OPTIONS: title:nil toc:nil date:nil author:nil H:6

** Macros :ignore:
# #+MACRO: acronym #+latex_header: \newacronym[description={$1}]{$2}{$2}{$3}
#+MACRO: glossaryentry #+latex_header: \newglossaryentry{$1}{name={$2},description={$3},sort={$4}}
#+MACRO: acronym #+latex_header: \newacronym{$1}{$2}{$3}
# #+MACRO: newline @@latex:\hspace{0pt}\\@@ @@html:<br>@@
# #+MACRO: fourstar @@latex:\bigskip{\centering\color{BrickRed}\FourStar\par}\bigskip@@
# #+MACRO: clearpage @@latex:\clearpage@@@@odt:<text:p text:style-name="PageBreak"/>@@

** LaTeX Export Headers and Options :noexport:
*** Packages :ignore:
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{amsmath,amssymb,amsfonts}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{todonotes}
*** Font Awesome icons
#+LATEX_HEADER: \usepackage{fontawesome}
*** Maths cancel
#+LATEX_HEADER: \usepackage[makeroom]{cancel}
*** Footnotes
#+LATEX_HEADER: \usepackage{footnote}
*** Tensor indexing (pre subscripts)
#+LATEX_HEADER: \usepackage{tensor}

*** Epigraph (chapter quotes)
#+LATEX_HEADER: \usepackage{epigraph}
*** Grey box for block quotes
#+LATEX_HEADER: \usepackage[most]{tcolorbox}
#+LATEX_HEADER: \definecolor{block-gray}{gray}{0.85}
#+LATEX_HEADER: \newtcolorbox{myquote}{colback=block-gray,boxrule=0pt,boxsep=0pt,breakable}
# #+LATEX_HEADER: \newtcolorbox{myquote}{colback=block-gray,grow to right by=-10mm,grow to left by=-10mm, boxrule=0pt,boxsep=0pt,breakable}
*** Acronym and Glossary :ignore:
#+latex_header: \usepackage[acronym]{glossaries}
#+latex_header: \makeglossaries

*** Equation Definitions

#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \newcommand{\defeq}{\vcentcolon=}

*** Create a Definition theorem
#+LATEX_HEADER: \newtheorem{definition}{Definition}[section]
#+LATEX_HEADER: \newtheorem{assumption}{Assumption}[section]
#+LATEX_HEADER: \newtheorem{theorem}{Theorem}[section]
#+LATEX_HEADER: \newtheorem{lemma}{Lemma}[section]
#+LATEX_HEADER: \newtheorem*{remark}{Remark}
*** Floating images configuration

By default,  if a figure consumes 60% of the page it will get its own float-page. To change that we have to adjust the value of the floatpagefraction derivative.
#+latex_header: \renewcommand{\floatpagefraction}{.8}%

See more information [[https://tex.stackexchange.com/questions/68516/avoid-that-figure-gets-its-own-page][here]].

*** Hyperref
Self-explanatory.
#+latex_header: \usepackage[colorlinks=true, citecolor=BrickRed, linkcolor=BrickRed, urlcolor=BrickRed]{hyperref}

*** Cleverref
#+latex_header: \usepackage[capitalise,noabbrev]{cleveref}
*** Bookmarks

The bookmark package implements a new bookmark (outline) organisation for package hyperref. This lets us change the "tree-navigation" associated with the generated pdf and constrain the menu only to H:2.
#+latex_header: \usepackage{bookmark}
#+latex_header: \bookmarksetup{depth=2}

*** BBding

Symbols such as diamond suit, which can be used for aesthetically separating paragraphs, could be added with the package =fdsymbol=. I'll use bbding which offers the more visually appealing =\FourStar=. I took this idea from seeing the thesis of the mimosis package author.
#+latex_header: \usepackage{bbding}

*** CS Quotes
The [[https://ctan.org/pkg/csquotes][csquotes]] package offers context sensitive quotation facilities, improving the typesetting of inline quotes.

Already imported by mimosis class.
# #+latex_header: \usepackage{csquotes}

To enclose quote environments with quotes from csquotes, see [[https://tex.stackexchange.com/questions/365231/enclose-a-custom-quote-environment-in-quotes-from-csquotes][the following TeX SE thread]].

#+latex_header: \def\signed #1{{\leavevmode\unskip\nobreak\hfil\penalty50\hskip1em
#+latex_header:   \hbox{}\nobreak\hfill #1%
#+latex_header:   \parfillskip=0pt \finalhyphendemerits=0 \endgraf}}

#+latex_header: \newsavebox\mybox
#+latex_header: \newenvironment{aquote}[1]
#+latex_header: {\savebox\mybox{#1}\begin{quote}\openautoquote\hspace*{-.7ex}}
#+latex_header:    {\unskip\closeautoquote\vspace*{1mm}\signed{\usebox\mybox}\end{quote}}

And then use quotes as:
#+begin_example
# The options derivative adds text after the environment. We use it to add the author.
#+ATTR_LATEX: :options {\cite{Frahm1994}}
#+begin_aquote
/Current (fMRI) applications often rely on "effects" or "statistically significant differences", rather than on a proper analysis of the relationship between neuronal activity, haemodynamic consequences, and MRI physics./
#+end_aquote
#+end_example

Note that org-ref links won't work here because the attr latex will be pasted as-is in the .tex file.

*** Date Time

The date time package allows us to specify a "formatted" date object, which will print different formats according to the current locale & language. I use this in my title page.
#+latex_header: \usepackage[level]{datetime}

*** Bibliography
General configuration.
# #+latex_header: \usepackage[autocite=plain, backend=biber, doi=true, url=true, hyperref=true,uniquename=false, maxbibnames=99, maxcitenames=2, sortcites=true, style=authoryear-comp]{biblatex}
# #+LATEX_HEADER: \usepackage[citestyle=authoryear-comp, maxcitenames=2, maxbibnames=99, doi=false, isbn=false, eprint=false, backend=bibtex, hyperref=true, url=false, natbib=true, style=authoryear-comp]{biblatex}
#+LATEX_HEADER: \usepackage[citestyle=authoryear-comp, maxcitenames=2, maxbibnames=99, doi=false, isbn=false, eprint=false, backend=bibtex, hyperref=true, url=false, natbib=true, style=authoryear-comp]{biblatex}
# #+LATEX_HEADER: \addbibresource{~/Dropbox/org/ref/mendeley/library.bib}
#+LATEX_HEADER: \addbibresource{~/Dropbox/org/ref/zotero-library.bib}

Improvements provided with the Mimosis class.
# #+latex_header: \input{bibliography-mimosis}

# And fix the andothers to show et al in English as well:
# #+latex_header: \DefineBibliographyStrings{english}{andothers={\textit{et\, al\adddot}}} 
# #+latex_header:\DefineBibliographyStrings{english}{and={\textit{and}}}


Remove ISSN, DOI and URL to shorten the bibliography.
#+latex_header: \AtEveryBibitem{%
#+latex_header:   \clearfield{urlyear}
#+latex_header:   \clearfield{urlmonth}
#+latex_header:   \clearfield{note}
#+latex_header:  \clearfield{issn} % Remove issn
#+latex_header:  \clearfield{doi} % Remove doi
#+latex_header: \ifentrytype{online}{}{% Remove url except for @online
#+latex_header:   \clearfield{url}
#+latex_header: }
#+latex_header: }

And increase the spacing between the entries, as per default they are too small.
#+latex_header: \setlength\bibitemsep{1.1\itemsep}

Also reduce the font-size
#+latex_header: \renewcommand*{\bibfont}{\footnotesize}

*** Improve chapter font colors and font size
The following commands make chapter numbers BrickRed, which look like the Donders color.
#+latex_header: \makeatletter
#+latex_header: \renewcommand*{\chapterformat}{  \mbox{\chapappifchapterprefix{\nobreakspace}{\color{BrickRed}\fontsize{40}{45}\selectfont\thechapter}\autodot\enskip}}
#+latex_header: \renewcommand\@seccntformat[1]{\color{BrickRed} {\csname the#1\endcsname}\hspace{0.3em}}
#+latex_header: \makeatother

*** Setspace for controlling line spacing

Already imported when using mimosis.
# #+latex_header: \usepackage{setspace}
#+latex_header: \setstretch{1.25} 

*** Parskip

Fine tuning of spacing between paragraphs. See [[https://tex.stackexchange.com/questions/161254/smaller-parskip-than-half-for-koma-script][thread here]].

#+latex_header: \setparsizes{0em}{0.1\baselineskip plus .1\baselineskip}{1em plus 1fil}

*** Table of Contents improvements

# TOC only the chapters, not their content.
# #+latex_header: \setcounter{tocdepth}{1}
#+latex_header: \setcounter{tocdepth}{2}

*** Possible Equation improvements

Make the equation numbers follow the chapter, not the whole thesis.
#+latex_header: \numberwithin{equation}{chapter}

*** TikZ and bayesnet for graphical models
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{bayesnet}

*** Notes in margins
# #+LATEX_HEADER: \usepackage{geometry}
#+LATEX_HEADER: \setlength{\marginparwidth}{3cm}
# #+LATEX_HEADER: \xdef\marginnotetextwidth{\textwidth}
#+LATEX_HEADER: \usepackage{marginnote}
# #+LATEX_HEADER: \renewcommand*{\marginfont}{\footnotesize}
# #+LATEX_HEADER: \newcommand{\parmarginnote}[1]{\hspace{\z@}\marginnote{#1}\ignorespaces}
#+LATEX_HEADER: \newcommand{\parmarginnote}[1]{\marginnote{#1}}
*** Captions
# #+LATEX_HEADER: \usepackage{caption}
# #+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \ifCLASSOPTIONcompsoc \usepackage[caption=false,font=footnotesize,labelfon
#+LATEX_HEADER: t=it,textfont=it]{subfig} \else
#+LATEX_HEADER: \usepackage[caption=false,font=footnotesize]{subfig}
#+LATEX_HEADER: \fi
#+LATEX_HEADER: \usepackage[format=plain,labelfont={bf},textfont=it]{caption} % make captions italic
*** Maths diag
#+LATEX_HEADER: \newcommand{\diag}{\mathop{\mathrm{diag}}}
** Text Variables :noexport:
#+latex_header: \newcommand{\ThesisTitle}{{Probabilistic Inference for Learning \& Control in Multimodal Dynamical Systems}}
# #+latex_header: \newcommand{\ThesisTitle}{{Data Efficient Learning for Control in Multimodal Dynamical Systems}}
#+latex_header: \newcommand{\ThesisSubTitle}{Synergising Bayesian Inference and Riemannian Geometry for Control}
#+latex_header: \newcommand{\FormattedThesisDefenseDate}{\mbox{\formatdate{1}{1}{2100}}}
#+latex_header: \newcommand{\FormattedAuthorDateOfBirth}{\mbox{\formatdate{1}{1}{2000}}}
#+latex_header: \newcommand{\FormattedThesisDefenseTime}{\mbox{10:00}}
#+latex_header: \newcommand{\AuthorShortName}{\mbox{Aidan Scannell}}
#+latex_header: \newcommand{\AuthorFullName}{\mbox{Aidan J. Scannell}}
#+latex_header: \newcommand{\ThesisISBN}{\mbox{}}

** Math Variables :noexport:
#+LATEX_HEADER: \DeclareMathOperator{\R}{\mathbb{R}}
#+LATEX_HEADER: \DeclareMathOperator{\E}{\mathbb{E}}
#+LATEX_HEADER: \DeclareMathOperator{\V}{\mathbb{V}}
#+LATEX_HEADER: \DeclareMathOperator{\K}{\mathbf{K}}

*** Num Data / Mode / State Dimension / Control Dimension (k, d, t/n)
#+LATEX_HEADER: \newcommand{\numData}{\ensuremath{t}}
# #+LATEX_HEADER: \newcommand{\numData}{\ensuremath{n}}
#+LATEX_HEADER: \newcommand{\numEpisodes}{\ensuremath{e}}
#+LATEX_HEADER: \newcommand{\numTimesteps}{\ensuremath{t}}
#+LATEX_HEADER: \newcommand{\numInd}{\ensuremath{m}}
#+LATEX_HEADER: \newcommand{\stateDim}{\ensuremath{d}}
#+LATEX_HEADER: \newcommand{\controlDim}{\ensuremath{f}}
#+LATEX_HEADER: \newcommand{\modeInd}{\ensuremath{k}}
#+LATEX_HEADER: \newcommand{\modeDesInd}{\ensuremath{\text{des}}}
#+LATEX_HEADER: \newcommand{\testInd}{\ensuremath{*}}
#+LATEX_HEADER: \newcommand{\NumData}{\ensuremath{\MakeUppercase{\numData}}}
#+LATEX_HEADER: \newcommand{\NumInd}{\ensuremath{\MakeUppercase{\numInd}}}
# #+LATEX_HEADER: \newcommand{\StateDim}{\ensuremath{\MakeUppercase{\stateDim}}}
# #+LATEX_HEADER: \newcommand{\ControlDim}{\ensuremath{\MakeUppercase{\controlDim}}}
#+LATEX_HEADER: \newcommand{\StateDim}{\ensuremath{{D_x}}}
#+LATEX_HEADER: \newcommand{\ControlDim}{\ensuremath{{D_u}}}
#+LATEX_HEADER: \newcommand{\ModeInd}{\ensuremath{\MakeUppercase{\modeInd}}}
#+LATEX_HEADER: \newcommand{\NumEpisodes}{\MakeUppercase{\numEpisodes}}
#+LATEX_HEADER: \newcommand{\NumTimesteps}{\MakeUppercase{\numTimesteps}}

# Macros for single/all data notation
#+LATEX_HEADER: \newcommand{\singleData}[1]{\ensuremath{#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\allData}[1]{\ensuremath{\MakeUppercase{#1}}}
# #+LATEX_HEADER: \newcommand{\singleData}[1]{\ensuremath{#1_{\numData}}}
# #+LATEX_HEADER: \newcommand{\allData}[1]{\ensuremath{#1}}
# #+LATEX_HEADER: \newcommand{\allData}[1]{\ensuremath{#1_{1:\NumData}}}

# Macros for data dimensions
# #+LATEX_HEADER: \newcommand{\singleDataDim}[1]{\ensuremath{#1_{\stateDim, \numData}}}
#+LATEX_HEADER: \newcommand{\singleDataDim}[1]{\ensuremath{_{\stateDim}#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\singleDim}[1]{\ensuremath{#1_{\stateDim}}}
# #+LATEX_HEADER: \newcommand{\singleDim}[1]{\ensuremath{_{\stateDim}#1}}
# #+LATEX_HEADER: \newcommand{\singleDimi}[2]{\ensuremath{\tensor*[_{#2}]{#1}{}}}
#+LATEX_HEADER: \newcommand{\singleDim}[1]{\ensuremath{\singleDimi{#1}{\stateDim}}}

# Macros for mode k notation
# #+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{#1^{(\modeInd)}}}
# #+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{#1^{\modeInd}}}
# #+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{\tensor*[^{\modeInd}]{#1}{}}}
#+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{#1_{\modeInd}}}
#+LATEX_HEADER: \newcommand{\modeDes}[1]{\ensuremath{#1^{\modeDesInd}}}

#+LATEX_HEADER: \newcommand{\singleDimiMode}[2]{\ensuremath{\tensor*[_#2^\modeInd]{#1}{}}}
#+LATEX_HEADER: \newcommand{\singleDimMode}[1]{\ensuremath{\singleDimiMode{#1}{\stateDim}}}
#+LATEX_HEADER: \newcommand{\singleDimModeData}[1]{\ensuremath{\tensor*[_\stateDim^\modeInd]{#1}{_\numData}}}

*** Data set
# Dataset/inputs/outputs
#+LATEX_HEADER: \newcommand{\state}{\ensuremath{\mathbf{x}}}
#+LATEX_HEADER: \newcommand{\control}{\ensuremath{\mathbf{u}}}
# #+LATEX_HEADER: \newcommand{\control}{\ensuremath{\mathbf{a}}}

#+LATEX_HEADER: \newcommand{\x}{\ensuremath{\mathbf{x}}}
# #+LATEX_HEADER: \newcommand{\y}{\ensuremath{\mathbf{y}}}
#+LATEX_HEADER: \newcommand{\y}{\ensuremath{y}}
# #+LATEX_HEADER: \newcommand{\x}{\ensuremath{\hat{\state}}}
# #+LATEX_HEADER: \newcommand{\y}{\ensuremath{\Delta\state}}
#+LATEX_HEADER: \newcommand{\dataset}{\ensuremath{\mathcal{D}}}

# Single/all input/output notation
# #+LATEX_HEADER: \newcommand{\singleInput}{\ensuremath{\singleData{\x}}}
#+LATEX_HEADER: \newcommand{\singleInput}{\ensuremath{\x_{\numData-1}}}
#+LATEX_HEADER: \newcommand{\singleOutput}{\ensuremath{\singleData{\y}}}
#+LATEX_HEADER: \newcommand{\allInput}{\ensuremath{\allData{\x}}}
#+LATEX_HEADER: \newcommand{\allOutput}{\ensuremath{\allData{\y}}}

# Single/all state/control notation
#+LATEX_HEADER: \newcommand{\singleState}{\ensuremath{\state_{\numData-1}}}
#+LATEX_HEADER: \newcommand{\singleControl}{\ensuremath{\control_{\numData-1}}}
#+LATEX_HEADER: \newcommand{\allState}{\ensuremath{\allData{\state}}}
#+LATEX_HEADER: \newcommand{\allControl}{\ensuremath{\allData{\control}}}

*** Noise Vars
#+LATEX_HEADER: \newcommand{\noiseVar}{\ensuremath{\sigma}}
#+LATEX_HEADER: \newcommand{\noiseVarK}{\ensuremath{\mode{\noiseVar}}}
#+LATEX_HEADER: \newcommand{\noiseVarOneK}{\ensuremath{\singleDimiMode{\noiseVar}{1}}}
#+LATEX_HEADER: \newcommand{\noiseVarDK}{\ensuremath{\singleDimiMode{\noiseVar}{\StateDim}}}
#+LATEX_HEADER: \newcommand{\noiseVardK}{\ensuremath{\singleDimMode{\noiseVar}}}
# #+LATEX_HEADER: \newcommand{\noiseVarOneK}{\ensuremath{\noiseVarK_{1}}}
# #+LATEX_HEADER: \newcommand{\noiseVarDK}{\ensuremath{\noiseVarK_{\StateDim}}}
# #+LATEX_HEADER: \newcommand{\noiseVardK}{\ensuremath{\noiseVarK_{\stateDim}}}
# #+LATEX_HEADER: \newcommand{\noiseVardK2}{\ensuremath{\left(\noiseVardK\right)^2}}

*** Mode Indicator Variable
#+LATEX_HEADER: \newcommand{\modeVar}{\ensuremath{\alpha}}
#+LATEX_HEADER: \newcommand{\modeVarn}{\ensuremath{\singleData{\modeVar}}}
#+LATEX_HEADER: \newcommand{\ModeVar}{\ensuremath{\bm{\modeVar}}}
# #+LATEX_HEADER: \newcommand{\ModeVar}{\ensuremath{\allData{\bm{\modeVar}}}}
#+LATEX_HEADER: \newcommand{\modeVarK}{\ensuremath{\modeVarn=\modeInd}}
# #+LATEX_HEADER: \newcommand{\ModeVarK}{\ensuremath{\mode{\bm{\modeVar}}}}
#+LATEX_HEADER: \newcommand{\ModeVarK}{\ensuremath{\ModeVar_{\modeInd}}}

*** Tensor Indexing
# Experts indexing
#+LATEX_HEADER: \newcommand{\nkd}[1]{\ensuremath{#1_{\numData,\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\nkD}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\NkD}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\nKD}[1]{\ensuremath{#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\Nkd}[1]{\ensuremath{#1_{:,\modeInd,\stateDim}}}

# Gating function indexing
#+LATEX_HEADER: \newcommand{\nk}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\Nk}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\nK}[1]{\ensuremath{#1_{\numData}}}

# Experts Inducing indexing
#+LATEX_HEADER: \newcommand{\mkd}[1]{\ensuremath{#1_{\numData,\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\mkD}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\MkD}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\mKD}[1]{\ensuremath{#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\Mkd}[1]{\ensuremath{#1_{:,\modeInd,\stateDim}}}

# Gating Inducing indexing
#+LATEX_HEADER: \newcommand{\mk}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\Mk}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\mK}[1]{\ensuremath{#1_{\numData}}}

# Desired Mode Gating indexing
#+LATEX_HEADER: \newcommand{\MDes}[1]{\ensuremath{#1_{:, k^*}}}

*** Gating Network New
# Function notation
#+LATEX_HEADER: \newcommand{\gatingFunc}{\ensuremath{h}}
#+LATEX_HEADER: \newcommand{\hk}{\ensuremath{\mode{\gatingFunc}}}

# Single data notation
#+LATEX_HEADER: \newcommand{\hkn}{\ensuremath{\nk{\gatingFunc}}}
#+LATEX_HEADER: \newcommand{\hn}{\ensuremath{\nK{\mathbf{\gatingFunc}}}}

# All inputs set/vector/tensor notation
#+LATEX_HEADER: \newcommand{\GatingFunc}{\ensuremath{\mathbf{\gatingFunc}}}
#+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\MakeUppercase\GatingFunc}}
#+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\Nk{\GatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\allData{\GatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\allData{\mode{\GatingFunc}}}}

*** Experts New
# Function notation
#+LATEX_HEADER: \newcommand{\latentFunc}{\ensuremath{f}}
#+LATEX_HEADER: \newcommand{\LatentFunc}{\ensuremath{\mathbf{\latentFunc}}}
#+LATEX_HEADER: \newcommand{\fkd}{\ensuremath{\latentFunc_{\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\fk}{\ensuremath{\mathbf{\latentFunc}_{\modeInd}}}
# #+LATEX_HEADER: \newcommand{\fk}{\ensuremath{\latentFunc_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\f}{\ensuremath{\mathbf{f}}}

# Vector/Matrix/Tensor notation
#+LATEX_HEADER: \newcommand{\F}{\ensuremath{\MakeUppercase{\mathbf{\latentFunc}}}}
# #+LATEX_HEADER: \newcommand{\Fnkd}{\ensuremath{\latentFunc_{\numData, \modeInd, \stateDim}}}
# #+LATEX_HEADER: \newcommand{\Fnk}{\ensuremath{\mathbf{\latentFunc}_{\numData, \modeInd}}}
# #+LATEX_HEADER: \newcommand{\Fk}{\ensuremath{\F_{:,\modeInd}}}
# #+LATEX_HEADER: \newcommand{\Fn}{\ensuremath{\F_{\numData}}}
#+LATEX_HEADER: \newcommand{\Fnkd}{\ensuremath{\nkd{\latentFunc}}}
#+LATEX_HEADER: \newcommand{\Fnk}{\ensuremath{\nkD{\mathbf{\latentFunc}}}}
#+LATEX_HEADER: \newcommand{\Fk}{\ensuremath{\NkD{\F}}}
#+LATEX_HEADER: \newcommand{\Fn}{\ensuremath{\nKD{\F}}}
#+LATEX_HEADER: \newcommand{\F}{\ensuremath{\F}}

# #+LATEX_HEADER: \newcommand{\Fdk}{\ensuremath{\mathbf{\latentFunc}_{:,\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\Nkd{\mathbf{\latentFunc}}}}

# Single input notation
#+LATEX_HEADER: \newcommand{\fn}{\ensuremath{\Fn}}
#+LATEX_HEADER: \newcommand{\fkn}{\ensuremath{\Fnk}}
#+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\Fnkd}}

# All inputs set/vector/tensor notation
# #+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\Fdk}}

*** Params
#+LATEX_HEADER: \newcommand{\gatingParams}{\ensuremath{\bm\phi}}
#+LATEX_HEADER: \newcommand{\expertParams}{\ensuremath{\bm\theta}}
#+LATEX_HEADER: \newcommand{\gatingParamsK}{\ensuremath{\mode{\bm\phi}}}
#+LATEX_HEADER: \newcommand{\expertParamsK}{\ensuremath{\mode{\bm\theta}}}
*** Sparse GPs
**** Experts
***** Variables
#+LATEX_HEADER: \newcommand{\uf}{\ensuremath{u}}
#+LATEX_HEADER: \newcommand{\uFkd}{\ensuremath{\Mkd{\mathbf{\uf}}}}
#+LATEX_HEADER: \newcommand{\uFk}{\ensuremath{\MkD{\MakeUppercase{\mathbf{\uf}}}}}
#+LATEX_HEADER: \newcommand{\uF}{\ensuremath{\MakeUppercase{\mathbf{\uf}}}}

***** Inputs
# #+LATEX_HEADER: \newcommand{\zf}{\ensuremath{\bm{\zeta}}}
# #+LATEX_HEADER: \newcommand{\zFkd}{\ensuremath{\Mkd{\zf}}}
# #+LATEX_HEADER: \newcommand{\zFk}{\ensuremath{\MkD{\zf}}}
# #+LATEX_HEADER: \newcommand{\zF}{\ensuremath{\MKD{\zf}}}
#+LATEX_HEADER: \newcommand{\zf}{\ensuremath{\mathbf{Z}}}
#+LATEX_HEADER: \newcommand{\zFkd}{\ensuremath{\Mkd{\zf}}}
#+LATEX_HEADER: \newcommand{\zFk}{\ensuremath{\MkD{\zf}}}
#+LATEX_HEADER: \newcommand{\zF}{\ensuremath{\MKD{\zf}}}

**** Gating
***** Variables
#+LATEX_HEADER: \newcommand{\uh}{\ensuremath{U}}
#+LATEX_HEADER: \newcommand{\uHk}{\ensuremath{\Mk{\hat{\mathbf{\uh}}}}}
#+LATEX_HEADER: \newcommand{\uH}{\ensuremath{\hat{\MakeUppercase{\mathbf{\uh}}}}}

#+LATEX_HEADER: \newcommand{\hu}{\ensuremath{\uh}}
#+LATEX_HEADER: \newcommand{\Hku}{\ensuremath{\uHk}}
#+LATEX_HEADER: \newcommand{\Hu}{\ensuremath{\uH}}

***** Inputs
# #+LATEX_HEADER: \newcommand{\zh}{\ensuremath{\bm{\xi}}}
# #+LATEX_HEADER: \newcommand{\zHk}{\ensuremath{\Mk{\zh}}}
# #+LATEX_HEADER: \newcommand{\zH}{\ensuremath{\MK{\zh}}}
#+LATEX_HEADER: \newcommand{\zh}{\ensuremath{\hat{\mathbf{Z}}}}
#+LATEX_HEADER: \newcommand{\zHk}{\ensuremath{\Mk{\zh}}}
#+LATEX_HEADER: \newcommand{\zH}{\ensuremath{\MK{\zh}}}

# #+LATEX_HEADER: \newcommand{\zHDes}{\ensuremath{\zH_{:, k^*}}}
#+LATEX_HEADER: \newcommand{\zHDes}{\ensuremath{\MDes{\zH}}}

**** Misc
#+LATEX_HEADER: \newcommand{\Z}{\ensuremath{\mathbf{Z}}}
**** Old
# Sparse GP macro
# #+LATEX_HEADER: \newcommand{\inducing}[1]{\ensuremath{\hat{#1}}}

# #+LATEX_HEADER: \newcommand{\fu}{\ensuremath{\inducing{\latentFunc}}}
# #+LATEX_HEADER: \newcommand{\Fu}{\ensuremath{\inducing{\mathbf{\latentFunc}}}}
# #+LATEX_HEADER: \newcommand{\Fku}{\ensuremath{\mode{\inducing{\mathbf{\latentFunc}}}}}
# #+LATEX_HEADER: \newcommand{\Fkdu}{\ensuremath{\singleDim{\Fku}}}
# #+LATEX_HEADER: \newcommand{\hu}{\ensuremath{\inducing{\gatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hu}{\ensuremath{\inducing{\mathbf{\gatingFunc}}}}
# #+LATEX_HEADER: \newcommand{\Hku}{\ensuremath{\mode{\inducing{\mathbf{\gatingFunc}}}}}

# #+LATEX_HEADER: \newcommand{\Zfk}{\ensuremath{\mode{\mathbf{Z}}_{\latentFunc}}}
# #+LATEX_HEADER: \newcommand{\Zfk}{\ensuremath{\mode{\bm{\zeta}}}}
# #+LATEX_HEADER: \newcommand{\Zf}{\ensuremath{\mathbf{Z}}}
# #+LATEX_HEADER: \newcommand{\Zf}{\ensuremath{\mathbf{Z}_{\latentFunc}}}

# #+LATEX_HEADER: \newcommand{\Zhk}{\ensuremath{\mode{\mathbf{Z}}_{\gatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Zh}{\ensuremath{\bm{\xi}}}
# #+LATEX_HEADER: \newcommand{\Zhk}{\ensuremath{\mode{\Zh}}}

# #+LATEX_HEADER: \newcommand{\ZhDes}{\ensuremath{\modeDes{\zH}}}

*** Continuous
#+LATEX_HEADER: \newcommand{\derivative}[1]{\ensuremath{\dot{#1}}}
#+LATEX_HEADER: \newcommand{\stateDerivative}{\ensuremath{\derivative{\state}}}
# #+LATEX_HEADER: \newcommand{\stateDerivative}{\ensuremath{\dot{\mathbf{x}}}}

*** Prob Dists New
#+LATEX_HEADER: \newcommand{\pFkd}{\ensuremath{p\left(\Fkd \right)}}
*** Prob Dists
#+LATEX_HEADER: \newcommand{\pFkd}{\ensuremath{p\left(\Fkd \mid \allInput \right)}}
#+LATEX_HEADER: \newcommand{\pFk}{\ensuremath{p\left(\Fk \mid \allInput, \expertParams\right)}}

#+LATEX_HEADER: \newcommand{\pF}{\ensuremath{p\left(\F \mid \allInput, \expertParams\right)}}
#+LATEX_HEADER: \newcommand{\pfk}{\ensuremath{p\left(\fk \mid \allInput, \expertParamsK \right)}}
#+LATEX_HEADER: \newcommand{\pfknd}{\ensuremath{p\left(\fknd \mid \allInput\right)}}

#+LATEX_HEADER: \newcommand{\pFkGivenUk}{\ensuremath{p\left(\Fk \mid \uFk \right)}}
# #+LATEX_HEADER: \newcommand{\pYkGivenUk}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pYkGivenFku}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \uFk \right)}}

#+LATEX_HEADER: \newcommand{\qF}{\ensuremath{q\left(\F \right)}}
#+LATEX_HEADER: \newcommand{\qFu}{\ensuremath{q\left(\uF \right)}}
#+LATEX_HEADER: \newcommand{\qFku}{\ensuremath{q\left(\uFk \right)}}
#+LATEX_HEADER: \newcommand{\pFku}{\ensuremath{p\left(\uFk \mid \zFk \right)}}
#+LATEX_HEADER: \newcommand{\pFkuGivenX}{\ensuremath{p\left(\uFk \mid \zFk \right)}}
#+LATEX_HEADER: \newcommand{\pFuGivenX}{\ensuremath{p\left(\uF \mid \zF \right)}}
#+LATEX_HEADER: \newcommand{\qFk}{\ensuremath{q\left(\Fk \right)}}
#+LATEX_HEADER: \newcommand{\qfk}{\ensuremath{q\left(\fk \right)}}
#+LATEX_HEADER: \newcommand{\qfkn}{\ensuremath{q\left(\fkn \right)}}
#+LATEX_HEADER: \newcommand{\qfn}{\ensuremath{q\left(\fn \right)}}
#+LATEX_HEADER: \newcommand{\pFkGivenFku}{\ensuremath{p\left(\Fk \mid \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pfkGivenFku}{\ensuremath{p\left(\fkn \mid \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenFku}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pYGivenUX}{\ensuremath{p\left(\allOutput \mid \uF, \allInput \right)}}
#+LATEX_HEADER: \newcommand{\pYGivenU}{\ensuremath{p\left(\allOutput \mid \uF \right)}}


#+LATEX_HEADER: \newcommand{\pY}{\ensuremath{p\left(\allOutput \right)}}
# #+LATEX_HEADER: \newcommand{\pykGivenfk}{\ensuremath{p\left(\singleOutputK \mid \fkn \right)}}
# #+LATEX_HEADER: \newcommand{\pYkGivenFk}{\ensuremath{p\left(\allOutputK \mid \Fk \right)}}
# #+LATEX_HEADER: \newcommand{\pYkGivenX}{\ensuremath{p(\allOutputK \mid \allInput)}}
#+LATEX_HEADER: \newcommand{\pykGivenx}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenxNegF}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \singleInput, \neg\Fk \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenfk}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \fkn \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenfkd}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \fknd \right)}}
#+LATEX_HEADER: \newcommand{\pYkGivenFk}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \Fk \right)}}
#+LATEX_HEADER: \newcommand{\pYkGivenX}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \allInput \right)}}
#+LATEX_HEADER: \newcommand{\pYGivenX}{\ensuremath{p\left(\allOutput \mid \allInput \right)}}

**** Gating network
#+LATEX_HEADER: \newcommand{\PrA}{\ensuremath{\Pr\left(\ModeVarK \right)}}
#+LATEX_HEADER: \newcommand{\Pra}{\ensuremath{\Pr\left(\modeVarK \right)}}
#+LATEX_HEADER: \newcommand{\PaGivenhx}{\ensuremath{P\left(\modeVarn \mid \hn, \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\PraGivenx}{\ensuremath{\Pr\left(\modeVarn \mid \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\PraGivenhx}{\ensuremath{\Pr\left(\modeVarK \mid \hn, \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\PraGivenxNegH}{\ensuremath{\Pr\left(\modeVarK \mid \singleInput, \neg\Hall \right)}}
#+LATEX_HEADER: \newcommand{\PrAGivenX}{\ensuremath{\Pr\left(\ModeVarK \mid \allInput \right)}}

#+LATEX_HEADER: \newcommand{\pHGivenX}{\ensuremath{p\left(\Hall \mid \allInput\right)}}
#+LATEX_HEADER: \newcommand{\pHkGivenX}{\ensuremath{p\left(\Hk \mid \allInput\right)}}

*** Kernels
# #+LATEX_HEADER: \newcommand{\Kkxx}{\mode{\mathbf{K}}_{\allInput\allInput}}
#+LATEX_HEADER: \newcommand{\Kkxx}{\mode{\mathbf{K}}_{d, \allInput\allInput}}

# TO derivative kernels
#+LATEX_HEADER: \newcommand{\ddK}{\ensuremath{\partial^2\K_{**}}}
#+LATEX_HEADER: \newcommand{\dK}{\ensuremath{\partial\K_{*}}}
#+LATEX_HEADER: \newcommand{\Kxx}{\ensuremath{\K_{}}}
#+LATEX_HEADER: \newcommand{\iKxx}{\ensuremath{\Kxx^{-1}}}

#+LATEX_HEADER: \newcommand{\dKz}{\ensuremath{\partial\K_{*\zH}}}
#+LATEX_HEADER: \newcommand{\Kzz}{\ensuremath{\K_{\zH\zH}}}
#+LATEX_HEADER: \newcommand{\iKzz}{\ensuremath{\Kzz^{-1}}}
*** Desired Mode
# Function notation
#+LATEX_HEADER: \newcommand{\HDes}{\ensuremath{\MDes{\GatingFunc}}}
#+LATEX_HEADER: \newcommand{\uHDes}{\ensuremath{\MDes{\uH}}}

# Inducing points
#+LATEX_HEADER: \newcommand{\pDes}{\ensuremath{p\left( \uHDes \mid \zHDes \right)}}
#+LATEX_HEADER: \newcommand{\qDes}{\ensuremath{q\left( \uHDes \right)}}
#+LATEX_HEADER: \newcommand{\mDes}{\ensuremath{\MDes{\mathbf{m}}}}
#+LATEX_HEADER: \newcommand{\SDes}{\ensuremath{\MDes{\mathbf{S}}}}

*** Jacobian
# Single data notation
#+LATEX_HEADER: \newcommand{\singleTest}[1]{\ensuremath{#1_{\testInd}}}
#+LATEX_HEADER: \newcommand{\testInput}{\ensuremath{\singleTest{\state}}}

# Jacobian notation
#+LATEX_HEADER: \newcommand{\Jac}{\ensuremath{\mathbf{J}}}
#+LATEX_HEADER: \newcommand{\testJac}{\ensuremath{\singleTest{\Jac}}}
#+LATEX_HEADER: \newcommand{\muJac}{\ensuremath{\mu_{\Jac}}}
#+LATEX_HEADER: \newcommand{\covJac}{\ensuremath{\Sigma_{\Jac}}}

*** Old
**** Gating Network Old
# # Function notation
# #+LATEX_HEADER: \newcommand{\gatingFunc}{\ensuremath{h}}
# #+LATEX_HEADER: \newcommand{\hk}{\ensuremath{\mode{\gatingFunc}}}

# # Single data notation
# #+LATEX_HEADER: \newcommand{\hkn}{\ensuremath{\singleData{\hk}}}
# #+LATEX_HEADER: \newcommand{\hn}{\ensuremath{\singleData{\mathbf{\gatingFunc}}}}

# # All inputs set/vector/tensor notation
# #+LATEX_HEADER: \newcommand{\GatingFunc}{\ensuremath{\mathbf{\gatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\GatingFunc}}
# #+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\mode{\GatingFunc}}}
# # #+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\allData{\GatingFunc}}}
# # #+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\allData{\mode{\GatingFunc}}}}
**** Desired Mode Old
# #+LATEX_HEADER: \newcommand{\HDes}{\ensuremath{\modeDes{\GatingFunc}}}
# #+LATEX_HEADER: \newcommand{\HuDes}{\ensuremath{\modeDes{\Hu}}}
# #+LATEX_HEADER: \newcommand{\mDes}{\ensuremath{\modeDes{\mathbf{m}}}}
# #+LATEX_HEADER: \newcommand{\SDes}{\ensuremath{\modeDes{\mathbf{S}}}}

**** Experts Old
# # Function notation
# #+LATEX_HEADER: \newcommand{\latentFunc}{\ensuremath{f}}
# #+LATEX_HEADER: \newcommand{\f}{\ensuremath{f}}
# #+LATEX_HEADER: \newcommand{\fk}{\ensuremath{\mode{\latentFunc}}}
# # #+LATEX_HEADER: \newcommand{\fkd}{\ensuremath{\singleDim{\fk}}}
# #+LATEX_HEADER: \newcommand{\fkd}{\ensuremath{\singleDimMode{\f}}}

# # Single input notation
# #+LATEX_HEADER: \newcommand{\fn}{\ensuremath{\singleData{\mathbf{\latentFunc}}}}
# #+LATEX_HEADER: \newcommand{\fkn}{\ensuremath{\singleData{\mode{\mathbf{\latentFunc}}}}}
# # #+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\singleDim{\singleData{\fk}}}}
# # #+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\singleDimMode{\singleData{\f}}}}
# #+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\singleDimModeData{\f}}}

# # All inputs set/vector/tensor notation
# # #+LATEX_HEADER: \newcommand{\F}{\ensuremath{\allData{\mathbf{\f}}}}
# #+LATEX_HEADER: \newcommand{\F}{\ensuremath{\mathbf{\f}}}
# #+LATEX_HEADER: \newcommand{\Fk}{\ensuremath{\mode{\F}}}
# # #+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\singleDim{\Fk}}}
# #+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\singleDimMode{\F}}}

#+LATEX_HEADER: \newcommand{\allOutputK}{\ensuremath{\mode{\allOutput}}}
#+LATEX_HEADER: \newcommand{\singleOutputK}{\ensuremath{\mode{\singleOutput}}}

** Acronyms :noexport:
Use with
- \acrfull{gp} prints Gaussian Process (GP)
- \acrshort{gp} prints GP
- \acrlong{gp} prints Gaussian Process

{{{glossaryentry(LaTeX,\LaTeX,A document preparation system,LaTeX)}}}
{{{glossaryentry(Real Numbers,$\real$,The set of Real numbers,Real Numbers)}}}

{{{acronym(mogpe,MoGPE,Mixtures of Gaussian Process Experts)}}}
{{{acronym(moe,MoE,Mixture of Experts)}}}
{{{acronym(mosvgpe,MoSVGPE,Mixtures of Sparse Variational Gaussian Process Experts)}}}
{{{acronym(gp,GP,Gaussian process)}}}
{{{acronym(mdp,MDP,Markov decision process)}}}
{{{acronym(ard,ARD,Automatic Relevance Determination)}}}
{{{acronym(ode,ODE,Ordinary Differential Equation)}}}
{{{acronym(sde,SDE,Stochastic Differential Equation)}}}
{{{acronym(elbo,ELBO,Evidence Lower Bound)}}}
{{{acronym(vae,VAE,Variational Auto-Encoder)}}}
{{{acronym(mbrl,MBRL,Model-Based Reinforcement Learning)}}}
{{{acronym(hmm,HMM,Hidden Markov Model)}}}
{{{acronym(svi,SVI,Stochastic variational inference)}}}
* Frontmatter :ignore:
#+BEGIN_EXPORT latex
\frontmatter
#+END_EXPORT
** Title Page :ignore:noexport:

  #+BEGIN_EXPORT latex
  \begin{titlepage}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % First page: Thesis Title and Author Name
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    % Uncomment when adding the background figure to the cover.
    \BgThispage

    \cleardoublepage
    \pagestyle{empty}

    \begin{center}
      \null\vfill
      {\huge{\bfseries \ThesisTitle}\par}
      \vspace{\stretch{0.5}}
      {\large \ThesisSubTitle \par}
      \vspace{\stretch{2}}
      \vspace{\baselineskip}
      {\large By \AuthorFullName\par}
      \vspace{\stretch{2}}
      %\vspace{\baselineskip}
      %\vspace{\baselineskip}
      \vspace{\baselineskip}
      \includegraphics[scale=0.6]{./logos/bristolcrest_colour}
      \hspace{5mm}
      \includegraphics[scale=0.35]{./logos/UWE_insignia.png}\\
      \vspace{10mm}
      {\large Department of Aerospace Engineering\\
       \textsc{University of Bristol}}
       \\
       \&
       \\
       {\large Department of Engineering Design and Mathematics\\
       \textsc{University of the West of England}}\\

      %{\large Faculty of Engineering\\
      %\textsc{University of Bristol}}\\
      %\vspace{6mm}
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \begin{minipage}{10cm}
        A dissertation submitted to the University of Bristol and the University of the West of England in accordance with the requirements of the degree of \textsc{Doctor of Philosophy} in the Faculty of Engineering.
      \end{minipage}\\
       \vspace{\baselineskip}
      % \vspace{\stretch{1}}
      \vspace{\baselineskip}
      \vspace{\stretch{1}}
      \noindent
      \begin{tabular}{@{}l@{\hspace{22pt}}ll}
        \textbf{Supervisors}:          & Prof.\ Arthur Richards\\
                                       & Dr.\ Carl Henrik Ek\\
      \end{tabular} \\
      %\vspace{\stretch{1}}
      %\vspace{\baselineskip}
      %\vspace{\baselineskip}
      \vspace{9mm}
      {\large\textsc{January 2022}}
      \vspace{12mm}
      \vfill
    \end{center}

    \cleardoublepage
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % End Titlepage
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{titlepage}
  #+END_EXPORT

** Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:
#+BEGIN_EXPORT latex
\begin{SingleSpace}
%\initial{R}einforcement learning and data-driven control have seen significant advances over the last decade,
%especially in simulated environments.
%Real world systems are often highly nonlinear, exhibit stochasticity and multimodalities,
%are expensive to run (slow, energy intensive, subject to wear and tear) and
%must be controlled subject to constraints (for safety, efficiency, etc).

%From robotics, to industrial processing, to finanace, learning-based approaches to control
%help alleviate the dependence on domain exerts for system identification and controller design.
This dissertation is concerned with \textit{learning} and \textit{control}
in unknown, (or partially unknown), multimodal dynamical systems.
It is motivated by controlling robotic systems in uncertain environments,
where both the underlying dynamics modes,
and how the dynamics switches between them, are \textit{not fully known a priori}.

%For example, controlling a quadcopter subject to inoperable dynamics modes that are
%induced via spatially varying turbulence
%i.e. fly a quadcopter to a target location, whilst remaining in the operable (non turbulent) dynamics mode.

%This dissertation is concerned with \textbf{learning} and \textbf{control}
%in unknown, (or partially unknown), multimodal dynamical systems.
%It is motivated by controlling a quadcopter with inoperable dynamics modes that are
%induced via spatially varying turbulence.
%The operable mode corresponds to regions of the state space subject to \textbf{low turbulence}, and the
%inoperable mode(s) corresponds to regions subject to \textbf{high turbulence}.
%The goal is to fly the quadcopter from an initial state in the desired (operable) dynamics mode,
%to a target state, whilst remaining in the desired dynamics mode.

This dissertation first considers learning representations of multimodal dynamical systems, assuming
access to a historial data set of state transitions.
\todo{add comment about MoGPE vs SVGP}
The model resembles the Mixture of Gaussian Process Experts model with a gating network based on Gaussian processes.
Motivated by synergising model learning and control,
this model infers latent \textit{geometric} structure in the gating network,
that is later exploited by a geometry inspired control algorithm.
Well-calibrated uncertainty estimates and scalability are obtained via
stochastic variational inference.
%variational lower bound based on sparse approximations, that can be optimised with
%stochastic gradient methods.
%A novel variational lower bound based on sparse approximations, that can be optimised with
%stochastic gradient methods, is derived.
%It provides scalability as well as well-calibrated uncertainty estimates.

%Secondly, this work considers trajectory optimisation algorithms,
%that exploit the learned dynamics model to achieve the aformentioned goal.
%In a \textbf{risk-averse setting}, it is also desirable to avoid entering regions of a learned dynamics model with
%high \textit{epistemic uncertainty}.
%This is because the state-control trajectory cannot be predicted confidently, and thus,
%constraints may be violated i.e. the system may enter inoperable dynamics modes.
%Still assuming access to a historical data set, the first approach presented in this dissertation
%exploits concepts of Riemannian geometry (extended to probabilistic geometries) to encode the trajectory optimisation
%goals into an objective function.
%A second, alternative approach, formulates the control problem as probabilistic inference
%in a graphical model by conditioning on a mode assignment variable.
%Both methods are evaluated via experiments on a simulated quadcopter, as well as a data set of a
%DJI Tello quadcopter flying in the Bristol Robotics Laboratory.

Secondly, this dissertation considers driving a dynamical system from an initial state (in a desired dynamics mode),
to a target state, whilst remaining in the desired dynamics mode.
For example, consider controlling a quadcopter in an environment subject to two dynamics modes: 1) a turbulent
dynamics mode in front of a fan, and 2) a non turbulent dynamics mode everywhere else.
The goal in this scenario is to fly the quadcopter to a target location,
whilst remaining in the operable (non turbulent) dynamics mode.

In a \textbf{risk-averse setting}, it is desirable to avoid entering regions of a learned dynamics model with
high \textit{epistemic uncertainty}, as well as remaining in the desired dynamics mode.
This is because the trajectory cannot be predicted confidently, and may leave the operable dynamics mode.
Given a partially learned dynamics model, this dissertation develops two trajectory optimisation algorithms
aimed at solving this risk-averse setting.
The first approach
exploits concepts of Riemannian geometry (extended to probabilistic geometries) to encode both of the goals
into a geometry inspired objective function.
The second approach formulates the control problem as probabilistic inference
in a graphical model, and encodes the goals by conditioning on a mode assignment variable.
Both methods are evaluated via experiments on a simulated quadcopter, as well as a data set collected onboard
a DJI Tello quadcopter.
%A second, alternative approach is also presented.
%Instead of exploiting the geometry of the learned model, it formulates the control problem as probabilistic inference
%in a graphical model by conditioning on a mode assignment variable.
%Based on these two goals, this dissertation develops two trajectory optimisation algorithms that exploit
%the learned dynamics to achieve them.

Finally, this dissertation considers the active learning setting, where it does
not assume access to a historical data set.
To achieve this goal, a constrained exploration algorithm is introduced.
The algorithm exploits the \textit{epistemic uncertainty} associated with the learned model, to guide
exploration into regions of the dynamics that have not previously been observed.
This experience can then be used to update the model and reduce the associated \textit{epistemic uncertainty}.
Exploration is subject to chance constraints that prevent the system from leaving the desired dynamics
mode, resulting in an overconstrained problem.
Loosening the chance constraints enables the algorithm to incrementally explore the environment,
becoming more confident in the dynamics,
until it can find a trajectory to the target state that does not violate the chance constraints.



\end{SingleSpace}
#+END_EXPORT

** Declaration :noexport:
:PROPERTIES:
:UNNUMBERED: t
:END:
#+BEGIN_EXPORT latex
\begin{SingleSpace}
\begin{quote}
\initial{I} declare that the work in this dissertation was carried out in accordance with the requirements of  the University's Regulations and Code of Practice for Research Degree Programmes and that it  has not been submitted for any other academic award. Except where indicated by specific  reference in the text, the work is the candidate's own work. Work done in collaboration with, or with the assistance of, others, is indicated as such. Any views expressed in the dissertation are those of the author.

\vspace{1.5cm}
\noindent
\hspace{-0.75cm}\textsc{SIGNED: .................................................... DATE: ..........................................}
\end{quote}
\end{SingleSpace}
#+END_EXPORT

** Acknowledgements :noexport:
:PROPERTIES:
:UNNUMBERED: t
:END:
#+BEGIN_EXPORT latex
\begin{SingleSpace}
\initial{H}ere goes the dedication.
\end{SingleSpace}
#+END_EXPORT
* TOC and Mainmatter :ignore:
#+BEGIN_EXPORT latex
\tableofcontents
% This ensures that the subsequent sections are being included as root
% items in the bookmark structure of your PDF reader.
\begingroup
    \let\clearpage\relax
    \glsaddall
    \printglossary[type=\acronymtype]
    \newpage
    \printglossary
\endgroup
\printindex

\mainmatter
#+END_EXPORT

* Testing Maths Variables :noexport:
** Tables :ignore:
#+CAPTION: Variables
| Name                    | Symbol     | Equation                                                   |
|-------------------------+------------+------------------------------------------------------------|
| State                   | $\state$   | $\R^{\StateDim}$                                           |
| Control                 | $\control$ | $\R^{\ControlDim}$                                         |
| Time                    | $t$        | $\R$                                                       |
| State-action input      | $\x$       | $(\state, \control) \in \R^{\StateDim \times \ControlDim}$ |
| State difference        | $\y$       | $\state_{t} - \state_{t-1} \in \R^{\StateDim}$             |
| Mode indicator variable | $\modeVar$ | $\{1,\ldots,\ModeInd\}$                                    |
|                         |            |                                                            |

#+CAPTION: Variables at single data points
| Name                    | Symbol           | Equation                                                          |
|-------------------------+------------------+-------------------------------------------------------------------|
| State                   | $\singleState$   | $\R^{\StateDim}$                                                  |
| Control                 | $\singleControl$ | $\R^{\ControlDim}$                                                |
| State-Action input      | $\singleInput$   | $(\singleState, \singleControl) \in \R^{\StateDim + \ControlDim}$ |
| State Difference        | $\singleOutput$  | $\R^{\StateDim}$                                                  |
| Mode indicator variable | $\modeVarn$      | $\{1,\ldots,\ModeInd\}$                                           |

#+CAPTION: Variables at all data points
| Name                    | Symbol        | Equation                                                                      |
|-------------------------+---------------+-------------------------------------------------------------------------------|
| State                   | $\allState$   | $\R^{\NumData \times \StateDim}$                                              |
| Control                 | $\allControl$ | $\R^{\NumData \times \ControlDim}$                                            |
| State-Action input      | $\allInput$   | $(\allState, \allControl) \in \R^{\NumData \times (\StateDim + \ControlDim)}$ |
| State Difference        | $\allOutput$  | $\R^{\NumData \times \StateDim}$                                              |
| Mode indicator variable | $\ModeVarK$   | $\{\singleData{\modeVar}=k\}_{\numData=1}^{\NumData}$                         |

#+CAPTION: Gating network notation
|                | Name                                   | Symbol        | Equation                                                                         |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| Function       | Gating function k                      | $\hk$         | $\hk : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R$                    |
|                | Gating function                        | $\gatingFunc$ | $\gatingFunc : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R^{\ModeInd}$ |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| $\singleInput$ | Gating function k at $\singleInput$    | $\hkn$        | $\hk(\singleInput) \in \R$                                                       |
|                | Gating function at $\singleInput$      | $\hn$         | $\gatingFunc(\singleInput) \in \R^{\ModeInd}$                                    |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| $\allInput$    | Gating function k                      | $\Hk$         | $\hk(\allInput) \in \R^{\NumData}$                                               |
|                | Gating functions                       | $\Hall$       | $\gatingFunc(\allInput) \in \R^{\NumData \times \ModeInd}$                       |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| $\zH$          | Inducing variables - gating function k | $\uHk$        | $\hk(\zHk) \in \R^{\NumInd}$                                                     |
|                | Inducing variables - gating functions | $\uH$         | $\h(\zH) \in \R^{\NumInd \times \ModeInd}$                                       |


#+CAPTION: Transition dynamics function notation
|                | Name                                    | Symbol  | Equation                                                                                 |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Dimension d of mode k                   | $\fkd$  | $\fkd : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R$                           |
| Function       | Mode k                                  | $\fk$   | $\fk : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R^{\StateDim}$                |
|                | All modes function                                          | $\f$    | $\f : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R^{\ModeInd \times \StateDim}$ |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Dimension d mode k                      | $\fknd$ | $\fkd(\singleInput) \in \R$                                                              |
| $\singleInput$ | Mode k                                  | $\fkn$  | $\fk(\singleInput) \in \R^{\StateDim}$                                                   |
|                | All modes                               | $\fn$   | $\f(\singleInput) \in \R^{\ModeInd \times \StateDim}$                                    |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Dimension d mode k                      | $\Fkd$  | $\fkd(\allInput) \in \R^{\NumData}$                                                      |
| $\allInput$    | Mode k                                  | $\Fk$   | $\fk(\allInput) \in \R^{\NumData \times \StateDim}$                                      |
|                | All modes                               | $\F$    | $\f(\allInput) \in \R^{\NumData \times \ModeInd \times \StateDim}$                       |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Inducing variables - dimension d mode k | $\uFkd$ | $\fkd(\zFkd) \in \R^{\NumInd}$                                                           |
| $\zF$          | Inducing variables - mode k             | $\uFk$  | $\fk(\zFk) \in \R^{\NumInd \times \StateDim}$                                            |
|                | Inducing variables - all modes          | $\uF$   | $\f(\zF) \in \R^{\NumInd \times \ModeInd \times \StateDim}$                              |
** Experts
GP prior over each output dimension $d$ for each dynamics mode $k$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior-single-dim}
p\left(\Fkd \mid \allInput \right) &= \mathcal{N}\left( \Fkd \mid \singleDimMode{\mu}(\allInput), \singleDimMode{k}(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
Assume each output dimension is independent,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior}
\pFk &= \prod_{\stateDim=1}^{\StateDim} \pFkd
\end{align}
#+END_EXPORT
Assume each dynamics mode $k$ is independent,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-prior}
\pF &= \prod_{k=1}^{K} \pFk
\end{align}
#+END_EXPORT
The process noise in each mode is modelled as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-likelihood}
\pYkGivenFk = \prod_{\numData=1}^{\NumData} \pykGivenfk &= \prod_{\numData=1}^{\NumData} \mathcal{N}\left( \singleOutput \mid \fkn, \text{diag}\left[ \left(\noiseVarOneK\right)^{2}, \ldots, \left( \noiseVarDK \right)^{2} \right]  \right)
\end{align}
#+END_EXPORT
where $\noiseVardK$ represents the noise variance associated with the $d$^{\text{th}} dimension of the $k$^{\text{th}} mode.

Each expert is then given by marginalising its associated latent function values,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert}
\pYkGivenX = \int  \pYkGivenFk \pFk \text{d} \Fk
\end{align}
#+END_EXPORT

The dynamics modes are combined via a distribution over the mode indicator variable $\modeVar$.
The resulting marginal likelihood is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-prior}
\pYGivenX = \sum_{\modeInd=1}^{\ModeInd} \Pr(\ModeVarK) \pYkGivenX
\end{align}
#+END_EXPORT

** Mixture of Experts
Mixture model marginal likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixture-marginal-likelihood}
\pYGivenX = \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \Pr(\modeVarK) p(\singleOutput \mid \modeVarn=\modeInd, \singleInput)
\end{align}
#+END_EXPORT
Mixture of experts marginal likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-moe-marginal-likelihood}
\pYGivenX &= \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \PraGivenx \pykGivenx
\end{align}
#+END_EXPORT

** Gating Network
This work is interested in transition dynamics where the governing mode varies over the input domain

This work specifies a probability mass function over the mode indicator variable that is governed by a set of input-dependent
latent functions. These model how the transition dynamics switch between modes over the input domain.
In the literature they are commonly referred to as gating functions.
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mode-indicator-dist}
\PaGivenhx = \prod_{\modeInd=1}^{\ModeInd} \PraGivenhx^{[\modeVarn = \modeInd]},
\end{align}
#+END_EXPORT
The probabilities $\Pr(\modeVarn=\modeInd \mid \hn )$ are obtained by normalising the outputs of all the gating functions, e.g.
$\text{softmax}(\hn)$.
Following a Bayesian formulation independent GP priors are placed on each of the gating functions,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-funcs-prior}
\pHGivenX = \prod_{\modeInd=1}^{\ModeInd} \pHkGivenX
= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \Hk \mid \mode{\mu}(\allInput), \mode{k}(\allInput, \allInput) \right).
\end{align}
#+END_EXPORT
Each GP models the epistemic uncertainty associated with its gating function.
The probabilities $\PraGivenx$ associated with the probability mass function over
the mode indicator variable are then obtained by marginalising the
latent gating functions,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-indicator-mult}
\PraGivenxNegH
&= \int \text{softmax}_k(\hn) p(\hn \mid \singleInput, \neg\Hall) \text{d} \mathbf{h}_t.
\end{align}
#+END_EXPORT
This equation integrates out the uncertainty associated with the gating functions.
High variance in the gating function GPs tends the distribution over the mode indicator variable
to a uniform distribution.

** Marginal Likelihood
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-expert}
\pykGivenxNegF = \pyk
\end{align*}
#+END_EXPORT

Our marginal likelihood can be written with the same factorisation as the \acrshort{moe}
marginal likelihood in Equation ref:eq-moe-marginal-likelihood,

#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood}
\pYGivenX &= \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \underset{\text{Mixing probability}}{\PraGivenxNegH} \underset{\text{Dynamics mode } k}{\pykGivenxNegF}
\end{align}
#+END_EXPORT
The
$\PraGivenxNegH$ contains $\ModeInd$ GP conditionals with complexity

$\pykGivenxNegF$ contains a GP conditional with complexity

** Inference

* Introduction
** maths :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\targetState}{\ensuremath{\state_f}}
#+END_EXPORT
** Intro :ignore:
# *Dynamical Systems*
# The modern world is pervaded with dynamical systems that we seek to control to achieve a desired behaviour.
# Examples include autonomous vehicles, aircraft, robotic manipulators, financial markets and energy management systems.
# In the last decade, learning-based control citep:hewingLearningBased2020,sutton2018reinforcement has become
# a popular paradigm for controlling dynamical systems.
# This can be accounted to significant improvements in sensing and computational capabilities as well as
# recent successes in machine learning.

# with the /hope/ of addressing some of the shortcomings associated with pure control theoretic approaches?
# Perhaps with the /hope/ of addressing some of the shortcomings associated with pure control theoretic approaches.
# This can be accounted to recent successes in machine learning and the hope of
# and significant improvements in sensing and computational capabilities.

# Modern artificial intelligence seeks solutions that allow machines to /understand/ and /learn/.
# In the field of machine learning, these challenges are often solved using probabilistic models.
# In this setting, /understanding/ is achieved by separating signal from noise and removing redundancies in
# complex and noisy data.
# Leveraging learned models for control requires the models to extrapolate beyond training observations.
# That is, they must enable the machine to /reason/ about previously unseen inputs.
# In the field of machine learning, /understanding/ is achieved by separating signal from noise and removing redundancies in
# complex and noisy data.
# Leveraging learned models for control requires the models to extrapolate beyond training observations.
# That is, they must be able to /reason/ about previously unseen inputs.



# 1. Learning the system dynamics: model based control strategies rely on suitable and sufficiently accurate model
#    representations of the system dynamics. A promising approach is to learn /unknown/, or /partially unknown/
#    dynamics from observations. This enables control in previously uncontrollable systems, and can improve control
#    by learning (and accounting) for any model errors.
# 2. Learning the controller design:

# (coming from both the reinforcement learning
# cite:sutton2018reinforcement and control theory \todo{cite control theory book?} communities),



The modern world is pervaded with dynamical systems that we seek to control to achieve a desired behaviour.
Examples include autonomous vehicles, aircraft, robotic manipulators, financial markets and energy management systems.
In the last decade, reinforcement learning, and learning-based
control in general, have become
popular paradigms for controlling dynamical systems citep:hewingLearningBased2020,sutton2018reinforcement.
This can be accounted to significant improvements in sensing and computational capabilities, as well as
recent successes in machine learning.
# From robotics, to industrial processing, to finance, learning-based control
# offers promise of solving problems that could not previously be solved with purely control
# theoretic approaches.

This growing interest in learning-based control
\parmarginnote{real-world systems}
has emphasised the importance for real-world considerations.
Real-world systems are often highly /nonlinear/, exhibit /stochasticity/ and /multimodalities/,
are expensive to run (energy intensive, subject to wear and tear) and
must be controlled subject to /constraints/ (for safety, efficiency, and so on).
In contrast to simulation, the control of physical systems also has real-world consequences:
components may get damaged, the system may damage its environment, or the system may catastrophically fail.
As such, any learning-based control strategy deployed in the real-world should handle both the uncertainty inherent
to the environment, and the uncertainty introduced by learning from observations.
# As such, any control strategy deployed in the real-world should ensure the safety of itself and
# its surrounding environment.
# Therefore, any control strategy deployed in the real-world should ensure the safety of itself and
# its surrounding environment.

Many dynamical systems exhibit multimodalities (in their transition dynamics), where some dynamics modes
\parmarginnote{multimodal systems}
are believed to be /inoperable/ or /undesirable/.
These multimodalities may be due to spatially varying model parameters, for example,
process noise terms modelling aircraft turbulence, or friction coefficients modelling
surface-tyre interactions for ground vehicles over different terrain.
In these systems, it is desirable to avoid entering specific dynamics mode that are believed to be /inoperable/.
Perhaps they are hard to control due to stability, or the mode switch itself is hard to control.
Or perhaps it is desirable to avoid specific modes for /efficiency/ or /performance/ reasons.
Given these motivations, this dissertation is interested in control techniques for
driving dynamical systems from an initial state,
to a target state, whilst avoiding specific dynamics modes.

# In particular, it is primarily interested in controlling a quadcopter in an environment
# subject to two dynamics modes: 1) a turbulent
# dynamics mode in front of a fan, and 2) a non turbulent dynamics mode everywhere else.
# The objective in this environment is to control the quadcopter, whilst navigating to a target location,
# and remaining in the operable (non turbulent) dynamics mode.

Model-based control comprises a powerful set of techniques for finding controls of /constrained/ dynamical
\parmarginnote{model-based control}
systems, given a transition dynamics model describing the evolution of the controlled system.
It is commonly used for controlling aircraft, robotic manipulators, and walking
robots citep:vonstrykDirect1992,bettsSurvey1998,gargUnified2010.
One caveat is that it requires a relatively accurate mathematical model of the system.
Traditionally, these mathematical models are built using first principles based on physics.
However, accurately modelling the underlying transition dynamics can be challenging and
lead to the introduction of model errors.
For example, incorrectly specifying model parameters, or
models themselves (modelling a nonlinear system to be linear, or a multimodal system to be unimodal).
Incorrectly specifying these model parameters (and their associated uncertainty)
can have a detrimental impact on controller performance,
and is an active area of research in the robust
and stochastic optimal control communities citep:freemanRobust1996,stengelStochastic1986.
# For example, model parameters might be,
# 1) hard to specify accurately, e.g. friction coefficients associated with surfaces,
# 2) assumed constant when in reality they vary spatially, e.g. process noise terms modelling the effect of
#    turbulence on aircraft,
# 3) assumed constant when in reality they vary with time, e.g. mass reducing due to fuel consumption.

The difficulties associated with constructing mathematical representations of dynamical systems
\parmarginnote{learning dynamics models}
can be overcome by learning from observations citep:ljungSystem1999.
Learning dynamics models has the added benefit that it
alleviates the dependence on domain experts for specifying accurate models, in turn making it easier to
deploy more general techniques.
However, learning dynamics models for control introduces other difficulties.
For example, it is important to know
where the model cannot predict confidently due to a lack of training observations.
This concept is known as /epistemic uncertainty/ and is reduced in the limit of infinite data.
Correctly quantifying uncertainty is crucial for intelligent decision-making.
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Epistemic Uncertainty}
is the uncertainty attributed to incomplete knowledge about a phenomenon that limits our ability to model it.
It manifests in parameters taking a range of values, there being a range of viable models,
the level of modelling detail, and statistical confidence.
It can be reduced by the accumulation of additional information.
%In machine learning, \textit{epistemic uncertainty} arises due to lack of training observations and model mis-specification.
%This dissertation, uses the notion of \textit{epistemic uncertainty} to refer to a
%models lack of knowledge due to limited training data.
\end{myquote}
#+END_EXPORT

In a *risk-averse setting*, it is desirable for control strategies
to avoid entering regions of a learned dynamics model with high /epistemic uncertainty/.
\parmarginnote{decision-making under uncertainty}
This is because it is impossible to guarantee /constraint/ satisfaction in a learned model, i.e.
if the trajectory will avoid the undesired dynamics mode.
Conversely, in an *explorative setting*,
if the /epistemic uncertainty/ has been quantified, then it can be used to guide exploration into
regions of the dynamics that have not previously been observed.
This experience can then be used to update the model and reduce its associated /epistemic uncertainty/.

These two settings are the main concepts explored in this dissertation.
If the dynamics are not fully /known a priori/, an agent will not be able to confidently plan a
risk-averse trajectory to the target state.
How can the agent explore its environment, in turn reducing the /epistemic uncertainty/
associated with its dynamics model?
It is assumed that complete knowledge of the transition dynamics and how they
switch between modes is /not known a priori/.
Therefore, it is interested in jointly inferring the mode /constraints/ alongside the underlying dynamics modes,
through repeated interactions with the system.
Once the agent has explored enough, how can the learned model be exploited to plan risk-averse trajectories
that attempt to remain in a desired dynamics mode, and in regions of the learned
dynamics with low /epistemic uncertainty/?

# More generally, this dissertation is interested in learning and control in multimodal dynamical systems.
# In Chapter ref:chap-dynamics, it starts by formulating learning as approximate Bayesian inference in
# a probabilistic representation of the transition dynamics.
# Assuming access to a historical data set of state transitions from the entire domain, Chapter ref:chap-traj-opt
# develops two risk-averse trajectory optimisation algorithms.
# Given the dynamics model from Chapter ref:chap-dynamics, trained on these observations,
# these algorithms are capable of finding trajectories that remain in the desired dynamics mode, whilst avoiding
# regions of the learned dynamics with high /epistemic uncertainty/.
# Finally, Chapter ref:chap-active-learning addresses the question of active learning, i.e. incrementally
# learning the dynamics model through repeated interactions with the environment.

# It assumes that complete knowledge of the transition dynamics and how they
# switch between modes is /not known a priori/.
# In essence, the system /constraints/ on the mode are /not known a priori/ and need to be inferred from observations.
# Therefore, it is interested in jointly inferring the /constraints/ alongside the underlying dynamics modes,
# through repeated interactions with the system.

# certain enough in its dynamics, it will be able to plan a risk-averse trajectory
# Once the agent is certain enough in its dynamics, it will be able to plan a risk-averse trajectory
# to the target state.

# Collectively, these two settings are known as the exploration-exploitation trade-off,
# which is well-known in the reinforcement learning and optimal control communities.


# As a result, it is interested in learning dynamics models incrementally, without violating some notion
# of constraint on the dynamics modes.



# This dissertation is interested in learning dynamics models for multimodal dynamical systems with /unknown/, or
# /partially unknown/, transition dynamics, where safety is governed by the underlying dynamics modes.
# It is assumed that complete knowledge of how the transition dynamics switch between modes is also /not known a priori/.
# In essence, the safety constraints are /not known a priori/ and should be learned from observations.
# Therefore, this dissertation is interested in jointly inferring the /constraints/ alongside the underlying dynamics modes,
# through repeated interactions with the system.


# Safe control in systems where the environment is /known a priori/
# has been well studied by the control and formal methods communities.
# However, safe control when the environment is /not known a priori/ has been less well studied.
# Recent work by cite:berkenkampSafe2019 attempts to address safe learning-based control in the
# reinforcement learning setting, where the environment is /not known a priori/.
# In cite:schreiterSafe2015 the authors address active learning in Gaussian process dynamics models, by
# deploying a binary GP classification model that indicates safe and unsafe regions.
# # Although this dissertation is not directly interested in safe control,
# # their work on constrained exploration is particularly relevant.


# It is important to note that the definition of safety varies,
# and should be defined on a system by system basis;
# cite:berkenkampSafe2019 define safety in terms of closed-loop
# stability but it is also common to define safety in terms of constraint satisfaction (on the states and controls).
# They define safety in terms of stability and exploit Lyapanov functions to construct a safe RL framework.
# Other approaches


# This dissertation is motivated by controlling dynamical systems that exhibit multimodalities, which are
# either unknown, or partially unknown.
# Incorrectly specifying a multimodal parameter as unimodal, or incorrectly specifying how a multimodal
# parameter switches between modes will have a detrimental impact on controller performance.
# In some cases, it may even lead to catastrophic failure.

** Illustrative Example label:illustrative_example
*** intro :ignore:
# The methods developed throughout this dissertation are evaluated on an illustrative 2D quadcotper navigation example.
The methods developed throughout this dissertation are motivated by a 2D quadcopter navigation example.
See cref:fig-problem-statement for a schematic of the environment and details of the problem.
The goal is to fly the quadcopter from an initial state $\state_0$, to a target state $\state_{f}$.
However, it considers a quadcopter operating in an environment subject to spatially varying wind --
induced by a fan -- where the system can be represented by two dynamics modes,
- Mode 1 :: an inoperable, /turbulent/ mode in front of the fan,
- Mode 2 :: a /non-turbulent/ mode everywhere else.
The turbulent dynamics mode is subject to higher drift (in the negative $x$ direction) and
to higher diffusion (aka process noise).
It is hard to know the exact turbulent dynamics due to complex and uncertain interactions between the
quadcopter and the wind field.
Further to this, it is believed that controlling the system in the turbulent dynamics mode may be infeasible.
This is because the unpredictability of the turbulence may cause catastrophic failure.
Therefore, when flying the quadcopter from an initial state $\state_0$, to a target state $\state_{f}$
it is desirable to find trajectories that avoid entering this turbulent dynamics mode.
However, the underlying dynamics modes and how the system switches between them, are \textit{not fully known a priori}.
To this end, this dissertation is interested in synergising model learning and model-based control (or planning),
to solve this quadcopter navigation problem.

#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
%\includegraphics[width=1.0\textwidth]{./images/point-mass-problem-statement-scenario-4.pdf}
\includegraphics[width=1.0\textwidth]{./images/quadcopter-domain-collocation-ppt.png}
\caption{\label{fig-problem-statement}\textbf{Quadcopter navigation problem}
Diagram showing a top down view of an environment, representing a quadcopter
subject to two dynamics modes: 1) a turbulent dynamics mode
induced by a fan (green) and 2) a non-turbulent dynamics mode everywhere else (blue).
The goal is to find trajectories from a start state $\state_0$, to the target state $\targetState$ (red star),
whilst avoiding the turbulent dynamics mode.}
\end{figure}
#+END_EXPORT


# Further to this, controlling the turbulent dynamics mode is believed to be difficult, as it may
# lead to catastrophic failure, due to the high process noise.

# The methods developed throughout this dissertation are evaluated on an illustrative 2D quadcotper example.
# The example considers a quadcopter operating in an environment subject to spatially varying turbulence (induced by a fan).
# It is hard to know the exact transition dynamics due to complex and uncertain
# interactions between the quadcopter and the fan.
# The system's transition dynamics can be represented by two dynamics modes,
# - Mode 1 :: a /turbulent/ mode in front of the fan,
# - Mode 2 :: a /non-turbulent/ mode everywhere else.
# However, the underlying dynamics modes, and how the dynamics switches between them, are \textit{not fully known a priori}.
# Fig. ref:fig-problem-statement shows a graphical representation of this environment.

# #+BEGIN_EXPORT latex
# \begin{figure}[!h]
# \centering
#   %\includegraphics[width=0.6\columnwidth]{images/quadcopter_bimodal_domain.pdf}
#   \includegraphics[width=0.9\columnwidth]{images/quadcopter-domain-ppt.png}
#   \caption{
#   Diagram showing a top down view of the environment for the 2D quadcopter example.
#   The quadcopter is subject to two dynamics modes: 1) a \textit{turbulent}
#   dynamics mode induced by a fan (green), and, 2) a \texit{non-turbulent} dynamics mode everywhere else (blue).}
# %The goal is learn a factorised representation of the underlying dynamics modes to find trajectories between $\mathbf{x}_0$ and $\mathbf{x}_f$ that either prioritise
# %remaining in the non-turbulent mode or prioritise avoiding regions of the learned dynamics
# %with high epistemic uncertainty due to lack of training observations.}
# \label{fig-problem-statement}
# \end{figure}
# #+END_EXPORT

*** Motivation label:to_motivation :noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{./images/traj-opt/mode-remaining/baseline-svgp-traj-opt/trajectories_over_desired_gating_mask.pdf}
\subcaption{SVGP dynamics model trained on state transitions sampled from \textbf{both} dynamics modes.}
\label{fig-traj-opt-over-gating-mask-svgp-all-baseline}
\end{minipage}
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{./images/traj-opt/mode-remaining/baseline-traj-opt/trajectories_over_desired_gating_mask.pdf}
\subcaption{SVGP dynamics model trained on state transitions sampled from \textbf{only the desired} (operable) dynamics mode.}
\label{fig-traj-opt-over-gating-mask-svgp-desired-baseline}
\end{minipage}
\caption{Trajectory optimisation results obtained using a SVGP dynamics model trained on different data sets.
The optimised control trajectories are rolled out in the SVGP dynamics model (magenta) and in the environment (cyan).
The state trajectories are overlayed on the gating mask, which indicates where each mode governs the dynamics.
White indicates the turbulent dynamics mode and red indicates the desired (operable) mode.
The hashed box indicates a subset of the environment that has not been observed.}
\label{fig-traj-opt-over-gating-mask-svgp-baseline}
\end{figure}
#+END_EXPORT
Given a historical data set of state transitions from the environment,
a common approach is to first learn a single-step dynamics model using a Gaussian process
citep:deisenrothPILCO2011,doerrOptimizing2017,vinogradskaStability2016,rohrProbabilistic2021,hewingLearningBased2020,kollerLearningBased2018.
This learned dynamics model can then be leveraged for model-based control.

cref:fig-traj-opt-over-gating-mask-svgp-all-baseline demonstrates the shortcomings of performing trajectory
optimisation in a GP dynamics model trained on state transitions sampled from *both* dynamics modes.
It shows that the GP is not able to learn a representation of the dynamics which is true to the underlying system.
This is because it cannot model the multimodal behaviour present in the environment.
The optimised controls successfully drive the GP dynamics from the start state $\state_0$,
to the target state $\targetState$.
However, as the GP dynamics model does not correspond to the true underlying dynamics,
the optimised controls do not successfully navigate to the target state $\targetState$ in the environment.
This example demonstrates the issues associated with learning a dynamics model that cannot model the discontinuities
associated with multimodal transition dynamics.
Further to this, it demonstrates the more general shortcomings of model-based control when leveraging learned
dynamics models that cannot represent the true underlying dynamics.
# This motivates cref:chap-dynamics that looks at learning representations of multimodal dynamical systems.
# This example demonstrates the issues associated with learning a dynamics model that cannot correctly identify
# the underlying dynamics modes.
# without deploying a technique that can infer both the underlying dynamics modes
# and how the system switches between them.
# The optimised controls are rolled out in the SVGP dynamics model (magenta)
# and in the environment (cyan) to highlight the issues with both approaches.

In contrast,
cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline shows results after training on state transitions from
only the desired (operable) dynamics mode.
It is able to accurately predict state transitions in the desired dynamics mode.
However, as this approach only considers the dynamics of the desired mode,
trajectories in the environment deviate from those planned in the learned dynamics model when they pass through the
turbulent mode.
This approach is problematic because the trajectory does not reach the target state $\targetState$,
nor does it avoid the turbulent dynamics mode, which may lead to catastrophic failure.
It is also worth noting here that the underlying dynamics modes and how the system switches between them,
are /not fully known a priori/.
As such, partitioning the data set and learning the dynamics model in
cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline is not possible in realistic scenarios.

#+BEGIN_EXPORT latex
\begin{myquote}
Following standard methodologies, these trajectories were found by minimising the expected cost
under the state distribution, resulting from cascading single-step predictions through the GP dynamics model.
A cost function consisting of a terminal state cost term ensures that trajectories end at the target state.
A quadratic integral control cost term was used to regularise the controls to encode the notion of
"minimal effort" trajectories.
\end{myquote}
#+END_EXPORT

This
cref:chap-dynamics strives to address these issues by learning representations of multimodal dynamical systems that
correctly identify the underlying dynamics modes and how the system switches between them.
cref:chap-traj-opt-geometry

Conveniently, the \acrshort{mosvgpe} method from cref:chap-dynamics can be used to learn a factorised representation of the underlying
dynamics modes.
This method correctly identifies the underlying dynamics modes and provides informative latent spaces that
can be used to encode mode remaining behaviour into control strategies.
In particular, the GP-based gating network infers informative latent structure.
The remainder of this chapter is concerned with encoding mode remaining behaviour into trajectory optimisation
algorithms after training a \acrshort{mosvgpe} dynamics model on the historical data set of state transitions.

The main goals of the trajectory optimisation in this Chapter can be summarised as follows,
- Goal 1 :: Remain in a desired dynamics mode $\desiredMode$, label:to-goal-mode
- Goal 2 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/ i.e. that cannot be predicted confidently. For example, due to limited training observations. label:to-goal-unc
  - in the desired dynamics mode,
  - in how the system switches between the underlying dynamics modes.



# #+BEGIN_EXPORT latex
# \begin{myquote}
# Following standard methodologies, trajectories were found by minimising the expected cost under the state distribution,
# resulting from cascading single-step predictions through the SVGP dynamics model.
# The cost function consisted of a quadratic integral control cost and a terminal state cost.
# \end{myquote}
# #+END_EXPORT
\todo{update fig-traj-opt-baseline with trajectories which don't use nominal mean function so that they deviate in the region with high epistemic uncertainty.}


# The first example in cref:fig-traj-opt-over-gating-mask-svgp-all-baseline demonstrates the issues of
# learning a dynamics model without deploying a technique that can infer both the underlying dynamics modes
# and how the system switches between them.
# cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline then demonstrates the
# difficulties of finding trajectories from the start state $\state_0$, to the target state $\targetState$,
# without avoiding the turbulent dynamics mode.



# #+BEGIN_EXPORT latex
# \begin{figure}[t!]
# \centering
# \includegraphics[width=1.0\textwidth]{./images/traj-opt/mode-remaining/baseline-traj-opt/trajectories_over_desired_gating_mask.pdf}
# \caption{\label{fig-traj-opt-baseline}
# Trajectory optimisation results obtained using a SVGP dynamics model trained only on state transitions from the
# desired (non-turbulent) dynamics mode.
# To demonstrate the methods shortcomings, the optimised controls are rolled out in the desired
# mode's GP dynamics (magenta) and in the environment (cyan).
# The state trajectories are overlayed on the gating mask, indicating where each mode governs the dynamics.
# White indicates the turbulent dynamics mode and red indicates the operable mode.
# The hashed box indicates a subset of the environment that has not been observed.}
# \end{figure}
# #+END_EXPORT

*** Motivation label:to_motivation
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{./images/traj-opt/mode-remaining/baseline-svgp-traj-opt/trajectories_over_desired_gating_mask.pdf}
\subcaption{SVGP dynamics model trained on state transitions sampled from \textbf{both} dynamics modes.}
\label{fig-traj-opt-over-gating-mask-svgp-all-baseline}
\end{minipage}
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{./images/traj-opt/mode-remaining/baseline-traj-opt/trajectories_over_desired_gating_mask.pdf}
\subcaption{SVGP dynamics model trained on state transitions sampled from \textbf{only the desired} (operable) dynamics mode.}
\label{fig-traj-opt-over-gating-mask-svgp-desired-baseline}
\end{minipage}
\caption{Trajectory optimisation results obtained using a SVGP dynamics model trained on different data sets.
The optimised control trajectories are rolled out in the SVGP dynamics model (magenta) and in the environment (cyan).
The state trajectories are overlayed on the gating mask, which indicates where each mode governs the dynamics.
White indicates the turbulent dynamics mode and red indicates the desired (operable) mode.
The hashed box indicates a subset of the environment that has not been observed.}
\label{fig-traj-opt-over-gating-mask-svgp-baseline}
\end{figure}
#+END_EXPORT
Given a historical data set of state transitions from the environment,
a common approach is to first learn a single-step dynamics model using a Gaussian process
citep:deisenrothPILCO2011,doerrOptimizing2017,vinogradskaStability2016,rohrProbabilistic2021,hewingLearningBased2020,kollerLearningBased2018.
This learned dynamics model can then be leveraged for model-based control.

cref:fig-traj-opt-over-gating-mask-svgp-all-baseline demonstrates the shortcomings of performing trajectory
optimisation in a GP dynamics model trained on state transitions sampled from *both* dynamics modes.
It shows that the GP is not able to learn a representation of the dynamics which is true to the underlying system.
This is because it cannot model the multimodal behaviour present in the environment.
The optimised controls successfully drive the GP dynamics from the start state $\state_0$,
to the target state $\targetState$.
However, as the GP dynamics model does not correspond to the true underlying dynamics,
the optimised controls do not successfully navigate to the target state $\targetState$ in the environment.
This example demonstrates the issues associated with learning a dynamics model that cannot model the discontinuities
associated with multimodal transition dynamics.
Further to this, it demonstrates the more general shortcomings of model-based control when leveraging learned
dynamics models that cannot represent the true underlying dynamics.
# This motivates cref:chap-dynamics that looks at learning representations of multimodal dynamical systems.
# This example demonstrates the issues associated with learning a dynamics model that cannot correctly identify
# the underlying dynamics modes.
# without deploying a technique that can infer both the underlying dynamics modes
# and how the system switches between them.
# The optimised controls are rolled out in the SVGP dynamics model (magenta)
# and in the environment (cyan) to highlight the issues with both approaches.

In contrast,
cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline shows results after training on state transitions from
only the desired (operable) dynamics mode.
It is able to accurately predict state transitions in the desired dynamics mode.
However, as this approach only considers the dynamics of the desired mode,
trajectories in the environment deviate from those planned in the learned dynamics model when they pass through the
turbulent mode.
This approach is problematic because the trajectory does not reach the target state $\targetState$,
nor does it avoid the turbulent dynamics mode, which may lead to catastrophic failure.
It is also worth noting here that the underlying dynamics modes and how the system switches between them,
are /not fully known a priori/.
As such, partitioning the data set and learning the dynamics model in
cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline is not possible in realistic scenarios.

#+BEGIN_EXPORT latex
\begin{myquote}
Following standard methodologies, these trajectories were found by minimising the expected cost
under the state distribution, resulting from cascading single-step predictions through the GP dynamics model.
A terminal state cost term ensured trajectories ended at the target state and
a quadratic integral control cost term regularised the controls to encode the notion of
"minimal effort" trajectories.
\end{myquote}
#+END_EXPORT




# #+BEGIN_EXPORT latex
# \begin{myquote}
# Following standard methodologies, trajectories were found by minimising the expected cost under the state distribution,
# resulting from cascading single-step predictions through the SVGP dynamics model.
# The cost function consisted of a quadratic integral control cost and a terminal state cost.
# \end{myquote}
# #+END_EXPORT
\todo{update fig-traj-opt-baseline with trajectories which don't use nominal mean function so that they deviate in the region with high epistemic uncertainty.}


# The first example in cref:fig-traj-opt-over-gating-mask-svgp-all-baseline demonstrates the issues of
# learning a dynamics model without deploying a technique that can infer both the underlying dynamics modes
# and how the system switches between them.
# cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline then demonstrates the
# difficulties of finding trajectories from the start state $\state_0$, to the target state $\targetState$,
# without avoiding the turbulent dynamics mode.



# #+BEGIN_EXPORT latex
# \begin{figure}[t!]
# \centering
# \includegraphics[width=1.0\textwidth]{./images/traj-opt/mode-remaining/baseline-traj-opt/trajectories_over_desired_gating_mask.pdf}
# \caption{\label{fig-traj-opt-baseline}
# Trajectory optimisation results obtained using a SVGP dynamics model trained only on state transitions from the
# desired (non-turbulent) dynamics mode.
# To demonstrate the methods shortcomings, the optimised controls are rolled out in the desired
# mode's GP dynamics (magenta) and in the environment (cyan).
# The state trajectories are overlayed on the gating mask, indicating where each mode governs the dynamics.
# White indicates the turbulent dynamics mode and red indicates the operable mode.
# The hashed box indicates a subset of the environment that has not been observed.}
# \end{figure}
# #+END_EXPORT

** Contributions
This dissertation explores mode constrained control in multimodal dynamical systems that explicitly reasons
about the uncertainties that arise during learning and control.
The primary contributions of this dissertation are as follows:
- cref:chap-dynamics: is concerned with learning representations of multimodal dynamical systems.
  Due to the complexity of the problem,
  this work first considers having prior access to the environment, where it can perform prior flight
  trials and observe state transitions over the domain, i.e. collect a data set of state transitions $\mathcal{D}$.
  Given such a data set, this chapter addresses learning environment models that
  can be exploited for model-based control (or planning). It strives to address the issues previously mentioned
  by learning representations of multimodal dynamical systems that correctly identify the underlying dynamics modes and
  how the system switches between them.
  The method is evaluated on a simple data set that contains discontinuities (the motorcycle data set)
  as well as a real-world quadcopter data set.
- cref:chap-traj-opt-geometry: investigates /risk-averse/ trajectory optimisation algorithms
  that leverage the models latent
  structure -- after performing Bayesian inference with the historical data set $\mathcal{D}$ -- to solve
  the quadcopter navigation problem.
  Due to lack of training observations, the learned dynamics model may be uncertain which mode governs the
  dynamics at a particular region (epistemic uncertainty).
  This chapter address the risk-averse setting and finds trajectories that avoid entering these regions of high
  epistemic uncertainty.
- cref:chap-active-learning: then considers the more realistic scenario of not having prior access
  to the environment. In this scenario, the quadcopter does not have access to a historical data set for model learning.
  Instead, it must actively explore its environment to collect data, whilst simultaneously attempting to avoid
  the inoperable, turbulent dynamics mode.

# Given such a data set, cref:chap-dynamics addresses learning environment models that can be exploited for
# model-based control (or planning).
# This chapter strives to address the issues previously mentioned
# by learning representations of multimodal dynamical systems that correctly identify the underlying dynamics modes and
# how the system switches between them.
# cref:chap-traj-opt-geometry,chap-traj-opt-inference
# then investigate trajectory optimisation
# algorithms that leverage the models latent structure -- after performing Bayesian inference
# with the historical data set $\mathcal{D}$ -- to solve the quadcopter navigation problem.
# #+BEGIN_EXPORT latex
# \begin{remark}[Epistemic Uncertainty]
# %Given that the underlying dynamics modes and how the dynamics switches between them, are
# %\textit{not fully known a priori}, these methods should \textit{air on the side of caution}.
# Due to lack of training observations, the learned model may be uncertain which mode governs the
# dynamics at a particular region. This is known as epistemic uncertainty.
# In a risk-averse setting, trajectories should also avoid entering these regions of high epistemic uncertainty.
# \end{remark}
# #+END_EXPORT
# cref:chap-active-learning then considers the more realistic scenario of not having prior access to the environment.
# In this scenario, the quadcopter does not have access to a historical data set for model learning.
# Instead, it must actively explore its environment to collect data, whilst simultaneously attempting to avoid
# the inoperable, turbulent dynamics mode.

# This dissertation explores mode constrained control in multimodal dynamical systems that explicitly reasons
# about uncertainties during learning and control.
# The risk-averse trajectory optimisation algorithms /know what they do not know/, and only evaluate regions that
# they are confident in.
# Conversely, the exploratory trajectory optimisation algorithm exploits this information to guide
# the system into regions that it has not previously observed.
# The primary contributions of this dissertation are as follows:
# - Chapter ref:chap-dynamics: details an approach to learning the underlying dynamics modes (and
#   how they're separated) in multimodal dynamical systems.
#   The approach formulates a probabilistic representation of the transition dynamics resembling a Mixture of
#   Gaussian Process Experts model. It then performs approximate Bayesian inference via a novel variational lower
#   bound that principally handles uncertainty and provides scalability via stochastic gradient methods.
#   The method is tested on a real-world quadcopter data set and two data sets obtained from simulated environments ...
# - Chapter ref:chap-traj-opt: introduces two trajectory optimisation techniques that find trajectories that attempt
#   to remain in a desired dynamics mode, and in regions of the learned dynamics that have been observed, so can be
#   predicted confidently. The first approach (published in cite:scannellTrajectory2021)
#   exploits the latent /geometry/ and well-calibrated /epistemic uncertainty/ estimates
#   inferred by the probabilistic model from Chapter ref:chap-dynamics.
#   The second approach formulates trajectory optimisation as probabilistic inference in a graphical model,
#   and achieves the desired behaviour by conditioning on a mode indicator variable.
# - Chapter ref:chap-active-learning:

** Associated Publications
# The probabilistic model and variational inference scheme presented in Chapter ref:chap-dynamics is used to learn
# the transition dynamics of a DJI Tello quadcopter in cite:scannellTrajectory2021.
The first trajectory optimisation algorithm presented in Chapter ref:chap-traj-opt-geometry, as well as the approach
to learning multimodal dynamical systems in Chapter ref:chap-dynamics are published in:

#+BEGIN_EXPORT latex
{\color{BrickRed}\fullcite{scannellTrajectory2021}}
#+END_EXPORT

** Introduction traj opt paper :noexport:
Many physical systems operate under switching dynamics modes due to
changing environmental or internal conditions.
Examples include: robotic grasping where objects with different
properties have to be manipulated, robotic locomotion in environments with varying surface types
and the control of aircraft in environments subject to different levels of turbulence.
When controlling these systems, it may be preferred to find trajectories that remain
in a single dynamics mode.
This paper is interested in controlling a DJI Tello quadcopter in an environment
with spatially varying turbulence induced by a fan at the side of the room, shown
in Fig. ref:fig-problem-statement.
It is hard to know the exact transition dynamics due to complex and uncertain
interactions between the quadcopter and the fan.
The system's transition dynamics resemble a mixture of two modes: a turbulent mode in front of
the fan and a non-turbulent mode everywhere else.
When planning a trajectory from start state $\mathbf{x}_0$ to desired state $\mathbf{x}_f$
it is preferred to avoid entering the turbulent mode, as it
results in poor performance and sometimes even system failure.

#+BEGIN_EXPORT latex
\begin{figure}[!t]
\centering
  %\includegraphics[width=0.9\columnwidth]{images/quadcopter_bimodal_domain.pdf}
  \includegraphics[width=0.9\columnwidth]{images/quadcopter-domain-ppt.png}
  \caption{
This work seeks to velocity control a DJI Tello quadcopter in an indoor environment
subject to two modes of operation characterised by process noise (turbulence).
A high turbulence mode is induced by placing a desktop fan at the right side of the room.
Data from four trajectories following a single 2D $\mathbf{x}=(x,y)$ target trajectory captures the variability
(process noise) in the dynamics.
Our goal is to find trajectories between $\mathbf{x}_0$ and $\mathbf{x}_f$ that either prioritise
remaining in the non-turbulent mode or prioritise avoiding regions of the learned dynamics
with high epistemic uncertainty due to lack of training observations.}
\label{fig-problem-statement}
\end{figure}
#+END_EXPORT

Trajectory optimisation comprises a powerful set of techniques for finding open-loop controls of dynamical
systems such that an objective function is minimised whilst satisfying a set of
constraints.
It is commonly used for controlling aircraft, robotic manipulators, and walking
robots cite:VonStryk1992,Betts1998,Garg2010.
One caveat to trajectory optimisation is that it requires a relatively accurate mathematical model of
the system.
Traditionally, these mathematical models are built using first principles based on physics.
However, accurately modelling the underlying transition dynamics can be challenging and
lead to the introduction of model errors.
For example, both observation and process noise
are inherent in many real-world systems and can be hard to model
due to both spatial and temporal variations.
Incorrectly accounting for this uncertainty can have a detrimental impact on controller performance
and is an active area of research in the robust
and stochastic optimal control communities cite:FreemanRandyA.2009,Stengel1988.

The difficulties associated with constructing mathematical models can be overcome by learning from
observations cite:Ljung1997.
However, learning dynamics models for control introduces other difficulties.
For example, it is important to know
where the model cannot predict confidently due to a lack of training observations.
This concept is known as epistemic uncertainty and is reduced in the limit of infinite data.
Probabilistic models have been used to account for epistemic uncertainty and also
provide a principled approach to modelling stochasticity i.e. aleatoric uncertainty
cite:Schneider1996,Deisenroth2011.
For example, cite:Cutler,Deisenroth2011,Pan2014 use Gaussian processes (GPs) to learn
transition dynamics.
GPs lend themselves to data-efficient learning through the selection of informative priors, and
when used in a Bayesian setting offer well calibrated uncertainty estimates.
Methods for learning probabilistic multimodal transition dynamics have also been proposed:
cite:Mckinnon used a mixture of GP experts method,
cite:Moerland studied the used of deep generative models and
cite:Kaiser2020a proposed a Bayesian model that learns independent
dynamics modes whilst maintaining a
probabilistic belief over which mode is responsible for predicting at a given input location.

There has also been work developing control algorithms exploiting learned multimodal transition dynamics
cite:Herzallah2020.
However, our work differs as it seeks to find trajectories that
remain in a single dynamics mode
whilst avoiding regions of the transition dynamics that cannot be predicted confidently.
To the best of our knowledge, there is no previous work addressing such trajectory optimisation
in transition dynamics models.

Probabilistic modelling and Bayesian inference are a promising
avenue for learning dynamics models to be used for controlling real-world systems.
\parmarginnote{probabilistic modelling}
The Bayesian framework provides a principled approach to modelling both the
/epistemic uncertainty/ associated with the model,
and the /aleatoric uncertainty/ inherent to the system (e.g. process noise).
For example, cite:deisenrothPILCO2011,cutlerEfficient2015,panProbabilistic2014
use Gaussian processes (GPs) to learn
transition dynamics from observations.
GPs lend themselves to data-efficient learning through the selection of informative priors, and
when used in a Bayesian setting offer well calibrated uncertainty estimates.
Methods for learning probabilistic multimodal transition dynamics have also been proposed:
cite:mckinnonLearning2017 use a Mixture of GP Experts method,
cite:moerlandLearning2017 study the use of deep generative models and
cite:kaiserBayesian2020 propose a Bayesian model that learns independent
dynamics modes whilst maintaining a
probabilistic belief over which mode is responsible for predicting at a given input location.

* Motivation
** Motivation label:to_motivation
As this work is interested in learning and control in multimodal dynamical systems,
a primary interest is correctly /identifying/ the underlying dynamics modes.
That is, ensuring that the learned dynamics modes accurately model the true underlying dynamics modes.
\marginpar{identifiability}
Incorrectly identifying the underlying dynamics modes will have a detrimental impact on performance
and may even lead to system failure.
# Learning
# and inferring latent spaces that are convenient for control.
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{./images/traj-opt/mode-remaining/baseline-svgp-traj-opt/trajectories_over_desired_gating_mask.pdf}
\subcaption{SVGP dynamics model trained on state transitions sampled from \textbf{both} dynamics modes.}
\label{fig-traj-opt-over-gating-mask-svgp-all-baseline}
\end{minipage}
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{./images/traj-opt/mode-remaining/baseline-traj-opt/trajectories_over_desired_gating_mask.pdf}
\subcaption{SVGP dynamics model trained on state transitions sampled from \textbf{only the desired} (operable) dynamics mode.}
\label{fig-traj-opt-over-gating-mask-svgp-desired-baseline}
\end{minipage}
\caption{Trajectory optimisation results obtained using a SVGP dynamics model trained on different data sets.
The optimised control trajectories are rolled out in the SVGP dynamics model (magenta) and in the environment (cyan).
The state trajectories are overlayed on the gating mask, which indicates where each mode governs the dynamics.
White indicates the turbulent dynamics mode and red indicates the desired (operable) mode.
The hashed box indicates a subset of the environment that has not been observed.}
\label{fig-traj-opt-over-gating-mask-svgp-baseline}
\end{figure}
#+END_EXPORT
Given a historical data set of state transitions from the environment,
a common approach is to first learn a single-step dynamics model using a Gaussian process
citep:deisenrothPILCO2011,doerrOptimizing2017,vinogradskaStability2016,rohrProbabilistic2021,hewingLearningBased2020,kollerLearningBased2018.
This learned dynamics model can then be leveraged for model-based control.
cref:fig-traj-opt-over-gating-mask-svgp-all-baseline
demonstrates the shortcomings of learning a GP dynamics model for the quadcopter navigation problem in the
illustrative example from cref:illustrative_example.
It shows the results of performing trajectory
optimisation in a GP dynamics model trained on state transitions sampled from *both* dynamics modes.
The GP has not been able to learn a representation of the dynamics which is true to the underlying system,
due to the discontinuities associated with the multimodal transition dynamics (changing lengthscales/noise variances etc).
The optimised controls successfully drive the GP dynamics from the start state $\state_0$,
to the target state $\targetState$.
However, as the GP does not represent the true underlying dynamics accurately,
the optimised controls do not successfully navigate to the target state $\targetState$ in the environment.
This example motivates the need to correctly identify the underlying modes when learning dynamics models for
model-based control.
# Further to this, it demonstrates the more general shortcomings of model-based control when leveraging learned
# dynamics models that cannot represent the true underlying dynamics.
# This motivates cref:chap-dynamics that looks at learning representations of multimodal dynamical systems.
# This example demonstrates the issues associated with learning a dynamics model that cannot correctly identify
# the underlying dynamics modes.
# without deploying a technique that can infer both the underlying dynamics modes
# and how the system switches between them.
# The optimised controls are rolled out in the SVGP dynamics model (magenta)
# and in the environment (cyan) to highlight the issues with both approaches.

In contrast,
cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline shows results after training on state transitions from
only the desired (operable) dynamics mode.
It is able to accurately predict state transitions in the desired dynamics mode.
However, as this approach only considers the dynamics of the desired mode,
trajectories in the environment deviate from those planned in the learned dynamics model when they pass through the
turbulent mode.
This approach is problematic because the trajectory passes through the turbulent dynamics mode,
which may lead to catastrophic failure.
Further to this, the trajectory does not reach the target state $\targetState$.
Without inferring information regarding how the system switches between its underlying dynamics modes, it is not
obvious how mode remaining/avoiding behaviour can be encoded into control algorithms.
#+BEGIN_EXPORT latex
\begin{remark}
As the underlying dynamics modes and how the system switches between them,
are \textit{not fully known a priori}, partitioning the data set and learning the dynamics model in
\cref{fig-traj-opt-over-gating-mask-svgp-desired-baseline} is not possible in realistic scenarios.
\end{remark}
#+END_EXPORT
# This example motivates the need for dynamics models that can automatically infer the underlying dynamics modes, whilst
# also learning how the system switches between its underlying dynamics modes.

# This approach is problematic because the trajectory does not reach the target state $\targetState$,
# nor does it avoid the turbulent dynamics mode, which may lead to catastrophic failure.


# It is also worth noting here that the underlying dynamics modes and how the system switches between them,
# are /not fully known a priori/.
# As such, partitioning the data set and learning the dynamics model in
# cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline is not possible in realistic scenarios.

#+BEGIN_EXPORT latex
\begin{myquote}
Following standard methodologies, these trajectories were found by minimising the expected cost
under the state distribution, resulting from cascading single-step predictions through the GP dynamics model.
A terminal state cost term ensured trajectories ended at the target state and
a quadratic integral control cost term regularised the controls to encode the notion of
"minimal effort" control.
\end{myquote}
#+END_EXPORT




# #+BEGIN_EXPORT latex
# \begin{myquote}
# Following standard methodologies, trajectories were found by minimising the expected cost under the state distribution,
# resulting from cascading single-step predictions through the SVGP dynamics model.
# The cost function consisted of a quadratic integral control cost and a terminal state cost.
# \end{myquote}
# #+END_EXPORT
\todo{update fig-traj-opt-baseline with trajectories which don't use nominal mean function so that they deviate in the region with high epistemic uncertainty.}


# The first example in cref:fig-traj-opt-over-gating-mask-svgp-all-baseline demonstrates the issues of
# learning a dynamics model without deploying a technique that can infer both the underlying dynamics modes
# and how the system switches between them.
# cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline then demonstrates the
# difficulties of finding trajectories from the start state $\state_0$, to the target state $\targetState$,
# without avoiding the turbulent dynamics mode.



# #+BEGIN_EXPORT latex
# \begin{figure}[t!]
# \centering
# \includegraphics[width=1.0\textwidth]{./images/traj-opt/mode-remaining/baseline-traj-opt/trajectories_over_desired_gating_mask.pdf}
# \caption{\label{fig-traj-opt-baseline}
# Trajectory optimisation results obtained using a SVGP dynamics model trained only on state transitions from the
# desired (non-turbulent) dynamics mode.
# To demonstrate the methods shortcomings, the optimised controls are rolled out in the desired
# mode's GP dynamics (magenta) and in the environment (cyan).
# The state trajectories are overlayed on the gating mask, indicating where each mode governs the dynamics.
# White indicates the turbulent dynamics mode and red indicates the operable mode.
# The hashed box indicates a subset of the environment that has not been observed.}
# \end{figure}
# #+END_EXPORT

** latent spaces :ignore:
\newline

The introduction of /latent variables/ into probabilistic models is a key component providing them with interesting
and powerful capabilities for synergising model learning and control.
For example, cite:hafnerLearning2019,rybkinModelBased2021
\todo{add more latent space dynamics refs}
learn /latent spaces/ which provide convenient spaces for planning.
\marginpar{latent spaces for control}
cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline highlights the need for learning
informative latent variables representing how the system switches between the underlying dynamics modes.
Without such information, it is not possible to encode the notion of mode remaining/avoiding behaviour.
As such, this work is interested in learning /latent spaces/
that are rich with information regarding how a system switches between its underlying dynamics modes.

With this in mind, the main goals of this chapter are to construct a model which,
1. can accurately /identify/ the true underlying dynamics modes,
2. has convenient /latent spaces/ for planning/control,
   - rich with information regarding the mode switching behaviour.
   # - this could be achieved by providing handles for encoding informative domain knowledge, in turn, constraining the set of admissible functions,

* Literature Review
# * Background and Related Work
** maths :ignore:
#+begin_export latex
\newcommand{\gpDomain}{\ensuremath{\hat{\stateDomain}}}
\newcommand{\inputDomain}{\ensuremath{\hat{\stateDomain}}}
\newcommand{\outputDomain}{\ensuremath{\stateDomain}}
#+end_export
** intro :ignore:
This chapter provides an overview of learning-based control and details the relevant background information
for the remainder of the dissertation.
** Dynamical Systems :noexport:
*** intro :ignore:
Dynamical systems describe the behaviour of a system over time $t$ and
are a key component of both control theory and reinforcement learning.
At any given time a dynamical system has a state, which is
represented as a vector of real numbers $\mathbf{x}(t) \in \R^D$.
For example, the state of a 2-dimensional robotic system with state comprising of 2D Cartesian coordinates
and an orientation would be represented as $\mathbf{x}(t) = [x(t), y(t), \theta(t)]^{T}$.
The system can be controlled by applying control actions $\mathbf{u}(t) \in \R^F$ at any given time $t$.
This work considers continuous-time, continuous-state, nonlinear stochastic dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-unimodal-dynamics-cont}
\stateDerivative(t) &= f(\state(t), \control(t)) + \epsilon(t) \quad \forall t
\end{align}
#+END_EXPORT
where $f : \R^D \times \R^{F} \rightarrow \R^{D}$ represents the transition dynamics function and $\epsilon(t)$
is an i.i.d process noise term with $\E[\epsilon(t)] = 0$.
The process noise term accounts for unwanted (and, in general, unknown) disturbances of the system.
For example, it is extremely hard to model aerodynamic effects on aircraft so these could be accounted
for in the process noise term.
Throughout this dissertation it is assumed that the state $\mathbf{x}$ is observed directly and is not subject to
observation noise.
This is a standard assumption in the \acrfull{mdp} framework which is commonly adopted
in the reinforcement learning literature.
\todo[inline]{Slightly weird referring to MDP literature on continuous time setting}

\todo[inline]{Should I make a bigger point out of the observation noise assumption}

The system is controlled via a policy $\mathbf{u}(t) = \pi(\mathbf{x}(t), t)$, which given the state $\mathbf{x}(t)$
and time step $t$ decides which control action $\mathbf{u}(t)$ to apply to the system.
The policy can be time-dependent and can also depend on all past information up to time step $t$.
In the time-independent case the policy is denoted $\pi(\mathbf{x}(t))$ and
the resulting closed-loop system is denoted $f_{\pi}(\mathbf{x}) = f(\mathbf{x},\pi(\mathbf{x}))$.

** Dynamical Systems and Uncertainty Quantification :ignore:
** Dynamical Systems :noexport:
*** maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{\MakeUppercase{\timeInd}}}

\newcommand{\stateDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\controlDomain}{\ensuremath{\mathcal{U}}}

\newcommand{\dynamicsFunc}{\ensuremath{f}}
#+END_EXPORT

*** intro :ignore:
Dynamical systems describe the behaviour of a system over time, $t$, and
are a key component of both control theory and reinforcement learning.
At any given time, $t$, a dynamical system has a state, which is
represented as a vector of real numbers $\state(t) \in \stateDomain \subseteq \R^D$.
For example, the state of a 2-dimensional robotic system with state comprising of 2D Cartesian coordinates
and an orientation would be represented as $\state(t) = [x(t), y(t), \theta(t)]^{T}$.
The system can be controlled by applying control actions $\mathbf{u}(t) \in \controlDomain \subseteq \R^F$ at any given
time, $t$.
This work considers continuous-time, continuous-state, nonlinear stochastic dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-unimodal-dynamics-cont}
\stateDerivative(t) &= \dynamicsFunc(\state(t), \control(t)) + \epsilon(t) \quad \forall t
\end{align}
#+END_EXPORT
where $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ represents the transition dynamics function and $\epsilon(t)$
is an i.i.d process noise term with $\E[\epsilon(t)] = 0$.
The process noise term accounts for unwanted (and, in general, unknown) disturbances of the system.
For example, it is extremely hard to model aerodynamic effects on aircraft so these could be accounted
for in the process noise term.
Throughout this dissertation it is assumed that the state $\state$ is observed directly and is not subject to
observation noise.
This is a standard assumption in the \acrfull{mdp} framework which is commonly adopted
in the reinforcement learning literature.
\todo[inline]{Slightly weird referring to MDP literature on continuous time setting}

\todo[inline]{Should I make a bigger point out of the observation noise assumption}

The system is controlled via a policy $\control(t) = \pi(\state(t), t)$, which given the state $\state(t)$
and time step $t$ decides which control action $\control(t)$ to apply to the system.
The policy can be time-dependent and can also depend on all past information up to time step $t$.
In the time-independent case the policy is denoted $\pi(\state(t))$ and
the resulting closed-loop system is denoted $f_{\pi}(\state) = \dynamicsFunc(\state,\pi(\state))$.
*** Sources of Uncertainty
\parmarginnote{epistemic uncertainty}
If we have used observations of a system to train a predictive model then we can only be confident in predictions near our observations.
As we extrapolate away from the observations we can no longer be certain and this is known as
epistemic uncertainty.
It can be reduced by collecting more data and retraining a model.
This is shown in Figure [[ref:epistemic]].
# #+NAME: epistemic
# #+ATTR_LATEX: :width 1.\textwidth :placement [h] :center nil
# #+caption: Plot demonstrating the concept of epistemic uncertainty. We can be confident in the learnt function close to our observations but as we exptrapolate away from them we become uncertain what the function should look like. If we have a notion of epistemic uncertainty in a MBRL algorithm we can use it to encourage the agent to visit these areas and collect data, which in turn will reduce the epistemic uncertainty.
# [[file:images/limited_data.pdf]]


\parmarginnote{aleatoric uncertainty}
As mentioned previously, aleatoric uncertainty consists of process noise and observation noise; uncertainties that are inherent in a system and cannot be reduced.
# #+NAME: bimodal-dataset
# #+ATTR_LATEX: :width 1.\textwidth :placement [h] :center nil
# #+caption: An artificial 1D dataset with two levels of process noise.
# [[file:images/dataset.pdf]]
Figure [[ref:bimodal-dataset]] shows an artificial 1D dataset that demonstrates the concept of aleatoric uncertainty.
In our work we generally assume that there is no observation noise and therefore the aleatoric uncertainty only consists of process noise.

** Learning in Dynamical Systems
*** maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{\MakeUppercase{\timeInd}}}

\newcommand{\stateDomain}{\ensuremath{{\mathcal{X}}}}
\newcommand{\controlDomain}{\ensuremath{{\mathcal{U}}}}

\newcommand{\dynamicsFunc}{\ensuremath{f}}

\renewcommand{\u}{\ensuremath{\mathbf{u}}}

\renewcommand{\allInput}{\ensuremath{\hat{\state}_{1:\TimeInd}}}
\renewcommand{\allOutput}{\ensuremath{{\Delta\state}_{1:\TimeInd}}}
#+END_EXPORT

*** intro :ignore:
Dynamical systems describe the behaviour of a system over time, $t$, and
are a key component of both control theory and reinforcement learning.
At any given time, $t$, a dynamical system has a state, which is
represented as a vector of real numbers $\state(t) \in \stateDomain \subseteq \R^D$.
For example, the state of a 2-dimensional robotic system with state comprising of 2D Cartesian coordinates
and an orientation would be represented as $\state(t) = [x(t), y(t), \theta(t)]^{T}$.
The system can be controlled by applying control actions $\mathbf{u}(t) \in \controlDomain \subseteq \R^F$ at any given
time, $t$.
This work considers continuous-time, continuous-state, nonlinear stochastic dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-unimodal-dynamics-cont}
\stateDerivative(t) &= \dynamicsFunc(\state(t), \control(t)) + \epsilon(t) \quad \forall t
\end{align}
#+END_EXPORT
where $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ represents the transition dynamics function and $\epsilon(t)$
is an i.i.d process noise term with $\E[\epsilon(t)] = 0$.
The process noise term accounts for unwanted (and, in general, unknown) disturbances of the system.
For example, it is extremely hard to model aerodynamic effects on aircraft so these could be accounted
for in the process noise term.
Throughout this dissertation it is assumed that the state $\state$ is observed directly and is not subject to
observation noise.
This is a standard assumption in the \acrfull{mdp} framework which is commonly adopted
in the reinforcement learning literature.
\todo[inline]{Slightly weird referring to MDP literature on continuous time setting}

\todo[inline]{Should I make a bigger point out of the observation noise assumption}

# The system is controlled via a policy $\control(t) = \pi(\state(t), t)$, which given the state $\state(t)$
# and time step $t$ decides which control action $\control(t)$ to apply to the system.
# The policy can be time-dependent and can also depend on all past information up to time step $t$.
# In the time-independent case the policy is denoted $\pi(\state(t))$ and
# the resulting closed-loop system is denoted $f_{\pi}(\state) = \dynamicsFunc(\state,\pi(\state))$.

*** Probabilistic Modelling
/Mathematical models/ are compact representations (sets of assumptions) that attempt to capture key features of the
phenomenon of interest, in a precise mathematical form.
Probabilistic modelling provides the capability of constructing /mathematical models/
that can represent and manipulate uncertainty in data, models, decisions and predictions.
As such, linking observed data to underlying phenomena through probabilistic models,
is an interesting direction for modelling, analysing and controlling dynamical systems.

Dynamical systems give rise to temporal observations arriving as a sequence
$\state_{1:\TimeInd} = \{\state_1, \ldots, \state_\TimeInd\}$.
\parmarginnote{measurement noise}
These measurements are often corrupted by (observation) noise due to imperfections in the measurement process.
Even when it is known that there is uncertainty in the measurement process, there still remains uncertainty about its
form.

Given our current understanding of the real-world, many dynamical systems also appear to be
\parmarginnote{process noise}
inherently stochastic.
This is due to our inability to accurately model certain phenomena (e.g. turbulence).
Stochasticity arising from state transitions within a system is known as process noise.
Observation and process noise are the constituent sources of /aleatoric uncertainty/;
uncertainties that are inherent in a system and cannot be reduced.

The structure of models can also be uncertain and there may be unobserved (aka latent) variables present.
The introduction of latent variables in probabilistic models is one of the key components providing
them with interesting and powerful capabilities.
A further form of uncertainty in models arises from unknown model parameters, $\theta$.
\parmarginnote{epistemic uncertainty}
It is worth considering the implications of all these types of uncertainty.
If a predictive dynamics model is learned from observations, then the model
can only be confident predicting near the training observations.
As one extrapolates away from the training observations, one can no longer be certain in the prediction;
this is known as epistemic uncertainty and can be reduced by collecting more data and retraining a model.

Probability theory enables this uncertainty to be represented and manipulated;
It provide a systematic way to combine observations with existing knowledge,
via a /mathematical model/.



# If we have used observations of a system to train a predictive model then we can only be confident in predictions near our observations.
# As we extrapolate away from the observations we can no longer be certain and this is known as
# epistemic uncertainty.
# It can be reduced by collecting more data and retraining a model.
# This is shown in Figure [[ref:epistemic]].
# #+NAME: epistemic
# #+ATTR_LATEX: :width 1.\textwidth :placement [h] :center nil
# #+caption: Plot demonstrating the concept of epistemic uncertainty. We can be confident in the learnt function close to our observations but as we exptrapolate away from them we become uncertain what the function should look like. If we have a notion of epistemic uncertainty in a MBRL algorithm we can use it to encourage the agent to visit these areas and collect data, which in turn will reduce the epistemic uncertainty.
# [[file:images/limited_data.pdf]]


# #+NAME: bimodal-dataset
# #+ATTR_LATEX: :width 1.\textwidth :placement [h] :center nil
# #+caption: An artificial 1D dataset with two levels of process noise.
# [[file:images/dataset.pdf]]
# Figure [[ref:bimodal-dataset]] shows an artificial 1D dataset that demonstrates the concept of aleatoric uncertainty.
# In our work we generally assume that there is no observation noise and therefore the aleatoric uncertainty only consists of process noise.

state-space models
dynamics models

*** Discrete-Time Dynamical Systems
When learning dynamical systems it is common that observations of the system
are sampled from the underlying system at a fixed time step, $\Delta \timeInd=t_*$.
The state and control observations at time, $t$, are denoted
$\state_t \in \stateDomain \subseteq \R^\StateDim$ and
$\control_t \in \controlDomain \subseteq \R^\ControlDim$ respectively.
The concatenation of the state and control domains is
denoted as $\inputDomain \coloneqq \stateDomain \times \controlDomain$ and
a single state-control input is denoted as $\singleInput = (\state_{t-1}, \control_{t-1})$.
A time series of observations from time $a$ to time $b$ (inclusive)
is denoted by $\mathbf{x}_{a:b}$ (analogously for other variables).
Given a data set of state transitions $\mathcal{D} = \{\allInput, \allOutput\}$,
it is natural to consider the discrete-time representation of the dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-unimodal-dynamics-disc}
\singleOutput = \dynamicsFunc (\singleState, &\singleControl ; \Delta t = t_*) + \epsilon_{t-1}
\end{align}
#+END_EXPORT

*** Gaussian Processes
This section introduces Gaussian processes and the sparse approximations that are used throughout this work.
Inference techniques with GPs leverage multivariate Gaussian conditioning operations.
Introducing multivariate Gaussians is a natural place to start.

# In particular, it walks through the lower bounds (used to perform variational inference) that are the building blocks
# of the inference in this thesis.

**** Multivariate Gaussian Identities
Gaussian distributions are very popular in machine learning and control theory. This is not only due to their
natural emergence in statistical scenarios (central limit theorem) but also their intuitiveness and
mathematical properties that render their manipulation tractable and easy.

Consider a multivariate Gaussian whose random variables are partitioned into two vectors $\f$ and $\u$.
The joint distribution takes the following form,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-joint-gaussian}
\left[\begin{array}{c}
      \f \\
      \u
\end{array}\right]
\sim\ &\mathcal{N}\left(
\left[\begin{array}{c}
      \bm\mu_{\f} \\
      \bm\mu_{\u}
 \end{array}\right]
\left[\begin{array}{cc}
      \bm\Sigma_{\f\f} & \bm\Sigma_{\f\u} \\
      \bm\Sigma_{\u\f} & \bm\Sigma_{\u\u}
 \end{array}\right]\right),
\end{align*}
#+END_EXPORT
where $\bm\mu_{\f}$ and $\bm\mu_{\u}$ represent the mean vectors, $\bm\Sigma_{\f\f}$ and $\bm\Sigma_{\u\u}$
represent the covariance matrices,
and $\bm\Sigma_{\u\f}$ and $\bm\Sigma_{\f\u}$ represent the cross-covariance matrices.
The marginalisation property of Gaussian distributions states that for two jointly Gaussian random variables,
the marginals are also Gaussian,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gaussian-marginal}
p(\f) &= \int p(\f, \u) \text{d}\u = \mathcal{N} \left(\f \mid \bm\mu_{\f}, \bm\Sigma_{\f\f} \right), \\
p(\u) &= \int p(\f, \u) \text{d}\f = \mathcal{N} \left(\u \mid \bm\mu_{\u}, \bm\Sigma_{\u\u} \right).
\end{align}
#+END_EXPORT
Conveniently, the conditional densities are also Gaussian,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gaussian-conditional}
\f \mid \u &\sim \mathcal{N} \left(\bm\mu_{\f} + \bm\Sigma_{\f\u} \bm\Sigma_{\u\u}^{-1}(\u - \bm\mu_{\u}), \bm\Sigma_{\f\f} - \bm\Sigma_{\f\u}\bm\Sigma_{\u\u}^{-1}\bm\Sigma_{\u\f} \right), \\
\u \mid \f &\sim \mathcal{N} \left(\bm\mu_{\u} + \bm\Sigma_{\u\f} \bm\Sigma_{\f\f}^{-1}(\f - \bm\mu_{\f}), \bm\Sigma_{\u\u} - \bm\Sigma_{\u\f}\bm\Sigma_{\f\f}^{-1}\bm\Sigma_{\f\u} \right).
\end{align}
#+END_EXPORT
Consider the case where $\u$ represents some observations and $\f$ represents a new test location.
cref:eq-gaussian-conditional can be used to make
inferences in the location $\f$ given the observations $\u$, i.e. make
sophisticated interpolations on the measurements, based on their closeness.
In real-world scenarios, it is desirable to consider the entire input domain, instead of simply
pre-selecting a discrete set of locations.
Gaussian processes provide this mathematical machinery.

**** Gaussian Processes
Informally, GPs are a generalisation of the multivariate Gaussian distribution, indexed by an
input domain as opposed to an index set.
Similar to how a sample from an $N-\text{dimensional}$ multivariate Gaussian is an $N-\text{dimensional}$ vector,
a sample from a GP is a random function over its domain.
Formally, a GP is defined as follows,
#+BEGIN_EXPORT latex
\begin{definition}[Gaussian process]
A Gaussian process (GP) is a collection of random variables, any finite number of which
have a joint Gaussian distribution \cite{rasmussenGaussian2006}.
\end{definition}
#+END_EXPORT

# More intuitively, a Gaussian process is a distribution over functions $f : \R^{D_f} \rightarrow \R$
More intuitively, a Gaussian process  is a distribution over functions $f(\cdot): \gpDomain \rightarrow \R$
defined over an input domain $\gpDomain \in \R^{D_f}$.
Whilst Gaussian distributions represent distributions over finite-dimensional vectors, GPs represent
distributions over infinite-dimensional functions.
\marginpar{mean and covariance functions}
A GP is fully defined by a mean function $\mu(\cdot) : \gpDomain \rightarrow \R$ and a covariance
function $k(\cdot, \cdot) : \gpDomain \times \gpDomain \rightarrow \R$ (also known as a kernel),
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-mean-cov}
f(\cdot) &\sim \mathcal{GP}\left(\mu(\cdot), k(\cdot,\cdot) \right), \\
\mu(\cdot) &= \E[f(\cdot)], \\
k(\cdot, \cdot') &= \E[(f(\cdot) -\mu(\cdot)) (f(\cdot') - \mu(\cdot'))],
\end{align}
#+END_EXPORT
Importantly, for a given set of training inputs from the
\marginpar{Gaussian process prior}
functions domain $~{\mathbf{X} = \{ \mathbf{x}_1, \ldots, \mathbf{x}_N \}}$, the associated function values
$\mathbf{f} = \{f(\mathbf{x}_1), \ldots, f(\mathbf{x}_N) \}$,
are jointly Gaussian.
This results in an $N-\text{dimensional}$ multivariate Gaussian random variable $\mathbf{f}$, given by,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-marginal}
\mathbf{f} \sim \mathcal{N}(\mu(\mathbf{X}), k(\mathbf{X}, \mathbf{X})).
\end{equation}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{myquote}
A common kernel that is used throughout this dissertation is the Squared Exponential
kernel with Automatic Relevance Determination (ARD), given by,
\begin{equation} \label{eq-se-kernel}
k(x, x') = \sigma_f^2 \exp\left(-\frac{1}{2} \sum_{d=1}^{D_f} \left( \frac{x_{d}- x'_{i}}{l_d} \right) \right),
\end{equation}
where $\sigma_f^2$ represent the signal variance and $l_d$ is a lengthscale parameter associated with
input dimension $d$.
The lengthscale parameter determines the length of the "wiggles" in the function and
the signal variance $\sigma_f^2$ determines the average deviation of the function from its mean.
\end{myquote}
#+END_EXPORT
Given mean and kernel functions with parameters $\bm\theta$, the marginal distribution is given by,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-prior}
p(\mathbf{f} \mid \mathbf{X}) = \mathcal{N}(\mathbf{f} \mid \mu(\mathbf{X}), k(\mathbf{X}, \mathbf{X})).
\end{equation}
#+END_EXPORT
where the dependency on the parameters $\bm\theta$ has been dropped, i.e.
$p(\f \mid \mathbf{X}) = p(\f \mid \mathbf{X}, \bm\theta)$.
This simplification will be used throughout this dissertation for notational conciseness.
By definition, these observations $\mathbf{f}$ are jointly Gaussian with any un-observed function value
\marginpar{jointly Gaussian}
$f_* = f(\mathbf{x}_*)$ at a new test input,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-gp-joint}
\left[\begin{array}{c}
      \mathbf{f} \\
      f^{*}
\end{array}\right]
\sim\ &\mathcal{N}\left(
\left[\begin{array}{c}
      \mu(\mathbf{X}) \\
      \mu(\mathbf{x}_{*})
 \end{array}\right]
\left[\begin{array}{cc}
      k(\mathbf{X}, \mathbf{X}) & k(\mathbf{X},\mathbf{x}_{*}) \\
      k(\mathbf{x}_{*},\mathbf{X}) & k(\mathbf{x}_{*},\mathbf{x}_{*})
 \end{array}\right]\right).
\end{align*}
#+END_EXPORT
Given the multivariate Gaussian conditionals in cref:eq-gaussian-conditional, it is easy to see how
the distribution over the test function value $f_*$,
\marginpar{noise-free predictions}
can be obtained by conditioning on the observations,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-prediction}
p(f_{*} \mid \mathbf{x}_*, \mathbf{f}, \mathbf{X}) &= \mathcal{N}(f_* \mid \mu, \sigma^2)  \\
\mu &= \mu(\mathbf{x}_*) + k(\mathbf{x}_*, \mathbf{X}) k(\mathbf{X}, \mathbf{X})^{-1} (\mathbf{f} - \mu(\mathbf{X})) \nonumber\\
\sigma^2 &= k(\mathbf{x}_*, \mathbf{x}_*) - k(\mathbf{x}_*, \mathbf{X}) k(\mathbf{X}, \mathbf{X})^{-1} k(\mathbf{X}, \mathbf{x}_*). \nonumber
\end{align}
#+END_EXPORT

It is typical in real-world modelling scenarios that observations of the true function values $\f$
are not directly accessible.
\marginpar{predictions with noise}
Instead, observations are usually corrupted by noise,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-noisy}
\mathbf{y} = f(\mathbf{x}) + \epsilon, \quad \epsilon \sim \mathcal{N}\left( \mathbf{0}, \sigma^2_{n} \mathbf{I} \right).
\end{align}
#+END_EXPORT
where $\sigma^2_n$ is the noise variance.
In this scenario, the function values $\f$ become latent variables and a Gaussian likelihood
is introduced,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-likelihood}
p(\mathbf{y} \mid \f) = \mathcal{N}\left( \mathbf{y} \mid \f, \sigma^2_{n} \mathbf{I} \right),
\end{align}
#+END_EXPORT
to relate the observations to the latent function values $\f$.
The predictive distribution for a test input $\mathbf{x}_*$ follows from cref:eq-gaussian-conditional,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-prediction-noisy}
p(f_{*} \mid \mathbf{x}_*, \mathbf{y}, \mathbf{X}) &= \mathcal{N}(f_* \mid \mu, \sigma^2)  \\
\mu &= \mu(\mathbf{x}_*) + k(\mathbf{x}_*, \mathbf{X})
\left(k(\mathbf{X}, \mathbf{X}) + \sigma^2_n \mathbf{I} \right)^{-1} (\mathbf{y} - \mu(\mathbf{X})) \nonumber\\
\sigma^2 &= k(\mathbf{x}_*, \mathbf{x}_*) -
k(\mathbf{x}_*, \mathbf{X})
\left( k(\mathbf{X}, \mathbf{X}) + \sigma^2_n \mathbf{I} \right))^{-1}
k(\mathbf{X}, \mathbf{x}_*). \nonumber
\end{align}
#+END_EXPORT
This predictive distribution is the GP posterior.


**** Bayesian Model Selection label:sec-bayesian-model-selection
#+BEGIN_EXPORT latex
\newcommand{\weights}{\ensuremath{\mathbf{w}}}
\newcommand{\hyperparameters}{\ensuremath{\bm\theta}}
\newcommand{\modelStructures}{\ensuremath{\mathcal{H}_i}}
\newcommand{\weightPrior}{\ensuremath{p(\weights \mid \hyperparameters, \modelStructures)}}
\newcommand{\weightLikelihood}{\ensuremath{p(\mathbf{y} \mid \mathbf{X}, \weights, \modelStructures)}}
\newcommand{\weightMarginalLikelihood}{\ensuremath{p(\mathbf{y} \mid \mathbf{X}, \hyperparameters, \modelStructures)}}
\newcommand{\weightPosterior}{\ensuremath{p(\weights \mid \mathbf{y}, \mathbf{X}, \hyperparameters, \modelStructures)}}

\newcommand{\hyperPrior}{\ensuremath{p(\hyperparameters \mid \modelStructures)}}
\newcommand{\hyperLikelihood}{\ensuremath{p(\mathbf{y} \mid \mathbf{X}, \hyperparameters, \modelStructures)}}
\newcommand{\hyperMarginalLikelihood}{\ensuremath{p(\mathbf{y} \mid \mathbf{X}, \modelStructures)}}
\newcommand{\hyperPosterior}{\ensuremath{p(\hyperparameters \mid \mathbf{y}, \mathbf{X}, \modelStructures)}}

\newcommand{\modelPrior}{\ensuremath{p(\modelStructures)}}
\newcommand{\modelLikelihood}{\ensuremath{p(\mathbf{y} \mid \mathbf{X}, \modelStructures)}}
\newcommand{\modelMarginalLikelihood}{\ensuremath{p(\mathbf{y} \mid \mathbf{X})}}
\newcommand{\modelPosterior}{\ensuremath{p(\modelStructures \mid \mathbf{y}, \mathbf{X})}}
#+END_EXPORT
The /posterior/ distribution in cref:eq-gp-prediction-noisy considers a fixed covariance function.
In most practical applications it is hard to specify all components of the covariance function /a priori/.
Although some properties of the underlying function may be known, such as stationarity,
it may not be easy to specify the hyperparameters, e.g. lengthscales.
The parameters of the /likelihood/ may also be hard to specify /a priori/, e.g. noise variance.
In order to deploy Gaussian process models in practical applications it is important to address
this model selection problem.



Bayesian model selection provides a principled framework for performing learning in probabilistic models.
This section gives a brief overview of the main ideas in Bayesian model selection.
The overview is general but highlights the difficulties that arise when working with Gaussian
process models.

In machine learning it is common to use hierarchical models.
At the lowest level are model parameters $\weights$, such as the weights of a neural network,
or the weights in linear regression.
\marginpar{hierarchical models}
At the next level are hyperparameters $\hyperparameters$, which serve as parameters in the distributions
over the model parameters at the lowest level.
For example, kernel hyperparameters in Gaussian process regression or the weight decay term in
neural networks.
At the highest level is the underlying model structure,
that is, there may be a set of (discrete) model structures $\modelStructures$ to be considered.
For example, the functional form of the covariance function.

*Maximum Likelihood*
Let us quickly introduce a simple (non Bayesian) approach to finding the best
values for the parameters.
This approach finds the best parameter settings by maximising the likelihood $\weightLikelihood$
with respect to the parameters $\weights$.
\marginpar{maximum likelihood}
This approach is known as maximum likelihood and is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-maximum-likelihood}
\weights = \text{arg}\max_{\weights} \weightLikelihood,
\end{align}
#+END_EXPORT
This obtains a point estimate for the "best" parameter values $\weights$.
Unfortunately, as the likelihood function is higher for more complex model structures,
this approach leads to overfitting.

# This can be overcome by noticing that integrating out the parameters (instead of optimising them),
# automatically protects from overfitting.
A more principled approach -- that overcomes overfitting -- is to treat the parameters as random variables
(i.e. place priors over them), and integrate them out (instead of optimising them).
This is advantageous as it considers all possible settings of the parameters,
penalising complex models and preventing overfitting.
This is known as /Bayesian Occam's razor/ citep:mackayProbable1995,rasmussenOccam2001,murrayNote2005
named after the principle of /Occam's razor/ (William of Occam 1285-1349).
The principle says that "one should pick the simplest model that adequately explains the data".
# This provides automatic Occam's razor, penalising complex models and preventing overfitting.
# and returns the
# posterior distribution $p(\mathbf{\Theta} | \mathbf{Y})$ over the unknown variables,
# in contrast to the point estimate in maximum likelihood.

Model selection takes place one step at a time, by repeatedly applying the rules of probability theory,
see cite:mackayBayesian1992 for more details on this framework.
At the lowest level, the /posterior/ distribution over the model parameters $\weightPosterior$
\marginpar{level 1 inference}
is obtained via Bayes' rule,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-bayes-rule-1}
\weightPosterior = \frac{\weightLikelihood \weightPrior}{\weightMarginalLikelihood}
\end{align}
#+END_EXPORT
where $\weightLikelihood$ is the /likelihood/, a statistical model relating the data to the model
parameters and $\weightPrior$ is the /prior/ over the parameters.
The /prior/ distribution encodes our initial belief in the parameters.
It may take a particular form to reflect the underlying structure of the problem, or
it may be broad to reflect having little prior knowledge.
The /posterior/ is the object of interest because it
combines information from the /prior/ distribution, with information from the data,
via the /likelihood/.
The denominator $\weightMarginalLikelihood$ is a normalising constant and is independent of the model
parameters $\weights$. It is known as the /marginal likelihood/  (or evidence) and is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood-1}
\weightMarginalLikelihood = \int \weightLikelihood \weightPrior \text{d} \weights,
\end{align}
#+END_EXPORT
where the variables of interest (the model parameters $\weights$) have been integrated out of the
joint distribution.
So far so good, however, how should the hyperparameters $\hyperparameters$ be set?
The same process can be repeated to obtain the /posterior/ distribution over the
\marginpar{level 2 inference}
hyperparameters,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-bayes-rule-2}
\hyperPosterior = \frac{\hyperLikelihood \hyperPrior}{\hyperMarginalLikelihood}
\end{align}
#+END_EXPORT
where $\hyperPrior$ is the /hyper-prior/, a prior over the hyperparameters.
Notice that the /marginal likelihood/ from the lowest level $\weightMarginalLikelihood$
now takes the role of the /likelihood/. Similarly to before, the normalisation constant
is given by integrating out the object of interest (the hyperparameters $\hyperparameters$),
#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood-2}
\hyperMarginalLikelihood = \int \hyperLikelihood \hyperPrior \text{d} \hyperparameters.
\end{align}
#+END_EXPORT
Finally, at the top level, the /posterior/ distribution over the model structure $\modelPosterior$
\marginpar{level 3 inference}
is computed,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-bayes-rule-3}
\modelPosterior = \frac{\modelLikelihood \modelPrior}{\modelMarginalLikelihood},
\end{align}
#+END_EXPORT
where $\modelMarginalLikelihood = \sum_{i} \modelLikelihood \modelPrior$.
It is important to note how performing Bayesian inference requires the computation of
integrals in the denominator of Bayes' rule.
These integrals are often not analytically tractable, due to details of the model.
As such, approximate inference techniques are often required.
These approximations may be analytical or based on Markov Chain Monte Carlo (MCMC) methods.

*Type II Maximum Likelihood*
In Gaussian process methods, the marginalisation of the hyperparameters in cref:eq-marginal-likelihood-2
is often difficult.
\marginpar{type II maximum likelihood}
When it is not possible to marginalise all of the variables, the next best thing to a full Bayesian
treatment -- without resorting to Maximum Likelihood --
is to marginalise as many variables as possible, and optimise the remaining variables subject to maximising
the /marginal likelihood/.
This optimisation is known as Type II Maximum Likelihood and is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-type-2-maximum-likelihood}
\weights &= \text{arg}\max_{\weights} \weightMarginalLikelihood \\
&= \text{arg}\max_{\weights} \int \weightLikelihood \weightPrior \text{d} \weights,
\end{align}
#+END_EXPORT


**** old :noexport:
# The integral in cref:eq:type-2-ml is often always tractable so approximations

# In Bayesian machine learning it is common to use GPs to formulate prior distributions over functions
# $p(\f \mid \mathbf{X})$.
# Different likelihood functions can be used to formulate GP regression and GP classification.
# Selecting a Gaussian likelihood

# It is possible to use GPs to place priors over functions in both the regression and classification settings.

# Bayesian inference with GP priors and different likelihood functions
# Performing Bayesian inference with GP priors and different likelihood functions

#+BEGIN_EXPORT latex
\begin{myquote}
Bayesian machine learning seeks to update a prior belief over latent variables $p(\bm\theta)$
given observations $\mathcal{D}$.
Bayesian inference derives the posterior probability from the prior probability $p(\bm\theta)$
and the likelihood function $p(\mathcal{D} \mid \bm\theta)$ --
a statistical model for the observed data -- using Bayes rule,
\begin{align} \label{eq-bayes-rule}
\underbrace{p(\bm\theta \mid \mathcal{D})}_{\text{posterior}} = \frac{p(\mathcal{D} \mid \bm\theta) p(\bm\theta)}{p(\mathcal{D})}
\propto \underbrace{p(\mathcal{D} \mid \bm\theta)}_{\text{likelihood}} \underbrace{p(\bm\theta)}_{\text{\prior}}
\end{align}
where $p(\mathcal{D})$ is the model evidence (aka marginal likelihood).
Given the posterior distribution $p(\bm\theta \mid \mathcal{D})$, it is possible to make predictions by
marginalising the latent variables -- known as Bayesian model averaging.
\begin{align} \label{eq-bayes-ml-prediction}
p(\mathbf{y}_* \mid \mathbf{x}_*, \mathcal{D} = \int
p(\mathbf{y}_* \mid \mathbf{x}_*, \bm\theta, \mathcal{D})
\underbrace{p(\bm\theta \mid \mathcal{D})}_{\text{posterior}}
\text{d} \bm\theta
\end{align}
\end{myquote}
#+END_EXPORT






**** Model Selection for GP Regression
Bayesian principles are an appealing framework for inference.
However, for most interesting models arising in machine learning, the required computations
(integrals over the latent variables) are analytically intractable.
Gaussian process regression with Gaussian noise is a rare exception, where the posterior
is analytically tractable cref:eq-gp-prediction-noisy.

The power of Gaussian process regression arises from the tractability of the marginal likelihood of the
observed outputs given the observed inputs.
The marginal likelihood is the product of the Gaussian likelihood and the GP prior, with the latent
variables integrated out,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-marginal-likelihood}
p(\mathbf{y} \mid \mathbf{X}, \bm\theta) &= \int
\underbrace{p(\mathbf{y} \mid \f)}_{\text{likelihood}}
\underbrace{p(\f \mid \mathbf{X}, \bm\theta)}_{\text{GP prior}}
\text{d}\f.
%&= \mathcal{N} \left( \mathbf{y} \mid \mu(\mathbf{X}), k(\mathbf{X}, \mathbf{X}) + \sigma^2_n \mathbf{I} \right).
\end{align}
#+END_EXPORT
This marginalisation implies that a whole family of functions are simultaneously considered.
The covariance function determines the properties of the functions (such as smoothness)
and defines a nonparametric form of $f$.
In this setting, the Gaussian process is used as the /prior/ over the latent function $f$
and the Gaussian likelihood is used to relate the data to the model.
Conditioning the /prior/ on the observed data obtains the /posterior/ distribution, which
fits the data.

Although GPs are nonparametric, the mean and kernel functions may contain hyperparameters $\bm\theta$.
In practical settings, it is desirable to find the best hyperparameter settings given the observations.
Unfortunately, specifying (hyper-)priors over the hyperparameters and marginalising them
(cref:eq-marginal-likelihood-2), is often difficult for GP models.
As such, it is common to resort to Type II Maximum Likelihood and maximise the log marginal likelihood
with respect to the kernel hyperparameters $\bm\theta$ and noise variance $\sigma^2_n$.
The log marginal likelihood is given by,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-gp-log-marginal-likelihood}
\text{log} p(\mathbf{y} \mid \mathbf{X}, \bm\theta) =
- \underbrace{\frac{1}{2} \mathbf{y}^T (k(\mathbf{X}, \mathbf{X}) + \sigma^2_n \mathbf{I})^{-1} \mathbf{y}}_{\text{data fit}}
- \underbrace{\frac{1}{2} \log \left| k(\mathbf{X}, \mathbf{X}) + \sigma^2_n \mathbf{I} \right|}_{\text{complexity penalty}}
- \underbrace{\frac{n}{2} \log 2 \pi}_{\text{constant}}
\end{align}
\normalsize
#+END_EXPORT
The three terms in the log marginal likelihood have interpretable roles:
1. Data fit term,
   - $\frac{1}{2} \mathbf{y}^T (k(\mathbf{X}, \mathbf{X}) + \sigma^2_n \mathbf{I})^{-1} \mathbf{y}$,
2. Complexity penalty depending only on the covariance function and the inputs,
   - $\frac{1}{2} \log | k(\mathbf{X}, \mathbf{X}) + \sigma^2_n \mathbf{I} |$,
3. Normalisation constant,
   - $\frac{n}{2} \log 2 \pi$.
Importantly, as discussed in Section ref:sec-bayesian-model-selection,
maximising the log marginal likelihood automatically balances the trade-off
between model complexity and data fit, i.e. it provides automatic Occam's razor.
The resulting optimisation is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-type-2-maximum-likelihood}
\bm\theta = \text{arg}\max_{\bm\theta} \log p(\mathbf{y} \mid \mathbf{X}, \bm\theta)
= \text{arg}\max_{\bm\theta} \log \int p(\mathbf{y} \mid \f) p(\f \mid \mathbf{X}, \bm\theta) \text{d} \f.
\end{align}
#+END_EXPORT

# Making predictions in cref:eq-gp-prediction-noisy is computationally expensive due to conditioning


**** Sparse Gaussian Processes
The nonparametric nature of Gaussian process methods is responsible for their flexibility but also
their shortcomings, namely their memory and computational limitations.
In general, the computational complexity of Gaussian process methods scale with $\mathcal{O}(N^{3})$,
due to the inversion of the $N \times N$ covariance matrix $k(\mathbf{X},\mathbf{X})$.
This makes its application to data sets with more than a few thousand data points prohibitive.
The main direction in the literature attempting to overcome this limitation is sparse approximations
cite:titsiasVariational2009,snelsonSparse2005a,quinonero-candelaUnifying2005,leibfriedTutorial2021.
Many approximation schemes have been proposed to reduce the computational complexity, see
cite:quinonero-candelaUnifying2005 for a review.


# The log marginal likelihood in cref:eq-gp-log-marginal-likelihood is computationally
# expensive due to the inversion of the covariance matrix evaluated between all of the
# training inputs $\mathbf{X}$.
# In general, the computational complexity of Gaussian process methods scale with $\mathcal{O}(N^{3})$,
# due to the inversion of the covariance matrix $k(\mathbf{X},\mathbf{X})$.


***** Variational Sparse Gaussian Processes

**** Sparse Gaussian Processes for Regression label:sec-sparse-gps

Consider another Gaussian density $q(\u)$ over the vector $\u$, as well as $p(\u)$, given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gaussian-marginal-q}
q(\u) &= \mathcal{N} \left(\u \mid \mathbf{m}_{\u}, \mathbf{S}_{\u\u} \right)
\end{align}
#+END_EXPORT
with mean $\mathbf{m}_{\u}$ and covariance $\mathbf{S}_{\u\u}$.
The marginal distribution over $\f$ can be obtained by conditioning on $\u$ and marginalising w.r.t. this
new distribution $q(\f) = \int p(\f \mid \u) q(\u) \text{d} \u$.
This marginal distribution is also Gaussian,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gaussian-conditional-q}
\f \mid \u &\sim \mathcal{N} \left(\bm\mu_{\f} + \bm\Sigma_{\f\u} \bm\Sigma_{\u\u}^{-1}(\mathbf{m}_{\u} - \bm\mu_{\u}), \bm\Sigma_{\f\f} - \bm\Sigma_{\f\u}
\bm\Sigma_{\u\u}^{-1}
\left(\bm\Sigma_{\u\u} - \mathbf{S}_{\u\u} \right)
\bm\Sigma_{\u\u}^{-1}
\bm\Sigma_{\u\f} \right).
\end{align}
#+END_EXPORT
Replacing $q(\u)$ with $p(\u)$ would recover the original marginal distribution $p(\f)$ with
mean $\bm\mu_{\f}$ and covariance $\bm\Sigma_{\f}$, as expected from cref:eq-gaussian-marginal.




This work focuses on inducing point methods citep:snelsonSparse2005a, where the latent variables
are augmented with inducing input-output pairs known as inducing "inputs" $\Z \in \inputDomain$, and inducing
"variables" $\u = f(\Z)$.
Introducing a set of $M$ inducing points from the same GP prior
$p(\u \mid \Z) &= \mathcal{N}(\hat{\mathbf{f}} \mid \mu(\Z), k(\Z, \Z))$
can reduce the computational cost
if $M < N$.
Note that these inducing variables $\u$ are jointly gaussian with the latent function values $\mathbf{f}$,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-spare-gp-joint}
p(\mathbf{f}, \u \mid \mathbf{X}, \mathbf{Z}) =\ &\mathcal{N}\left(
\left[\begin{array}{c}
      \mathbf{f} \\
      \u
\end{array}\right]
      \mid \bm{0} ,
\left[\begin{array}{cc}
      \mathbf{K}_{nn} & \mathbf{K}_{nm} \\
      \mathbf{K}_{nm}^{T} & \mathbf{K}_{mm}
 \end{array}\right]\right).
\end{align*}
#+END_EXPORT
where $\K_{mm}$ is the covariance function evaluated between all inducing inputs $\Z$, and
$\K_{nm}$ is the covariance function evaluated between the data inputs $\mathbf{X}$ and the inducing inputs $\Z$.
This joint distribution can be re-written using the properties of multivariate normal distributions as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-joint-cond}
p(\mathbf{f}, \u \mid \mathbf{X}, \mathbf{Z}) &= p(\mathbf{f} \mid \u) p(\u \mid \mathbf{Z}) \\
&= \mathcal{N}(\mathbf{f} \mid \mathbf{K}_{nm} \mathbf{K}_{mm}^{-1} \u, \mathbf{K}_{nn} - \mathbf{Q}_{nn})
\mathcal{N}(\u \mid \mathbf{0}, \mathbf{K}_{mm})
\end{align}
#+END_EXPORT
where $\mathbf{Q}_{nn} = \mathbf{K}_{nm}\mathbf{K}_{mm}^{-1}\mathbf{K}_{nm}^{T}$.
The full joint distribution is then given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-full-joint}
p(\mathbf{y}, \mathbf{f}, \u \mid \mathbf{X}, \mathbf{Z}) &=  p(\mathbf{y} \mid \mathbf{f}) p(\mathbf{f} \mid \u, \mathbf{X}) p(\u \mid \mathbf{Z})
\end{align}
#+END_EXPORT
Computationally efficient inference is obtained by approximating the integration over $\mathbf{f}$.
The popular Fully Independent Training Conditional (FITC) method (in the case of a Gaussian likelihood)
is obtained via the factorisation: $p(\mathbf{y} \mid \u) \approx \prod^{N}_{n=1} p(y_{n} \mid \u)$.
To obtain a variational approximation Jensen's inequality is first used to bound this conditional,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-bound-cond}
\text{log}p(\mathbf{y}\mid\u) \geq \E_{p(\F \mid \u)} \left[ \text{log} p(\mathbf{y} \mid \F) \right] := \mathcal{L}_{1}.
\end{align}
#+END_EXPORT
This is the standard bound shown in cite:hensmanGaussian2013 ($\mathcal{L}_{1}$ defined in Eq. 1).
In cite:titsiasVariational2009 this bound is then substituted into the marginal likelihood
to obtain a tractable lower bound,
\marginpar{Titsias' bound}
#+BEGIN_EXPORT latex
\begin{align}
\text{log} p(\mathbf{y} \mid \mathbf{X})
&= \text{log} \E_{p(\u \mid \mathbf{Z})} \left[ p(\mathbf{y} \mid \u) \right] \\
&\geq \text{log} \E_{p(\u \mid \mathbf{Z})} \left[ \text{exp}(\mathcal{L}_{1}) \right] := \mathcal{L}_{\text{titsias}} \label{eq-titsias-bound-2} \\
&= \text{log}\mathcal{N}(\mathbf{y} \mid \mathbf{0}, \mathbf{Q}_{nn} + \sigma^{2} \mathbf{I})
- \frac{1}{2\sigma^2}\text{tr}(\K_{nn} - \mathbf{Q}_{nn}) \label{eq-titsias-bound},
\end{align}
#+END_EXPORT
where $\sigma^2$ is the noise variance associated with the Gaussian likelihood.
The bound in cref:eq-titsias-bound becomes exact (i.e. recovers the true log marginal likelihood)
when the inducing inputs $\Z$ equal the
data inputs $\mathbf{X}$ as $\K_{nm}=\K_{mm}=\K_{nn}$ so $\mathbf{Q}_{nn}=\K_{nn}$.

cite:hensmanGaussian2013 noted that this bound has complexity $O(NM^{2})$ so
introduced additional variational parameters to achieve a more computationally scalable bound
with complexity $O(M^{3})$.
Instead of collapsing the inducing points in cref:eq-titsias-bound
they explicitly represent them as a variational distribution,
#+BEGIN_EXPORT latex
\begin{align} \label{eq--inducing-dist}
q(\u) = \mathcal{N}\left(\u \mid \mathbf{m}, \mathbf{S} \right),
\end{align}
#+END_EXPORT
which they use to lower bound the marginal likelihood $p(\mathbf{y} \mid \mathbf{X})$ by
\marginpar{Hensman's bound}
further bounding $\mathcal{L}_{\text{titsias}}$,
#+BEGIN_EXPORT latex
\begin{align}
\mathcal{L}_{\text{titsias}} &= \text{log} \E_{p(\u \mid \mathbf{Z})} \left[ \text{exp}(\mathcal{L}_{1}) \right] \\
&\geq \E_{q(\u)} \left[ \mathcal{L}_{1} \right]
- \text{KL}\left[q(\u) \mid\mid p(\u \mid \mathbf{Z})\right] \\
&\geq \E_{q(\F)} \left[ p(\mathbf{y} \mid \mathbf{f}) \right]
- \text{KL}\left[q(\u) \mid\mid p(\u \mid \mathbf{Z})\right] := \mathcal{L}_{\text{hensman}}, \label{eq-hensman-bound}
\end{align}
#+END_EXPORT
where $q(\mathbf{f}) = \int p(\mathbf{f} \mid \u) q(\u) \text{d} \u$.
This is equivalent to $\mathcal{L}_{3}$ defined in Eq. 4 cite:hensmanGaussian2013.
If the likelihood factorizes across data $p(\mathbf{y} \mid \mathbf{f}) = \prod_{n=1}^{N} p(y_{n} \mid f_{n})$
only the marginals of $q(\mathbf{f})$ are needed to calculate cref:eq-hensman-bound,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-bound-3-fact}
\mathcal{L}_{\text{hensman}}
= \sum_{n=1}^{N}\E_{q(f_{n})} \left[ \text{log} p(y_{n} \mid f_{n}) \right]
- \text{KL}\left[q(\u) \mid\mid p(\u \mid \mathbf{Z}) \right].
\end{align}
#+END_EXPORT
Importantly, this bound is written as a sum over input-output pairs which has induced the necessary conditions
to preform stochastic gradient methods on $q(\u)$.



**** Variational Inference :noexport:
\todo{seek to convert inference into an optimisation}

Variational approaches seek to lower bound the marginal likelihood (or evidence) $p(\mathbf{Y})$ using a functional that depends on variational parameters $q(\mathbf{\Theta})$.

#+BEGIN_EXPORT latex
\begin{align}
	\text{log}\ p(\mathbf{Y}) &= \text{log} \int p(\mathbf{Y}, \mathbf{\Theta}) \text{d}\mathbf{\Theta} = \text{log} \int p(\mathbf{Y} | \mathbf{\Theta}) p(\mathbf{\Theta}) \text{d}\mathbf{\Theta}
	\\
	&= \text{log} \int \frac{q(\mathbf{\Theta})}{q(\mathbf{\Theta})} p(\mathbf{Y}, \mathbf{\Theta}) \text{d}\mathbf{\Theta}
	\\
	&= \text{log} \int q(\mathbf{\Theta}) \frac{p(\mathbf{Y}, \mathbf{\Theta})}{q(\mathbf{\Theta})} \text{d}\mathbf{\Theta}
	\\
	&= \text{log} \E_{q(\mathbf{\Theta})} \bigg[ \frac{p(\mathbf{Y}, \mathbf{\Theta})}{q(\mathbf{\Theta})} \bigg]
	\\
	&\geq \E_{q(\mathbf{\Theta})} \bigg[ \text{log} \frac{p(\mathbf{Y}, \mathbf{\Theta})}{q(\mathbf{\Theta})} \bigg]
\end{align}
#+END_EXPORT

The bound becomes exact if $q(\mathbf{\Theta}) = p(\mathbf{\Theta} | \mathbf{Y})$. The true posterior is intractable but the variational posterior $q(\mathbf{\Theta})$ must be constrained (so that it is tractable). It is common to factorise $q(\mathbf{\Theta})$ with respect to groups of elements belonging to parameter set $\mathbf{\Theta}$, this is known as mean field approximation.



**** Sparse Gaussian Processes :noexport:
A GP cite:edwardrasmussenGaussian2010 is a distribution over functions $f : \R^{D_f} \rightarrow \R$
fully defined by a mean function $\mu(\cdot)$ and a covariance function $k(\cdot, \cdot)$.
For a given set of inputs  from the
functions domain $\mathbf{X} = \{ \mathbf{x}_1, \ldots, \mathbf{x}_N \}$ the associated function values
$\mathbf{f} = \{f(\mathbf{x}_1), \ldots, f(\mathbf{x}_N) \}$
are jointly Gaussian,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-prior}
p(\mathbf{f} \mid \mathbf{X}) = \mathcal{N}(\mathbf{f} \mid \bm\mu_{\mathbf{X}}, \mathbf{K}_{\mathbf{X}, \mathbf{X}})
\end{equation}
#+END_EXPORT
where $\bm\mu_{\mathbf{X}}= \mu(\mathbf{X})$ is the mean vector, and
$\mathbf{K}_{\mathbf{X},\mathbf{X}} = k(\mathbf{X}, \mathbf{X})$ is the covariance function evaluated
between the inputs $\mathbf{X}$.
In this work, the squared exponential covariance function with Automatic Relevance Determination
is used for all GPs.
The distribution over the function value $f_*$ at a new input $\mathbf{x}_*$ (i.e. to make a prediction)
is given by the conditional,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-gp-prediction}
p(f_{*} \mid \mathbf{x}_*, \mathbf{f}, \mathbf{X}) &= \mathcal{N}(f_* \mid \mu, \sigma^2) \numberthis \\
\mu &= \mu_{\mathbf{x}_*} + \mathbf{k}_{\mathbf{x}_*, \mathbf{X}} \mathbf{K}_{\mathbf{X}, \mathbf{X}}^{-1} (\mathbf{f} - \bm\mu_{\mathbf{X}}) \\
\sigma^2 &= k_{\mathbf{x}_*, \mathbf{x}_*} - \mathbf{k}_{\mathbf{x}_*, \mathbf{X}} \mathbf{K}_{\mathbf{X}, \mathbf{X}}^{-1} \mathbf{k}_{\mathbf{X}, \mathbf{x}_*}.
\end{align*}
#+END_EXPORT
The conditional in cref:eq-gp-prediction is computationally expensive due to conditioning
on all of the training data $\mathbf{X}, \mathbf{f}$.
Introducing a set of $M$ inducing points from the same GP prior can reduce the computational cost
if $M < N$.
The inducing inputs are denoted $\bm\xi$ and outputs as
$\hat{\mathbf{f}} = f(\bm\xi)$.
The inducing variables
$\hat{\mathbf{f}}$ are jointly Gaussian with the latent function values $\mathbf{f}$.
The GP predictive distribution can be approximated by conditioning on this
smaller set of inducing variables,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-prediction}
p({f}_{*} \mid \mathbf{x}_*, \mathbf{f}, \mathbf{X}) &\approx
p({f}_* \mid \mathbf{x}_*, \hat{\mathbf{f}}, \bm\xi) \\
p(\hat{\mathbf{f}}) &= \mathcal{N}(\hat{\mathbf{f}} \mid \mathbf{m}, \mathbf{S}).
\end{align}
#+END_EXPORT
The approximation becomes exact when the
inducing variables $\hat{\mathbf{f}}$ are a sufficient statistic
for the latent function values $\mathbf{f}$ cite:Titsias2009.
# The predicted latent function values are then mutually independent given the inducing variables.
# A central assumption is that given enough well
# placed inducing variables $\hat{\mathbf{f}}$ they are a
# sufficient statistic for the latent function values $\mathbf{f}$.

*** Gaussian Processes :noexport:
This section introduces Gaussian processes and the sparse approximations that are used throughout this work.
In particular, it walks through the lower bounds (used to perform variational inference) that are the building blocks
of the inference in this thesis.


#+BEGIN_EXPORT latex
\begin{definition}[Gaussian process]
A Gaussian process (GP) is a collection of random variables, any finite number of which
have a joint Gaussian distribution \cite{rasmussenGaussian2006}.
\end{definition}
#+END_EXPORT

# More intuitively, a Gaussian process is a distribution over functions $f : \R^{D_f} \rightarrow \R$
More intuitively, a Gaussian process  is a distribution over functions $f(\cdot): \inputDomain \rightarrow \R$
defined over an input domain $\inputDomain \in \R^{D_f}$.
Whilst Gaussian distributions represent distributions over finite-dimensional vectors, GPs represent
distributions over infinite-dimensional functions. Vector indexes in multivariate Gaussian random variables
correspond to evaluation points $\mathbf{X} \in \inputDomain$ in GP random functions.
\marginpar{mean and covariance functions}
A GP is fully defined by a mean function $\mu(\cdot) : \inputDomain \rightarrow \R$ and a covariance
function $k(\cdot, \cdot) : \inputDomain \times \inputDomain \rightarrow \R$ (also known as a kernel),
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-mean-cov}
f(\cdot) &\sim \mathcal{GP}\left(\mu(\cdot), k(\cdot,\cdot) \right), \\
\mu(\cdot) &= \E[f(\cdot)], \\
k(\cdot, \cdot') &= \E[(f(\cdot) -\mu(\cdot)) (f(\cdot') - \mu(\cdot'))],
\end{align}
#+END_EXPORT
Importantly, for a given set of training inputs from the
\marginpar{Gaussian process prior}
functions domain $~{\mathbf{X} = \{ \mathbf{x}_1, \ldots, \mathbf{x}_N \}}$, the associated function values
$\mathbf{f} = \{f(\mathbf{x}_1), \ldots, f(\mathbf{x}_N) \}$,
are jointly Gaussian.
This results in an $N-\text{dimensional}$ multivariate Gaussian random variable $\mathbf{f}$.
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-prior}
\mathbf{f} \sim \mathcal{N}(\mu(\mathbf{X}), k(\mathbf{X}, \mathbf{X})).
\end{equation}
#+END_EXPORT
By definition, the training observations $\mathbf{f}$ are jointly Gaussian with any un-observed function value
\marginpar{jointly Gaussian}
$f_*$ at any new test input $\mathbf{x}_*$,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-spare-gp-joint}
\left[\begin{array}{c}
      \mathbf{f} \\
      f^{*}
\end{array}\right]
\sim\ &\mathcal{N}\left(
\left[\begin{array}{c}
      \mu(\mathbf{X}) \\
      \mu(\mathbf{x}_{*})
 \end{array}\right]
\left[\begin{array}{cc}
      k(\mathbf{X}, \mathbf{X}) & k(\mathbf{X},\mathbf{x}_{*}) \\
      k(\mathbf{x}_{*},\mathbf{X}) & k(\mathbf{x}_{*},\mathbf{x}_{*})
 \end{array}\right]\right).
\end{align*}
#+END_EXPORT

# The prior distribution over $\mathbf{f}$ is then given by,
# #+BEGIN_EXPORT latex
# \begin{equation} \label{eq-gp-prior}
# p(\mathbf{f} \mid \mathbf{X}) = \mathcal{N}(\mathbf{f} \mid \mu(\mathbf{X}), k(\mathbf{X}, \mathbf{X})).
# \end{equation}
# #+END_EXPORT
# By definition, the training observations $\mathbf{f}$ are jointly Gaussian with any un-observed function value
# \marginpar{jointly Gaussian}
# $f_*$ at a new test input $\mathbf{x}_*$,
# #+BEGIN_EXPORT latex
# \begin{align*} \label{eq-spare-gp-joint}
# \left[\begin{array}{c}
#       \mathbf{f} \\
#       f^{*}
# \end{array}\right]
# \sim\ &\mathcal{N}\left(
#       \bm{0} ,
# \left[\begin{array}{cc}
#       k(\mathbf{X}, \mathbf{X}) & k(\mathbf{X},\mathbf{x}_{*}) \\
#       k(\mathbf{x}_{*},\mathbf{X}) & k(\mathbf{x}_{*},\mathbf{x}_{*})
#  \end{array}\right]\right).
# \end{align*}
# #+END_EXPORT
The distribution over the test function value $f_*$ (i.e. to make a prediction)
\marginpar{noise-free predictions}
can therefore easily be obtained by conditioning on the prior observations.
This is easily obtained using the properties of multivariate Normal distributions to give
the predictive conditional distribution,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-prediction}
p(f_{*} \mid \mathbf{x}_*, \mathbf{f}, \mathbf{X}) &= \mathcal{N}(f_* \mid \mu, \sigma^2)  \\
\mu &= \mu(\mathbf{x}_*) + k(\mathbf{x}_*, \mathbf{X}) k(\mathbf{X}, \mathbf{X})^{-1} (\mathbf{f} - \mu(\mathbf{X})) \nonumber\\
\sigma^2 &= k(\mathbf{x}_*, \mathbf{x}_*) - k(\mathbf{x}_*, \mathbf{X}) k(\mathbf{X}, \mathbf{X})^{-1} k(\mathbf{X}, \mathbf{x}_*). \nonumber
\end{align}
#+END_EXPORT
The conditional in cref:eq-gp-prediction is computationally expensive due to conditioning
on all of the training data $\mathbf{X}, \mathbf{f}$.
The computational complexity of Gaussian process methods scale with $\mathcal{O}(N^{3})$
due to the inversion of the covariance matrix $k(\mathbf{X},\mathbf{X})$.
Many approximation schemes have been proposed to reduce the computational complexity, see
cite:quinonero-candelaUnifying2005 for a review.

**** Bayesian Inference with Gaussian Processes


\todo{introduce noisy outputs and Gaussian likelihood}

Suppose we have some observations $\mathbf{Y}$ generated from a family of models $p(\mathbf{Y}, \mathbf{\Theta})$, where $\mathbf{\Theta}$ represents the unknown random variables that the model depends on.

*Maximum Likelihood*
In maximum likelihood we seek to find the best model by maximising the likelihood $p(\mathbf{Y} | \mathbf{\Theta})$. We obtain a point estimate for the "best" variables $\mathbf{\Theta}$. The likelihood function is higher for more complex model structures, leading to overfitting.

*Type II MLE/MAP*
Bayesian methods overcome overfitting by treating the model parameters as random variables and averaging over the likelihood for different settings of the parameters. They achieve this by maximising the logarithm of the marginal likelihood (or evidence) $p(\mathbf{Y})$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq:type-2-ml}
	\text{log}\ p(\mathbf{Y}) &= \text{log} \int p(\mathbf{Y}, \mathbf{\Theta}) \text{d}\mathbf{\Theta} = \text{log} \int p(\mathbf{Y} | \mathbf{\Theta}) p(\mathbf{\Theta}) \text{d}\mathbf{\Theta}.
\end{align}
#+END_EXPORT
This is advantageous as we now take a weighted average over all possible settings of $\mathbf{\Theta}$ and we obtain the posterior $p(\mathbf{\Theta} | \mathbf{Y})$ for the unknown variables, as opposed to just a point estimate as in maximum likelihood. This provides automatic Occam's razor, penalising complex models and preventing overfitting.

The integral in cref:eq:type-2-ml is often always tractable so approximations

**** Variational Inference
\todo{seek to convert inference into an optimisation}

Variational approaches seek to lower bound the marginal likelihood (or evidence) $p(\mathbf{Y})$ using a functional that depends on variational parameters $q(\mathbf{\Theta})$.

#+BEGIN_EXPORT latex
\begin{align}
	\text{log}\ p(\mathbf{Y}) &= \text{log} \int p(\mathbf{Y}, \mathbf{\Theta}) \text{d}\mathbf{\Theta} = \text{log} \int p(\mathbf{Y} | \mathbf{\Theta}) p(\mathbf{\Theta}) \text{d}\mathbf{\Theta}
	\\
	&= \text{log} \int \frac{q(\mathbf{\Theta})}{q(\mathbf{\Theta})} p(\mathbf{Y}, \mathbf{\Theta}) \text{d}\mathbf{\Theta}
	\\
	&= \text{log} \int q(\mathbf{\Theta}) \frac{p(\mathbf{Y}, \mathbf{\Theta})}{q(\mathbf{\Theta})} \text{d}\mathbf{\Theta}
	\\
	&= \text{log} \E_{q(\mathbf{\Theta})} \bigg[ \frac{p(\mathbf{Y}, \mathbf{\Theta})}{q(\mathbf{\Theta})} \bigg]
	\\
	&\geq \E_{q(\mathbf{\Theta})} \bigg[ \text{log} \frac{p(\mathbf{Y}, \mathbf{\Theta})}{q(\mathbf{\Theta})} \bigg]
\end{align}
#+END_EXPORT

The bound becomes exact if $q(\mathbf{\Theta}) = p(\mathbf{\Theta} | \mathbf{Y})$. The true posterior is intractable but the variational posterior $q(\mathbf{\Theta})$ must be constrained (so that it is tractable). It is common to factorise $q(\mathbf{\Theta})$ with respect to groups of elements belonging to parameter set $\mathbf{\Theta}$, this is known as mean field approximation.




**** Sparse Gaussian Processes for Regression label:sec-sparse-gps

Consider another Gaussian density $q(\u)$ over the vector $\u$, as well as $p(\u)$, given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gaussian-marginal-q}
q(\u) &= \mathcal{N} \left(\u \mid \mathbf{m}_{\u}, \mathbf{S}_{\u\u} \right)
\end{align}
#+END_EXPORT
with mean $\mathbf{m}_{\u}$ and covariance $\mathbf{S}_{\u\u}$.
The marginal distribution over $\f$ can be obtained by conditioning on $\u$ and marginalising w.r.t. this
new distribution $q(\f) = \int p(\f \mid \u) q(\u) \text{d} \u$.
This marginal distribution is also Gaussian,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gaussian-conditional-q}
\f \mid \u &\sim \mathcal{N} \left(\bm\mu_{\f} + \bm\Sigma_{\f\u} \bm\Sigma_{\u\u}^{-1}(\mathbf{m}_{\u} - \bm\mu_{\u}), \bm\Sigma_{\f\f} - \bm\Sigma_{\f\u}
\bm\Sigma_{\u\u}^{-1}
\left(\bm\Sigma_{\u\u} - \mathbf{S}_{\u\u} \right)
\bm\Sigma_{\u\u}^{-1}
\bm\Sigma_{\u\f} \right).
\end{align}
#+END_EXPORT
Replacing $q(\u)$ with $p(\u)$ would recover the original marginal distribution $p(\f)$ with
mean $\bm\mu_{\f}$ and covariance $\bm\Sigma_{\f}$, as expected from cref:eq-gaussian-marginal.


Consider a set of $N$ random variable with a joint Gaussian distribution,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-prior}
\f \sim \mathcal{N}(\mathbf{f} \mid \bm\mu, \bm\Sigma_{\f\f})
= (2\pi)^{-\frac{n}{2}} -\K_{ff}^{-\frac{1}{2}}\exp\left( - \frac{1}{2} \mathbf{f}^T \K_{ff}^{-1} \mathbf{F} \right)
\end{equation}
#+END_EXPORT




The prior distribution over $\mathbf{f}$ is then given by,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-prior}
p(\mathbf{f} \mid \mathbf{X}) = \mathcal{N}(\mathbf{f} \mid \mu(\mathbf{X}), k(\mathbf{X}, \mathbf{X}))
= (2\pi)^{-\frac{n}{2}} -\K_{ff}^{-\frac{1}{2}}\exp\left( - \frac{1}{2} \mathbf{f}^T \K_{ff}^{-1} \mathbf{F} \right)
\end{equation}
#+END_EXPORT


This work focuses on inducing point methods citep:snelsonSparse2005a, where the latent variables
are augmented with inducing input-output pairs known as inducing "inputs" $\Z \in \inputDomain$, and inducing
"variables" $\uF = f(\Z)$.
Introducing a set of $M$ inducing points from the same GP prior
$p(\uF \mid \Z) &= \mathcal{N}(\hat{\mathbf{f}} \mid \mu(\Z), k(\Z, \Z))$
can reduce the computational cost
if $M < N$.
Note that these inducing variables $\uF$ are jointly gaussian with the latent function values $\F$,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-spare-gp-joint}
p(\mathbf{f}, \uF \mid \mathbf{X}) =\ &\mathcal{N}\left(
\left[\begin{array}{c}
      \mathbf{f} \\
      \uF
\end{array}\right]
      \mid \bm{0} ,
\left[\begin{array}{cc}
      \mathbf{K}_{nn} & \mathbf{K}_{nm} \\
      \mathbf{K}_{nm}^{T} & \mathbf{K}_{mm}
 \end{array}\right]\right).
\end{align*}
#+END_EXPORT
where $\K_{mm}$ is the covariance function evaluated between all inducing inputs $\Z$, and
$\K_{nm}$ is the covariance function evaluated between the data inputs $\mathbf{X}$ and the inducing inputs $\Z$.
This joint distribution can be re-written using the properties of multivariate normal distributions as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-joint-cond}
p(\mathbf{f}, \uF \mid \mathbf{X}) &= p(\mathbf{f} \mid \uF) p(\uF \mid \mathbf{X}) \\
&= \mathcal{N}(\mathbf{f} \mid \mathbf{K}_{nm} \mathbf{K}_{mm}^{-1} \uF, \mathbf{K}_{nn} - \mathbf{Q}_{nn})
\mathcal{N}(\uF \mid \mathbf{0}, \mathbf{K}_{mm})
\end{align}
#+END_EXPORT
where $\mathbf{Q}_{nn} = \mathbf{K}_{nm}\mathbf{K}_{mm}^{-1}\mathbf{K}_{nm}^{T}$.
The full joint distribution is then given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-full-joint}
p(\mathbf{y}, \mathbf{f}, \uF \mid \mathbf{X}) &=  p(\mathbf{y} \mid \mathbf{f}) p(\mathbf{f} \mid \uF) p(\uF \mid \mathbf{X})
\end{align}
#+END_EXPORT
Computationally efficient inference is obtained by approximating the integration over $\mathbf{f}$.
\marginpar{FITC}
The popular Fully Independent Training Conditional (FITC) method (in the case of a Gaussian likelihood)
is obtained via the factorisation: $p(\mathbf{y} \mid \uF) \approx \prod^{N}_{n=1} p(y_{n} \mid \uF)$.
To obtain a variational approximation Jensen's inequality is first used to bound this conditional,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-bound-cond}
\text{log}p(\mathbf{y}\mid\uF) \geq \E_{p(\F \mid \uF)} \left[ \text{log} p(\mathbf{y} \mid \F) \right] := \mathcal{L}_{1}.
\end{align}
#+END_EXPORT
This is the standard bound shown in cite:hensmanGaussian2013 ($\mathcal{L}_{1}$ defined in Eq. 1).
In cite:titsiasVariational2009 this bound is then substituted into the marginal likelihood
to obtain a tractable lower bound,
\marginpar{Titsias' bound}
#+BEGIN_EXPORT latex
\begin{align}
\text{log} p(\mathbf{y} \mid \mathbf{X})
&= \text{log} \E_{p(\uF \mid \mathbf{X})} \left[ p(\mathbf{y} \mid \uF) \right] \\
&\geq \text{log} \E_{p(\uF \mid \mathbf{X})} \left[ \text{exp}(\mathcal{L}_{1}) \right] := \mathcal{L}_{\text{titsias}} \label{eq-titsias-bound-2} \\
&= \text{log}\mathcal{N}(\mathbf{y} \mid \mathbf{0}, \mathbf{Q}_{nn} + \sigma^{2} \mathbf{I})
- \frac{1}{2\sigma^2}\text{tr}(\K_{nn} - \mathbf{Q}_{nn}) \label{eq-titsias-bound},
\end{align}
#+END_EXPORT
where $\sigma^2$ is the noise variance associated with the Gaussian likelihood.
The bound in cref:eq-titsias-bound becomes exact (i.e. recovers the true log marginal likelihood)
when the inducing inputs $\Z$ equal the
data inputs $\mathbf{X}$ as $\K_{nm}=\K_{mm}=\K_{nn}$ so $\mathbf{Q}_{nn}=\K_{nn}$.

cite:hensmanGaussian2013 noted that this bound has complexity $O(NM^{2})$ so
introduced additional variational parameters to achieve a more computationally scalable bound
with complexity $O(M^{3})$.
Instead of collapsing the inducing points in cref:eq-titsias-bound
they explicitly represent them as a variational distribution,
#+BEGIN_EXPORT latex
\begin{align} \label{eq--inducing-dist}
q(\uF) = \mathcal{N}\left(\uF \mid \mathbf{m}, \mathbf{S} \right),
\end{align}
#+END_EXPORT
which they use to lower bound the marginal likelihood $p(\mathbf{y} \mid \mathbf{X})$ by
\marginpar{Hensman's bound}
further bounding $\mathcal{L}_{\text{titsias}}$,
#+BEGIN_EXPORT latex
\begin{align}
\mathcal{L}_{\text{titsias}} &= \text{log} \E_{p(\uF \mid \mathbf{X})} \left[ \text{exp}(\mathcal{L}_{1}) \right] \\
&\geq \E_{q(\uF)} \left[ \mathcal{L}_{1} \right]
- \text{KL}\left[q(\uF) \mid\mid p(\uF \mid \mathbf{X})\right] \\
&\geq \E_{q(\F)} \left[ p(\mathbf{y} \mid \F) \right]
- \text{KL}\left[q(\uF) \mid\mid p(\uF \mid \mathbf{X})\right] := \mathcal{L}_{\text{hensman}}, \label{eq-hensman-bound}
\end{align}
#+END_EXPORT
where $q(\F) = \int p(\F \mid \uF) q(\uF) \text{d} \uF$.
This is equivalent to $\mathcal{L}_{3}$ defined in Eq. 4 cite:hensmanGaussian2013.
If the likelihood factorizes across data $p(\mathbf{y} \mid \F) = \prod_{n=1}^{N} p(y_{n} \mid f_{n})$
only the marginals of $q(\F)$ are needed to calculate cref:eq-hensman-bound,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-bound-3-fact}
\mathcal{L}_{\text{hensman}}
= \sum_{n=1}^{N}\E_{q(f_{n})} \left[ \text{log} p(y_{n} \mid f_{n}) \right]
- \text{KL}\left[q(\uF) \mid\mid p(\uF \mid \mathbf{X}) \right].
\end{align}
#+END_EXPORT
Importantly, this bound is written as a sum over input-output pairs which has induced the necessary conditions
to preform stochastic gradient methods on $q(\uF)$.


**** Sparse Gaussian Processes :noexport:
A GP cite:edwardrasmussenGaussian2010 is a distribution over functions $f : \R^{D_f} \rightarrow \R$
fully defined by a mean function $\mu(\cdot)$ and a covariance function $k(\cdot, \cdot)$.
For a given set of inputs  from the
functions domain $\mathbf{X} = \{ \mathbf{x}_1, \ldots, \mathbf{x}_N \}$ the associated function values
$\mathbf{f} = \{f(\mathbf{x}_1), \ldots, f(\mathbf{x}_N) \}$
are jointly Gaussian,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-prior}
p(\mathbf{f} \mid \mathbf{X}) = \mathcal{N}(\mathbf{f} \mid \bm\mu_{\mathbf{X}}, \mathbf{K}_{\mathbf{X}, \mathbf{X}})
\end{equation}
#+END_EXPORT
where $\bm\mu_{\mathbf{X}}= \mu(\mathbf{X})$ is the mean vector, and
$\mathbf{K}_{\mathbf{X},\mathbf{X}} = k(\mathbf{X}, \mathbf{X})$ is the covariance function evaluated
between the inputs $\mathbf{X}$.
In this work, the squared exponential covariance function with Automatic Relevance Determination
is used for all GPs.
The distribution over the function value $f_*$ at a new input $\mathbf{x}_*$ (i.e. to make a prediction)
is given by the conditional,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-gp-prediction}
p(f_{*} \mid \mathbf{x}_*, \mathbf{f}, \mathbf{X}) &= \mathcal{N}(f_* \mid \mu, \sigma^2) \numberthis \\
\mu &= \mu_{\mathbf{x}_*} + \mathbf{k}_{\mathbf{x}_*, \mathbf{X}} \mathbf{K}_{\mathbf{X}, \mathbf{X}}^{-1} (\mathbf{f} - \bm\mu_{\mathbf{X}}) \\
\sigma^2 &= k_{\mathbf{x}_*, \mathbf{x}_*} - \mathbf{k}_{\mathbf{x}_*, \mathbf{X}} \mathbf{K}_{\mathbf{X}, \mathbf{X}}^{-1} \mathbf{k}_{\mathbf{X}, \mathbf{x}_*}.
\end{align*}
#+END_EXPORT
The conditional in cref:eq-gp-prediction is computationally expensive due to conditioning
on all of the training data $\mathbf{X}, \mathbf{f}$.
Introducing a set of $M$ inducing points from the same GP prior can reduce the computational cost
if $M < N$.
The inducing inputs are denoted $\bm\xi$ and outputs as
$\hat{\mathbf{f}} = f(\bm\xi)$.
The inducing variables
$\hat{\mathbf{f}}$ are jointly Gaussian with the latent function values $\mathbf{f}$.
The GP predictive distribution can be approximated by conditioning on this
smaller set of inducing variables,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-prediction}
p({f}_{*} \mid \mathbf{x}_*, \mathbf{f}, \mathbf{X}) &\approx
p({f}_* \mid \mathbf{x}_*, \hat{\mathbf{f}}, \bm\xi) \\
p(\hat{\mathbf{f}}) &= \mathcal{N}(\hat{\mathbf{f}} \mid \mathbf{m}, \mathbf{S}).
\end{align}
#+END_EXPORT
The approximation becomes exact when the
inducing variables $\hat{\mathbf{f}}$ are a sufficient statistic
for the latent function values $\mathbf{f}$ cite:Titsias2009.
# The predicted latent function values are then mutually independent given the inducing variables.
# A central assumption is that given enough well
# placed inducing variables $\hat{\mathbf{f}}$ they are a
# sufficient statistic for the latent function values $\mathbf{f}$.

*** Learning in Multimodal Dynamical Systems
From deep structured mixture of expets

products of experts

Naive-Local-Experts (NLE) cite:trespBayesian2000a,vasudevanGaussian2009a

Products-of-Experts (PoE) cite:cohenHealing2020s

Generalised  cite:gaddEnriched2020


mixture of experts

Gaussian process state-space models
state-space models
cite:doerrProbabilistic2018,schonSystem2011
cite:eleftheriadisIdentification2017
*** Mixtures of Gaussian Process Experts
Gaussian processes (GPs) are the state-of-the art approach for Bayesian nonparametric regression.
However, they suffer from two important limitations.
Firstly, the covariance function is commonly assumed to be stationary
due to the challenge of parameterising them to be non-stationary.
This limits their modelling flexibility.
For example, if the function has a discontinuity due to different underlying lengthscales
in different parts of the input space then a stationary covariance function will not be adequate.
Similarly, if the observations are subject to different noise variances in different regions
of the input space then conventional homoscedastic regression will not suffice.
Secondly, GPs cannot model multimodal predictive distributions,
i.e. where there are multiple
regions of high probability mass with regions of smaller probability mass in between.
GP regression with a Gaussian likelihood models a Gaussian predictive distribution
that is not capable of modelling such multimodal distributions.
Using any likelihood but a Gaussian requires approximate inference techniques which
are usually accompanied by increased computational cost.
# A motivating factor for adopting GP methods is their associated uncertainty quantification.
# #+begin_export latex
# \hl{Standard GP regression (with a Gaussian likelihood) models a Gaussian predictive distribution
# that is not capable of modelling such multimodal distributions.
# Using any likelihood but a Gaussian also results in inference no longer being
# analytically tractable. These likelihoods require approximate inference techniques which
# are usually accompanied by computational issues.}
# #+end_export
# We can think of the first limitation as fitting the predictive mean to the observations
# and the second with correctly quantifying uncertainty in the predictive posterior.

Mixture models have been proposed that
for a set of input $\mathbf{x}$ and output $\mathbf{y}$ observations
can model a multimodal distribution over the output
$p(\mathbf{y} | \mathbf{x}) = \sum_{k=1}^K \pi_k\ p(\mathbf{y} | \alpha=k, \mathbf{x})$.
The predictive distribution $p(\mathbf{y} | \mathbf{x})$ consists of
$k$ mixture components $p(\mathbf{y} | \alpha=k, \mathbf{x})$ that are weighted according to
the mixing coefficients $\pi_k$.
The mixture components may take the form of any distribution, for
example, Bernoulli or Gaussian.
Mixture models assume that each observation belongs to one of the components and then try to infer the
distribution of each component separately.
The capability of these models can be further extended by allowing the mixing coefficients themselves
to be a function of the input variable $x$.
This was introduced by cite:Jacobs1991 in the mixture of experts (ME) model where the mixing coefficients
are known as gating functions $\pi_k(\mathbf{x}) = p(\alpha=k | \mathbf{x})$
(and collectively the gating network).
The individual component densities $p(\mathbf{y} | \alpha=k, \mathbf{x})$ are then referred to as experts
because at different regions in the input space different components are responsible for predicting.
The gating network is governed by an expert indicator variable $\alpha_n \in \{1, ..., K\}$
that assigns each observation to one of the $K$ experts.
# are given by $\pi_k(\mathbf{x}) = p(\alpha=k | \mathbf{x})$ and
# referred to as gating functions (or collectively the gating network).
# This results in the predictive distribution
# $p(\mathbf{y} | \mathbf{x}) = \sum_{k=1}^K \pi_k(\mathbf{x}) p(\mathbf{y} | \alpha=k, \mathbf{x})$.
# The role of the gating network is to indicate which expert is most likely responsible for generating
# the data in a given region of the input space.

Modelling the experts as GPs gives rise to a class of powerful models known as
mixture of GP experts (MoGPE).
These models address both of the limitations that we discussed.
They can model multimodal predictive distributions as they model a mixture of distributions
over the outputs, usually a Gaussian mixture in the regression setting (cite:Tresp,Rasmussen2002).
They are able to model non-stationary functions as
each expert can learn different lengthscales, noise variances etc and the gating
network can turn each one "on" and "off" in different regions of the input space.
Many approaches to MoGPE have been proposed and in general the key differences between them are
the formulation of the gating network
and their inference algorithms (and associated approximations) cite:Yuksel2012.
The original MoGPE work by cite:Tresp proposed a gating network as a softmax of $K$ GPs
(bottom left plot in Figure [[ref:gating_network_comparison]]).
An EM inference scheme is proposed, which assuming there are $N$ observations requires
$\mathcal{O}(3KN^3)$ computations per iteration.
# The original MoGPE work by cite:Tresp proposed a gating network resembling a
# GP classification model (bottom left plot in Figure [[ref:gating_network_comparison]]).
# An EM inference scheme is proposed, requiring $\mathcal{O}(3KN^3)$ computations per iteration,
# assuming that there are $N$ observations.

Although this gating function divides up the input space cite:Rasmussen2002 argue that
data not assigned to a GP expert will lead to bias near the boundaries.
Instead cite:Rasmussen2002 formulate the gating network in terms of conditional
distributions on the expert indicator variable, on which they place an input-dependent
Dirichlet process prior.
cite:Meeds2005 proposed an alternative infinite MoGPE similar to cite:Rasmussen2002 except
that they specify a full generative model over the input and output space $p(\mathbf{y}, \mathbf{x})$
as opposed to just a
conditional model $p(\mathbf{y} | \mathbf{x})$.
Each experts inputs are now Gaussian distributed
(middle top plot of Figure [[ref:gating_network_comparison]]) giving rise to the gating
network in the middle bottom plot of Figure [[ref:gating_network_comparison]].
This gating network is not capable of turning an expert "on" in multiple regions
of the input space. It will instead generate a new cluster and assign it a new expert.
These approaches reduce the computational burden by associating each expert with a
subset of the observations but rely on Markov Chain Monte Carlo (MCMC) inference schemes.
# #+NAME: gating_network_comparison
# #+ATTR_LATEX: :width 0.7\textwidth :placement [h] :center t
# #+caption: Comparison of different gating networks (one per column).
# #+caption: The bottom plots show the expert mixing probabilities $p(\alpha=k|x)$
# #+caption: and the top plots show any associated distributions over the inputs.
# #+caption: The left and right regions (shaded green) are subsets of the domain where expert
# #+caption: one has generated the observations (observations are not shown) and the middle region (shaded blue) where expert two has.
# #+caption: The bottom left plot shows the mixing probabilities of a gating network based on GPs.
# #+caption: The middle top plot shows three separate Gaussian pdfs representing clustering of the inputs
# #+caption: (we have introduced a cluster indicator variable $z$).
# #+caption: The middle bottom plot shows the associated expert mixing probabilities (assuming $z=\alpha$).
# #+caption: In the right top plot we have assumed that there are only two experts (with $z\neq \alpha$) and
# #+caption: marginalised the cluster indicator variable from the plot to the left - leading to
# #+caption: a Gaussian mixture over the inputs.
# #+caption: The plot below shows the resulting mixing probabilities.
# [[file:images/gating-network-comparison-2by3-cropped.pdf]]
cite:Yuan proposed a MoGPE with a variational inference scheme which is much
faster than using MCMC.
They assume that the expert indicators $\alpha$ are generated by a Dirichlet distribution and
that given the expert indicator the inputs follow a Gaussian mixture model (GMM), i.e. they
have introduced a cluster indicator variable $z$.
This contrasts earlier approaches that let the expert indicator act as the input cluster
indicator.
This is illustrated in the right hand plots of Figure [[ref:gating_network_comparison]] which
show two Gaussian mixtures over the inputs (one for each expert) and
the resulting expert mixing probabilities.
Introducing the separate cluster indicator variable has given rise to a
gating network that can turn a single expert "on" in multiple regions of the input space.
However, it does not provide a handle for encoding prior information like the GP based gating network.
# This resembles two separate mixture models, one over the inputs and one over the outputs.

We note that mixture models are inherently unidentifiable as
different combinations of mixture components
and mixing coefficients can generate the same predictive distributions.
The gating network can be considered a handle for encoding prior knowledge that can be used to constrain
the set of admissible functions.
This can improve identifiability and lead to learned representations that reflect
our understanding of the system. The simplest case being reordering the experts.
Figure ref:gating_network_comparison provides a visual comparison of
different gating network formulations,
providing intuition for how they can differ in restricting the set of admissible functions.

# Modelling the relationship between the  input space and the expert indicator variable (gating network)

# with GPs enables
# us to efficiently capture the dependencies through the choice of GP prior.
# Modelling the gating network with GPs enables
# us to efficiently capture the dependencies through the choice of mean and covariance functions.
Modelling the gating network with GPs enables us
to encode informative prior knowledge through the choice of mean and covariance functions.
For example, adopting a squared exponential covariance function would encode prior belief that
the mixing probabilities should vary smoothly across the input space.
If modelling a dynamical system with oscillatory behaviour (e.g. an engine)
a periodic kernel could be adopted.
Prior knowledge of the mixing probability values can be encoded through
the choice of mean function.
We may also be interested in exploiting techniques from differential geometry
e.g. finding probabilistic geodesics (cite:Tosi2014) on the gating network.
Again, our choice of covariance function can be used to encode how differentiable
the gating network should be.
Learning better representations will also improve the models
ability to extrapolate as it will be more true to the underlying system.
# Again, our choice of covariance function can encode how differentiable
# our gating network should be.

# We may also be interested in harnessing the structure learnt by the gating network,
# for example, finding probabilistic geodesics (cite:Tosi2014).
# This would require a differential covariance function to be used.
# Learning better representations will also improve the models
# ability to extrapolate as it will be more true to the underlying system.
# For example, if one wanted to exploit techniques from differential geometry (e.g. finding geodesics)
# they could utilise a differentiable covariance function.
# For example, if one wanted to exploit techniques from differential geometry (e.g. finding geodesics)
# they could utilise a differentiable covariance function.

# The extension from the middle plot of Figure ref:gating_network_comparison
# to modelling a GMM over the inputs (bottom plot) leads to a
# more powerful gating network from a modelling perspective - as it can turn a single expert "on"
# Introducing the separate cluster indicator variable (middle plot of
# Figure ref:gating_network_comparison to bottom plot) gives rise to a
# gating network that can turn a single expert "on" in multiple regions of the input space.
# However, it does not provide a handle for encoding prior information like a GP based gating network.
# The formulation of the gating network is commonly motivated by improving
# the computational complexity of the inference scheme.
# As such, we consider our method (GP based gating network) as trading in the computational
# benefits of gating networks based on GMMs
# (bottom plots of Figure ref:gating_network_comparison)
# for the ability to improve identifiability through informative gating network priors.
# \todo{Insert reference to periodic gating function in results once it's added.}


We can also consider the implications of the mentioned gating networks from an inference perspective.
It is known that developing inference algorithms for propagating uncertain (Gaussian) inputs
through GPs is itself a challenging problem (see cite:Ustyuzhaninov2019).
Therefore, developing a scalable inference algorithm to propagate inputs that are distributed according to
a Gaussian mixture will likely lead to valuable information loss.
Theoretically the approaches by cite:Rasmussen2002,Meeds2005 are able to achieve very
accurate results but their inference relies on MCMC sampling methods,
which can be slow to converge.
# Previous work that formulated a gating network based on GPs cite:Tresp used an EM inference scheme
# which decouples the learning of the gating network and the experts.
# The main contributions of this paper are twofold,
# \todo{What is a true MoE model???}
# 1. We re-formulate a gating network based on GPs to a) improve indentifiability and b) achieve a true mixture of experts model, i.e. not split the observations between experts,
# 2. We derive a variational lower bound which improves the complexity issues associated with inference when adopting a GP based gating network.
#    - Motivated by learning representations of dynamical systems with two regimes we instantiate
#      the model with two experts as it is  a special case where the gating network can be calculated
#      in closed form.
#      We seek to learn an operable mode with one expert and explain away the inoperable mode
#      with the other. With this representation the gating network indicates which regions of the
#      input space correspond to the operable mode and provides a convenient space for planning.
# The GMM formulation of the gating network is motivated by improving
# the computational complexity of the inference scheme.
# We consider our approach as trading in the computational benefits of a GMM gating network
# for the ability to improve identifiability by placing informative GP priors on the gating network.
We consider our approach as trading in the computational benefits that can be obtained through
the formulation of the gating network
for the ability to improve identifiability with informative GP priors.
The main contributions of this paper are twofold. Firstly, we re-formulate a gating network
based on GPs to improve indentifiability.
Secondly, we derive a variational lower bound
which improves on the complexity issues associated with inference when adopting such a gating network.
Motivated by learning representations of dynamical systems with two regimes we instantiate
the model with two experts as it is  a special case where the gating network can be calculated
in closed form.
This is because we seek to learn an operable mode with one expert and explain away the inoperable mode
with the other. This results
in the gating network indicating which regions of the
input space are operable, providing a convenient space for planning.
# This is because we seek to learn an operable mode with one expert and explain away the inoperable mode
# with the other.
# The gating network then indicates which regions of the
# input space are operable, providing a convenient space for planning.
# with the other. With this representation the gating network indicates which regions of the
# input space correspond to the operable mode which provides a convenient space for planning.

The remainder of the paper is organised as follows. We first introduce our model
in Section ref:sec-modelling
and then derive our variational lower
bound in Section ref:sec-variational-approximation.
In Section ref:sec-model-validation we test our method on an artificial data set where we show
the benefits of adopting informative covariance functions in the gating network.
We then test our model on the motorcycle data set and compare it to a sparse variational GP.
# The remainder of the paper is organised as follows. First we introduce our generative model
# in Section ref:sec-modelling where we compare our marginal likelihood to the literature.
# We then derive our variational lower
# bound in Section ref:sec-variational-approximation and detail how we optimise it and make
# predictions.
# In Section ref:sec-model-validation we test our model on an artificial data set where we show
# the benefits of adopting informative covariance functions in the gating network GP.
# We then test our model on the motorcycle data set and compare its performance
# to a sparse variational GP.

*** Learning Multimodal Dynamical Systems
\newline

This chapter is motivated by learning representations of multimodal dynamical systems
that can be exploited by model-based control algorithms.
Probabilistic models provide the capability of constructing mathematical
models that can represent and manipulate uncertainty in data, models, decisions and
predictions.
\marginpar{probabilistic models}
Learning representations of dynamical systems for control using probabilistic models has shown much promise.
For example, see
cite:deisenrothPILCO2011,schneiderExploiting1996,chuaDeep2018,hafnerLearning2019,cutlerEfficient2015,deisenrothPILCO2011,panProbabilistic2014.
# In particular, use Gaussian processes (GPs) to learn

Methods for learning probabilistic multimodal transition dynamics have been proposed.
cite:moerlandLearning2017 use deep generative models, namely a conditional \acrfull{vae},
to learn multimodal transition dynamics for \acrfull{mbrl}.
However, encoding expert domain knowledge into \acrshort{vae}s is difficult, which makes
it hard to ensure that the correct underlying dynamics modes are learned.

Early work combining Hidden Markov Models and linear dynamical systems, known as
switching state-space models citep:ghahramaniVariational2000,ghahramaniSwitching1996,
offer a dynamical generalisation of Mixture of Experts.
These models have been extended to nonlinear dynamics modes but are limited in
the switching behaviour that they can model.
In particular, they do not provide mechanisms for placing informative spatial priors on the switching behaviour.
\todo{is this right about HMM? Can they incorporate GP priors}

# cite:kaiserBayesian2020 use a Bayesian model that learns independent
# dynamics modes whilst maintaining a
# probabilistic belief over which mode is responsible for predicting at a given input location.
# The methods differs as it does not assign observations to modes.
cite:mckinnonLearning2017 use a \acrshort{mogpe} method to learn switching
robot dynamics online.
Their model uses a gating network based on a Dirichlet process to model how the system switches between the underlying
dynamics modes. The Dirichlet process models the switching behaviour via clustering.
As such, it cannot model complex nonlinear dependencies between the switching behaviour
and the state-control inputs.
For example, if a dynamical system oscillates between two dynamics modes over its state-control input space,
their method would assign new dynamics modes at each oscillation.
In contrast, we seek to learn only the two true dynamics modes and turn them
"on" and "off" in different regions of the input space.

*** Gating Networks and Identifiability
#+BEGIN_EXPORT latex
\begin{figure}[t]
    \centering
    \begin{minipage}[r]{0.49\textwidth}
        \includegraphics{images/quadcopter_domain_two_experts.png}
        \subcaption{}
        \label{fig-quadcopter-domain-two-experts}
    \end{minipage}
    \begin{minipage}[r]{0.49\textwidth}
        \subcaption{}
        \includegraphics{images/quadcopter_domain_three_experts.png}
        \label{fig-quadcopter-domain-three-experts}
    \end{minipage}
    \caption{}
    \label{fig-quadcopter-domain--two-vs-three-experts}
\end{figure}
#+END_EXPORT

*** On Mixtures of Gaussian Process Experts and their Identifiability
Gaussian processes are the state-of-the art approach for Bayesian nonparametric regression
and they provide a powerful mechanism for encoding expert domain knowledge.
As such, \acrshort{mogpe} methods are a promising direction for modelling multimodal systems.
Motivated by improving identifiability and learning latent spaces for control, we now
review the \acrshort{mogpe} literature, starting from their origin, the mixture model.

*Mixture Models*
Mixture models are a natural choice for modelling multimodal systems.
Given an input $\singleInput$ and an output $\singleOutput$, mixture models
model a mixture of distributions over the output,
\marginpar{mixture models}
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-mixture-model}
\singleMoeEvidence = \sum_{\modeInd=1}^\ModeInd
\underbrace{\Pr(\singleModeVarK \mid \gatingParams)}_{\text{mixing probability}}
\underbrace{p(\singleOutput \mid \modeVarK, \singleInput, \expertParamsK)}_{\text{component } k}.
\end{equation}
#+END_EXPORT
where $\gatingParams$ and $\expertParams$ represent model parameters
and $\singleModeVar$ is a discrete indicator variable assigning observations to components.
The predictive distribution $\singleMoeEvidence$ consists of
$K$ mixture components ${p(\singleOutput \mid \modeVarK, \singleInput, \expertParamsK)}$ that are weighted according to
their mixing probabilities $\Pr(\modeVarK \mid \gatingParams)$.

*Mixture of Experts* The \acrfull{moe} model citep:jacobsAdaptive1991 is
an extension where the mixing probabilities
depend on the input variable $\moeGatingPosterior$, and are
collectively referred to as the *gating network*.
The individual component densities $\moeExpertPosterior$ are then referred to as *experts*,
as at different regions in the input space, different components are responsible for predicting.
Given $\ModeInd$ experts $\moeExpertPosterior$, each with parameters $\expertParamsK$,
the MoE marginal likelihood is given by,
\marginpar{mixture of experts}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixture-marginal-likelihood}
\moeEvidence = \prod_{\numData=1}^\NumData \sum_{\modeInd=1}^{\ModeInd}
\underbrace{\moeGatingPosterior}_{\text{gating network}}
\underbrace{\moeExpertPosterior}_{\text{expert } k},
\end{align}
#+END_EXPORT
where $\singleModeVar$ is the expert indicator variable assigning observations to experts.
The probability mass function over the expert indicator variable is referred to as the gating network and
indicates which expert governs the model at a given input location.
See cite:yukselTwenty2012 for a survey of \acrshort{moe} methods.

**** Nonparametric Mixtures of Experts :ignore:

\newline
*Nonparametric Mixtures of Experts*
Modelling the experts as GPs gives rise to a class of powerful models known as \acrfull{mogpe}.
They can model multimodal distributions as they model a mixture of distributions
over the outputs, usually a Gaussian mixture in the regression setting citep:trespMixtures2000a,rasmussenInfinite2001.
They are able to model non-stationary functions as
each expert learns separate hyperparameters (lengthscales, noise variances etc).
Many \acrshort{mogpe} methods have been proposed and in general they differ via
the formulation of their gating network and their approximate inference algorithms.
# and the gating network can turn each expert "on" and "off" in different regions of the input space.

As highlighted by cite:rasmussenInfinite2001,
the traditional MoE marginal likelihood does not apply when the
experts are nonparametric.
This is because the model assumes that the observations are i.i.d. given the model parameters,
which is contrary to \acrshort{gp} models, which model the dependencies in the joint distribution, given the
hyperparameters.
\marginpar{mixtures of nonparametric experts}
cite:rasmussenInfinite2001 point out that there is a joint distribution corresponding to every possible
combination of assignments (of observations to experts).
The marginal likelihood is then a sum over exponentially many ($\ModeInd^{\NumData}$) sets of assignments,
#+BEGIN_EXPORT latex
\small
\begin{align}  \label{eq-np-moe-marginal-likelihood-assign}
\moeEvidence &= \sum_{\allModeVar} \npmoeGatingPosterior \npmoeExpertPosterior  \nonumber \\
&= \sum_{\allModeVar} \npmoeGatingPosterior
\left[ \prod_{\modeInd=1}^\ModeInd p\left(\allOutputK \mid \allInputK, \expertParamsK \right) \right],
\end{align}
\normalsize
#+END_EXPORT
where $\allModeVar = \{\modeVar_1, \ldots, \modeVar_\NumData \}$ represents a set of assignments for all observations.
Note that $\allInputK = \{\singleInput : \singleModeVarK \}_{\numData=1}^{\NumData_{\modeInd}}$ and
$\allOutputK = \{\singleOutput : \singleModeVarK \}_{\numData=1}^{\NumData_{\modeInd}}$
represent the $\NumData_{\modeInd}$ inputs and outputs assigned to the $\modeInd^{\text{th}}$
expert respectively.
This distribution factors into the product over experts, where each expert models the joint Gaussian distribution
over the observations assigned to it.
Assuming a mixture of Gaussian process regression models,
the marginal likelihood in cref:eq-np-moe-marginal-likelihood-assign
can be expanded to show each of the experts' latent variables,
#+BEGIN_EXPORT latex
\marginpar{marginal likelihood}
\begin{align} \label{eq-np-moe-marginal-likelihood}
%\moeEvidence
\evidence
%&= \sum_{\allModeVar} \npmoeGatingPosterior \npmoeExpertPosterior  \\
%&= \sum_{\allModeVar} \npmoeGatingPosterior
%\left[ \prod_{\modeInd=1}^\ModeInd p\left(\allOutputK \mid \allInputK, \expertParamsK \right) \right]   \\
&= \sum_{\allModeVar} \npmoeGatingPosterior
\left[ \prod_{\modeInd=1}^\ModeInd
\E_{\expertPrior \right} \left[
\prod_{\numData=1}^{\NumData_{\modeInd}}
p\left(\singleOutput \mid \mode{\latentFunc}(\singleInput) \right)
\right] \right],
\end{align}
#+END_EXPORT
where each expert follows the standard Gaussian likelihood model,
\marginpar{expert's Gaussian likelihood}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-likelihood}
\singleOutput &= \mode{f}(\singleInput) + \epsilon_{k}, \quad \epsilon_{k} \sim \mathcal{N}(0, \mode{\noiseVar}^2), \\
\singleExpertLikelihood &= \mathcal{N}\left( \singleOutput \mid \mode{f}(\singleInput), \mode{\noiseVar}^2 \right),
%\singleOutput \mid \modeVar, \mode{\latentFunc}(\singleInput) &\sim \mathcal{N}\left( \mode{f}(\singleInput), \mode{\noiseVar}^2 \right),
\end{align}
#+END_EXPORT
with $\mode{f}$ and $\mode{\noiseVar}^2$ representing the latent function and the noise variance associated
with the $\modeInd$^{\text{th}} expert.
Note that for notational conciseness the dependency on $\modeVarK$ is dropped from
$\singleExpertLikelihood$ as it is implied by the mode indexing $\mode{\latentFunc}$.
The dependence on $\gatingParams$ and $\expertParams$ is also dropped from here on in.
Independent GP priors are placed on each of the expert's latent functions,
\marginpar{expert's GP prior}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior}
%\expertPrior &= \mathcal{N}\left( \mode{f}(\allInput) \mid \expertMeanFunc(\allInput), \expertCovFunc(\allInput, \allInput) \right) \\
%\mode{\latentFunc}(\allInput) &\sim \mathcal{N}\left( \expertMeanFunc(\allInput), \expertCovFunc(\allInput, \allInput) \right) \\
\mode{\latentFunc}(\allInputK) &\sim \mathcal{N}\left( \expertMeanFunc(\allInputK), \expertCovFunc(\allInputK, \allInputK) \right)
\end{align}
#+END_EXPORT
where $\expertMeanFunc(\cdot)$ and $\expertCovFunc(\cdot, \cdot)$ represent the mean and
covariance functions associated with the $\modeInd^{\text{th}}$ expert respectively.
This leads to each expert resembling a standard \acrshort{gp} regression model with a Gaussian likelihood.
For notational conciseness the dependence on the inputs $\allInputK$ and hyperparameters $\expertParamsK$
is dropped for each \acrshort{gp} prior,
i.e. $\expertPrior = p\left( \mode{f}(\allInputK) \mid \allInputK, \expertParamsK \right)$.
cref:fig-graphical-model-npmoe shows the graphical model representation of this model.
# Note that the expectation over each experts' latent function in cref:eq-np-moe-marginal-likelihood
# is inside the marginalisation of the expert indicator variable $\modeVar$.

**** npmoe graphical model :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
    \begin{minipage}[r]{0.49\textwidth}
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInputK$};

      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\allInputK)$};

      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[latent, below=of x, xshift=0.0cm] (a) {$\modeVarnk$};

      \node[const, right=of a, xshift=-0.4cm] (phik) {$\gatingParams$};

      \node[obs, below=of f, yshift=0.4cm] (y) {$\allOutputK$};

      \node[const, left=of y] (sigmak) {$\noiseVarK$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      %\draw[post] (f)--(yk);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)--(a);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(a);
      %\draw[post] (sigmak)|-(yk);
      \draw[post] (sigmak)|-(y);
      \draw[post] (f)--(y);

      \plate {} {(x) (yk) (a)} {$\NumData_{\modeInd}$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      %\plate {} {(f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(x) (y) (f) (sigmak) (thetak) (a)} {$\ModeInd$};
    \end{tikzpicture}
    %}
    \subcaption{}
\label{fig-graphical-model-npmoe}
\end{minipage}
    \begin{minipage}[r]{0.49\textwidth}
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInputK$};

      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\allInputK)$};
      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[latent, below=of x, xshift=0.0cm] (a) {$\modeVarnk$};
      \node[latent, right=of a, yshift=0.0cm] (h) {$\mode{\gatingFunc}(\allInput)$};
      \node[const, right=of h, xshift=-0.4cm] (phik) {$\gatingParamsK$};

      \node[obs, below=of f, yshift=0.4cm] (y) {$\allOutputK$};
      \node[const, left=of y] (sigmak) {$\noiseVarK$};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      \draw[post] (x)-|(h);
      \draw[post] (h)--(a);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      %\draw[post] (sigmak)|-(yk);
      \draw[post] (sigmak)|-(y);
      \draw[post] (f)--(y);

      \plate {} {(x) (yk) (a)} {$\NumData_{\modeInd}$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      %\plate {} {(f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(h) (phik)} {$\ModeInd$};
      \plate {} {(x) (y) (f) (sigmak) (thetak)  (a)} {$\ModeInd$};
    \end{tikzpicture}
    %}
    \subcaption{}
\label{fig-graphical-model-gp-gating-network}
\end{minipage}
  %\caption{Graphical model where the output $\singleOutput$}
  \caption{
  Graphical models where the outputs $\allOutput = \{\allOutputK\}_{\modeInd=1}^{\ModeInd}$
are generated by mapping the inputs $\allInput = \{\allInputK\}_{\modeInd=1}^{\ModeInd}$ through the latent process.
  An input assigned to expert $\modeInd$ is denoted $\singleInputK$
  and the sets of all $\NumData_{\modeInd}$ inputs and outputs assigned to expert $\modeInd$ are denoted
  $\allInputK$ and $\allOutputK$ respectively.
The experts are shown on the left of each model and the gating network on the right.
The generative process involves evaluating the gating network
and sampling an expert mode indicator variable $\singleModeVar$.
The indicated mode's latent function $\mode{f}$ and noise model $\mathcal{N}(0, \noiseVarK)$ are
then evaluated to generate the output $\singleOutput$.
  (\subref{fig-graphical-model-npmoe}) shows
   the Mixture of Gaussian Process Experts model first presented in
  \cite{rasmussenInfinite2001} but without the Dirichlet process prior on the gating network.
  This represents the basic conditional model, not the full generative model over both the inputs and outputs as
  presented in \cite{NIPS2005_f499d34b}.
  (\subref{fig-graphical-model-gp-gating-network}) shows our model with a GP-based gating network
which involves evaluating $\ModeInd$ latent gating functions $\GatingFunc$
and normalising their output to obtain the mixing probabilities $\singleGatingLikelihood$.
The mode indicator variable $\singleModeVar$ is then sampled from the Categorical distribution
governed by these probabilities.}
\label{fig-graphical-model-comparison}
\end{figure}
#+END_EXPORT


**** Gating Networks and Identifiability
# *** Gating Networks :ignore:
# \newline
# *Gating Networks*
Mixture models are inherently unidentifiable as different combinations of mixture components
and mixing probabilities can generate the same distributions over the output.
The gating network can be considered a handle for encoding prior knowledge that can be used to constrain
the set of admissible functions.
This can improve identifiability and lead to learned representations that better reflect
our understanding of the system. The simplest case being reordering the experts.
cref:gating_network_comparison provides a visual comparison of
different gating network formulations,
providing intuition for how they can differ in restricting the set of admissible functions.

# An EM inference scheme is proposed, which assuming there are $N$ observations, requires
# $\mathcal{O}(3KN^3)$ computations per iteration.
# Importantly, this gating network is capable of turning a single expert "on" in multiple regions of the input space.
The original MoGPE work by cite:trespMixtures2000a, proposed a gating network resembling a GP classification
model, i.e. a softmax likelihood with independent GPs placed over each of the $K$ latent functions.
Importantly, this gating network provides a mechanism for encoding informative prior knowledge,
that can improve identifiability by restricting the set of admissible functions.
This is achieved by placing informative GP priors over the gating functions.
As such, this gating network is capable of modelling complex nonlinear dependencies between the
input-space and the expert indicator variable.
For example, it is capable of turning a single expert "on" in multiple regions of the input space.
This is visualised in the bottom left plot in cref:gating_network_comparison, where
the left and right regions (shaded green) are subsets of the domain where expert
one has generated the observations (observations are not shown) and the middle region (shaded blue) where expert two has.

#+NAME: gating_network_comparison
#+ATTR_LATEX: :width 0.95\textwidth :placement [h] :center t
#+caption: Comparison of different gating networks (one per column).
#+caption: The bottom plots show the expert mixing probabilities $p(\alpha=k|x)$
#+caption: and the top plots show any associated distributions over the inputs.
#+caption: The left and right regions (shaded green) are subsets of the domain where expert
#+caption: one has generated the observations (observations are not shown) and the middle region (shaded blue) where expert two has.
#+caption: The bottom left plot shows the mixing probabilities of a gating network based on GPs.
#+caption: The middle top plot shows three separate Gaussian pdfs representing clustering of the inputs
#+caption: (we have introduced a cluster indicator variable $z$).
#+caption: The middle bottom plot shows the associated expert mixing probabilities (assuming $z=\alpha$).
#+caption: In the right top plot we have assumed that there are only two experts (with $z\neq \alpha$) and
#+caption: marginalised the cluster indicator variable from the plot to the left - leading to
#+caption: a Gaussian mixture over the inputs.
#+caption: The plot below shows the resulting mixing probabilities.
[[file:images/model/gating-network-comparison-2by3.pdf]]
# [[file:images/model/gating-network-comparison-2by3-cropped.pdf]]

# Although this gating network divides up the input space, cite:rasmussenInfinite2001 argue that
# data not assigned to a GP expert will lead to bias near the boundaries.
# Instead they formulate the gating network in terms of conditional
cite:rasmussenInfinite2001 introduced the infinite Mixtures of Gaussian Process Experts method
which automatically infers the number of experts from observations via an input-dependent
Dirichlet process prior.
cite:NIPS2005_f499d34b proposed an alternative infinite MoGPE that models the joint distribution
over the input and output space $p(\allOutput, \allInput)$,
as opposed to just a conditional model $p(\allOutput | \allInput)$.
These methods are not capable of turning a single expert "on" in multiple regions
of the input space. Instead they will generate a new expert.
This is illustrated in the middle column of cref:gating_network_comparison.
The top plot shows clustering of the input space
where each Gaussian cluster $z=\{1,\ldots, \ModeInd\}$
corresponds to the inputs for a particular expert $\alpha = \{1, \ldots, \ModeInd\}$.
These models assume that the cluster indicator variable $z$ equals the expert indicator variable $z=\alpha$.
This gives rise to the gating network in the middle bottom plot of cref:gating_network_comparison, where
a third expert $k=3$ has been introduced to model the right hand region associated with expert one.

# cite:rasmussenInfinite2001 formulate the gating network in terms of conditional
# distributions on the expert indicator variable, on which they place an input-dependent
# Dirichlet process prior.
# cite:Meeds2005 proposed an alternative infinite MoGPE similar to cite:rasmussenInfinite2001 except
# that they specify a full generative model over the input and output space $p(\mathbf{y}, \mathbf{x})$,
# as opposed to just a conditional model $p(\mathbf{y} | \mathbf{x})$.
# The input space is divided into Gaussian clusters where each cluster $z=\{1,\ldots, K\}$
# corresponds to the inputs for a particular expert $\alpha = \{1, \ldots, K\}$.
# This is shown in the middle top plot of Figure [[ref:gating_network_comparison]]
# where the cluster indicator variable equals the expert indicator variable $z=\alpha$.
# This gives rise to the gating network in the middle bottom plot of Figure [[ref:gating_network_comparison]], where
# a third expert $k=3$ has been introduced to model the right hand region associated with expert one.
# This gating network is not capable of turning an expert "on" in multiple regions
# of the input space. Instead it generates a new cluster and assign it a new expert.
# Although these approaches reduce the computational burden by associating each expert with a
# subset of the observations, they rely on Markov Chain Monte Carlo (MCMC) inference schemes.

cite:NIPS2008_f4b9ec30 proposed a MoGPE method with a variational inference scheme, which is much faster
than the previous approaches that rely on MCMC.
Their method introduces a separate cluster indicator variable $z$, resulting in each experts' inputs
following a Gaussian mixture model (GMM).
Introducing the separate cluster indicator variable enables the
gating network to turn a single expert "on" in multiple regions of the input space.
This is illustrated in the right hand plots of Figure [[ref:gating_network_comparison]], which
show Gaussian mixtures over each experts' inputs (top) and the resulting expert mixing probabilities (bottom).
This method enables a single expert to be turned "on" in multiple regions of the input space.
However, it still does not provide a handle for encoding prior information like the GP based gating network.
# prevents an explosion in the number of experts

# cite:Yuan proposed a MoGPE with a variational inference scheme which is much faster than using MCMC.
# They assume that the expert indicators $\alpha$ are generated by a Dirichlet distribution and
# that given the expert indicator the inputs follow a Gaussian mixture model (GMM), i.e. they
# introduce a separate cluster indicator variable $z$.
# This contrasts earlier approaches that let the expert indicator act as the input cluster
# indicator.
# This is illustrated in the right hand plots of Figure [[ref:gating_network_comparison]] which
# show two Gaussian mixtures over the inputs (one for each expert) and
# the resulting expert mixing probabilities.
# Introducing the separate cluster indicator variable has given rise to a
# gating network that can turn a single expert "on" in multiple regions of the input space.
# However, it does not provide a handle for encoding prior information like the GP based gating network.

*GP-based Gating Network* Modelling the gating network with a set of input-dependent gating functions
and a softmax likelihood, enables complex nonlinear dependencies between the expert indicator
variable and the input-space to be modelled.
If knowledge regarding how the model switches between the experts over the input-space is /known a priori/,
then this can be encoded via the choice of mean and covariance functions.
For example, adopting a squared exponential covariance function would encode prior belief that
the mixing probabilities should vary smoothly across the input space.
If modelling a dynamical system which oscillates between different dynamics modes over the input-space
(with a constant frequency), then a periodic kernel could be adopted.
More accurately modelling this dependency will improve identifiability and result in superior generalisation,
i.e. the model will be able to interpolate and extrapolate more accurately.

With regards to constructing convenient /latent spaces/ for control, formulating the gating network GPs
with differentiable mean and covariance functions, enables techniques from Riemannian geometry
to be deployed on the gating functions citep:carmoRiemannian1992.
In particular, our work is interested in finding length minimising trajectories on the
gating functions' GP posteriors, aka geodesic trajectories citep:tosiMetrics2014.


# Modelling the gating network with GPs (resembling a GP classification model) enables
# informative prior knowledge to be encoded through the choice of mean and covariance functions.
# For example, adopting a squared exponential covariance function would encode prior belief that
# the mixing probabilities should vary smoothly across the input space.
# If modelling a dynamical system with oscillatory behaviour (e.g. an engine)
# a periodic kernel could be adopted.
# Prior knowledge of the mixing probability values can be encoded through
# the choice of mean function.
# Our work is interested in exploiting the gating network for techniques from differential geometry,
# in particular, finding geodesics citep:tosiMetrics2014.
# Selecting mean and covariance functions which are differentiable with respect to their inputs, enables
# techniques from Riemannian geometry citep:carmoRiemannian1992 to be deployed on the gating functions.
# Learning better representations will also improve the models
# ability to extrapolate as it will be more true to the underlying system.

More recent MoGPE methods also exist citep:trappDeep2020,nguyenStochastic2018,gaddEnriched2020.
The method by cite:trappDeep2020 presents a Deep Structured Mixtures of Gaussian Processes
based on sum product networks.
It provides exact inference and attractive computational and memory costs but results in worse predictive densities
than a Sparse Variational Gaussian Process (SVGP).
This is indicated by worse Negative Log Predictive Density (NLPD) scores than a SVGP on 6 out of the 7
benchmark data sets it is tested on.
In contrast, our work seeks improved NLPD scores -- relative to a SVGP -- as we seek learned representations that
more accurately model the data distribution.
# cite:gaddEnriched2020 present a method that models the joint distribution of the inputs and targets explicitly.

\todo{talk more about nguyenStochastic2018,gaddEnriched2020}

# *Inference* We can also consider the implications of the mentioned gating networks from an inference perspective.
# It is known that developing inference algorithms for propagating uncertain (Gaussian) inputs
# through GPs is itself a challenging problem (see cite:ustyuzhaninovCompositional2020).
# Therefore, developing a scalable inference algorithm to propagate inputs that are distributed according to
# a Gaussian mixture will likely lead to valuable information loss.
# We consider our approach as trading in the computational benefits that can be obtained through
# the formulation of the gating network for the ability to improve identifiability with informative GP priors.

# Theoretically the approaches by cite:rasmussenInfinite2001,NIPS2005_f499d34b are able to achieve very
# accurate results but their inference relies on MCMC sampling methods, which can be slow to converge.

# The gating network models how the dynamics switch between modes over the input space and is of particular importance
# in this work.
# The gating network is of particular importance in this work as it models how the dynamics switch
# between modes over the input space.

**** Gating Networks and Identifiability
# *** Gating Networks :ignore:
# \newline
# *Gating Networks*
Mixture models are inherently unidentifiable as different combinations of mixture components
and mixing probabilities can generate the same distributions over the output.
The gating network can be considered a handle for encoding prior knowledge that can be used to constrain
the set of admissible functions.
This can improve identifiability and lead to learned representations that better reflect
our understanding of the system. The simplest case being reordering the experts.

cite:rasmussenInfinite2001 introduced the infinite Mixtures of Gaussian Process Experts method
which automatically infers the number of experts from observations via an input-dependent
Dirichlet process prior.
cite:NIPS2005_f499d34b proposed an alternative infinite MoGPE that models the joint distribution
over the input and output space $p(\allOutput, \allInput)$,
as opposed to just a conditional model $p(\allOutput | \allInput)$.
These methods are not capable of turning a single expert "on" in multiple regions
of the input space. Instead they will generate a new expert.

cite:NIPS2008_f4b9ec30 proposed a MoGPE method that
introduced a separate cluster indicator variable $z$, resulting in each experts' inputs
following a Gaussian mixture model (GMM).
Introducing the separate cluster indicator variable enables the
gating network to turn a single expert "on" in multiple regions of the input space.
More recent MoGPE methods also exist citep:trappDeep2020,nguyenStochastic2018,gaddEnriched2020.
However, none of these models provide a handle for encoding prior information like the GP-based gating
network from the original MoGPE work by cite:trespMixtures2000a.

cite:trespMixtures2000a proposed a gating network resembling a GP classification
model, i.e. a softmax likelihood with independent GPs placed over each of the $K$ gating functions.
Modelling the gating network with a set of input-dependent gating functions
and a softmax likelihood, enables complex nonlinear dependencies between the expert indicator
variable and the input-space to be modelled.
If knowledge regarding how the model switches between the experts over the input-space is /known a priori/,
then this can be encoded via the choice of mean and covariance functions.
For example, adopting a squared exponential covariance function would encode prior belief that
the mixing probabilities should vary smoothly across the input space.
Further to this, it is capable of turning a single expert "on" in multiple regions of the input space.
If modelling a dynamical system which oscillates between different dynamics modes over the input-space
(with a constant frequency), then a periodic kernel could be adopted.
More accurately modelling this dependency will improve identifiability and result in superior generalisation.

The inductive bias of a model encodes what solutions one thinks are /a-priori/ likely.
Just as the kernel function controls the support and inductive biases in GP models,
the complex interaction between the gating network and experts takes on this role in \acrshort{mogpe} models.
The gating network models how the system switches between dynamics modes over the input space.
It can be seen as a handle for encoding prior knowledge that can be used to constrain the set of admissible functions.
It is of particular interest in this work as it
can improve identifiability and lead to learned representations that better reflect our understanding of the system.
This work formulates a gating network aimed at,
# The simplest case being reordering the experts.
# This work derives a variational approximation that can,
# As highlighted previously, the gating network can be formulated to hard assign observations to experts with the
# goal of improving the computational problems associated with inference and prediction in MoGPE methods.
# This work derives a variational approximation that can,
# This work chooses to sacrifice these computational benefits in favour of constructing a gating network that can,
1) Improving *identifiability* in MoGPE methods,
   - Selecting informative mean and covariance functions for each gating function GP prior can ensure that each expert corresponds to the underlying dynamics mode that it was intended to model. Learning representations that are more true to the underlying system will also enhance the models ability to generalise away from training observations.
2) Inferring *informative latent structure* that can be exploited for *control*,
   - The goal of cref:chap-traj-opt-geometry is to construct control techniques that attempt to find trajectories that remain in a single dynamics mode and avoid regions of the dynamics that cannot be predicted confidently, e.g. because they have not been observed.
     The GP-based gating network presented here, infers informative geometric structure regarding how the dynamics switches between modes over the input space, whilst providing a principled approach to modelling the epistemic uncertainty associated with the gating functions. This makes the gating network a convenient latent space to project the control problem onto.




# Modelling the gating network with GPs (resembling a GP classification model) enables
# informative prior knowledge to be encoded through the choice of mean and covariance functions.
# For example, adopting a squared exponential covariance function would encode prior belief that
# the mixing probabilities should vary smoothly across the input space.
# If modelling a dynamical system with oscillatory behaviour (e.g. an engine)
# a periodic kernel could be adopted.
# Prior knowledge of the mixing probability values can be encoded through
# the choice of mean function.
# Our work is interested in exploiting the gating network for techniques from differential geometry,
# in particular, finding geodesics citep:tosiMetrics2014.
# Selecting mean and covariance functions which are differentiable with respect to their inputs, enables
# techniques from Riemannian geometry citep:carmoRiemannian1992 to be deployed on the gating functions.
# Learning better representations will also improve the models
# ability to extrapolate as it will be more true to the underlying system.


\todo{talk more about nguyenStochastic2018,gaddEnriched2020}

# *Inference* We can also consider the implications of the mentioned gating networks from an inference perspective.
# It is known that developing inference algorithms for propagating uncertain (Gaussian) inputs
# through GPs is itself a challenging problem (see cite:ustyuzhaninovCompositional2020).
# Therefore, developing a scalable inference algorithm to propagate inputs that are distributed according to
# a Gaussian mixture will likely lead to valuable information loss.
# We consider our approach as trading in the computational benefits that can be obtained through
# the formulation of the gating network for the ability to improve identifiability with informative GP priors.

# Theoretically the approaches by cite:rasmussenInfinite2001,NIPS2005_f499d34b are able to achieve very
# accurate results but their inference relies on MCMC sampling methods, which can be slow to converge.

# The gating network models how the dynamics switch between modes over the input space and is of particular importance
# in this work.
# The gating network is of particular importance in this work as it models how the dynamics switch
# between modes over the input space.

** Optimal Control
*** maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\costFunc}{\ensuremath{c}}
\newcommand{\constraintsFunc}{\ensuremath{g}}

\newcommand{\terminalCostFunc}{\ensuremath{C_T}}
\newcommand{\integralCostFunc}{\ensuremath{C}}

\newcommand{\stateTraj}{\ensuremath{\bar{\state}}}
\newcommand{\controlTraj}{\ensuremath{\bar{\control}}}

\newcommand{\policySpace}{\ensuremath{\Pi}}
\newcommand{\policy}{\ensuremath{\pi}}
#+END_EXPORT

*** intro :ignore:
The problem of (deterministic) optimal control can be summarised as follows:
#+BEGIN_EXPORT latex
\begin{myquote}
Given a dynamical system (Eq. \ref{eq-unimodal-dynamics-cont}) with
states, $\state (t) \in \stateDomain \subseteq \R^{\StateDim}$, and
controls, $\control(t) \in \controlDomain \subseteq \R^{\ControlDim}$,
find a set of controls, $\controlTraj$,
over a time period, $t \in [t_0, t_f]$, that optimises an objective function $J$.
The objective function might be formulated to solve a particular task, or to make the dynamical
system behave in a certain way.
Typically this objective function is given by,
\begin{align} \label{eq-objective}
J \defeq \terminalCostFunc(\state(t_{f}), t_{f})
+ \int^{t_{f}}_{t_{0}}
\integralCostFunc(\state(t), \control(t), t) \text{d}t,
\end{align}
which consists of two terms:
\begin{itemize}
\item a terminal cost $\terminalCostFunc : \stateDomain \times \R \rightarrow \R$,
\item an integral cost (or Lagrangian) $\integralCostFunc : \stateDomain \times \controlDomain \times \R \rightarrow \R$.
\end{itemize}
\end{myquote}
#+END_EXPORT
In practice, the system is often corrupted by process noise, or is not completely known, so the resulting
dynamical system is actually stochastic.
As such, the definition above must be adapted for stochastic systems.
In stochastic systems, a control sequence is not associated with a specific
trajectory, but rather a distribution over trajectories.
The resulting problem is known as stochastic optimal control (SOC).

This section reviews the relevant background on SOC,
in particular, trajectory optimisation algorithms for controlling
stochastic systems that have been learned from observations.

*** Stochastic Optimal Control
Let us formalise the stochastic optimal control problem here.
Given a stochastic, fully-observed, nonlinear system,
with transition dynamics,
${\dynamicsFunc: \stateDomain \times \controlDomain \rightarrow \stateDomain}$,
SOC seeks to find the control sequence (trajectory), $\controlTraj$, over a horizon,
$\TimeInd$, that minimises a cost
function, ${\integralCostFunc: \stateDomain \times \controlDomain \rightarrow \R}$,
in expectation over the stochastic dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-soc-problem-statement}
\min_{\controlTraj} &\E_{\state(\cdot), \control(\cdot) \mid \state(0), \policy}\left[ \terminalCostFunc(\state(T))
+ \int_{t_0}^{T} \integralCostFunc(\state(t), \control(t), t) \right] \\
\text{s.t.} \quad &\state(t) = \dynamicsFunc(\state(t), \control(t)) \text{d}t + \epsilon_\modeInd,
\quad \epsilon \sim \mathcal{N}(\mathbf{0}, \Sigma_{\epsilon})
\end{align}
#+END_EXPORT
*Policy Space* The policy space, $\policySpace$, defines the set of policies which optimisation is performed over,
i.e. it is the optimisation domain.
In this dissertation, a policy, $\policy$, shall be a conditional distribution over $\controlDomain$.
In (deterministic) optimal control the policy space is usually given by open loop policies, i.e.
only conditioned on time $\policy(\control \mid t)$.
In contrast, SOC (and RL) are mainly interested in feedback policies, where the controls are conditioned on both time and
state, i.e. $\policy(\control(t) \mid t, \state(t))$
\footnote{for notational conciseness dependency on time is assumed from here on, i.e.
$\policy(\control(t) \mid t, \state(t) = \policy(\control(t) \mid \state(t))$}.
A special case are deterministic policies, given by $\policy(\control \mid \state) = \delta_{\control=f(\state)}$,
where $\delta$ is the dirac delta distribution and $f$ is a function.

*** Overview
#+BEGIN_EXPORT latex
\begin{figure}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}
  %[scale=.7,auto=center,every node/.style={fill=blue!20}] % here, node/.style is the style pre-defined, that will be the default layout of all the nodes. You can also create different forms for different nodes.
  %\tikzstyle{highlight}  = [fill=red!20, color=red!60]
  [
    every node/.style={rectangle, fill=blue!20, very thick, minimum size=7mm},
    highlight/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
    discretegeneration/.style={rectangle, draw=red!60, fill=blude!20, very thick, minimum size=7mm},
  ]

  \node (oc) at (1,2) {Optimal Control};
  \node[highlight, below=of oc, xshift=-4.8cm] (ol) {Open Loop};
  \node[highlight, below=of oc, xshift=4.8cm] (cl) {Closed Loop};

  \node[highlight, below=of oc] (mpc) {MPC};

  \node[highlight, below=of ol, xshift=-2.6cm] (i) {Indirect Methods};
  \node[highlight, below=of ol, xshift=2.6cm] (d) {Direct Methods};

  \node[highlight, below=of d] (gd) {GD};

  \node[highlight, below=of cl, xshift=-2.6cm] (dp) {Dynamic Programming};
  \node[below=of cl, xshift=2.6cm] (hjb) {HJB/HJI};

  \node[discretegeneration, below=of dp] (ilqr) {iLQR};
  \node[left=of ilqr] (lqr) {LQR};
  \node[right=of ilqr] (ddp) {DDP};

  \draw[->] (oc) -- (cl);
  \draw[->] (oc) -- (ol);
  \draw[->] (ol) -- (i);
  \draw[->] (ol) -- (d);
  \draw[->] (cl) -- (dp);
  \draw[->] (cl) -- (hjb);
  \draw[->] (ol) -- (mpc);
  \draw[->] (mpc) -- (cl);
  \draw[->] (dp) -- (ilqr);
  \draw[->] (dp) -- (lqr);
  \draw[->] (dp) -- (ddp);
\end{tikzpicture}
}
\caption{Roadmap of optimal control where red indicates applicability in discrete-time setting.}
\label{fig-optimal-control-roadmap}
\end{figure}
#+END_EXPORT

*** Problem Statement :noexport:
Let us formalise the stochastic optimal control problem here.
Given a stochastic, discrete-time, fully-observed, nonlinear system,
with transition dynamics,
${\dynamicsFunc: \stateDomain \times \controlDomain \rightarrow \stateDomain}$,
SOC seeks to find the control sequence (trajectory), $\controlTraj$, over a horizon,
$\TimeInd$, that minimises a cost
function, ${\integralCostFunc: \stateDomain \times \controlDomain \rightarrow \R}$,
in expectation over the stochastic dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-soc-problem-statement}
\min_{\controlTraj} &\E_{\state(\cdot), \control(\cdot) \mid \state(0), \policy}\left[ \terminalCostFunc(\state_\TimeInd)
+ \sum_{\timeInd=1}^{\TimeInd-1} \integralCostFunc(\state_\timeInd, \control_\timeInd) \right] \\
\text{s.t.} \quad &\state_{\timeInd+1} = \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \epsilon_\modeInd,
\quad \epsilon \sim \mathcal{N}(\mathbf{0}, \Sigma_{\epsilon})
\end{align}
#+END_EXPORT

*** Dynamic Programming

Differential dynamic programming (DDP) citep:jacobsonDifferential1970, iteratively constructs
local Taylor series approximations of the dynamics and cost to locally approximate the
value function. As such, it exploits the temporal structure to obtain a closed-loop controller.
The extension of DDP to stochastic dynamics (SDDP) citep:theodorouStochastic2010, considers
expected costs under Gaussian disturbances.
Iterative linear quadratic Gaussian (iLQG) control citep:todorovGeneralized2005 deploys
a similar approach to DDP but instead
uses a linear approximation of the dynamics --- trading in accuracy for a computational speed-up.
Guided policy search cite:levineGuided2013

\todo{Add details on LQG etc here}

*** Model-based Control
- robust optimal control
- stochastic optimal control
*** Trajectory Optimisation
**** maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{\MakeUppercase{\timeInd}}}

\newcommand{\stateDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\controlDomain}{\ensuremath{\mathcal{U}}}

\newcommand{\dynamicsFunc}{\ensuremath{f}}
\newcommand{\costFunc}{\ensuremath{c}}
\newcommand{\constraintsFunc}{\ensuremath{g}}

\newcommand{\stateTraj}{\ensuremath{\bar{\state}}}
\newcommand{\controlTraj}{\ensuremath{\bar{\control}}}
#+END_EXPORT

**** intro :ignore:
Formally, trajectory optimisation seeks to find the state and
control trajectories,
$\stateTraj = \{\state(\timeInd) \in \stateDomain \mid \forall \timeInd \in [\timeInd_0, \timeInd_f] \}$,
and
$\controlTraj = \{\control(\timeInd) \in \controlDomain \mid \forall \timeInd \in [\timeInd_0, \timeInd_f] \}$,
respectively,
that minimise some cost function $\costFunc$, whilst satisfying constraints $\constraintsFunc$,
and boundary conditions.
The trajectory optimisation problem is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-traj-opt}
\min_{\state(t), \control(t)} &\int_{t_0}^{t_f} \costFunc(\state(t), \control(t)) \text{d}t \quad \forall t \\
\text{s.t. }&\text{Eq. \ref{eq-unimodal-dynamics-cont}}  \\
&\constraintsFunc(\mathbf{x}(t)) \leq 0 \quad \forall t \\
&\state(t_0) = \state_0  \quad \state(t_f) = \state_f
\end{align}
#+END_EXPORT
Solving this problem is often computationally expensive and
is an active area of research.
Many (approximate) solutions have been introduced in the literature,
that seek to balance the computational complexity
(for real-time control) and accuracy trade-off differently
cite:bettsSurvey1998.
This section briefly reviews several approaches to trajectory
optimisation that can be used with learned dynamics models.
It then provides an in-depth review of the control-as-inference
framework, which is used in Chapter ref:chap-traj-opt.

*Iterative Linear Quadratic Regulator (iLQR)*
can be used to generate trajectories for non-linear systems by
iteratively approximating
the dynamics to be linear around a nominal trajectory and optimising for the controls.
iLQR works well for quadratic cost functions but can be used with any cost function by approximating the cost
function with a second order Taylor expansion. However, in this case iLQR is susceptible to converging
to terrible (local) optima if the true cost function is highly non-convex.
cite:boedeckerApproximate2014 present a real-time iLQR controller
based on sparse GPs.
cite:rohrProbabilistic2021 propose a novel LQR
controller synthesis for
linearised GP dynamics that yields robust controllers with
respect to a probabilistic stability margin.
In cite:marcoAutomatic2016 they parameterise the weight matrices
$\stateMatrix$ and $\controlMatrix$ with parameters $\theta$
and perform BO (entropy search) on $J(\theta)$. Their method handles model mis-specification by adapting the parameters.

*Iterative Linear Quadratic Gaussian (iLQG)* is an extension of
iLQR to a stochastic setting, where the process
noise is assumed to be Guassian.
cite:mitrovicAdaptive2010 combine learned dynamics models with iLQG to develop an adaptive optimal feedback controller.

*Differential Dynamic Programming (DDP)* iLQR and iLQG  are variants of the classic Differential Dynamic Programming (DDP) algorithm cite:rosenbrockDifferential1972,
the main difference being that only a /first-order/ approximation of the dynamics is used, instead of a /second-order/.
cite:liaoAdvantages1992 show that DDP is comparable to a full Gauss-Newton step (for nonlinear least squares)
over the entire control sequence.
Therefore, care must be taken when either the Hessian isn't positive-definite or the minimum isn't close
and the quadratic model is inaccurate.
\todo{remember improvements to standard iLQR/DDP from cite:tassaSynthesis2012}
DDP solutions provides greater accuracy than iLQG/iLQR but
at the cost
of increased computational time cite:tassaSynthesis2012.
The extension of DDP to stochastic state and controls is addressed in cite:theodorouStochastic2010.
DDP under parametric uncertainty using generalised polynomial chaos cite:aoyamaReceding2021.

cite:panProbabilistic2014 present a probabilistic trajectory optimisation
framework for systems with unknown dynamics called probabilistic
differential dynamic programming (PDDP).
It extends DDP to explicitly account
for uncertainty in dynamics modelled using gaussian processes.
In contrast to typical gradient based policy search methods, PDDP does not
require a policy parameterisation as it learns a locally optimal,
time-varying control policy.
Based on the second-order local approximation of the value function,
PDDP performs Dynamic Programming around a nominal trajectory in Gaussian belief spaces.

*Path Integral*  control is a method for developing optimal controls based on stochastic
sampling of trajectories cite:theodorouGeneralized2010.
This work was originally limited to control affine systems cite:williamsModel2017
but has since been extended to general nonlinear
dynamical systems cite:williamsInformation2017,williamsInformationTheoretic2018.

**** Collocation
**** Trajectory Optimisation as Probabilistic Inference
***** maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\startStateDist}{\ensuremath{p(\state_{1})}}
\newcommand{\transitionDist}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)}}
\newcommand{\trajectoryDist}{\ensuremath{p(\stateControlTraj)}}
\newcommand{\controlDist}{\ensuremath{p(\control_\timeInd \mid \state_\timeInd)}}

\newcommand{\trajectoryVarDist}{\ensuremath{q(\state_{1:\TimeInd}, \control_{1:\TimeInd})}}
\newcommand{\controlTrajVarDist}{\ensuremath{q(\control_{1:\TimeInd} \mid \state_{1:\TimeInd})}}
\newcommand{\controlVarDist}{\ensuremath{q(\control_{\timeInd})}}

\newcommand{\optimalVar}{\ensuremath{\mathcal{O}}}
\newcommand{\monotonicFunc}{\ensuremath{f_{\text{mon}}}}

\newcommand{\optimalProb}{\ensuremath{\Pr(\optimalVar_\timeInd = 1 \mid \state_\timeInd, \control_\timeInd)}}
\newcommand{\optimalDist}{\ensuremath{P(\optimalVar_\timeInd \mid \state_\timeInd, \control_\timeInd)}}

\newcommand{\terminalCostDist}{\ensuremath{\Pr(\optimalVar_\TimeInd=1 \mid \state_\TimeInd)}}

\newcommand{\marginalLikelihood}{\ensuremath{p(\optimalVar_{1:\TimeInd} = 1)}}
\newcommand{\jointDist}{\ensuremath{p(\optimalVar_{1:\TimeInd}=1, \state_{1:\TimeInd}, \control_{1:\TimeInd})}}

\newcommand{\temperature}{\ensuremath{\gamma}}
#+END_EXPORT
***** intro :ignore:
The second trajectory algorithm optimisation in Chapter ref:chap-traj-opt
builds on top of the control-as-inference framework
citep:toussaintProbabilistic2006,kappenOptimal2013,
which frames optimal control as probabilistic inference.
Translating optimal control to inference can be viewed as a subset of probabilistic numerics
citep:hennigProbabilistic2015, where statistical methods are used to solve numerical problems,
providing uncertainty quantification, regularisation and faster convergence.

Control-as-inference has been considered since the beginning of modern optimal control due to
the concurrent development of LQR, Kalman filtering and LQG citep:kalmanNew1960,hoBayesian1964.
Recent work on control-as-inference stems from path integral cite:kappenOptimal2013,williamsModel2017

cite:rawlikStochastic2013

cite:toussaintRobot2009 formulate trajectory optimisation as
approximate inference and derive a local approximation
where the maximum likelihood solution returns the deterministic optimal trajectory, and the local Gaussian belief
approximation is equivalent to the LQG perturbation model around the optimal trajectory.
cite:levineReinforcement2018 followed a similar approach formulating RL as probabilistic inference
and showing how different approximate inference algorithms instantiated different popular RL algorithms.


Recent work by cite:watsonStochastic2021 has formulated
stochastic optimal control as input estimation

***** Cost Functions as Likelihoods
\newline
To formulate optimal control as probabilistic inference
it is first embedded into a graphical model (see Figure ref:fig-basic-control-graphical-model).
The joint probability model (over a trajectory) is augmented with an
additional variable to encode the notion of cost (or reward)
over the trajectory (see Figure ref:fig-augmented-control-graphical-model).
The new variable is a Bernoulli random variable $\optimalVar_\timeInd \in \{0, 1\}$, that indicates if
time step $\timeInd$ is /optimal/ $\optimalVar_\timeInd=1$, or not /optimal/ $\optimalVar_\timeInd=0$.
The likelihood distribution can then be formulated
by mapping the negative
cost through a monotonically increasing function $\monotonicFunc$,
giving the likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-monotonicOptimalityLikelihood}
\optimalProb &\coloneqq \monotonicFunc( -\costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
A common (and convenient) approach is to formulate
the likelihood using an exponential transform of
the cost. This results in a Boltzmann distribution where the
inverse temperature, $\temperature$, is used to scale the cost,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-exponentialOptimalityLikelihood}
\optimalProb &\propto \exp ( -\temperature\costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
The resulting negative log-likelihood is an affine transformation of the cost, which preserves convexity.
#+BEGIN_EXPORT latex
\todo[inline]{add detials on Cross Entropy Method (CEM) cite:chuaDeep2018 and other methods}
\begin{myquote}
As shown in \cite{okadaVariational2020}, the choice of monotonic function,
$\monotonicFunc$,
leads to the inference algorithm resembling different well-known algorithms.
For example, selecting $\monotonicFunc$ to be the exponential function,
\begin{align} \label{eq-}
\optimalProb &\propto \text{exp}( -\costFunc(\state_\timeInd, \control_\timeInd))
\end{align}
leads to the algorithm resembling Model Predictive Path Integral (MPPI)
\cite{williamsModel2017,williamsInformation2017}, whilst
selecting $\monotonicFunc$ to as,
\begin{align} \label{eq-}
\optimalProb &\propto \mathbb{I}\left[( \costFunc(\state_\timeInd, \control_\timeInd)) \leq \costFunc_{\text{thd}} \right]
\end{align}
recovers an algorithm resembling the Cross Entropy Method (CEM).
\end{myquote}
#+END_EXPORT

***** Joint Probability Model
\newline
Given the "cost" likelihood
(cref:eq-monotonicOptimalityLikelihood)
the joint probability for an optimal trajectory (i.e.
for $\optimalVar_\timeInd=1$ for all $\timeInd \in \{1,\ldots,\TimeInd\}$), is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-traj-opt-joint-dist}
\jointDist =
& \startStateDist \underbrace{\terminalCostDist}_{\text{Terminal Cost}}  \\
&\prod_{\timeInd=1}^{\TimeInd-1}
\underbrace{\optimalProb}_{\text{Cost}}
\underbrace{\transitionDist}_{\text{Dynamics}}
\underbrace{\controlDist}_{\text{Controller}} \nonumber
\end{align}
#+END_EXPORT
where $\controlDist$ denotes the controller or policy.
In the case of RL the policy is parameterised with parameters, $\theta$,
whereas the control setting is usually interested in
optimising the states and controls directly.
The object of interest in this model is the distribution over the control, $\control_\timeInd$,
conditioned on the state, $\state_\timeInd$, and all future time steps being optimal,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\control_{\timeInd} \mid \state_{\timeInd}, \optimalVar_{\timeInd:\TimeInd}=1) =
\frac{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)
\controlDist p(\state_\timeInd)}{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd)
p(\state_\timeInd)}.
\end{align}
#+END_EXPORT
The distributions $p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)$
and
$p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd)$,
can be obtained via recursive message passing,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd)
&= p(\state_\timeInd)
\prod_{\timeInd}^{\TimeInd-1}
\int \int
\optimalProb p(\state_{\timeInd+1} \mid \state_{\timeInd}, \control_{\timeInd}) \controlDist
\text{d}\state_{\timeInd+1} \text{d}\control_{\timeInd+1} \nonumber \\
p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd)
&=
\int p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd)
p(\control_\timeInd \mid \state_\timeInd)
\text{d}\control_{\timeInd}.
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo[inline]{Considering process noise in the dynamics separately from the epistemic (parametric) uncertainty,
\begin{align} \label{eq-}
p(\optimalVar_{1:\TimeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})
= \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \optimalDist
p(\state_{\timeInd+1} \mid \dynamicsFunc(\state_\timeInd)) p(\dynamicsFunc(\state_\timeInd) \mid \state_\timeInd, \control_\timeInd)
\controlDist
\end{align}
\begin{align} \label{eq-}
\optimalProb &= \monotonicFunc( -\costFunc(\state_\timeInd, \dynamicsFunc(\state_{\timeInd-1}), \control_\timeInd)) \\
\end{align}
}
#+END_EXPORT

***** graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \begin{subfigure}{.45\textwidth}
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);
    \end{tikzpicture}
    %}
  \caption{Graphical model with states and controls.}
\label{fig-basic-control-graphical-model}
\end{subfigure}
\begin{subfigure}{.45\textwidth}
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[obs, above=of u1] (o1) {$\optimalVar_1$};
      \node[obs, right=of o1] (o2) {$\optimalVar_2$};
      \node[obs, right=of o2] (o3) {$\optimalVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);
    \end{tikzpicture}
    %}
  \caption{Graphical model with optimality variable.}
\label{fig-augmented-control-graphical-model}
\end{subfigure}
\caption{Graphical models of control formulated as inference.}
\label{fig-control-graphical-model}
\end{figure}
#+END_EXPORT

***** Variational Inference
In variational inference the distribution over the latent variables
$p(\state_{1:\TimeInd}, \control_{1:\TimeInd})$
is approximated with another, potentially simpler
(e.g. factorised) distribution $q(\state_{1:\TimeInd}, \control_{1:\TimeInd})$.
In this case, the aim is to approximate the distribution over the optimal policy,
$p(\control_{1:\TimeInd} \mid \state_{1:\TimeInd}, \optimalVar_{1:\TimeInd}=1)$.
Introducing the variational distribution,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\trajectoryVarDist = \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \transitionDist \controlVarDist
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\text{log} \marginalLikelihood
&= \text{log} \E_\trajectoryVarDist \left[ \frac{\jointDist}{\trajectoryVarDist} \right] \\
&= \text{log} \E_\trajectoryVarDist \left[
\frac{\cancel{\startStateDist} \prod_{\timeInd=1}^{\TimeInd} \optimalProb \cancel{\transitionDist} \controlDist}{\cancel{\startStateDist} \prod_{\timeInd=1}^\TimeInd \cancel{\transitionDist} \controlVarDist}
\right] \\
&\geq \E_\trajectoryVarDist \left[
\sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb - \sum_{\timeInd=1}^\TimeInd \text{log}
\frac{\controlVarDist}{\controlDist} \right] \\
&= \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \costFunc(\state_\timeInd, \control_\timeInd)
- \text{log}\controlVarDist\right]
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\text{log} \marginalLikelihood &\geq
 \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb
- \text{log}\controlVarDist\right] \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \text{log} \optimalProb \right]
- \E_{\controlVarDist q(\state_{\timdInd})} \left[ \text{log}\controlVarDist \right] \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
+ \sum_{\timeInd=1}^{\TimeInd} \underbrace{- \E_{\controlVarDist} \left[ \text{log} \controlVarDist \right]}_{\text{Entropy}} \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
+ \sum_{\timeInd=1}^{\TimeInd} \mathcal{H}(\control_{\timeInd})
\end{align}
#+END_EXPORT

** Model-Based Control :noexport:
*** Dynamical Systems :noexport:
**** intro :ignore:
Dynamical systems describe the behaviour of a system over time, $t$, and
are a key component of both control theory and reinforcement learning.
At any given time a dynamical system has a state, which is
represented as a vector of real numbers $\state(t) \in \stateDomain \subseteq \R^D$.
For example, the state of a 2-dimensional robotic system with state comprising of 2D Cartesian coordinates
and an orientation would be represented as $\state(t) = [x(t), y(t), \theta(t)]^{T}$.
The system can be controlled by applying control actions $\mathbf{u}(t) \in \controlDomain \subseteq \R^F$ at any given time $t$.
This work considers continuous-time, continuous-state, nonlinear stochastic dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-unimodal-dynamics-cont}
\stateDerivative(t) &= \dynamicsFunc(\state(t), \control(t)) + \epsilon(t) \quad \forall t
\end{align}
#+END_EXPORT
where $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ represents the transition dynamics function and $\epsilon(t)$
is an i.i.d process noise term with $\E[\epsilon(t)] = 0$.
The process noise term accounts for unwanted (and, in general, unknown) disturbances of the system.
For example, it is extremely hard to model aerodynamic effects on aircraft so these could be accounted
for in the process noise term.
Throughout this dissertation it is assumed that the state $\state$ is observed directly and is not subject to
observation noise.
This is a standard assumption in the \acrfull{mdp} framework which is commonly adopted
in the reinforcement learning literature.
\todo[inline]{Slightly weird referring to MDP literature on continuous time setting}

\todo[inline]{Should I make a bigger point out of the observation noise assumption}

The system is controlled via a policy $\control(t) = \pi(\state(t), t)$, which given the state $\state(t)$
and time step $t$ decides which control action $\control(t)$ to apply to the system.
The policy can be time-dependent and can also depend on all past information up to time step $t$.
In the time-independent case the policy is denoted $\pi(\state(t))$ and
the resulting closed-loop system is denoted $f_{\pi}(\state) = \dynamicsFunc(\state,\pi(\state))$.

*** Optimal Control :ignore:
Optimal control is a branch of mathematical optimisation that seeks to find a set of controls
over a time period $t \in [t_0, t_f]$ that optimises an objective function $\mathbf{J}_{\pi}(\x)$.
The objective function might be formulated to solve a particular task or to make the dynamical
system behave in a certain way.

Typically this objective function is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-objective}
\mathbf{J} \defeq \phi(\x(t_{f}), t_{f}) + \int^{t_{f}}_{t_{0}} L(\mathbf{x}(t), \mathbf{u}(t), t) \text{d}t,
\end{align}
#+END_EXPORT
which consists of two terms:
1. a terminal cost term $\phi : \R^{D} \times \R \rightarrow \R$,
2. an integral cost term (or Lagrangian) $L : \R^{D} \times \R^{F} \times \R \rightarrow \R$.

Model-based control ...

*** Trajectory Optimisation
**** intro :ignore:
Formally, trajectory optimisation seeks to find the state and
control trajectories,
$\stateTraj = \{\state(\timeInd) \in \stateDomain \mid \forall \timeInd \in [\timeInd_0, \timeInd_f] \}$,
and
$\controlTraj = \{\control(\timeInd) \in \controlDomain \mid \forall \timeInd \in [\timeInd_0, \timeInd_f] \}$,
respectively,
that minimise some cost function $\costFunc$, whilst satisfying constraints $\constraintsFunc$,
and boundary conditions.
The trajectory optimisation problem is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-traj-opt}
\min_{\state(t), \control(t)} &\int_{t_0}^{t_f} \costFunc(\state(t), \control(t)) \text{d}t \quad \forall t \\
\text{s.t. }&\text{Eq. \ref{eq-unimodal-dynamics-cont}}  \\
&\constraintsFunc(\mathbf{x}(t)) \leq 0 \quad \forall t \\
&\state(t_0) = \state_0  \quad \state(t_f) = \state_f
\end{align}
#+END_EXPORT
**** Collocation
**** Trajectory Optimisation as Probabilistic Inference
***** maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\startStateDist}{\ensuremath{p(\state_{1})}}
\newcommand{\transitionDist}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)}}
\newcommand{\trajectoryDist}{\ensuremath{p(\stateControlTraj)}}
\newcommand{\controlDist}{\ensuremath{p(\control_\timeInd \mid \state_\timeInd)}}

\newcommand{\trajectoryVarDist}{\ensuremath{q(\state_{1:\TimeInd}, \control_{1:\TimeInd})}}
\newcommand{\controlTrajVarDist}{\ensuremath{q(\control_{1:\TimeInd} \mid \state_{1:\TimeInd})}}
\newcommand{\controlVarDist}{\ensuremath{q(\control_{\timeInd})}}

\newcommand{\optimalVar}{\ensuremath{\mathcal{O}}}
\newcommand{\monotonicFunc}{\ensuremath{f_{\text{mon}}}}

\newcommand{\optimalProb}{\ensuremath{\Pr(\optimalVar_\timeInd = 1 \mid \state_\timeInd, \control_\timeInd)}}
\newcommand{\optimalDist}{\ensuremath{P(\optimalVar_\timeInd \mid \state_\timeInd, \control_\timeInd)}}

\newcommand{\terminalCostDist}{\ensuremath{\Pr(\optimalVar_\TimeInd=1 \mid \state_\TimeInd)}}

\newcommand{\marginalLikelihood}{\ensuremath{p(\optimalVar_{1:\TimeInd} = 1)}}
\newcommand{\jointDist}{\ensuremath{p(\optimalVar_{1:\TimeInd}=1, \state_{1:\TimeInd}, \control_{1:\TimeInd})}}

\newcommand{\temperature}{\ensuremath{\gamma}}
#+END_EXPORT
***** intro :ignore:
The second trajectory algorithm optimisation in Chapter ref:chap-traj-opt
builds on top of the control-as-inference framework
citep:toussaintProbabilistic2006,kappenOptimal2013,
which frames optimal control as probabilistic inference.
Translating optimal control to inference can be viewed as a subset of probabilistic numerics
citep:hennigProbabilistic2015, where statistical methods are used to solve numerical problems,
providing uncertainty quantification, regularisation and faster convergence.

Control-as-inference has been considered since the beginning of modern optimal control due to
the concurrent development of LQR, Kalman filtering and LQG cite:kalmanNew1960,hoBayesian1964.
Recent work on control-as-inference stems from path integral cite:kappenOptimal2013,williamsModel2017

cite:rawlikStochastic2013

cite:toussaintRobot2009 formulate trajectory optimisation as
approximate inference and derive a local approximation
where the maximum likelihood solution returns the deterministic optimal trajectory, and the local Gaussian belief
approximation is equivalent to the LQG perturbation model around the optimal trajectory.
cite:levineReinforcement2018 followed a similar approach formulating RL as probabilistic inference
and showing how different approximate inference algorithms instantiated different popular RL algorithms.


Recent work by cite:watsonStochastic2021 has formulated
stochastic optimal control as input estimation

***** Cost Functions as Likelihoods
To formulate optimal control as probabilistic inference
it is first embedded into a graphical model (see Figure ref:fig-basic-control-graphical-model).
The joint probability model (over a trajectory) is augmented with an
additional variable to encode the notion of cost (or reward)
over the trajectory (see Figure ref:fig-augmented-control-graphical-model).
The new variable is a Bernoulli random variable $\optimalVar_\timeInd \in \{0, 1\}$, that indicates if
time step $\timeInd$ is /optimal/ $\optimalVar_\timeInd=1$, or not /optimal/ $\optimalVar_\timeInd=0$.
The likelihood distribution can then be formulated
by mapping the negative
cost through a monotonically increasing function $\monotonicFunc$,
giving the likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-monotonicOptimalityLikelihood}
\optimalProb &\coloneqq \monotonicFunc( -\costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
A common (and convenient) approach is to formulate
the likelihood using an exponential transform of
the cost. This results in a Boltzmann distribution where the
inverse temperature, $\temperature$, is used to scale the cost,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-exponentialOptimalityLikelihood}
\optimalProb &\propto \exp ( -\temperature\costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
The resulting negative log-likelihood is an affine transformation of the cost, which preserves convexity.
#+BEGIN_EXPORT latex
\todo[inline]{add detials on Cross Entropy Method (CEM) cite:chuaDeep2018 and other methods}
\begin{myquote}
As shown in \cite{okadaVariational2020}, the choice of monotonic function,
$\monotonicFunc$,
leads to the inference algorithm resembling different well-known algorithms.
For example, selecting $\monotonicFunc$ to be the exponential function,
\begin{align} \label{eq-}
\optimalProb &\propto \text{exp}( -\costFunc(\state_\timeInd, \control_\timeInd))
\end{align}
leads to the algorithm resembling Model Predictive Path Integral (MPPI)
\cite{williamsModel2017,williamsInformation2017}, whilst
selecting $\monotonicFunc$ to as,
\begin{align} \label{eq-}
\optimalProb &\propto \mathbb{I}\left[( \costFunc(\state_\timeInd, \control_\timeInd)) \leq \costFunc_{\text{thd}} \right]
\end{align}
recovers an algorithm resembling the Cross Entropy Method (CEM).
\end{myquote}
#+END_EXPORT

***** Joint Probability Model
Given the "cost" likelihood
(cref:eq-monotonicOptimalityLikelihood)
the joint probability for an optimal trajectory (i.e.
for $\optimalVar_\timeInd=1$ for all $\timeInd \in \{1,\ldots,\TimeInd\}$), is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-traj-opt-joint-dist}
\jointDist =
& \startStateDist \underbrace{\terminalCostDist}_{\text{Terminal Cost}} \\
&\prod_{\timeInd=1}^{\TimeInd-1}
\underbrace{\optimalProb}_{\text{Cost}}
\underbrace{\transitionDist}_{\text{Dynamics}}
\underbrace{\controlDist}_{\text{Controller}}
\end{align}
#+END_EXPORT
where $\controlDist$ denotes the policy.
In the case of RL the policy is parameterised with parameters, $\theta$, but in control we are usually interested in
optimising the states and controls directly.
The object of interest in this model is the distribution over the control, $\control_\timeInd$,
conditioned on the state, $\state_\timeInd$, and all future time steps being optimal,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\control_{\timeInd} \mid \state_{\timeInd}, \optimalVar_{\timeInd:\TimeInd}=1) =
\frac{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)
\controlDist p(\state_\timeInd)}{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd)
p(\state_\timeInd)}.
\end{align}
#+END_EXPORT
The distributions $p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd)$,
$p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd)$,
$p(\modeVar_{\timeInd:\TimeInd}=\modeInd \mid \state_\timeInd, \control_\timeInd)$ and
$p(\modeVar_{\timeInd:\TimeInd}=\modeInd \mid \state_\timeInd)$
can be obtained via recursive message passing,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd) =
\prod_{\timeInd+1}^{\TimeInd-1}
\int \int \optimalProb p(\state_\timeInd \mid \state_{\timeInd-1}, \control_{\timeInd-1}) \controlDist \text{d}\state_{\timeInd} \text{d}\control_{\timeInd}.
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\todo[inline]{Considering process noise in the dynamics separately from the epistemic (parametric) uncertainty,
\begin{align} \label{eq-}
p(\optimalVar_{1:\TimeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})
= \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \optimalDist
p(\state_{\timeInd+1} \mid \dynamicsFunc(\state_\timeInd)) p(\dynamicsFunc(\state_\timeInd) \mid \state_\timeInd, \control_\timeInd)
\controlDist
\end{align}
\begin{align} \label{eq-}
\optimalProb &= \monotonicFunc( -\costFunc(\state_\timeInd, \dynamicsFunc(\state_{\timeInd-1}), \control_\timeInd)) \\
\end{align}
}
#+END_EXPORT

***** graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \begin{subfigure}{.45\textwidth}
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);
    \end{tikzpicture}
    %}
  \caption{Graphical model with states and controls.}
\label{fig-basic-control-graphical-model}
\end{subfigure}
\begin{subfigure}{.45\textwidth}
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[obs, above=of u1] (o1) {$\optimalVar_1$};
      \node[obs, right=of o1] (o2) {$\optimalVar_2$};
      \node[obs, right=of o2] (o3) {$\optimalVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);
    \end{tikzpicture}
    %}
  \caption{Graphical model with optimality variable.}
\label{fig-augmented-control-graphical-model}
\end{subfigure}
\caption{Graphical models of control formulated as inference.}
\label{fig-control-graphical-model}
\end{figure}
#+END_EXPORT

***** Variational Inference
In variational inference the distribution over the latent variables
$p(\state_{1:\TimeInd}, \control_{1:\TimeInd})$
is approximated with another, potentially simpler
(e.g. factorised) distribution $q(\state_{1:\TimeInd}, \control_{1:\TimeInd})$.
In this case, the aim is to approximate the distribution over the optimal policy,
$p(\control_{1:\TimeInd} \mid \state_{1:\TimeInd}, \optimalVar_{1:\TimeInd}=1)$.
Introducing the variational distribution,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\trajectoryVarDist = \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \transitionDist \controlVarDist
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\text{log} \marginalLikelihood
&= \text{log} \E_\trajectoryVarDist \left[ \frac{\jointDist}{\trajectoryVarDist} \right] \\
&= \text{log} \E_\trajectoryVarDist \left[
\frac{\cancel{\startStateDist} \prod_{\timeInd=1}^{\TimeInd} \optimalProb \cancel{\transitionDist} \controlDist}{\cancel{\startStateDist} \prod_{\timeInd=1}^\TimeInd \cancel{\transitionDist} \controlVarDist}
\right] \\
&\geq \E_\trajectoryVarDist \left[
\sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb - \sum_{\timeInd=1}^\TimeInd \text{log}
\frac{\controlVarDist}{\controlDist} \right] \\
&= \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \costFunc(\state_\timeInd, \control_\timeInd)
- \text{log}\controlVarDist\right]
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\text{log} \marginalLikelihood &\geq
 \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb
- \text{log}\controlVarDist\right] \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \text{log} \optimalProb \right]
- \E_{\controlVarDist q(\state_{\timdInd})} \left[ \text{log}\controlVarDist \right] \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
+ \sum_{\timeInd=1}^{\TimeInd} \underbrace{- \E_{\controlVarDist} \left[ \text{log} \controlVarDist \right]}_{\text{Entropy}} \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
+ \sum_{\timeInd=1}^{\TimeInd} \mathcal{H}(\control_{\timeInd})
\end{align}
#+END_EXPORT

*** Model-Based Control in Uncertain Systems
- robust optimal control
- stochastic optimal control
*** Literature Review (iLQR/iLQG/DDP/MPPI for Trajectory Generation)
*Iterative LQR/LQG* can be used to generate trajectories for non-linear systems by iteratively approximating
the dynamics to be linear around a nominal trajectory and optimising for the controls.
iLQR works well for quadratic cost functions and can be used with any cost function by approximating the cost
function with a second order Taylor expansion. However, in this case iLQR is susceptible to converging
to terrible (local) optima if the true cost function is highly non-convex.

cite:mitrovicAdaptive2010 combine learned dynamics models with iLQG to develop an adaptive optimal feedback controller.

cite:boedeckerApproximate2014 present a real-time iLQR based on sparse GPs.

cite:rohrProbabilistic2021 propose a novel controller synthesis for
linearised GP dynamics that yields robust controllers with
respect to a probabilistic stability margin.

In cite:marcoAutomatic2016 they parameterise the weight matrices
$\stateMatrix$ and $\controlMatrix$ with parameters $\theta$
and perform BO (entropy search) on $J(\theta)$. Their method handles model mis-specification
by adapting the parameters.

*Differential Dynamic Programming* iLQR and iLQG  are variants of the classic Differential Dynamic Programming (DDP) algorithm cite:rosenbrockDifferential1972,
the main difference being that only a /first-order/ approximation of the dynamics is used, instead of /second-order/.
cite:liaoAdvantages1992 show that DDP is comparable to a full Newton step for the entire control
sequence. Therefore, care must be taken when either the Hessian isn't positive-definite or the minimum isn't close
and the quadratic model is inaccurate.
\todo{remember improvements to standard iLQR/DDP from cite:tassaSynthesis2012}

The extension of DDP to stochastic state and controls is addressed in cite:theodorouStochastic2010.

DDP under parametric uncertainty using generalised polynomial chaos cite:aoyamaReceding2021.


cite:panProbabilistic2014 present a probabilistic trajectory optimisation
framework for systems with unknown dynamics called probabilistic
differential dynamic programming (PDDP).
It extends differential dynamic programming to explicitly account
for uncertainty in the learned dynamics modelled using gaussian processes.
In contrast to typical gradient based policy search methods, PDDP does not
require a policy parameterisation as it learns a locally optimal,
time-varying control policy.
Based on the second-order local approximation of the value function,
PDDP performs Dynamic Programming around a nominal trajectory in Gaussian belief spaces.

*Path Integral*  control is a method for developing optimal controls based on stochastic
sampling of trajectories cite:theodorouGeneralized2010.
This work was originally limited to control affine systems cite:williamsModel2017
but has since been extended to general nonlinear
dynamical systems cite:williamsInformation2017,williamsInformationTheoretic2018.

** Exploration in Dynamical Systems
*** Bandits and Bayesian Optimisation
*** Information Theoretic Active Learning with Gaussian Processe
cite:krauseNonmyopic2007,houlsbyBayesian2011
*** Active Learning in Dynamical Systems
cite:yuActive2021,schreiterSafe2015,caponeLocalized2020,buisson-fenetActively2020

** Old :noexport:
In cite:NhatAnhNguyen2018 they highlight that the previously mentioned approaches are limited as
each expert is trained using only the local data
assigned to it. This means that they do not take into account the global information, i.e. the
correlations between clusters.
The experts are therefore likely to overfit to their local training data.
cite:NhatAnhNguyen2018 propose a model based on both global and local sparse GPs to overcome this
and also employ stochastic optimisation providing greater scalability.
They reduce the computational complexity in their bound through the variational
approximation on the indicator variable as well as the sparse GP framework cite:Hensman2013.

Our approach does not lead to over fitting as each GP is trained using all of the observations.
However, each of our experts has a set of inducing inputs that are unique to the expert which
greatly improves computational complexity (at training and prediction).
These inducing inputs enable stochastic variational inference in our model which also significantly speeds
up inference.

In contrast, our main motivation is learning good representations of the underlying functions
(both the experts and the gating network).
Mixtures of GP experts models are inherently unidentifiable as
different combinations of experts
and gating networks can generate the same distributions over the observations.
As such, we trade in the computational benefits of the gating networks used by much
of the literature for the modelling advantages offered by a gating network based on GPs.
This provides a handle for encoding prior knowledge that can be used to constrain
the set of admissible functions that we wish to learn within.
This in turn improves identifiability and results in learned representations that are more true
to our understanding of the system.
It is worth noting that we do derive a variational lower bound with competitive complexity.
To the best of our knowledge the complexity of our variational lower bound is
state of the art for \gls{mgp} models with GP based gating networks.




Before comparing our work to the literature we first provide some intuition for the modelling
assumptions behind different gating networks in Figure [[ref:gating_network_comparison]].
We denote the inputs $x$, expert indicator variable $\alpha$ and the cluster indicator variable $z$.
The expert mixing probabilities are given by $P(\alpha=k|x)$ and the distributions over the inputs
are given by $p(x | \alpha=k, z=c)$.
\todo{dist or density?}
The top plot is a gating
network based on GPs and importantly we can see that it has been able to turn expert 1 "on" in
multiple regions of the domain and is not limited to a single local subset of the domain.
Alternatively, many approaches utilise some notion of clustering the inputs locally and the middle right plot
shows three Gaussian pdfs representing this.
It is common to assign each cluster to a single expert,
i.e. let the cluster indicator equal the expert indicator ($k=c$).
This leads to improved computational complexity by dividing up the domain and decomposing the
covariance matrix into a set of smaller matrices.
However, this leads to fitting separate experts to the left and right regions (shaded green) which may be
contrary to our knowledge of the generation of observations.
We apply Bayes' rule to recover the expert mixing probabilities $P(\alpha=k |x)$ shown in the
middle left plot (so that we can compare it to the GP based gating network above).

One can maintain separate expert and cluster indicator variables (as seen in cite:Yuan) which
leads to each experts inputs being modelled as a mixture of Gaussians. This is shown in the bottom
right plot where all we have done is take the middle right plot and simply marginalising the
cluster indicator variable.
The distribution over the inputs associated with expert
one $p(x | \alpha=1)$ (dashed green) has high *cluster* mixing
probabilities for the clusters in the left and right (green) regions and a low *cluster* mixing
probability in the middle (blue) region.
This leads to a gating network that is also able to turn expert 1 "on" in multiple regions of the domain
like the GP based gating network.

This is good but we now consider the added modelling benefit of a GP based gating network.
\todo[inline]{insert text/example about encoding prior info and making hyperparameters not trainable etc}

We can also consider the implications from an inference perspective.
It is known that developing inference algorithms for propagating uncertain (Gaussian) inputs
through GPs is itself an unsolved problem (see cite:Ustyuzhaninov).
Therefore developing an inference algorithm to propagate inputs that are distributed according to
a Gaussian mixture model will likely lead to valuable information loss.
This is a purely intuitive statement and we have not performed any experiments.

# Stochastic variational inference enables GP models to scale to larger data sets so we would like
# to develop such an algorithm.

Hopefully this provides some intuition about the different assumptions that can be encoded (or not)
by different gating network formulations.
We now highlight XXX of the factors that might make a GP based gating network desirable.

1. Handle for encoding prior information,
   - Constrain the set of admissible functions.
2. Information loss,
   - Propagating Gaussian inputs through GPs is an active area of research cite:Ustyuzhaninov and we
     argue that any approximations required to propagate inputs that are distributed according to
     a Gaussian mixture model will likely lose valuable information.

\todo[inline]{We can encode more info into a GP prior that into a GMM?}
\todo[inline]{Can GMMs extrapolate away from data??}
\todo[inline]{With GP gating function we can ensure it is a Riemannian manifold i.e. differentiable. Is this the case with GMMs?}


\todo{Yuan uses VI and changed gating network structure}
cite:Yuan proposed a mixture of GP experts with a variational inference scheme which is much
faster than using MCMC.
They assume that the expert indicators are generated by a Dirichlet distribution and
that given the expert indicator the inputs follow a Gaussian mixture model, i.e. they
have introduced a cluster indicator variable.
This is in contrast to earlier approaches that let the expert indicator act as the input cluster
indicator.
Intuitively we think that this gives rise to a gating network similar
to the bottom plot of Figure [[ref:gating_network_comparison]].
However, the expert indicator does not depend on the inputs, rather the inputs depend on the cluster
indicator which depends on the expert indicator.
\todo{However, the "gating network" is no longer input dependent, unless a similar modification to cite:Rasmussen2002 is made to the DP.}

In cite:NhatAnhNguyen2018 they highlight that these approaches are limited as
each expert is trained using only the local data
assigned to it. This means that they do not take into account the global information, i.e. the
correlations between clusters.
The experts are therefore likely to overfit to their local training data.
cite:NhatAnhNguyen2018 propose a model based on both global and local sparse GPs to overcome this
and also employ stochastic optimisation providing greater scalability.
They reduce the computational complexity in their bound through the variational
approximation on the indicator variable as well as the sparse GP framework cite:Hensman2013.
Our approach does not lead to over fitting as each GP is trained using all of the observations.
However, each of our experts has a set of inducing inputs that are unique to the expert which
greatly improves computational complexity (at training and prediction).
These inducing inputs enable stochastic variational inference in our model which also significantly speeds
up inference.

Our gating function only requires a single GP as we instantiate our model with only two experts.
We exploit the probit link function (as opposed to the softmax function used by cite:Tresp)
because it leads to analytic mixing probabilities
and factorisation across the data given $M$ global inducing points.
We do not introduce a variational distribution over the indicator variable as we are able
to analytically marginalise it in our evidence lower bound without a significant computational burden.


Our model is similar to cite:Tresp but overcomes the limitations highlighted
by cite:Rasmussen2002.
That is, we simultaneously improve the models computational complexity and
the data assignment at prediction time.
Each of our experts predicts on the basis of a set of inducing inputs that are unique to the expert.
These inducing inputs enable stochastic variational inference in our model which significantly speeds
up inference.

** Ignore :noexport:

The \glspl{mgp} literature commonly highlights two main issues that they
seek to address.
Firstly, \glspl{gp} cannot handle multi-modality, i.e. they assume that the observations
were generated by a single underlying function.
Secondly, they suffer from both time and memory complexity issues
arising from the calculation and storage of the inverse covariance matrix.
Given a data set with $N$ observations, training has time and memory complexity of $\mathcal{O}(N^3)$
and $\mathcal{O}(N^2)$ respectively.
Many approaches to a \gls{mgp} method have been proposed which attempt
to address these issues cite:Tresp,Rasmussen2002,Meeds2005,Yuan,NhatAnhNguyen2018.
However, most previous work appears to be motivated by addressing the complexity issues
associated with \glspl{gp}.
That is, they are interested in decomposing the $N \times N$ covariance matrix into a set of smaller
matrices.
In contrast, our main motivation is learning good representations of the underlying functions
(both the experts and the gating network).
Mixtures of GP experts models are inherently unidentifiable as
different combinations of experts
and gating networks can generate the same distributions over the observations.
As such, we trade in the computational benefits of the gating networks used by much
of the literature for the modelling advantages offered by a gating network based on GPs.
This provides a handle for encoding prior knowledge that can be used to constrain
the set of admissible functions that we wish to learn within.
This in turn improves identifiability and results in learned representations that are more true
to our understanding of the system.
It is worth noting that we do derive a variational lower bound with competitive complexity.
To the best of our knowledge the complexity of our variational lower bound is
state of the art for \gls{mgp} models with GP based gating networks.

In cite:NhatAnhNguyen2018 they exploit a global GP to coarsely model the
entire data set as well as local GP experts to overcome this limitation.
Our approach does not lead to over fitting as each GP is trained using all of the observations.
We exploit the factorization achieved via the sparse variational GP framework to
couple the learning of the gating network and the experts.
Our bound achieves this with little extra computational burden.

A significant amount of recent work builds on cite:Rasmussen2002 with the
computational benefits as a key driving factor.
However, we highlight that there is also a modelling assumption that distinguishes these approaches.
The gating network of cite:Tresp is able to model spatial structure.
That is, it is capable of turning each expert "on" and "off" in different regions of the input space
i.e. expert $k$ can be turned on in multiple regions.
This contrasts placing a GMM on the inputs and associating each expert with the observations
associated with specific components.
This approach improves the computational complexity but is not capable of capturing spatial structure.
For example, if expert $k$ is responsible for generating the data in multiple areas of the
input space then a GP based gating network will be capable of capturing this.

\todo[inline]{what do we mean by spatial structure??? Essentially we have GP compared to GMM. Do Gaussian mixture models capture spatial structure??}


We do not directly assign each observation to a single expert but instead assume that
it is generated as a mixture of the experts.

cite:Nguyen2014 also provide fast variational inference based on the sparse GP framework.
Their model also "splits" the dataset such that each expert acts only on the
observations assigned to it.
Each observation is assigned to an expert based on its proximity to the experts centroids.


A limitation of these approaches is that each expert is trained using only the local data
assigned to it. This means that they do not take into account the global information, i.e. the
correlations between clusters.
The experts are therefore likely to overfit to their local training data.

Our approach is similar to cite:Yuan in the sense that we break the dependency among the training
outputs enabling variational inference.
However, we achieve this through the sparse variational GP framework seen in cite:Hensman2013.
We actually induce the necessary conditions for stochastic variational inference as
we obtain factorisation over data points given a set of $M$ global inducing points.

cite:NhatAnhNguyen2018 propose a model based on both global and local sparse GPs
and also employ stochastic optimisation providing greater scalability.
They reduce the computational complexity in their bound through the variational
approximation on the indicator variable as well as the sparse GP framework cite:Hensman2013.
We do not introduce a variational distribution over the indicator variable as we are able
to analytically marginalise it in our evidence lower bound without a significant computational burden.
\todo[inline]{Variational dists allow us to condider the unc in a variable and perform full Bayesian inference. What is the significance of what we have done? I think that ideally we would analytically marginalise all our latent variables so what we have done is very nice.}
cite:NhatAnhNguyen2018 use a GMM gating network which and achieve complexity $\mathcal{O}(NM^2K)$.
We want spatial structure so use GP based gating network which leads to $\mathcal{O}(NM^3K)$.


The gating network enforces that each component of the GP mixture is localized.
In cite:Lazaro-Gredilla2011 they formulate a mixture without the use of a gating function
which enables a given
location in the input space to be associated with multiple outputs. This could be used for modelling
multiple objects in a tracking system and is known as the data association problem.
We are not interested in the data association problem but instead wish to encode spatial structure
through the use of a gating function.
We introduce our model from a generative model perspective which provides insight
into how we can easily switch between the data association setting and a mixture of GPs by
simply marginalising the indicator variable.

Our gating function only requires a single GP as we instantiate our model with only two experts.
We exploit the probit link function (as opposed to the softmax function used by cite:Tresp)
because it leads to analytic mixing probabilities
and factorisation across the data given $M$ global inducing points.


Our model is similar to cite:Tresp but overcomes the limitations highlighted
by cite:Rasmussen2002.
That is, we simultaneously improve the models computational complexity and
the data assignment at prediction time.
Each of our experts predicts on the basis of a set of inducing inputs that are unique to the expert.
These inducing inputs enable stochastic variational inference in our model which significantly speeds
up inference.

* Probabilistic Inference for Learning Multimodal Dynamical Systems label:chap-dynamics
#+begin_export latex
\epigraph{All models are wrong, but some are useful.}{\textit{George Box}}
#+end_export
** Maths Symbols :ignore:
*** Domains :ignore:
#+BEGIN_EXPORT latex
\newcommand{\stateDomain}{\ensuremath{\mathcal{X}}}
%\renewcommand{\stateDomain}{\ensuremath{\mathcal{S}}}
\renewcommand{\controlDomain}{\ensuremath{\mathcal{U}}}
\renewcommand{\modeDomain}{\ensuremath{\mathcal{A}}}
%\renewcommand{\inputDomain}{\ensuremath{\mathcal{X}}}
\renewcommand{\inputDomain}{\ensuremath{\mathcal{Z}}}

%\renewcommand{\state}{\ensuremath{\mathbf{s}}}
\renewcommand{\state}{\ensuremath{\mathbf{x}}}

\renewcommand{\nominalDynamics}{\ensuremath{\mathbf{n}}}
\renewcommand{\unknownDynamics}{\ensuremath{\mathbf{f}}}
\renewcommand{\nominalDynamicsK}{\ensuremath{\mode{\mathbf{n}}}}
\renewcommand{\unknownDynamicsK}{\ensuremath{\mode{\mathbf{f}}}}

\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{T}}
\newcommand{\inputDim}{\ensuremath{d}}
\newcommand{\InputDim}{\ensuremath{D}}
#+END_EXPORT

*** Bounds :ignore:
#+BEGIN_EXPORT latex
\newcommand{\tightBound}{\ensuremath{\mathcal{L}_{\text{tight}}}}
\newcommand{\furtherBound}{\ensuremath{\mathcal{L}_{\text{further}}}}
\newcommand{\furtherBoundTwo}{\ensuremath{\mathcal{L}_{\text{further}^2}}}
#+END_EXPORT
*** maths :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\mode}[1]{\ensuremath{#1_{\modeInd}}}
\renewcommand{\modei}[2]{\ensuremath{#1_{#2}}}
%\renewcommand{\singleOutput}{\ensuremath{\Delta x_{\numData}}}
%\renewcommand{\allOutput}{\ensuremath{\Delta\mathbf{\state}}}
\newcommand{\singleModeVar}{\ensuremath{\singleData{\modeVar}}}
\newcommand{\allModeVar}{\ensuremath{\bm{\modeVar}}}
\newcommand{\singleModeVarK}{\ensuremath{\singleModeVar = \modeInd}}
\newcommand{\allModeVarK}{\ensuremath{\bm{\modeVar}_{\modeInd}}}
%\newcommand{\allModeVarK}{\ensuremath{\{\singleModeVarK\}_{\numData=1}^\NumData}}
\newcommand{\modeVarnk}{\ensuremath{\modeVar_{\numData,\modeInd}}}

% new
\renewcommand{\numData}{\ensuremath{n}}
\renewcommand{\NumData}{\ensuremath{N}}
\renewcommand{\singleOutput}{\ensuremath{y_{\numData}}}
\renewcommand{\singleInput}{\ensuremath{\mathbf{x}_{\numData}}}
\renewcommand{\allInput}{\ensuremath{\mathbf{X}}}
\renewcommand{\allOutput}{\ensuremath{\mathbf{y}}}
\renewcommand{\allOutput}{\ensuremath{\mathbf{y}}}
%\renewcommand{\allInputK}{\ensuremath{\{\singleInput : \singleModeVarK \}}}
%\renewcommand{\allOutputK}{\ensuremath{\{\singleOutput : \singleModeVarK\}}}
%\renewcommand{\allInputK}{\ensuremath{\allInput^{\modeInd}}}
%\renewcommand{\allOutputK}{\ensuremath{\allOutput^{\modeInd}}}
\renewcommand{\singleInputK}{\ensuremath{\mathbf{x}_{\numData, \modeInd}}}
\renewcommand{\allInputK}{\ensuremath{\mode{\allInput}}}
\renewcommand{\allOutputK}{\ensuremath{\mode{\allOutput}}}

%\renewcommand{\x}{\ensuremath{\mathbf{z}}}
%\renewcommand{\y}{\ensuremath{y}}
%\renewcommand{\singleInput}{\ensuremath{\mathbf{z}_{\numData}}}
%\renewcommand{\allInput}{\ensuremath{\mathbf{Z}}}
%\renewcommand{\singleInputK}{\ensuremath{\mathbf{z}_{\numData, \modeInd}}}
%\renewcommand{\allInputK}{\ensuremath{\mode{\allInput}}}

%\newcommand{\expertPrior}{\ensuremath{p\left(\mode{f}(\allInput) \right)}}
\newcommand{\expertPrior}{\ensuremath{p\left(\mode{f}(\allInputK) \right)}}
\newcommand{\expertsPrior}{\ensuremath{p\left(\LatentFunc(\allInput) \right)}}
\newcommand{\expertMeanFunc}{\ensuremath{\mode{\mu}}}
\newcommand{\expertCovFunc}{\ensuremath{\mode{k}}}
\newcommand{\expertLikelihood}{\ensuremath{p\left(\allOutput \mid \mode{f}(\allInput)\right)}}
\newcommand{\singleExpertLikelihood}{\ensuremath{p(\singleOutput \mid \mode{f}(\singleInput))}}
%\newcommand{\allExpertLikelihood}{\ensuremath{p(\allOutput \mid \mode{f}(\allInput))}}
%\newcommand{\allExpertLikelihood}{\ensuremath{p(\allOutputK \mid \mode{f}(\allInputK))}}
\newcommand{\allExpertLikelihood}{\ensuremath{p(\allOutputK \mid \mode{f}(\allInputK))}}
\newcommand{\expertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVarK, \allInput \right)}}
\newcommand{\singleExpertPosterior}{\ensuremath{p\left(\singleOutput \mid \singleModeVarK, \allInput \right)}}
% \newcommand{\expertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVarK \right)}}

\newcommand{\gatingPrior}{\ensuremath{p\left(\GatingFunc(\allInput ) \right)}}
\newcommand{\gatingMeanFunc}{\ensuremath{\mode{\hat{\mu}}}}
\newcommand{\gatingCovFunc}{\ensuremath{\mode{\hat{k}}}}
\newcommand{\singleGatingLikelihood}{\ensuremath{\Pr\left(\singleModeVarK \mid \GatingFunc(\singleInput) \right)}}
%\newcommand{\allGatingLikelihood}{\ensuremath{\Pr\left(\allModeVarK \mid \GatingFunc(\allInput) \right)}}
\newcommand{\allGatingLikelihood}{\ensuremath{p\left(\allModeVar \mid \GatingFunc(\allInput) \right)}}
\newcommand{\gatingLikelihood}{\ensuremath{P\left(\singleModeVar \mid \GatingFunc(\singleInput) \right)}}
%\newcommand{\gatingPosterior}{\ensuremath{\Pr\left( \singleModeVar \mid \singleInput \right)}}
\newcommand{\gatingPosterior}{\ensuremath{\Pr\left( \allModeVarK \mid \allInput \right)}}
\newcommand{\singleGatingPosterior}{\ensuremath{\Pr\left( \singleModeVarK \mid \singleInput, \gatingParams \right)}}
\newcommand{\evidence}{\ensuremath{p\left(\allOutput \mid \allInput \right)}}

\newcommand{\moeExpertPosterior}{\ensuremath{p\left(\singleOutput \mid \singleModeVarK, \singleInput, \expertParamsK \right)}}
\newcommand{\moeGatingPosterior}{\ensuremath{\Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right)}}
\newcommand{\moeEvidence}{\ensuremath{p\left(\allOutput \mid \allInput, \expertParams, \gatingParams \right)}}
\newcommand{\singleMoeEvidence}{\ensuremath{p\left(\singleOutput \mid \singleInput, \expertParams, \gatingParams \right)}}

\newcommand{\npmoeExpertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVar, \allInput, \expertParams \right)}}
\newcommand{\npmoeGatingPosterior}{\ensuremath{p\left(\allModeVar \mid \allInput, \gatingParams \right)}}

\newcommand{\moeLikelihood}{\ensuremath{p\left(\allOutput \mid \LatentFunc(\allInput), \GatingFunc (\allInput) \right)}}
\newcommand{\singleMoeLikelihood}{\ensuremath{p\left(\singleOutput \mid \mode{\latentFunc}(\allInput), \GatingFunc (\allInput) \right)}}

#+END_EXPORT
*** kernels :ignore:
#+BEGIN_EXPORT latex
%\renewcommand{\expertKernelnn}{\ensuremath{k_{\singleInput\singleInput}}}
%\renewcommand{\expertKernelnM}{\ensuremath{\mathbf{k}_{\singleInput \expertInducingInput}}}
%\renewcommand{\expertKernelMM}{\ensuremath{\mathbf{K}_{\expertInducingInput\expertInducingInput}}}
%\renewcommand{\expertKernelMn}{\ensuremath{\mathbf{k}_{\expertInducingInput \singleInput}}}
\renewcommand{\expertKernelnn}{\ensuremath{k_{\modeInd \numData \numData}}}
\renewcommand{\expertKernelNN}{\ensuremath{\mathbf{K}_{\modeInd \NumData \NumData}}}
\renewcommand{\expertKernelnM}{\ensuremath{\mathbf{k}_{\modeInd \numData \NumInducing}}}}
\renewcommand{\expertKernelNM}{\ensuremath{\mathbf{K}_{\modeInd \NumData \NumInducing}}}}
\renewcommand{\expertKernelMM}{\ensuremath{\mathbf{K}_{\modeInd \NumInducing \NumInducing}}}
\renewcommand{\expertKernelMn}{\ensuremath{\mathbf{k}_{\modeInd \NumInducing \numData}}}
\renewcommand{\expertKernelMN}{\ensuremath{\mathbf{K}_{\modeInd \NumInducing \NumData}}}
\renewcommand{\expertKernelsM}{\ensuremath{\mathbf{k}_{\modeInd * \NumInducing}}}}
\renewcommand{\expertKernelss}{\ensuremath{k_{\modeInd **}}}
\renewcommand{\expertKernelSM}{\ensuremath{\mathbf{K}_{\modeInd * \NumInducing}}}}
\renewcommand{\expertKernelSS}{\ensuremath{\mathbf{K}_{\modeInd **}}}

%\renewcommand{\gatingKernelnn}{\ensuremath{\hat{k}_{\singleInput\singleInput}}}
%\renewcommand{\gatingKernelnM}{\ensuremath{\hat{\mathbf{k}}_{\singleInput \gatingInducingInput}}}
%\renewcommand{\gatingKernelMM}{\ensuremath{\hat{\mathbf{K}}_{\gatingInducingInput\gatingInducingInput}}}
%\renewcommand{\gatingKernelMn}{\ensuremath{\hat{\mathbf{k}}_{\gatingInducingInput \singleInput}}}
\renewcommand{\gatingKernelnn}{\ensuremath{\hat{k}_{\modeInd \numData \numData}}}
\renewcommand{\gatingKernelNN}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumData \NumData}}}
\renewcommand{\gatingKernelnM}{\ensuremath{\hat{\mathbf{k}}_{\modeInd \numData \NumInducing}}}
\renewcommand{\gatingKernelNM}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumData \NumInducing}}}
\renewcommand{\gatingKernelMM}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumInducing \NumInducing}}}
\renewcommand{\gatingKernelMn}{\ensuremath{\hat{\mathbf{k}}_{\modeInd \NumInducing \numData}}}
\renewcommand{\gatingKernelss}{\ensuremath{\hat{k}_{\modeInd **}}}
\renewcommand{\gatingKernelsM}{\ensuremath{\hat{\mathbf{k}}_{\modeInd * \NumInducing}}}
\renewcommand{\gatingKernelMs}{\ensuremath{\hat{\mathbf{k}}_{\modeInd \NumInducing *}}}
\renewcommand{\gatingKernelSM}{\ensuremath{\hat{\mathbf{K}}_{\modeInd * \NumInducing}}}
\renewcommand{\gatingKernelMS}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumInducing *}}}
\renewcommand{\gatingKernelSS}{\ensuremath{\hat{\mathbf{K}}_{\modeInd **}}}

\renewcommand{\expertA}{\ensuremath{\mode{\mathbf{A}}}}
\renewcommand{\gatingA}{\ensuremath{\mode{\hat{\mathbf{A}}}}}
#+END_EXPORT

*** inference :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\input}{\ensuremath{\hat{\state}}}
\renewcommand{\output}{\ensuremath{\Delta \state}}

\newcommand{\kernel}{\ensuremath{k}}
\newcommand{\expertKernel}{\ensuremath{\mode{\kernel}}}
\newcommand{\gatingKernel}{\ensuremath{\mode{\hat{\kernel}}}}

\newcommand{\numInducing}{\ensuremath{m}}
\newcommand{\NumInducing}{\ensuremath{\MakeUppercase{\numInducing}}}
%\newcommand{\inducingInput}{\ensuremath{\mathbf{Z}}}
\newcommand{\inducingInput}{\ensuremath{\bm{\zeta}}}
\newcommand{\inducingOutput}{\ensuremath{\mathbf{u}}}

%\newcommand{\expertInducingInput}{\ensuremath{\mode{\inducingInput}}}
%\newcommand{\expertsInducingInput}{\ensuremath{\inducingInput}}}
\newcommand{\expertInducingInput}{\ensuremath{\mode{\bm{\zeta}}}}
\newcommand{\expertsInducingInput}{\ensuremath{\bm{\zeta}}}
%\newcommand{\expertInducingOutput}{\ensuremath{\mode{\inducingOutput}}}
%\newcommand{\expertsInducingOutput}{\ensuremath{\MakeUppercase{\inducingOutput}}}
%\newcommand{\expertInducingOutput}{\ensuremath{\mode{\hat{\mathbf{\latentFunc}}}}}
%\newcommand{\expertsInducingOutput}{\ensuremath{\hat{\MakeUppercase{\mathbf{\latentFunc}}}}}
%\newcommand{\expertInducingOutput}{\ensuremath{\mode{\tilde{\mathbf{\latentFunc}}}}}
%\newcommand{\expertsInducingOutput}{\ensuremath{\tilde{\MakeUppercase{\mathbf{\latentFunc}}}}}
\newcommand{\expertInducingOutput}{\ensuremath{\mode{\latentFunc}(\expertInducingInput)}}
\newcommand{\expertsInducingOutput}{\ensuremath{\mathbf{\latentFunc}(\expertsInducingInput)}}

%\newcommand{\gatingInducingInput}{\ensuremath{\hat{\inducingInput}}}
\newcommand{\gatingInducingInput}{\ensuremath{\bm{\xi}}}
%\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\hat{\inducingOutput}}}}
%\newcommand{\gatingsInducingOutput}{\ensuremath{\hat{\MakeUppercase{\inducingOutput}}}}
%\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\hat{\mathbf{\gatingFunc}}}}}
%\newcommand{\gatingsInducingOutput}{\ensuremath{\hat{\MakeUppercase{\mathbf{\gatingFunc}}}}}
%\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\tilde{\mathbf{\gatingFunc}}}}}
%\newcommand{\gatingsInducingOutput}{\ensuremath{\tilde{\MakeUppercase{\mathbf{\gatingFunc}}}}}
\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\gatingFunc}(\gatingInducingInput)}}
\newcommand{\gatingsInducingOutput}{\ensuremath{\mathbf{\gatingFunc}(\gatingInducingInput)}}

%\newcommand{\expertInducingPrior}{\ensuremath{p(\mode{\latentFunc}(\expertInducingInput))}}
%\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput \mid \expertInducingInput)}}
%\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput \mid \expertsInducingInput)}}
\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput)}}
\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput)}}
%\newcommand{\expertInducingVariational}{\ensuremath{q(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\expertInducingVariational}{\ensuremath{q(\expertInducingOutput)}}
\newcommand{\expertsInducingVariational}{\ensuremath{q(\expertsInducingOutput)}}
\newcommand{\expertVariational}{\ensuremath{q(\mode{\latentFunc}(\singleInput))}}
\newcommand{\expertsVariational}{\ensuremath{q(\LatentFunc_\numData)}}
%\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \expertInducingOutput)}}
\newcommand{\allExpertGivenInducing}{\ensuremath{p(\allOutput \mid \expertInducingOutput)}}
%\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \expertInducingOutput)}}
\newcommand{\allLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\allInput) \mid \expertInducingOutput)}}


%\newcommand{\gatingInducingPrior}{\ensuremath{p(\GatingFunc(\gatingInducingInput))}}
%\newcommand{\gatingInducingPrior}{\ensuremath{p(\gatingInducingOutput \mid \gatingInducingInput)}}
%\newcommand{\gatingsInducingPrior}{\ensuremath{p(\gatingsInducingOutput \mid \gatingInducingInput)}}
\newcommand{\gatingInducingPrior}{\ensuremath{p(\gatingInducingOutput)}}
\newcommand{\gatingsInducingPrior}{\ensuremath{p(\gatingsInducingOutput)}}
%\newcommand{\gatingInducingVariational}{\ensuremath{q(\GatingFunc(\gatingInducingInput))}}
\newcommand{\gatingInducingVariational}{\ensuremath{q(\gatingInducingOutput)}}
\newcommand{\gatingsInducingVariational}{\ensuremath{q(\gatingsInducingOutput)}}
\newcommand{\gatingsVariational}{\ensuremath{q(\GatingFunc_\numData)}}
\newcommand{\singleGatingGivenInducing}{\ensuremath{\Pr(\singleModeVarK \mid \gatingsInducingOutput)}}
\newcommand{\allGatingGivenInducing}{\ensuremath{\Pr(\allModeVarK \mid \gatingInducingOutput)}}
\newcommand{\allGatingsGivenInducing}{\ensuremath{\Pr(\allModeVarK \mid \gatingsInducingOutput)}}
\newcommand{\singleLatentGatingsGivenInducing}{\ensuremath{p(\GatingFunc(\singleInput) \mid \gatingsInducingOutput)}}
\newcommand{\allLatentGatingsGivenInducing}{\ensuremath{p(\GatingFunc(\allInput) \mid \gatingsInducingOutput)}}
\newcommand{\singleLatentGatingGivenInducing}{\ensuremath{p(\mode{\gatingFunc}(\singleInput) \mid \gatingInducingOutput)}}

\newcommand{\expertKL}{\ensuremath{\text{KL}\left( \expertInducingVariational \mid\mid \expertInducingPrior \right)}}
\newcommand{\expertsKL}{\ensuremath{\sum_{\modeInd=1}^\ModeInd\text{KL}\left( \expertInducingVariational \mid\mid \expertInducingPrior \right)}}
\newcommand{\gatingKL}{\ensuremath{\text{KL}\left( \gatingInducingVariational \mid\mid \gatingInducingPrior \right)}}
\newcommand{\gatingsKL}{\ensuremath{\sum_{\modeInd=1}^\ModeInd \text{KL}\left( \gatingInducingVariational \mid\mid \gatingInducingPrior \right)}}
#+END_EXPORT
** Intro :ignore:
This chapter is interested in /learning/ representations of multimodal dynamical systems,
that can be leveraged to control robotic systems in uncertain environments, where both the underlying
dynamics modes, and how the system switches between them, are /not fully known a priori/.
This chapter assumes access to a data set of state transitions $\dataset$, that
have previously been sampled from the system at a constant frequency, i.e. with a fixed time-step.

# However, it does assume that the dynamics modes vary over the
# input (state, control) domain, such that the mode switching can be modelled by input-dependent functions.
# This set of functions governing the mode switching is commonly referred to as a gating network.

# This work is motivated by learning synergising model learning for controlling multimodal dynamical systems.

# This work is motivated by learning /latent/ structure that can be exploited for model-based control.

# In particular

# As such, correctly identifying the underlying dynamics modes is extremely important

# Identifiability

# Given such a data set, this chapter first constructs a discrete-time representation of
# multimodal dynamical systems.
# It then formulates a probabilistic representation of this model and details an approach to performing Bayesian
# inference in the model.

# This chapter is motivated by synergising model learning and control.
Following the motivation in cref:to_motivation, this chapter
seeks to correctly identify the underlying dynamics modes, whilst inferring
latent structure that can be exploited for control.
The main goals of this chapter can be summarised as follows,
1. accurately /identify/ the true underlying dynamics modes,
2. learn /latent spaces/ for planning/control,
   - rich with information regarding the mode switching behaviour.

The probabilistic model constructed in this chapter
resembles a \acrfull{mogpe} with a \acrshort{gp}-based gating network and  is named \acrfull{mosvgpe}.
Following other \acrshort{mogpe} methods, the \acrshort{mosvgpe} is evaluated on the motorcyle data set citep:Silverman1985.
It is then tested on the real-world quadcopter data set from the illustrative example detailed in cref:illustrative_example.
The work in this chapter is implemented in GPflow/TensorFlow and is available on GitHub[fn::Code accompanying this Chapter can be found @ [[https://github.com/aidanscannell/mogpe][\icon{\faGithub}/aidanscannell/mogpe]].]
citep:matthewsGPflow2017,GPflow2017,tensorflow2015-whitepaper.




# and a data set obtained from a
# velocity controlled point mass simulation.

# The model and inference are tested on the real-world quadcopter data set and a data set obtained from a
# velocity controlled point mass simulation.
# \todo{Add that this method is tested on mcycle?}
# The work in this chapter is implemented in GPflow/TensorFlow and is available on GitHub
# cite:matthewsGPflow2017,GPflow2017,tensorflow2015-whitepaper[fn:1:Code accompanying this Chapter can be found @ [[https://github.com/aidanscannell/mogpe][\icon{\faGithub}/aidanscannell/mogpe]].].

# This chapter first introduces the set of continuous-time multimodal dynamical systems



# Based on this assumption, this chapter constructs a discrete-time representation of the system's transition dynamics
# and then formulates it as a probabilistic model based on GPs.

# This thesis is interested in multimodal systems where the dynamics modes vary over the
# input (state, control) domain, such that the mode switching can by modelled
# by input-dependent functions.
# This set of functions governing the mode switching is commonly referred to as a gating network.

# Motivated by data-efficient learning this work formulates multimodal on probabilistic models


# This work is motivated by data-efficient learning and specifically focuses on gating networks
# where prior knowledge of the system can be encoded into the model via informative priors.
# As a result, the probabilistic model constructed in this chapter resembles a \acrfull{mogpe}
# with a \acrshort{gp}-based gating network.
# This work derives a novel variational lower bound based on sparse GPs that
# enables the model to be trained with stochastic gradient methods.


# that this work considers and then details an approach to performing Bayesian inference in such models.

# Motivated by control,
# Given this data set $\dataset$, this chapter constructs a discrete-time, probabilistic
# representation of the multimodal transition dynamics
# and then formulates it as a probabilistic model based on GPs.

# the system's transition dynamics
# and then formulates it as a probabilistic model based on GPs.

# that pairs of input $\x$ and output $\y$ observations
# have previously been sampled from the system at a constant frequency,
# (i.e. with a fixed time-step) to give a data set $\dataset$.

# introduces the class of continuous-time multimodal dynamical systems

# This chapter introduces the class of continuous-time multimodal dynamical systems
# that this work considers and then details an approach to performing Bayesian inference in such models.

# Throughout this chapter it is assumed that pairs of input $\x$ and output $\y$ observations
# have previously been sampled from the system at a constant frequency,
# (i.e. with a fixed time-step) to give a data set $\dataset$.
# Based on this assumption, this chapter constructs a discrete-time representation of the system's transition dynamics
# and then formulates it as a probabilistic model based on GPs.

# This thesis is interested in multimodal systems where the dynamics modes vary over the
# input (state, control) domain, such that the mode switching can by modelled
# by input-dependent functions.
# This set of functions governing the mode switching is commonly referred to as a gating network.

# This work is motivated by data-efficient learning and specifically focuses on gating networks
# where prior knowledge of the system can be encoded into the model via informative priors.
# As a result, the probabilistic model constructed in this chapter resembles a \acrfull{mogpe}
# with a \acrshort{gp}-based gating network.
# This work derives a novel variational lower bound based on sparse GPs that
# enables the model to be trained with stochastic gradient methods.
# The work in this chapter is implemented in GPflow/TensorFlow and is available on GitHub
# cite:matthewsGPflow2017,GPflow2017,tensorflow2015-whitepaper[fn:1:Code accompanying this Chapter can be found @ [[https://github.com/aidanscannell/mogpe][\icon{\faGithub}/aidanscannell/mogpe]].].

** Problem Statement
\todo{change t-1 to t and t to t+1 to be consistent with control section}
This work considers /unknown/, or /partially unknown/,  multimodal,
stochastic, nonlinear dynamical systems,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-multimodal-dynamics-disc}
\Delta \state_{\timeInd+1}
&= \unknownDynamics(\state_{t}, \control_{t}; \Delta t = t_*) + \bm\epsilon & \nonumber \\
&= \unknownDynamicsK (\state_{\timeInd}, \control_{\timeInd} ; \Delta t = t_*) + \mode{\bm\epsilon}
&\text{if} \quad \modeVar_{\timeInd} = \modeInd
\end{align}
#+END_EXPORT
with continuous states $\state \in \stateDomain \subseteq \R^\StateDim$ and controls
$\control \in \controlDomain \subseteq \R^\ControlDim$.
The state and control at time $t$ are denoted as $\state_t \in \stateDomain$ and $\control_t \in \controlDomain$
respectively and
$\Delta \state_{\timeInd+1} = \state_{\timeInd+1} - \state_{\timeInd}$ denotes the change in state between time $\timeInd-1$
and $\timeInd$.
At any given time step $t$, one of the $\ModeInd$ dynamics modes
$\{\unknownDynamicsK : \stateDomain \times \controlDomain \rightarrow \stateDomain \}_{\modeInd=1}^\ModeInd$,
and associated noise models
$\mode{\bm\epsilon} &\sim \mathcal{N}\left(\mathbf{0}, \bm\Sigma_{\mode{\bm\epsilon}} \right)$, where
$\bm\Sigma_{\mode{\epsilon}} = \text{diag}\left[ \sigma_{\modeInd,1}^{2}, \ldots, \sigma_{\modeInd,\StateDim}^2 \right]$,
are selected by a discrete mode indicator variable,
$\alpha_t \in \modeDomain = \mathbb{Z} \cap [1,\ModeInd]$.
This work considers systems that are stochastic (subject to process noise), but
are not subject to observation noise.
Thus, the $\mode{\bm\epsilon}$ term solely represents the process noise.

# The nominal dynamics
# $\{\nominalDynamicsK : \stateDomain \times \controlDomain \rightarrow \stateDomain \}_{\modeInd=1}^\ModeInd$
# represents our /a priori/ knowledge of the dynamics (i.e. the assumed dynamics)
# and the additive dynamics
# $\{\unknownDynamicsK : \stateDomain \times \controlDomain \rightarrow \stateDomain \}_{\modeInd=1}^\ModeInd$
# represents the initially unknown dynamics, which are to be learned from observations.


This chapter assumes access to historical data comprising state transitions from $\NumEpisodes$ trajectories
of length $T$, sampled  with a  fixed  time step $\Delta t=t_{*}$.
The data set has ${N=ET}$ elements
and we abuse notation by appending the independent trajectories along
time to get the data set $\mathcal{D}=\{(\state_{\timeInd},\control_{\timeInd}),\Delta \state_{t+1}\}^\NumData_{\timeInd=1}$.

To help with understanding and ease of notation, our modelling only considers a single output dimension,
i.e. $\stateDomain \subseteq \R$ as $\fk : \stateDomain \times \controlDomain \rightarrow \stateDomain$.
The extension to multiple output dimensions follows from standard GP methodologies and is detailed where necessary.
To further ease notation, the state-control input domain is denoted
$\inputDomain = \stateDomain \times \controlDomain \subseteq \R^{\InputDim}$
and a single state-control input is dentoed
$\singleInput = (\state_{\timeInd},\control_{\timeInd})$.
Given this formulation, the goal of this chapter is to learn the mapping $\unknownDynamics$,
which switches between $\ModeInd$ different functions $\mode{\latentFunc}$.
This is a regression problem,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-multimodal-dynamics-disc-regression}
\underbrace{\Delta \state_{t+1}}_{\text{\singleOutput}}
= \underbrace{\unknownDynamicsK(\state_{t}, \control_{t} ; \Delta t = t_*)}_{\text{\mode{\latentFunc} (\singleInput)}}
+ \mode{\epsilon}
\quad \text{if} \quad \modeVarK,
\end{align}
#+END_EXPORT
where both the latent dynamics functions $\unknownDynamics$ and how the system switches between them
$\modeVar$, must be inferred from observations.
A single observation is denoted as $(\singleInput, \singleOutput) = ((\state_{t},\control_{t}), \Delta \state_{t+1})$.
The set of all inputs is denoted as $\allInput \in \R^{\NumData \times \InputDim$
and the set of all outputs as $\allOutput \in \R^{\NumData \times 1}$.
With this notation, the regression problem can be written as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-multimodal-dynamics-disc-regression-simple}
\singleOutput
= \mode{\latentFunc}(\singleInput) + \mode{\epsilon}
\quad \text{if} \quad \modeVarK.
\end{align}
#+END_EXPORT


# from inputs $\singleInput$ to
# outputs $\singleOutput$,


# Given this notation, our task is to learn the mapping $\f$, from inputs $\singleInput$ to
# outputs $\singleOutput$,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-multimodal-dynamics-disc}
# \singleOutput = \mode{\latentFunc} (\singleInput) + \mode{\epsilon}
# \quad \text{if} \quad \modeVarK,
# \end{align}
# #+END_EXPORT
# where the mapping switches between $\ModeInd$ different functions $\mode{\latentFunc}$.

# The $\stateDim^{\text{th}}$ dimension of the $\modeInd^{\text{th}}$ mode's latent transition dynamics
# function $\fk$, evaluated at $\singleInput$,
# is denoted $\fknd = \fkd (\singleInput)$,
# for all dimensions as $\fkn$ and at all data points as $\Fk$.

# A time series of observations from time $a$ to time $b$ (inclusive)
# is denoted by $\mathbf{x}_{a:b}$ (analogously for other variables).
# The concatenation of the state and control domains is
# denoted $\inputDomain \coloneqq \stateDomain \times \controlDomain$.
# A single state-control input is denoted as
# $\singleInput = (\state_{t-1}, \control_{t-1})$, all inputs are denoted as
# $\allInput$, and the set of all outputs as $\allOutput$.
# The $\stateDim^{\text{th}}$ dimension of the $\modeInd^{\text{th}}$ mode's latent transition dynamics
# function $\fk$, evaluated at $\singleInput$,
# is denoted $\fknd = \fkd (\singleInput)$,
# for all dimensions as $\fkn$ and at all data points as $\Fk$.

# This chapter assumes access to a historical data set, $\dataset$, comprising state transitions
# from $\NumEpisodes$ trajectories (episodes), sampled with a fixed time step, $\Delta t=t_*$, resulting in
# $\NumTimesteps$ steps per episode.
# This gives a data set with $N = \NumEpisodes \NumTimesteps$ elements.


# giving the data set,
# $~{\mathcal{D} = \left\{ \left\{ (\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t} \right\}_{\numTimesteps=1}^{\NumTimesteps} \right\}_{\numEpisodes=1}^{\NumEpisodes}}$.

# This chapter assumes access to a historical data set, $\dataset$, comprising state transitions
# from $\NumEpisodes$ trajectories (episodes), sampled with a fixed time step, $\Delta t=t_*$, resulting in
# $\NumTimesteps$ steps per episode.
# This gives a data set with $N = \NumEpisodes \NumTimesteps$ elements.
# We combine the independent episodes to get the data set
# $\mathcal{D} = \left\{ \left\{ (\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t} \right\}_{\numTimesteps=1}^{\NumTimesteps} \right\}_{\numEpisodes=1}^{\NumEpisodes}$.
# We abuse notation (considering time indexing) and denote the data set
# $\mathcal{D} = \left\{ (\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t} \right\}_{\numTimesteps=1}^{\NumTimesteps}$.

# We combine the independent episodes to get the data set
# $\mathcal{D} = \left\{(\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t}\right\}_{t=1}^{T}$
# where we have abused the time indexing notation.

# This work learns a discrete-time representation of cref:eq-multimodal-dynamics-cont,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-multimodal-dynamics-disc}
# \singleOutput = \fk (\singleState, &\singleControl ; \Delta t = t_*) + \mode{\epsilon_{t-1}}
# \quad \text{if} \quad \modeVarK,
# \end{align}
# #+END_EXPORT
# where $\state_t \in \R^D$ and $\control_t \in \R^F$ are the states and controls
# at time $t$ respectively, and $\alpha_t \in \modeDomain = \{1, \dotsc, \ModeInd\}$ is the mode indicator variable (that
# indicates one of $\ModeInd$ dynamics modes) at time $t$.

# A time series of observations from time $a$ to time $b$ (inclusive)
# is denoted by $\mathbf{x}_{a:b}$ (analogously for other variables).
# The concatenation of the state and control domains is
# denoted $\inputDomain \coloneqq \stateDomain \times \controlDomain$.
# A single state-control input is denoted as
# $\singleInput = (\state_{t-1}, \control_{t-1})$, all inputs are denoted as
# $\allInput$, and the set of all outputs as $\allOutput$.
# The $\stateDim^{\text{th}}$ dimension of the $\modeInd^{\text{th}}$ mode's latent transition dynamics
# function $\fk$, evaluated at $\singleInput$,
# is denoted $\fknd = \fkd (\singleInput)$,
# for all dimensions as $\fkn$ and at all data points as $\Fk$.

** Preliminaries
Gaussian processes are the state-of-the art approach for Bayesian nonparametric regression
and they provide a powerful mechanism for encoding expert domain knowledge.
As such, \acrshort{mogpe} methods are a promising direction for modelling multimodal systems.
This section recaps the \acrshort{mogpe} concepts that this chapter builds upon.
*** mixture models :ignore:

\newline

*Mixture Models*
Mixture models are a natural choice for modelling multimodal systems.
Given an input $\singleInput$ and an output $\singleOutput$, mixture models
model a mixture of distributions over the output,
\marginpar{mixture models}
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-mixture-model}
\singleMoeEvidence = \sum_{\modeInd=1}^\ModeInd
\underbrace{\Pr(\singleModeVarK \mid \gatingParams)}_{\text{mixing probability}}
\underbrace{p(\singleOutput \mid \modeVarK, \singleInput, \expertParamsK)}_{\text{component } k}.
\end{equation}
#+END_EXPORT
where $\gatingParams$ and $\expertParams$ represent model parameters
and $\singleModeVar$ is a discrete indicator variable assigning observations to components.
The predictive distribution $\singleMoeEvidence$ consists of
$K$ mixture components ${p(\singleOutput \mid \modeVarK, \singleInput, \expertParamsK)}$ that are weighted according to
their mixing probabilities $\Pr(\modeVarK \mid \gatingParams)$.

*** mixture of experts :ignore:

\newline

*Mixture of Experts* The \acrfull{moe} model citep:jacobsAdaptive1991 is
an extension where the mixing probabilities
depend on the input variable $\moeGatingPosterior$, and are
collectively referred to as the *gating network*.
The individual component densities $\moeExpertPosterior$ are then referred to as *experts*,
as at different regions in the input space, different components are responsible for predicting.
Given $\ModeInd$ experts $\moeExpertPosterior$, each with parameters $\expertParamsK$,
the MoE marginal likelihood is given by,
\marginpar{mixture of experts}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixture-marginal-likelihood}
\moeEvidence = \prod_{\numData=1}^\NumData \sum_{\modeInd=1}^{\ModeInd}
\underbrace{\moeGatingPosterior}_{\text{gating network}}
\underbrace{\moeExpertPosterior}_{\text{expert } k},
\end{align}
#+END_EXPORT
where $\singleModeVar$ is the expert indicator variable assigning observations to experts.
The probability mass function over the expert indicator variable is referred to as the gating network and
indicates which expert governs the model at a given input location.
See cite:yukselTwenty2012 for a survey of \acrshort{moe} methods.
*** Nonparametric Mixtures of Experts :ignore:

\newline
*Nonparametric Mixtures of Experts*
Modelling the experts as GPs gives rise to a class of powerful models known as \acrfull{mogpe}.
They can model multimodal distributions as they model a mixture of distributions
over the outputs, usually a Gaussian mixture in the regression setting citep:trespMixtures2000a,rasmussenInfinite2001.
They are able to model non-stationary functions as
each expert learns separate hyperparameters (lengthscales, noise variances etc).
Many \acrshort{mogpe} methods have been proposed and in general they differ via
the formulation of their gating network and their approximate inference algorithms.
# and the gating network can turn each expert "on" and "off" in different regions of the input space.

*** npmoe graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
    \begin{minipage}[r]{0.49\textwidth}
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInputK$};

      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\allInputK)$};

      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[latent, below=of x, xshift=0.0cm] (a) {$\modeVarnk$};

      \node[const, right=of a, xshift=-0.4cm] (phik) {$\gatingParams$};

      \node[obs, below=of f, yshift=0.4cm] (y) {$\allOutputK$};

      \node[const, left=of y] (sigmak) {$\noiseVarK$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      %\draw[post] (f)--(yk);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)--(a);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(a);
      %\draw[post] (sigmak)|-(yk);
      \draw[post] (sigmak)|-(y);
      \draw[post] (f)--(y);

      \plate {} {(x) (yk) (a)} {$\NumData_{\modeInd}$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      %\plate {} {(f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(x) (y) (f) (sigmak) (thetak) (a)} {$\ModeInd$};
    \end{tikzpicture}
    %}
    \subcaption{}
\label{fig-graphical-model-npmoe}
\end{minipage}
    \begin{minipage}[r]{0.49\textwidth}
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInputK$};

      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\allInputK)$};
      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[latent, below=of x, xshift=0.0cm] (a) {$\modeVarnk$};
      \node[latent, right=of a, yshift=0.0cm] (h) {$\mode{\gatingFunc}(\allInput)$};
      \node[const, right=of h, xshift=-0.4cm] (phik) {$\gatingParamsK$};

      \node[obs, below=of f, yshift=0.4cm] (y) {$\allOutputK$};
      \node[const, left=of y] (sigmak) {$\noiseVarK$};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      \draw[post] (x)-|(h);
      \draw[post] (h)--(a);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      %\draw[post] (sigmak)|-(yk);
      \draw[post] (sigmak)|-(y);
      \draw[post] (f)--(y);

      \plate {} {(x) (yk) (a)} {$\NumData_{\modeInd}$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      %\plate {} {(f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(h) (phik)} {$\ModeInd$};
      \plate {} {(x) (y) (f) (sigmak) (thetak)  (a)} {$\ModeInd$};
    \end{tikzpicture}
    %}
    \subcaption{}
\label{fig-graphical-model-gp-gating-network}
\end{minipage}
  %\caption{Graphical model where the output $\singleOutput$}
  \caption{
  Graphical models where the outputs $\allOutput = \{\allOutputK\}_{\modeInd=1}^{\ModeInd}$
are generated by mapping the inputs $\allInput = \{\allInputK\}_{\modeInd=1}^{\ModeInd}$ through the latent process.
  An input assigned to expert $\modeInd$ is denoted $\singleInputK$
  and the sets of all $\NumData_{\modeInd}$ inputs and outputs assigned to expert $\modeInd$ are denoted
  $\allInputK$ and $\allOutputK$ respectively.
The experts are shown on the left of each model and the gating network on the right.
The generative process involves evaluating the gating network
and sampling an expert mode indicator variable $\singleModeVar$.
The indicated mode's latent function $\mode{f}$ and noise model $\mathcal{N}(0, \noiseVarK)$ are
then evaluated to generate the output $\singleOutput$.
  (\subref{fig-graphical-model-npmoe}) shows
   the Mixture of Gaussian Process Experts model first presented in
  \cite{rasmussenInfinite2001} but without the Dirichlet process prior on the gating network.
  This represents the basic conditional model, not the full generative model over both the inputs and outputs as
  presented in \cite{NIPS2005_f499d34b}.
  (\subref{fig-graphical-model-gp-gating-network}) shows our model with a GP-based gating network
which involves evaluating $\ModeInd$ latent gating functions $\GatingFunc$
and normalising their output to obtain the mixing probabilities $\singleGatingLikelihood$.
The mode indicator variable $\singleModeVar$ is then sampled from the Categorical distribution
governed by these probabilities.}
\label{fig-graphical-model-comparison}
\end{figure}
#+END_EXPORT

*** after graphical model :ignore:

\newline

As highlighted by cite:rasmussenInfinite2001,
the traditional MoE marginal likelihood does not apply when the
experts are nonparametric.
This is because the model assumes that the observations are i.i.d. given the model parameters,
which is contrary to \acrshort{gp} models, which model the dependencies in the joint distribution, given the
hyperparameters.
\marginpar{mixtures of nonparametric experts}
cite:rasmussenInfinite2001 point out that there is a joint distribution corresponding to every possible
combination of assignments (of observations to experts).
The marginal likelihood is then a sum over exponentially many ($\ModeInd^{\NumData}$) sets of assignments,
#+BEGIN_EXPORT latex
\small
\begin{align}  \label{eq-np-moe-marginal-likelihood-assign}
\moeEvidence &= \sum_{\allModeVar} \npmoeGatingPosterior \npmoeExpertPosterior  \nonumber \\
&= \sum_{\allModeVar} \npmoeGatingPosterior
\left[ \prod_{\modeInd=1}^\ModeInd p\left(\allOutputK \mid \allInputK, \expertParamsK \right) \right],
\end{align}
\normalsize
#+END_EXPORT
where $\allModeVar = \{\modeVar_1, \ldots, \modeVar_\NumData \}$ represents a set of assignments for all observations.
cref:fig-graphical-model-npmoe shows its graphical model representation.
Note that $\allInputK = \{\singleInput : \singleModeVarK \}_{\numData=1}^{\NumData_{\modeInd}}$ and
$\allOutputK = \{\singleOutput : \singleModeVarK \}_{\numData=1}^{\NumData_{\modeInd}}$
represent the $\NumData_{\modeInd}$ inputs and outputs assigned to the $\modeInd^{\text{th}}$
expert respectively.
This distribution factors into the product over experts, where each expert models the joint Gaussian distribution
over the observations assigned to it.
Assuming a mixture of Gaussian process regression models,
the marginal likelihood in cref:eq-np-moe-marginal-likelihood-assign
can be expanded to show each of the experts' latent variables,
#+BEGIN_EXPORT latex
\marginpar{marginal likelihood}
\begin{align} \label{eq-np-moe-marginal-likelihood}
%\moeEvidence
\evidence
%&= \sum_{\allModeVar} \npmoeGatingPosterior \npmoeExpertPosterior  \\
%&= \sum_{\allModeVar} \npmoeGatingPosterior
%\left[ \prod_{\modeInd=1}^\ModeInd p\left(\allOutputK \mid \allInputK, \expertParamsK \right) \right]   \\
&= \sum_{\allModeVar} \npmoeGatingPosterior
\left[ \prod_{\modeInd=1}^\ModeInd
\E_{\expertPrior \right} \left[
\prod_{\numData=1}^{\NumData_{\modeInd}}
p\left(\singleOutput \mid \mode{\latentFunc}(\singleInput) \right)
\right] \right],
\end{align}
#+END_EXPORT
where each expert follows the standard Gaussian likelihood model,
\marginpar{expert's Gaussian likelihood}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-likelihood}
\singleOutput &= \mode{f}(\singleInput) + \epsilon_{k}, \quad \epsilon_{k} \sim \mathcal{N}(0, \mode{\noiseVar}^2), \\
\singleExpertLikelihood &= \mathcal{N}\left( \singleOutput \mid \mode{f}(\singleInput), \mode{\noiseVar}^2 \right),
%\singleOutput \mid \modeVar, \mode{\latentFunc}(\singleInput) &\sim \mathcal{N}\left( \mode{f}(\singleInput), \mode{\noiseVar}^2 \right),
\end{align}
#+END_EXPORT
with $\mode{f}$ and $\mode{\noiseVar}^2$ representing the latent function and the noise variance associated
with the $\modeInd$^{\text{th}} expert.
Note that for notational conciseness the dependency on $\modeVarK$ is dropped from
$\singleExpertLikelihood$ as it is implied by the mode indexing $\mode{\latentFunc}$.
The dependence on $\gatingParams$ and $\expertParams$ is also dropped from here on in.
Independent GP priors are placed on each of the expert's latent functions,
\marginpar{expert's GP prior}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior}
%\expertPrior &= \mathcal{N}\left( \mode{f}(\allInput) \mid \expertMeanFunc(\allInput), \expertCovFunc(\allInput, \allInput) \right) \\
%\mode{\latentFunc}(\allInput) &\sim \mathcal{N}\left( \expertMeanFunc(\allInput), \expertCovFunc(\allInput, \allInput) \right) \\
\mode{\latentFunc}(\allInputK) &\sim \mathcal{N}\left( \expertMeanFunc(\allInputK), \expertCovFunc(\allInputK, \allInputK) \right)
\end{align}
#+END_EXPORT
where $\expertMeanFunc(\cdot)$ and $\expertCovFunc(\cdot, \cdot)$ represent the mean and
covariance functions associated with the $\modeInd^{\text{th}}$ expert respectively.
This leads to each expert resembling a standard \acrshort{gp} regression model with a Gaussian likelihood.
For notational conciseness the dependence on the inputs $\allInputK$ and hyperparameters $\expertParamsK$
is dropped for each \acrshort{gp} prior,
i.e. $\expertPrior = p\left( \mode{f}(\allInputK) \mid \allInputK, \expertParamsK \right)$.
# Note that the expectation over each experts' latent function in cref:eq-np-moe-marginal-likelihood
# is inside the marginalisation of the expert indicator variable $\modeVar$.

** Related Work :noexport:
Modelling the experts as GPs gives rise to a class of powerful models known as
Mixture of Gaussian Process Experts (MoGPE).
These models address both of the limitations that we discussed.
They can model multimodal predictive distributions as they model a mixture of distributions
over the outputs, usually a Gaussian mixture in the regression setting (cite:Tresp,Rasmussen2002).
They are able to model non-stationary functions as
each expert can learn different lengthscales, noise variances etc and the gating
network can turn each one "on" and "off" in different regions of the input space.
Many approaches to MoGPE have been proposed and in general the key differences between them are
the formulation of the gating network
and their inference algorithms (and associated approximations) cite:Yuksel2012.
The original MoGPE work by cite:Tresp proposed a gating network as a softmax of $K$ GPs
(bottom left plot in Figure [[ref:gating_network_comparison]]).
An EM inference scheme is proposed, which assuming there are $N$ observations requires
$\mathcal{O}(3KN^3)$ computations per iteration.


Although this gating function divides up the input space cite:Rasmussen2002 argue that
data not assigned to a GP expert will lead to bias near the boundaries.
Instead cite:Rasmussen2002 formulate the gating network in terms of conditional
distributions on the expert indicator variable, on which they place an input-dependent
Dirichlet process prior.
cite:Meeds2005 proposed an alternative infinite MoGPE similar to cite:Rasmussen2002 except
that they specify a full generative model over the input and output space $p(\mathbf{y}, \mathbf{x})$
as opposed to just a
conditional model $p(\mathbf{y} | \mathbf{x})$.
Each experts inputs are now Gaussian distributed
(middle top plot of Figure [[ref:gating_network_comparison]]) giving rise to the gating
network in the middle bottom plot of Figure [[ref:gating_network_comparison]].
This gating network is not capable of turning an expert "on" in multiple regions
of the input space. It will instead generate a new cluster and assign it a new expert.
These approaches reduce the computational burden by associating each expert with a
subset of the observations but rely on Markov Chain Monte Carlo (MCMC) inference schemes.

cite:Yuan proposed a MoGPE with a variational inference scheme which is much
faster than using MCMC.
They assume that the expert indicators $\alpha$ are generated by a Dirichlet distribution and
that given the expert indicator the inputs follow a Gaussian mixture model (GMM), i.e. they
have introduced a cluster indicator variable $z$.
This contrasts earlier approaches that let the expert indicator act as the input cluster
indicator.
This is illustrated in the right hand plots of Figure [[ref:gating_network_comparison]] which
show two Gaussian mixtures over the inputs (one for each expert) and
the resulting expert mixing probabilities.
Introducing the separate cluster indicator variable has given rise to a
gating network that can turn a single expert "on" in multiple regions of the input space.
However, it does not provide a handle for encoding prior information like the GP based gating network.

We note that mixture models are inherently unidentifiable as
different combinations of mixture components
and mixing coefficients can generate the same predictive distributions.
The gating network can be considered a handle for encoding prior knowledge that can be used to constrain
the set of admissible functions.
This can improve identifiability and lead to learned representations that better reflect
our understanding of the system. The simplest case being reordering the experts.
Figure ref:gating_network_comparison provides a visual comparison of
different gating network formulations,
providing intuition for how they can differ in restricting the set of admissible functions.


Modelling the gating network with GPs enables us
to encode informative prior knowledge through the choice of mean and covariance functions.
For example, adopting a squared exponential covariance function would encode prior belief that
the mixing probabilities should vary smoothly across the input space.
If modelling a dynamical system with oscillatory behaviour (e.g. an engine)
a periodic kernel could be adopted.
Prior knowledge of the mixing probability values can be encoded through
the choice of mean function.
We may also be interested in exploiting techniques from differential geometry
e.g. finding probabilistic geodesics (cite:Tosi2014) on the gating network.
Again, our choice of covariance function can be used to encode how differentiable
the gating network should be.
Learning better representations will also improve the models
ability to extrapolate as it will be more true to the underlying system.

We can also consider the implications of the mentioned gating networks from an inference perspective.
It is known that developing inference algorithms for propagating uncertain (Gaussian) inputs
through GPs is itself a challenging problem (see cite:Ustyuzhaninov2019).
Therefore, developing a scalable inference algorithm to propagate inputs that are distributed according to
a Gaussian mixture will likely lead to valuable information loss.
Theoretically the approaches by cite:Rasmussen2002,Meeds2005 are able to achieve very
accurate results but their inference relies on MCMC sampling methods,
which can be slow to converge.

We consider our approach as trading in the computational benefits that can be obtained through
the formulation of the gating network
for the ability to improve identifiability with informative GP priors.
The main contributions of this paper are twofold. Firstly, we re-formulate a gating network
based on GPs to improve indentifiability.
Secondly, we derive an evidence lower bound
which improves on the complexity issues associated with inference when adopting such a gating network.
Motivated by learning representations of dynamical systems with two regimes we instantiate
the model with two experts as it is  a special case where the gating network can be calculated
in closed form.
This is because we seek to learn an operable mode with one expert and explain away the inoperable mode
with the other. This results
in the gating network indicating which regions of the
input space are operable, providing a convenient space for planning.

# The remainder of the paper is organised as follows. We first introduce our model
# in Section ref:sec-modelling
# and then derive our variational lower
# bound in Section ref:sec-variational-approximation.
# In Section ref:sec-model-validation we test our method on an artificial data set where we show
# the benefits of adopting informative covariance functions in the gating network.
# We then test our model on the motorcycle data set and compare it to a sparse variational GP.

** Identifiable Mixtures of Gaussian Process Experts
*** intro :ignore:noexport:
# Motivated by synergising model learning and control, the probabilistic model constructed in this chapter
# Given this motivation, the probabilistic model constructed in this chapter
# is a \acrfull{mogpe} with a \acrshort{gp}-based gating network.
# Motivated by identifiability this work adopts a GP-based gating network resembling a GP classification model.
# Such a gating network is able to turn a single expert on in multiple subsets of the domain.

# The \acrshort{gp}-based gating network infers latent /geometric/ structure that is exploited by the control
# algorithms in cref:chap-traj-opt-geometry.

*** intro :ignore:noexport:
Motivated by synergising model learning and control, the probabilistic model constructed in this chapter
resembles a \acrfull{mogpe} with a \acrshort{gp}-based gating network.
The \acrshort{gp}-based gating network infers latent /geometric/ structure that is exploited by the control
algorithm in Section ref:sec-traj-opt-geometric.
# This work derives a novel variational lower bound based on sparse GP approximations, that provides
# well-calibrated uncertainty estimates and scalability via stochastic gradient methods.

# Given a factorised likelihood for each expert (e.g. Gaussian),
# an alternative approach to cref:eq-np-moe-marginal-likelihood-assign is to let each expert
# predict on the basis of all observations and leave the gating network to soft assign the observations.
# An alternative approach to cref:eq-np-moe-marginal-likelihood-assign is to let each expert
# predict on the basis of all observations (i.e. $\ModeVarK = \{\modeVarK\}_{\numData=1}^{\NumData}$)
# and leave the gating network to soft assign the observations.
# The marginal likelihood of this model is then given by,

The marginal likelihood in cref:eq-np-moe-marginal-likelihood-assign
can be expanded to show each of the experts' latent variables,
#+BEGIN_EXPORT latex
\marginpar{marginal likelihood}
\begin{align} \label{eq-np-moe-marginal-likelihood}
\moeEvidence &= \sum_{\allModeVar} \npmoeGatingPosterior \npmoeExpertPosterior  \\
&= \sum_{\allModeVar} \npmoeGatingPosterior
\left[ \prod_{\modeInd=1}^\ModeInd p\left(\allOutputK \mid \allInputK, \expertParamsK \right) \right]   \\
&= \sum_{\allModeVar} \npmoeGatingPosterior
\left[ \prod_{\modeInd=1}^\ModeInd
\prod_{\numData=1}^{\NumData}
p\left(\singleOutput \mid \mode{\latentFunc}(\singleInput) \right)
p\left(\mode{\latentFunc}(\allInputK) \mid \allInputK \right)
\right],
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \marginpar{marginal likelihood}
# \begin{align} \label{eq-np-moe-marginal-likelihood}
# \moeEvidence &=
# \sum_{\modeInd=1}^\ModeInd
# \underbrace{ \Pr\left(\allModeVarK \mid \allInput, \gatingParams \right)}_{\text{gating network}}
# \underbrace{ p(\allOutput \mid \allModeVarK, \allInput, \expertParamsK) }_{\text{expert } \modeInd} \\
# &=
# \sum_{\modeInd=1}^\ModeInd
# \underbrace{ \left[ \prod_{\numData=1}^\NumData
# \Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right) \right]}_{\text{gating network}}
# \underbrace{\E_{\expertPrior} \left[ \prod_{\numData=1}^\NumData \singleExpertLikelihood \right]}_{\text{expert } \modeInd},
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \marginpar{marginal likelihood}
# \begin{align} \label{eq-np-moe-marginal-likelihood}
# \moeEvidence &=
# \prod_{\numData=1}^\NumData \sum_{\modeInd=1}^\ModeInd
# \underbrace{\Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right)}_{\text{gating network}}
# \underbrace{\E_{\expertPrior} \left[ \singleExpertLikelihood \right]}_{\text{expert } \modeInd},
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \marginpar{marginal likelihood}
# \begin{align} \label{eq-np-moe-marginal-likelihood}
# \moeEvidence &=
# \underbrace{\E_{\expertPrior}}_{\text{expert } k \text{ prior}} \left[
# \prod_{\numData=1}^\NumData \sum_{\modeInd=1}^\ModeInd
# \underbrace{\Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right)}_{\text{gating network}}
# \underbrace{ \singleExpertLikelihood}_{\text{expert } k \text{ likelihood}}
# \right],
# \end{align}
# #+END_EXPORT
where each expert follows the standard Gaussian likelihood model,
\marginpar{expert's Gaussian likelihood}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-likelihood}
\singleOutput &= \mode{f}(\singleInput) + \epsilon_{k}, \quad \epsilon_{k} \sim \mathcal{N}(0, \mode{\noiseVar}^2), \\
\singleExpertLikelihood &= \mathcal{N}\left( \singleOutput \mid \mode{f}(\singleInput), \mode{\noiseVar}^2 \right),
%\singleOutput \mid \modeVar, \mode{\latentFunc}(\singleInput) &\sim \mathcal{N}\left( \mode{f}(\singleInput), \mode{\noiseVar}^2 \right),
\end{align}
#+END_EXPORT
with $\mode{f}$ and $\mode{\noiseVar}^2$ representing the latent function and the noise variance associated
with the $\modeInd$^{\text{th}} expert.
Note that the dependency on $\modeVarK$ is dropped from $\singleExpertLikelihood$ for notational conciseness.
Independent GP priors are placed on each of the expert's latent functions,
\marginpar{expert's GP prior}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior}
%\expertPrior &= \mathcal{N}\left( \mode{f}(\allInput) \mid \expertMeanFunc(\allInput), \expertCovFunc(\allInput, \allInput) \right) \\
%\mode{\latentFunc}(\allInput) &\sim \mathcal{N}\left( \expertMeanFunc(\allInput), \expertCovFunc(\allInput, \allInput) \right) \\
\mode{\latentFunc}(\allInputK) &\sim \mathcal{N}\left( \expertMeanFunc(\allInputK), \expertCovFunc(\allInputK, \allInputK) \right)
\end{align}
#+END_EXPORT
where $\expertMeanFunc(\cdot)$ and $\expertCovFunc(\cdot, \cdot)$ represent the mean and
covariance functions associated with the $\modeInd^{\text{th}}$ expert respectively.
This leads to each expert resembling a standard GP regression model with a Gaussian likelihood.
For notational conciseness the dependence on the inputs $\allInputK$ and hyperparameters $\expertParamsK$
is dropped for each GP prior,
i.e. $\expertPrior = p\left( \mode{f}(\allInputK) \mid \allInputK, \expertParamsK \right)$.
\todo{add reference to section number}
Note that the expectation over each experts' latent function in cref:eq-np-moe-marginal-likelihood
is inside the marginalisation of the expert indicator variable $\modeVar$.
# Importantly, this model retains the uncertainty in the assignment of observations to experts
# and will lead to each expert not overfitting to the observations assigned to it.
# This approach can be see as trading in the computational benefits of assigning observations to experts
# (Equation ref:eq-np-moe-marginal-likelihood-assign) in favour
# of directly capturing the correlations between all observations.

For ease of notation and understanding, only a single output dimension has been considered,
although in most scenarios the output dimension will be greater than $1$.
It is trivial to extend this work to multiple output dimensions following multioutput GP
methodologies cite:vanderwilkFramework2020.
# The extension to multiple output dimensions is fairly straight forward.
\todo{Can I just say it's left out because trivial?}

*** Identifiable Mixtures of Sparse Variational Gaussian Process Experts :ignore:
# Motivated by synergising model learning and control, the probabilistic model constructed in this chapter
# Given this motivation, the probabilistic model constructed in this chapter
# is a \acrfull{mogpe} with a \acrshort{gp}-based gating network.

Motivated by improving identifiability and learning latent spaces for control,
this work adopts a GP-based gating network resembling a GP classification model,
similar to that used in the original \acrshort{mogpe} model citep:trespMixtures2000a.
The marginal likelihood is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood-assign}
\evidence &=
\sum_{\allModeVar}
\underbrace{\E_{\gatingPrior} \left[
%\frac{1}{Z}
\prod_{\numData=1}^{\NumData}
\gatingLikelihood
% p(\modeVarn \mid \GatingFunc(\singleInput))
 \right]}_{\text{GP gating network}}
  \underbrace{\prod_{\modeInd=1}^\ModeInd p\left(\{\singleOutput : \singleModeVarK \} \mid \{\singleInput : \singleModeVarK \} \right)}_{\text{experts}}
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-augmented-marginal-likelihood}
# \evidence &\approx
# \E_{\expertsInducingPrior \gatingPrior} \left[
# \prod_{\numData=1}^{\NumData}
# \sum_{\modeInd=1}^{\ModeInd}
#  \singleGatingLikelihood \singleExpertGivenInducing \right],
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-marginal-likelihood}
# \evidence &=
# \sum_{\modeInd=1}^\ModeInd
# \underbrace{\E_{\gatingPrior} \left[ \prod_{\numData=1}^\NumData \singleGatingLikelihood \right]}_{\text{gating network}}
# \underbrace{\E_{\expertPrior} \left[ \prod_{\numData=1}^\NumData \singleExpertLikelihood \right]}_{\text{expert } k}.
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-marginal-likelihood}
# \evidence &=
# \E_{\gatingPrior \expertPrior} \left[
#  \sum_{\modeInd=1}^\ModeInd
#  \prod_{\numData=1}^\NumData
# \underbrace{\singleGatingLikelihood}_{\text{gating network}}
# \underbrace{\singleExpertLikelihood}_{\text{expert } k}
# \right]
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-marginal-likelihood}
# \evidence &=
# \prod_{\numData=1}^\NumData \sum_{\modeInd=1}^\ModeInd
# \underbrace{\E_{\gatingPrior} \left[ \singleGatingLikelihood \right]}_{\text{gating network}}
# \underbrace{\E_{\expertPrior} \left[ \singleExpertLikelihood \right]}_{\text{expert } k}.
# \end{align}
# #+END_EXPORT
where the gating network resembles a Gaussian process
classification model, with a factorised classification likelihood $\gatingLikelihood$ dependent on
input dependent functions
$\GatingFunc = \{\mode{\gatingFunc} : \inputDomain \rightarrow \R \}_{\modeInd=1}^\ModeInd$,
known as gating functions.
The probability mass function over the expert indicator variable is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-prob-mass}
\gatingLikelihood &= \prod_{\modeInd=1}^\ModeInd \left(\singleGatingLikelihood\right)^{[\singleModeVarK]},
%\singleModeVar \mid \GatingFunc(\singleInput) &\sim \text{Categorical}\left(\ModeInd, \text{softmax}(\GatingFunc(\singleInput)) \right)
%= \prod_{\modeInd=1}^\ModeInd \left(\singleGatingLikelihood\right)^{[\singleModeVarK]}
\end{align}
#+END_EXPORT
where $[\singleModeVarK]$ denotes the Iverson bracket.
The probabilities of this Categorical distribution $\singleGatingLikelihood$
are governed by a classification likelihood (Bernoulli/Softmax).
Fig. [[ref:fig-graphical-model-gp-gating-network]] shows the graphical model where the $\ModeInd$ latent gating functions $\GatingFunc$
are evaluated and normalised to obtain the mixing probabilities
$\singleGatingLikelihood$.
The mode indicator variable $\singleModeVar$ is then sampled from the Categorical distribution
governed by these probabilities.
The indicated mode's latent function $\mode{f}$ and process noise $\noiseVarK$ are
then evaluated to generate the output $\singleOutput$.


*Softmax ($\ModeInd>2$)* In the general case, that is, when there are more than two experts,
$\ModeInd > 2$, the gating network's likelihood is defined as the Softmax function,
\marginpar{softmax}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-softmax}
\singleGatingLikelihood = \text{softmax}_{\modeInd}\left(\GatingFunc(\singleInput)\right) = \frac{\text{exp}\left(\mode{\gatingFunc}(\singleInput)\right)}{\sum_{j=1}^{\ModeInd} \text{exp}\left(\gatingFunc_j(\singleInput) \right)}.
\end{align}
#+END_EXPORT
Each gating function $\mode{\gatingFunc}$ describes how its corresponding mode's mixing
probability varies over the input space.
Modelling the gating network with input-dependent functions enables
informative prior knowledge to be encoded through the placement of GP priors on each gating function.
Further to this, if the modes are believed to only vary over a subset of the state-control input space,
then the gating functions can depend only on this subset.
Independent GP priors are placed on each gating function, giving the gating network prior,
\marginpar{GP priors}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-prior}
%\gatingPrior &= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \mode{\gatingFunc}(\allInput) \mid \gatingMeanFunc(\allInput), \gatingCovFunc(\allInput, \allInput) \right) \\
\GatingFunc(\allInput) &\sim \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \gatingMeanFunc(\allInput), \gatingCovFunc(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
where $\gatingMeanFunc(\cdot)$ and $\gatingCovFunc(\cdot,\cdot)$ are the mean and covariance functions
associated with the $\modeInd^\text{th}$ gating function.
Similarly to the experts, dependence on the inputs and hyperparameters is dropped from the gating network's GP prior,
i.e. $\gatingPrior = p(\GatingFunc(\allInput) \mid \allInput, \gatingParams)$.
In contrast to the experts, partitioning of the data set is not desirable for the gating network GPs,
as each gating function should depend on all of the training observations.

# Each mode's mixing probability $\singleGatingPosterior$ is then obtained by marginalising
# *all* of the gating functions,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-gating-posterior}
# \singleGatingPosterior
# &= \int \gatingPrior \singleGatingLikelihood \text{d} \GatingFunc(\allInput).
# %&= \E_{\gatingPrior}\left[ \singleGatingLikelihood \right].
# \end{align}
# #+END_EXPORT
# In the general case where $\singleGatingLikelihood$ uses the softmax function
# (cref:eq-softmax) this integral is intractable, so it is approximated with Monte carlo quadrature.
Each mode's mixing probability $\singleGatingPosterior$ is then obtained by marginalising
*all* of the gating functions.
In the general case where $\singleGatingLikelihood$ uses the softmax function
(cref:eq-softmax) this integral is intractable, so it is approximated with Monte carlo quadrature.

*Bernoulli ($\ModeInd=2$)* Instantiating the model with two experts, $\singleModeVar \in \{1, 2\}$, is a special case
where only a single gating function is needed.
\marginpar{two experts}
This is because the output of a function $\gatingFunc(\singleInput)$ can be mapped through a sigmoid
function $\text{sig} : \R \rightarrow [0, 1]$ and interpreted as a probability,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sigmoid}
\Pr(\singleModeVar=1 \mid \modei{\gatingFunc}{1}(\singleInput)) = \text{sig}(\modei{\gatingFunc}{1}(\singleInput)).
\end{align}
#+END_EXPORT
If this sigmoid function satisfies the point symmetry condition then
the following holds,
$\Pr(\singleModeVar=2 \mid \modei{\gatingFunc}{1}(\singleInput)) = 1 - \Pr(\singleModeVar=1 \mid \modei{\gatingFunc}{1}(\singleInput))$.
This only requires a single gating function and no normalisation term needs to be calculated.
If the sigmoid function in cref:eq-sigmoid is selected
to be the Gaussian cumulative distribution function
$\Phi(\gatingFunc(\cdot)) = \int^{\gatingFunc(\cdot)}_{-\infty} \mathcal{N}(\tau | 0, 1) \text{d} \tau$,
then the mixing probability can be calculated in closed-form,
# then the integral in cref:eq-gating-posterior can be calculated in closed-form,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixing-probabilities-analytic}
\Pr(\modeVarn=1 \mid \singleInput, \gatingParams) &=
 \int \Phi\left(\gatingFunc(\singleInput)\right) \mathcal{N}\left(\gatingFunc(\singleInput) \mid \mu_h, \sigma^2_h \right) \text{d} \gatingFunc(\singleInput) \nonumber \\
&= \Phi \left(\frac{\mu_{h}}{\sqrt{1 + \sigma^2_{h} }}\right),
\end{align}
#+END_EXPORT
where $\mu_h$ and $\sigma^2_{h}$ represent the mean and variance of the gating GP at $\singleInput$ respectively.


# This model makes single-step probabilistic predictions,
# where the predictive distribution over the output $\singleOutput$ is
# given by a mixture of Gaussians.
# This provides the model flexibility to model multimodal transition dynamics $f$ as mixtures of $\modeInd$ experts.
# In the two expert case the marginal likelihood is a mixture of two Gaussians and can be calculated in
# closed-form.
# \todo{can it be calculated in closed form?}
# It has complexity $\mathcal{O}(\ModeInd\NumData^4)$ to evaluate
# and the general case (with more than two experts) has complexity $\mathcal{O}(\ModeInd^2\NumData^4)$.

*** graphical model :ignore:noexport:

#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};

      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\singleInput)$};

      \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\expertInducingOutput$};
      \node[const, left=of uk, xshift=0.4cm] (zk) {$\expertInducingInput$};


      \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\expertParamsK$};
%      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[const, below=of f] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      \node[latent, right=of a, yshift=1.4cm, xshift=-0.4cm] (h) {$\mode{\gatingFunc}(\allInput)$};
      \node[const, right=of h, xshift=-0.4cm] (phik) {$\gatingParamsK$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \draw[post] (uk)--(f);
      \draw[post] (zk)--(uk);
      \draw[post] (thetak)--(f);

      \plate {} {(x) (y) (a) (f)} {$\NumData$};
      \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    %}
  \caption{Graphical model where the output $\singleOutput$
is generated by mapping the input $\singleInput$ through the latent process. The experts are shown on the
left and the gating network is shown on the right.
The geneartive process involves evaluating the $\ModeInd$ latent gating functions $\GatingFunc$
and normalising their output to obtain the mixing probabilities $\singleGatingLikelihood$.
The mode indicator variable $\singleModeVar$ is then sampled from the Categorical distribution
governed by these probabilities.
The indicated mode's latent function $\mode{f}$ and noise model $\mathcal{N}(\mathbf{0}, \noiseVarK)$ are
then evaluated to generate the output $\singleOutput$.}
\label{fig-graphical-model}
\end{figure}
#+END_EXPORT

*** graphical model non sparse experts :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\allInput)$};
     % \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\hk$};


      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[const, below=of f] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      \node[latent, right=of a, yshift=1.4cm, xshift=-0.4cm] (h) {$\mode{\gatingFunc}(\allInput)$};
      \node[const, right=of h, xshift=-0.4cm] (phik) {$\gatingParamsK$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    %}
  \caption{Graphical model of the transition dynamics where the state difference $\singleOutput$
is generated by pushing the state and control $\singleInput$ through the latent process.}
\label{fig-graphical-model}
\end{figure}
#+END_EXPORT


# #+BEGIN_EXPORT latex
# \begin{figure}[t]
#   \centering
#   \resizebox{0.8\columnwidth}{!}{
#     \begin{tikzpicture}[
#       pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
#       post/.style={->,shorten >=0.4pt,>=stealth',semithick}
#       ]
#       \node[const] (x) {$\hat{\mathbf{x}}_{t-1}$};
#       \node[latent, left=of x, yshift=-1.4cm] (f) {$\mathbf{f}^{(k)}_t$};
#       %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
#       \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {${h}^{(k)}_t$};

#       \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\hat{\mathbf{f}}^{(k)}$};
#       \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\hat{\mathbf{h}}^{(k)}$};
#       \node[const, left=of uk, xshift=0.4cm] (zk) {$\bm\xi^{(k)}_f$};
#       \node[const, right=of uh, xshift=-0.4cm] (zh) {$\bm\xi^{(k)}_h$};

#       \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\bm\theta^{(k)}$};
#       \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\bm\phi^{(k)}$};

#       \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\sigma^{(k)}$};

#       \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\Delta\mathbf{x}_t$};
#       %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
#       \node[latent, right=of y, xshift=-0.4cm] (a) {${\alpha}_t$};

#       %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

#       \factor[above=of a] {h-a} {left:Cat} {h} {a};

#       \draw[post] (a)--(y);
#       \draw[post] (x)-|(f);
#       %\draw[post] (f)--(yk);
#       \draw[post] (f)--(y);
#       %\draw[post] (yk)--(y);
#       %\draw[post] (h)--(a);
#       \draw[post] (x)-|(h);
#       \draw[post] (uk)--(f);
#       \draw[post] (uh)--(h);
#       \draw[post] (zk)--(uk);
#       \draw[post] (zh)--(uh);
#       \draw[post] (thetak)--(f);
#       \draw[post] (phik)--(h);
#       \draw[post] (sigmak)|-(y);

#       \plate {} {(x) (y) (a) (f) (h)} {$T$};
#       %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
#       \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$K$};
#       \plate {} {(zh) (uh) (h) (phik)} {$K$};
#     \end{tikzpicture}
#     }
#   \caption{Graphical model of the transition dynamics
# where the state difference $\Delta\mathbf{x}_{t}$
# is generated by pushing the state and control $\hat{\mathbf{x}}_{t-1}$
# through the latent process.}
# %\label{fig:graphical_model}
# \end{figure}
# #+END_EXPORT

*** Generative Model :noexport:
# The marginal likelihood of this model can be written to clearly show the factorised likelihood (mixture of Gaussians)
# and the expectation over the latent variables,
With this formulation, the marginal likelihood of this model can be written to clearly show the
expectations over the latent variables,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood}
\evidence &=
\sum_{\modeInd=1}^\ModeInd
\underbrace{\E_{\gatingPrior} \left[ \prod_{\numData=1}^\NumData \singleGatingLikelihood \right]}_{\text{gating network}}
\underbrace{\E_{\expertPrior} \left[ \prod_{\numData=1}^\NumData \singleExpertLikelihood \right]}_{\text{expert } k}.
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood-factorised}
\evidence &=
\sum_{\modeInd=1}^\ModeInd
\E_{\gatingPrior \expertsPrior} \left[
\prod_{\numData=1}^\NumData
\singleGatingLikelihood \singleExpertLikelihood \right],
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-marginal-likelihood-factorised}
# \evidence &=
# \E_{\gatingPrior \expertsPrior} \left[
# \sum_{\modeInd=1}^\ModeInd
# \prod_{\numData=1}^\NumData
# \singleGatingLikelihood \singleExpertLikelihood \right],
# \end{align}
# #+END_EXPORT
# where $\expertsPrior = \prod_{\modeInd=1}^\ModeInd \expertPrior$ is the experts' prior and
where $\expertPrior$ is the $\modeInd^{\text{th}}$ expert's prior and
$\gatingPrior$ is the gating network's prior.
Observe that  the  GP  priors  have  removed  the  factorisation  over  data
which  is  present  in  the  ME  marginal  likelihood  cref:eq-mixture-marginal-likelihood.
Fig. [[ref:fig-graphical-model]]
shows the graphical model where the $\ModeInd$ latent gating functions $\GatingFunc$
are evaluated and normalised to obtain the mixing probabilities
$\singleGatingLikelihood$.
The mode indicator variable $\singleModeVar$ is then sampled from the Categorical distribution
governed by these probabilities.
The indicated mode's latent function $\mode{f}$ and process noise $\noiseVarK$ are
then evaluated to generate the state difference $\singleOutput$.

This model makes single-step probabilistic predictions,
where the predictive distribution over the output $\singleOutput$ is
given by a mixture of Gaussians.
This provides the model flexibility to model multimodal transition dynamics $f$ as mixtures of $\modeInd$ experts.
In the two expert case the marginal likelihood is a mixture of two Gaussians and can be calculated in
closed-form.
\todo{can it be calculated in closed form?}
It has complexity $\mathcal{O}(\ModeInd\NumData^4)$ to evaluate
and the general case (with more than two experts) has complexity $\mathcal{O}(\ModeInd^2\NumData^4)$.

For ease of notation and understanding, only a single output dimension has been considred,
although in most scenarios the state dimension will be greater than $1$.
This work considers independent output dimensions which follow trivially from multioutput GP
methodologies.
# The extension to multiple output dimensions is fairly straight forward.
\todo{Can I just say it's left out because trivial?}
*** Gating Network :noexport:
The gating network governs how the dynamics switch between modes.
This work is interested in spatially varying modes so
formulates an input dependent Categorical distribution over $\alpha_t$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-prob-mass}
P\left(\alpha_t \mid \mathbf{h}_t\right) = \prod_{k=1}^K \left(\Pr(\alpha_t=k \mid \mathbf{h}_t)\right)^{[\alpha_t = k]},
\end{align}
#+END_EXPORT
where $[\alpha_t=k]$ denotes the Iverson bracket.
It should be noted that other gating networks have been proposed that provide computational benefits
for inference. However, these gating networks often lack a handle for encoding domain knowledge
that can be used to restrict the set of possible

\todo[inline]{insert gating network references}
The probabilities of this Categorical distribution $\Pr(\alpha_t=k \mid \mathbf{h}_t)$
are obtained by evaluating $K$ latent
gating functions $\{h^{(k)}\}_{k=1}^K$ and normalising their output.
Each gating function evaluated at $\hat{\mathbf{x}}_{t-1}$ is denoted
as $h^{(k)}_t = h^{(k)}(\hat{\mathbf{x}}_{t-1})$
and at all observations ${h}^{(k)}_{1:T}$.
The set of all gating functions evaluated at $\hat{\mathbf{x}}_{t-1}$ is denoted
as $\mathbf{h}_t$ and at all observations as $\mathbf{h}_{1:T}$.
Each gating function $h^{(k)}$ describes how its corresponding mode's mixing
probability varies over the input space.


This work is interested in finding trajectories that can avoid areas of the transition dynamics model
that cannot be predicted confidently.
Placing independent sparse GP priors on each gating function
provides a principled approach to
modelling the epistemic uncertainty associated with each gating function.
The gating function's posterior covariance is a quantitative value that can be exploited by the
trajectory optimisation.
# GPs also provide data-efficient and interpretable learning through the selection of informative mean
# and covariance functions.
# For example, if the transition dynamics modes are subject to oscillatory mixing then a periodic
# covariance function could be selected.

Each gating function's inducing inputs are denoted
$\bm\xi_h^{(k)}$ and outputs as $\hat{\mathbf{h}}^{(k)}$.
For all gating functions, they are collected as $\bm\xi_h$ and $\hat{\mathbf{h}}$ respectively.
The probability that the $t^{\text{th}}$ observation is generated by mode $k$
given the inducing inputs is obtained by marginalising $\mathbf{h}_t$,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha=\modeInd \mid \mathbf{h}(\cdot) )
&= \E_{p(\mathbf{h}(\cdot))} \left[ \Pr(\alpha_t=k \mid \mathbf{h}(\cdot)) \right],
\end{align*}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha=\modeInd \mid \mathbf{h}(\cdot) )
&= \E_{p(\mathbf{h}(\cdot))} \left[ \text{softmax}_k(\mathbf{h}(\cdot)) \right],
\end{align*}
#+END_EXPORT
#+BEGIN_EXPORT latex
% \small
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha_t=k \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}} )
&= \int \text{softmax}_k(\mathbf{h}_t) p(\mathbf{h}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}) \text{d} \mathbf{h}_t,
\end{align*}
#+END_EXPORT
where $p(\mathbf{h}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}) = \prod_{k=1}^K p\left({h}^{(k)}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}^{(k)}\right)$
is the $K$ independent sparse GP priors on the gating functions.

**** Two Experts
Instantiating the model with two experts is a special case where only a single gating function is needed.
The output of a function can be mapped through a sigmoid
function $\text{sig} : \R \rightarrow [0, 1]$ and interpreted as
a probability $\Pr(\alpha=1 \mid \gatingFunc(\cdot))$.
If this sigmoid function satisfies the point symmetry condition then
we know that $\Pr(\alpha=2 \mid \gatingFunc(\cdot)) = 1 - \Pr(\alpha =1 \mid \gatingFunc(\cdot))$.

The distribution over $\gatingFunc(\cdot)$

We note that $p({h}_n^{(k)} \mid \mathbf{h}_{\neg n}, \mathbf{X})$
is a GP conditional and denote its mean $\mu_h$ and variance $\sigma^2_h$.
Choosing the sigmoid as the Gaussian cdf
$\Phi(h_n) = \int^{h_n}_{-\infty} \mathcal{N}(\tau | 0, 1) \text{d} \tau$
leads to,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixing-probabilities-analytic}
\Pr(\alpha_n=1 | \mathbf{X}) &=
 \int \Phi({h}_n) \mathcal{N}(h_n \mid \mu_h, \sigma^2_h) \text{d} {h}_n
= \Phi \left(\frac{\mu_{h}}{\sqrt{1 + \sigma^2_{h} }}\right).
\end{align}
#+END_EXPORT
Given this gating network our marginal likelihood is now an analytic mixture of two Gaussians.

can analytically marginalise.

** Approximate Inference [[label:sec-inference]]
#+begin_export latex
\epigraph{Nature laughs at the difficulties of integration.}{\textit{Pierre-Simon Laplace}}
#+end_export
*** intro :ignore:
Performing Bayesian inference involves finding the posterior over the latent variables,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\GatingFunc(\allInput), \LatentFunc(\allInput) \mid \allInput, \allOutput, \gatingParams, \expertParams)
&= \frac{\sum_{\allModeVar} \allGatingLikelihood \gatingPrior
\prod_{\modeInd=1}^{\ModeInd} \allExpertLikelihood \expertPrior}{\evidence}
\end{align}
#+END_EXPORT
where the denominator is the marginal likelihood from cref:eq-marginal-likelihood-assign.
Exact inference in our model is intractable, so we resort to a variational approximation.
The rich structure of our model makes it hard to construct an ELBO that can
be evaluated in closed-form, whilst accurately modelling the complex dependencies.
Further to this, the marginal likelihood is extremely expensive to evaluate,
as there are $\ModeInd^{\NumData}$ sets of assignments $\allModeVar$ that need to be marginalised.
For each set of assignments, there are $\ModeInd$ GP experts that need to be evaluated, each with
complexity $\mathcal{O}(\NumData^{3})$.
\todo{add marginal likelihood's correct complexity}
For these reasons, this work derives a variational approximation based on inducing variables, that provides scalability
by utilising stochastic gradient-based optimisation.
# The resulting complexity is  $\mathcal{O}(\ModeInd^{\NumData} \NumData_{\modeInd}^3)$,
# where $\NumData_{\modeInd}$ represents the largest number of data points assigned to an expert for particular
# set of assignments.
# and is also intractable due to the marginalisation of $\mathbf{h}$ in the gating network.

\acrfull{svi} citep:hoffmanStochastic2013 relies upon having a set of local variables
factorised across observations and a set of global variables.
The marginalisation over the set of expert indicator variables $\allModeVar$
in cref:eq-np-moe-marginal-likelihood is prohibitive to \acrshort{svi}.
Following the approach by cite:titsiasVariational2009, the probability space is augmented
with a set of $\NumInducing$ inducing variables for each GP.
However, instead of collapsing these inducing variables they are explicitly
represented as variational distributions and used to lower bound
(and then further bound) the marginal likelihood, similar to
cite:hensmanGaussian2013,hensmanScalable2015.
cref:fig-graphical-model-sparse shows the graphical model of the augmented joint probability space.

# The marginal likelihood in cref:eq-marginal-likelihood-factorised is extremely expensive
# to evaluate $\mathcal{O}(\ModeInd^2 \NumData^{4})$
# \todo{add marginal likelihood's correct complexity}
# and is also intractable due to the marginalisation of $\mathbf{h}$ in the gating network
# (except for the two expert case).
# For these reasons, this work derives a variational approximation based on inducing variables that provides scalability
# by utilising stochastic gradient-based optimisation.


# *Inducing Variables* As this approach essentially parameterises a nonparametric model,
# it is interesting to pause here and consider the implications of defining inducing inputs in different ways.
# For example, what are the implications of having shared or separate inducing inputs
# for the gating network GPs, for the expert GPs and for combinations of the experts and gating functions?

# 1. *Separate inducing inputs* for each *expert* GP, i.e. $\expertInducingOutput=\mode{\latentFunc}(\expertInducingInput)$,
#    - Introducing separate inducing inputs for each expert's GP can loosely be seen as "partitioning"
#       the observations between experts. This requires less inducing points for each expert and achieves data partitioning behaviour like other MoGPE methods.
# - Introducing separate inducing inputs for each expert's GP can loosely be seen as "partitioning"
#   the observations between experts,
# - Less inducing inputs needed for each expert,
# - Achieves data partitioning behaviour like other MoGPE methods.
# 3. *Shared inducing inputs* for the *gating network* GPs, i.e. $\gatingInducingOutput=\mode{\gatingFunc}(\gatingInducingInput)$,
#    - Partitioning of the data set is not desirable for the gating function GPs as each gating function
#       should depend on all of the training observations. For this reason the inducing inputs should be shared between each gating function GP.
# - Partitioning of the data set is not desirable for the gating function GPs as each gating function
#   should depend on all of the training observations,
# - For this reason the inducing inputs should be shared between each gating function GP.

*** sparse graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\singleInput)$};
      %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
      \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\mode{\gatingFunc}(\singleInput)$};

      \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\expertInducingOutput$};
      \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\gatingInducingOutput$};
      \node[const, left=of uk, xshift=0.4cm] (zk) {$\expertInducingInput$};
      \node[const, right=of uh, xshift=-0.4cm] (zh) {$\gatingInducingInput$};

      \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\expertParamsK$};
      \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\gatingParamsK$};

      \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (uk)--(f);
      \draw[post] (uh)--(h);
      \draw[post] (zk)--(uk);
      \draw[post] (zh)--(uh);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a) (f) (h)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(uh) (h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    }
  \caption{Graphical model of the augmented probability space where the joint distribution over the data is captured by the inducing variables $\expertInducingOutput=\mode{\latentFunc}(\expertInducingInput)$ and $\gatingInducingOutput=\mode{\gatingFunc}(\gatingInducingInput)$.
  The observations assigned to expert $\modeInd$ are modelled by the
  inducing points $\{\expertInducingInput, \expertInducingOutput\}_{\modeInd=1}^{\ModeInd}$.
  This model avoids the hard assignment of observations to experts by letting the gating network
  softly assign them in the ELBO.}
\label{fig-graphical-model-sparse}
\end{figure}
#+END_EXPORT

*** Augmented Probability Space :ignore:

*Augmented experts*
This work sidesteps the hard assignment of observations to experts by augmenting each expert with a set
of separate independent inducing points,
$(\expertInducingInput, \expertInducingOutput)$.
Each expert's inducing points are assumed to be from its GP prior,
# Each set of inducing points are assumed to be from the GP prior associated with the expert,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-inducing-prior}
\expertsInducingPrior
&= \prod_{\modeInd=1}^\ModeInd \expertInducingPrior
= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \expertInducingOutput \mid
\expertMeanFunc(\expertInducingInput),
\expertCovFunc(\expertInducingInput, \expertInducingInput) \right),
\end{align}
#+END_EXPORT
where the set of all inducing inputs associated with the experts has been denoted $\expertsInducingInput$
and the  set of all inducing variables as $\expertsInducingOutput$.
Note that the dependence on the inducing inputs has dropped for notational conciseness.
Introducing separate inducing points from each expert's GP can loosely be seen as "partitioning"
the observations between experts.
However, as the assignment of observations to experts is /not known a priori/, the inducing inputs
$\expertInducingInput$ and variables $\expertInducingOutput$, must be inferred from observations.
# , i.e. the inducing points can be seen as approximating the data partition if
# $\expertInducingOutput \approx \mode{\latentFunc}(\allInputK)$ and $\expertInducingInput \approx \allInputK$.

*Augmented gating network* Following a similar approach for the gating network, each gating function is augmented with a
set of $\NumInducing$ inducing points from its corresponding GP prior,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-inducing-prior}
\gatingsInducingPrior
&= \prod_{\modeInd=1}^\ModeInd \gatingInducingPrior
= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \gatingInducingOutput \mid
\gatingMeanFunc(\gatingInducingInput),
\gatingCovFunc(\gatingInducingInput, \gatingInducingInput) \right),
\end{align}
#+END_EXPORT
where  $\gatingInducingOutput = \mode{\gatingFunc}(\gatingInducingInput)$
are the inducing points associated with the $\modeInd^{\text{th}}$ gating function.
Again, the dependence on the inducing inputs has been dropped for notational conciseness.
The set of all inducing variables associated with the gating network has been
denoted $\gatingsInducingOutput$.
Partitioning of the data set is not desirable for the gating function GPs as each gating function
should depend on all of the training observations.
For this reason the gating functions share the same inducing inputs $\gatingInducingInput$.

*Marginal likelihood* These inducing points are used to approximate the marginal likelihood with a factorisation over observations
that is favourable for constructing a GP-based gating network.
Our approximate marginal likelihood is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-augmented-marginal-likelihood}
\evidence &\approx
\E_{\gatingsInducingPrior \expertsInducingPrior} \left[
\prod_{\numData=1}^{\NumData}
\sum_{\modeInd=1}^{\ModeInd}
 \singleGatingGivenInducing \singleExpertGivenInducing \right],
\end{align}
#+END_EXPORT
where the conditional distributions $\singleExpertGivenInducing$ and $\singleGatingGivenInducing$
follow from standard sparse GP methodologies,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\singleExpertGivenInducing &= \E_{\singleLatentExpertGivenInducing} \left[ \singleExpertLikelihood \right] \\
\singleGatingGivenInducing &= \E_{\singleLatentGatingsGivenInducing} \left[ \singleGatingLikelihood \right] \\
\singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
\expertKernelnM \expertKernelMM^{-1} \expertInducingOutput,
\expertKernelnn - \expertKernelnM \expertKernelMM^{-1} \expertKernelMn \right), \\
\singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mode{\gatingFunc}(\singleInput) \mid
\gatingKernelnM \gatingKernelMM^{-1} \gatingInducingOutput,
\gatingKernelnn - \gatingKernelnM \gatingKernelMM^{-1} \gatingKernelMn \right),
\end{align}
#+END_EXPORT
where
$\expertKernelMM = k_{\modeInd} (\expertInducingInput,\expertInducingInput)$
represents the $\modeInd^{\text{th}}$ expert's kernel evaluated between its inducing inputs,
$\expertKernelnn = k_k (\singleInput, \singleInput)$
represents it evaluated between the $\numData^{\text{th}}$ training input and
$\expertKernelnM = k_k (\singleInput, \expertInducingInput)$
between the $\numData^{\text{th}}$ training input and its inducing inputs.
Similarly for the gating network.
Figure ref:fig-graphical-model-sparse shows the graphical model of this augmented joint probability model.

Our work follows from sparse GP methodologies, that assume,
given the inducing variables, the latent function values factorise over observations.
Our approximation assumes that given the inducing points,
the marginalisation over every possible assignment of data points to experts, can be factorised over data.
In a similar spirit to the FITC approximation citep:naish-guzmanGeneralized2008,quinonero-candelaUnifying2005,
this can be viewed as a likelihood approximation,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-likelihood-approximation}
p(\allOutput \mid \expertsInducingOutput)
&\approx \prod_{\numData=1}^{\NumData} p(\singleOutput \mid \expertsInducingOutput)
= \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \moeGatingPosterior
\prod_{\modeInd=1}^{\ModeInd} \singleExpertGivenInducing.
\end{align}
#+END_EXPORT
Importantly, the factorisation over observations has been moved
outside of the marginalisation over the expert indicator variable, i.e.
the expert indicator variable can be marginalised for each data point separately.
This approximation assumes that the inducing variables,
$\{\expertInducingOutput\}_{\modeInd=1}^\ModeInd$, are
a sufficient statistic for their associated latent function values,
$\{\mode{\latentFunc}(\allInputK) \}_{\modeInd=1}^\ModeInd$
and the set of assignments $\allModeVar$.
This approximation becomes exact in the limit $\ModeInd\NumInducing=\NumData$,
if each expert's inducing points represent the true data partition
$(\expertInducingInput, \expertInducingOutput) = (\allInputK, \mode{\latentFunc}(\allInputK))$.
It is also worth noting that cref:eq-augmented-marginal-likelihood captures a rich approximation of each
expert's covariance but as $\ModeInd\NumInducing \ll \NumData$ the computational complexity is
much lower.
This approximation efficiently couples the gating network and the experts by marginalising the expert
indicator variable for each data point separately.

Our approximate marginal likelihood captures
the joint distribution over the data and assignments through the inducing variables
$\expertsInducingOutput$.
As such, information regarding the assignment of observations to experts must pass through the bottleneck of the
inducing variables.
This approximation induces a local factorisation over observations and a set of global variables
-- the necessary conditions for \acrshort{svi}.
cref:fig-graphical-model-sparse shows the graphical model of this augmented joint probability model.
This approach can loosely be viewed as parameterising the full nonparametric model
in cref:eq-np-moe-marginal-likelihood-assign to obtain a desirable
factorisation for 1) constructing a GP-based gating network and 2) deriving an ELBO that can
be optimised with stochastic gradient methods,
whilst still capturing the complex dependencies between the gating network and experts.

*** Augmented probability space :ignore:noexport:

\acrfull{svi} citep:hoffmanStochastic2013 relies upon having a set of local variables
factorised across observations and a set of global variables.
Annoyingly, the order of the marginalisation over the expert indicator variable
and the product over observations $\NumData$
in cref:eq-marginal-likelihood is prohibitive to \acrshort{svi}.


# which is detailed in Section ref:sec-sparse-gps Eqs. ref:eq-titsias-bound - ref:eq-hensman-bound,
*Augmented Probability Space* Following the approach by cite:titsiasVariational2009 and cite:hensmanGaussian2013,
the probability space is first augmented with a set of $\NumInducing$ inducing variables from each GP prior,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-inducing-prior}
\expertsInducingPrior
&= \prod_{\modeInd=1}^\ModeInd \expertInducingPrior
= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \expertInducingOutput) \mid
\expertMeanFunc(\expertInducingInput),
\expertCovFunc(\expertInducingInput, \expertInducingInput) \right) \\
\gatingsInducingPrior
&= \prod_{\modeInd=1}^\ModeInd \gatingInducingPrior
= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \gatingInducingOutput) \mid
\gatingMeanFunc(\gatingInducingInput),
\gatingCovFunc(\gatingInducingInput, \gatingInducingInput) \right).
\end{align}
#+END_EXPORT
where  $\gatingInducingOutput = \mode{\gatingFunc}(\gatingInducingInput)$
are the inducing points associated with the $\modeInd^{\text{th}}$ gating function.
Partitioning of the data set is not desirable for the gating function GPs as each gating function
should depend on all of the training observations.
For this reason the gating functions share the same inducing inputs $gatingInducingInput$.
The set of all inducing variables associated with the gating network has been
denoted $\gatingsInducingOutput$.

Our approximate marginal likelihood is then given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-augmented-marginal-likelihood}
\evidence &\approx
\E_{\gatingsInducingPrior \expertsInducingPrior} \left[
\prod_{\numData=1}^{\NumData}
\sum_{\modeInd=1}^{\ModeInd}
 \singleGatingGivenInducing \singleExpertGivenInducing \right],
\end{align}
#+END_EXPORT
where the joint distribution over the data is captured by the inducing variables
$(\gatingsInducingOutput, \expertsInducingOutput)$.
As such, information regarding the assignment of observations to experts must pass through the bottleneck of the
inducing variables.
This approximation induces a local factorisation over observations and a set of global variables -- the inducing
variables -- the necessary conditions for \acrshort{svi}.
Figure ref:fig-graphical-model-sparse shows the graphical model of this augmented joint probability model.


The augmented marginal likelihood is then given by,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-augmented-marginal-likelihood}
# \evidence &=
# \E_{\gatingsInducingPrior \expertsInducingPrior} \left[
# \sum_{\modeInd=1}^{\ModeInd} \allGatingGivenInducing \allExpertGivenInducing
# \right],
# \end{align}
# #+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-augmented-marginal-likelihood-not-factorised}
\evidence &=
\E_{\gatingsInducingPrior} \left[
\sum_{\modeInd=1}^{\ModeInd}
 \allGatingsGivenInducing
\E_{\expertInducingPrior} \left[ \allExpertGivenInducing \right]
\right],
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-augmented-marginal-likelihood}
# \evidence &\approx
# \sum_{\modeInd=1}^{\ModeInd}
# \E_{\gatingsInducingPrior} \left[ \prod_{\numData=1}^\NumData \singleGatingGivenInducing \right]
# \E_{\expertsInducingPrior} \left[ \prod_{\numData=1}^\NumData \singleExpertGivenInducing \right]
# \right],
# \end{align}
# #+END_EXPORT
where the conditional distributions $\allExpertGivenInducing$ and $\allGatingGivenInducing$
follow from standard sparse GP methodologies,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\allExpertGivenInducing &= \E_{\allLatentExpertGivenInducing} \left[ \allExpertLikelihood \right] \\
\allGatingGivenInducing &= \E_{\allLatentGatingsGivenInducing} \left[ \allGatingLikelihood \right] \\
%\allLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\allInput) \mid
%\mode{\mathbf{A}} \expertInducingOutput,
%\expertKernel(\allInput, \allInput) -
%\mode{\mathbf{A}}
%\expertKernel(\expertInducingInput, \expertInducingInput)
%\mode{\mathbf{A}}^T \right), \\
%\allLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mathbf{\gatingFunc}(\allInput) \mid
%\mode{\hat{\mathbf{A}}} \gatingInducingOutput,
%\gatingKernel(\allInput, \allInput) -
%\mode{\hat{\mathbf{A}}}
%\gatingKernel(\gatingInducingInput, \gatingInducingInput)
%\mode{\hat{\mathbf{A}}}^T \right), \\
\allLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\allInput) \mid
\expertKernelNM \expertKernelMM^{-1} \expertInducingOutput,
\expertKernelNN - \expertKernelNM \expertKernelMM^{-1} \expertKernelMN \right), \\
\allLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd}
\mathcal{N}\left( \mode{\gatingFunc}(\allInput) \mid
\gatingKernelNM \gatingKernelMM^{-1} \gatingInducingOutput,
\gatingKernelNN - \gatingKernelNM \gatingKernelMM^{-1} \gatingKernelMN \right),
\end{align}
#+END_EXPORT
where
$\expertKernelMM = k_{\modeInd} (\expertInducingInput,\expertInducingInput)$
represents the $\modeInd^{\text{th}}$ experts kernel evaluated between its inducing inputs,
$\expertKernelNN = k_k (\allInput, \allInput)$
represents it evaluated between the training inputs and
$\expertKernelNM = k_k (\allInput, \expertInducingInput)$
between the training inputs and its inducing inputs.
Similarly for the gating network.

A central assumption of our work follows from sparse GP methodologies, that assume,
given the inducing variables, the latent function values factorise over observations.
In a similar spirit to the FITC approximation cite:naish-guzmanGeneralized2008,quinonero-candelaUnifying2005,
we propose the following likelihood approximation,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-likelihood-approximation}
p(\allOutput \mid \gatingsInducingOutput, \expertsInducingOutput)
\approx \prod_{\numData=1}^{\NumData} p(\singleOutput \mid \gatingsInducingOutput, \expertsInducingOutput)
= \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \singleGatingGivenInducing \singleExpertGivenInducing.
\end{align}
#+END_EXPORT
# This approximation factorises over the observed outputs given the inducing points.
Importantly, this approximation moves the factorisation over observations
outside of the marginalisation over the expert indicator variable.
It captures a rich approximation of each expert's covariance whilst marginalising the expert
indicator variable.
This approximation assumes that the inducing variables,
$\{\gatingInducingOutput, \expertInducingOutput\}_{\modeInd=1}^\ModeInd$, are
a sufficient statistic for their associated latent function values,
$\{\mode{\latentFunc}(\allInput), \mode{\gatingFunc}(\allInput) \}_{\modeInd=1}^\ModeInd$.
When all of the expert's inducing inputs $\expertInducingInput$
and the gating network's inducing inputs
$\gatingInducingInput$ are equal to the training inputs
$\expertInducingInput = \gatingInducingInput=\allInput$, this approximation is exact.
This approximation becomes exact in the limit $M=N$.
Our approximate marginal likelihood is then given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-augmented-marginal-likelihood}
\evidence &\approx
\E_{\gatingsInducingPrior \expertsInducingPrior} \left[
\prod_{\numData=1}^{\NumData}
\sum_{\modeInd=1}^{\ModeInd}
 \singleGatingGivenInducing \singleExpertGivenInducing \right],
\end{align}
#+END_EXPORT
where the joint distribution over the data is captured by the inducing variables
$(\gatingsInducingOutput, \expertsInducingOutput)$.
As such, information regarding the assignment of observations to experts must pass through the bottleneck of the
inducing variables.
This approximation induces a local factorisation over observations and a set of global variables -- the inducing
variables -- the necessary conditions for \acrshort{svi}.
Figure ref:fig-graphical-model-sparse shows the graphical model of this augmented joint probability model.


# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \allExpertGivenInducing &= \prod_{\numData=1}^{\NumData} \singleExpertGivenInducing
# = \prod_{\numData=1}^{\NumData} \E_{\singleLatentExpertGivenInducing} \left[ \singleExpertLikelihood \right] \\
# \allGatingsGivenInducing &= \prod_{\numData=1}^{\NumData} \singleGatingGivenInducing
# = \prod_{\numData=1}^{\NumData} \E_{\singleLatentGatingsGivenInducing} \left[ \singleGatingLikelihood \right] \\
# %\singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
# %\mode{\mathbf{A}} \expertInducingOutput,
# %\expertKernel(\singleInput, \singleInput) -
# %\mode{\mathbf{A}} \expertKernel(\expertInducingInput, \expertInducingInput) \mode{\mathbf{A}}^T \right), \\
# %\singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mathbf{\gatingFunc}(\singleInput) \mid
# %\mode{\hat{\mathbf{A}}} \gatingInducingOutput,
# %\gatingKernel(\singleInput, \singleInput) -
# %\mode{\hat{\mathbf{A}}} \gatingKernel(\gatingInducingInput, \gatingInducingInput) \mode{\hat{\mathbf{A}}}^T \right), \\
# %\singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
# %\expertA \expertInducingOutput,
# %\expertKernelnn - \expertA \expertKernelMM \expertA^T \right), \\
# %\singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mathbf{\gatingFunc}(\singleInput) \mid
# %\gatingA \gatingInducingOutput,
# %\gatingKernelnn - \gatingA \gatingKernelMM \gatingA^T \right), \\
# \singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
# \expertKernelnM \expertKernelMM^{-1} \expertInducingOutput,
# \expertKernelnn - \expertKernelnM \expertKernelMM^{-1} \expertKernelMn \right), \\
# \singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mode{\gatingFunc}(\singleInput) \mid
# \gatingKernelnM \gatingKernelMM^{-1} \gatingInducingOutput,
# \gatingKernelnn - \gatingKernelnM \gatingKernelMM^{-1} \gatingKernelMn \right),
# \end{align}
# #+END_EXPORT
# where,
# $\mathbf{K}_{\modeInd \numInducing \numInducing} = k_{\modeInd} (
# ${k}_{\modeInd \numData \numData} = k_k (\singleInput, \singleInput)
# $\mathbf{k}_{\modeInd \numData \numInducing} = k_k (\singleInput, \eInput)
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \mode{\mathbf{A}} &= \expertKernelnM \expertKernelMM^{-1} \\
# \mode{\mathbf{A}} &= \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1} \\
# \mode{\hat{\mathbf{A}}} &= \gatingKernel(\singleInput, \gatingInducingInput)\left(\gatingKernel(\gatingInducingInput, \gatingInducingInput)\right)^{-1}.
# \end{align}
# #+END_EXPORT


# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \singleExpertGivenInducing &= \E_{\singleLatentExpertGivenInducing} \left[ \singleExpertLikelihood \right] \\
# \singleGatingGivenInducing &= \E_{\singleLatentGatingsGivenInducing} \left[ \singleGatingLikelihood \right] \\
# \singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
# \mode{\mathbf{A}} \expertInducingOutput,
# \expertKernel(\singleInput, \singleInput) -
# \mode{\mathbf{A}}
# \expertKernel(\expertInducingInput, \expertInducingInput)
# \mode{\mathbf{A}}^T
# \right), \\
# \singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mathbf{\gatingFunc}(\singleInput) \mid
# \mode{\hat{\mathbf{A}}} \gatingInducingOutput,
# \gatingKernel(\singleInput, \singleInput) -
# \mode{\hat{\mathbf{A}}}
# \gatingKernel(\gatingInducingInput, \gatingInducingInput)
# \mode{\hat{\mathbf{A}}}^T
# \right),
# \end{align}
# %\mode{\K}
# %\mode{\tilde{\K}}
# #+END_EXPORT
# where,
# # #+BEGIN_EXPORT latex
# # \begin{align} \label{eq-}
# # \mode{\mathbf{A}} &= \expertKernel(\allInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1} \\
# # \mode{\hat{\mathbf{A}}} &= \gatingKernel(\allInput, \gatingInducingInput)\left(\gatingKernel(\gatingInducingInput, \gatingInducingInput)\right)^{-1}.
# # \end{align}
# # #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \mode{\mathbf{A}} &= \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1} \\
# \mode{\hat{\mathbf{A}}} &= \gatingKernel(\singleInput, \gatingInducingInput)\left(\gatingKernel(\gatingInducingInput, \gatingInducingInput)\right)^{-1}.
# \end{align}
# #+END_EXPORT
# $\mode{\mathbf{A}} = \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}$ and
# $\mode{\hat{\mathbf{A}}} = \gatingKernel(\singleInput, \gatingInducingInput)\left(\expertKernel(\gatingInducingInput, \gatingInducingInput)\right)^{-1}$.

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \singleExpertGivenInducing &= \E_{\singleLatentExpertGivenInducing} \left[ \singleExpertLikelihood \right] \\
# \singleGatingGivenInducing &= \E_{\singleLatentGatingsGivenInducing} \left[ \singleGatingLikelihood \right] \\
# \singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
# \expertKernel(\singleInput, \expertInducingInput) \expertKernel^{-1}(\expertInducingInput, \expertInducingInput) \expertInducingOutput,
# \mode{\K}
# \right), \\
# \singleLatentGatingsGivenInducing &= \mathcal{N}\left( \mathbf{\gatingFunc}(\singleInput) \mid
# \gatingKernel(\singleInput, \gatingInducingInput) \gatingKernel^{-1}(\gatingInducingInput, \gatingInducingInput) \gatingInducingOutput,
# \mode{\tilde{\K}}
# \right),
# \end{align}
# #+END_EXPORT
# where,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \mode{\K} = \expertKernel(\singleInput, \singleInput) - \expertKernel(\singleInput, \gatingInducingInput) (\expertKernel(\gatingInducingInput,\gatingInducingInput))^{-1} \expertKernel(\gatingInducingInput, \singleInput) \\
# \mode{\tilde{\K}} = \gatingKernel(\singleInput, \singleInput) - \gatingKernel(\singleInput, \gatingInducingInput) (\gatingKernel(\gatingInducingInput,\gatingInducingInput))^{-1} \gatingKernel(\gatingInducingInput, \singleInput).
# \end{align}
# #+END_EXPORT


# $\expertInducingInput=\allInput$, and for the gating network's inducing inputs,
# $\gatingInducingInput=\allInput$, this approximation becomes exact in the limit $M=N$.

# For each expert's inducing inputs,
# $\expertInducingInput=\allInput$, and for the gating network's inducing inputs,
# $\gatingInducingInput=\allInput$, this approximation becomes exact in the limit $M=N$.

# This minimises the KL divergence and ensures that $\expertsInducingInput$ are distributed amongst the training
# inputs $\allInput$ such that ....

*** Evidence Lower Bounds
Instead of collapsing the inducing variables as seen in cite:titsiasVariational2009,
they can be explicitly represented as variational distributions,
$(\expertsInducingVariational, \gatingsInducingVariational)$
and used to obtain a variational lower bound, aka \acrfull{elbo}.
This section derives three \acrshort{elbo}s.
The first bound is the tightest but requires approximating $\NumInducing$ dimensional integrals
for each expert and gating function.
Two further lower bounds which replace some (or all) of the $\NumInducing$ dimensional integrals
with one dimensional integrals are derived.
These further bounds offer improved computational properties at the cost of loosening the bound.
All three of these bounds are evaluated in cref:sec-mcycle-results.

# \newline
*Tight lower bound*
Following a similar approach to cite:hensmanGaussian2013,hensmanScalable2015,
a lower bound on cref:eq-augmented-marginal-likelihood can be obtained,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-lower-bound-tight}
\text{log} \evidence &\geq  \sum_{\numData=1}^\NumData \E_{\gatingsInducingVariational \expertsInducingVariational}
\left[ \text{log} \left( \sum_{\modeInd=1}^\ModeInd
\singleGatingGivenInducing \singleExpertGivenInducing \right) \right] \nonumber \\
&- \gatingsKL \nonumber \\
&- \expertsKL := \tightBound,
\end{align}
#+END_EXPORT
# From cref:eq-lower-bound-tight it is clear that the optimal distribution for gating network's variational
# distribution consists of independent Guassians.
# The experts' variational posterior is not tractable so we assume a Gaussian approximate posterior.
where we parameterise the variational posteriors to be independent Gaussians,
# Our variational posteriors are then given by,
# From cref:eq-lower-bound-tight it is clear that the optimal distribution for each of the variational distributions
# is Guassian, so we parameterise them as such,
# The posterior is not tractable so we assume a Gaussian approximate posterior.
#+BEGIN_EXPORT latex
\begin{align} \label{eq-variational-inducing-dist}
\expertsInducingVariational &= \prod_{\modeInd=1}^{\ModeInd} \expertInducingVariational
= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left(\expertInducingOutput \mid \mode{\mathbf{m}}, \mode{\mathbf{S}} \right) \\
\gatingsInducingVariational &= \prod_{\modeInd=1}^{\ModeInd} \gatingInducingVariational
= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left(\gatingInducingOutput \mid \mode{\hat{\mathbf{m}}}, \mode{\hat{\mathbf{S}}} \right).
\end{align}
#+END_EXPORT
The bound in cref:eq-lower-bound-tight meets the necessary conditions to perform stochastic gradient methods on
$\expertsInducingVariational$ and $\gatingsInducingVariational$, as the expected log likelihood (first term)
is written as a sum over input-output pairs.
However, this expectation cannot be calculated in closed-form and must be approximated.
The joint distributions over the inducing variables for each expert GP $\expertInducingVariational$
and each gating function GP $\gatingInducingVariational$,
are $\NumInducing$ dimensional multivariate normal distributions.
Therefore, each expectation requires an $\NumInducing$ dimensional integral to be approximated.
# integral being approximated is $\NumInducing$ dimensional.
\todo{add more on why we don't want M dimensional integral}

*Further lower bound* Following cite:hensmanScalable2015, these issues can be overcome
by further bounding the bound in cref:eq-lower-bound-tight ($\tightBound$).
This removes the $\NumInducing$ dimensional integrals associated with each of the gating functions.
Jensen's inequality can be applied to the conditional probability $\singleLatentGatingGivenInducing$,
obtaining the further bound,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-lower-bound-further}
\tightBound \geq \sum_{\numData=1}^\NumData &\E_{\gatingsVariational \expertsInducingVariational}
\left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertGivenInducing \right] \nonumber \\
&- \gatingsKL \nonumber \\
&- \expertsKL := \furtherBound,
\end{align}
#+END_EXPORT
where $\gatingsVariational$ represents the variational posterior given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-variational-posteriors}
%\expertsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{ \expertInducingVariational} \left[ \singleLatentExpertGivenInducing \right] \\
\gatingsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{\gatingInducingVariational} \left[ \singleLatentGatingGivenInducing \right].
\end{align}
#+END_EXPORT
Moving the marginalisation over the latent gating functions $\GatingFunc(\singleInput)$
outside of the marginalisation over the expert indicator
variable is possible because the mixing probabilities are dependent on *all* of the gating functions and
not just their associated gating function.
In contrast, moving the marginalisation over each expert's latent function $\mode{\latentFunc}(\singleInput)$
outside of the marginalisation over the expert
indicator variable, corresponds to changing the underlying model, in particular, the likelihood
approximation in cref:eq-likelihood-approximation.
# This is not the case for the experts' as they only depend on their corresponding latent function.

*Further^2 lower bound*
Nevertheless, we proceed and further bound the experts for comparison.
Jensen's inequality is applied to the conditional probability $\singleLatentExpertGivenInducing$,
obtaining the further^2 bound,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-lower-bound-further-2}
\furtherBound \geq \sum_{\numData=1}^\NumData &\E_{\gatingsVariational \expertsVariational}
\left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \nonumber \\
&- \gatingsKL \nonumber \\
&- \expertsKL := \furtherBoundTwo,
\end{align}
#+END_EXPORT
where $\expertsVariational$ represents the variational posterior given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-variational-posteriors}
\expertsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{ \expertInducingVariational} \left[ \singleLatentExpertGivenInducing \right].
\end{align}
#+END_EXPORT
Intuitively, this bound can be seen as modifying the likelihood approximation in cref:eq-likelihood-approximation.
Instead of mixing the GPs associated with each expert, this approximation simply mixes their associated
noise models.

# *Further Lower Bound* Following cite:hensmanScalable2015, the bound in
# cref:eq-lower-bound-tight ($\tightBound$) can be further bounded to remove the $\NumInducing$ dimensional integrals.
# Applying Jensen's inequality to the conditional probability $\singleGatingGivenInducing \singleExpertGivenInducing$,
# obtains a further bound,
# # #+BEGIN_EXPORT latex
# # \begin{align} \label{eq-further-bound}
# # \singleGatingGivenInducing \singleExpertGivenInducing \geq
# # \E_{\singleLatentExpertGivenInducing \singleLatentGatingsGivenInducing}
# # \left[ \text{log} \left( \singleGatingLikelihood \singleExpertLikelihood \right ) \right]
# # \end{align}
# # #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-lower-bound-further}
# \tightBound \geq \sum_{\numData=1}^\NumData &\E_{\gatingsVariational \expertsVariational}
# \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \nonumber \\
# &- \gatingsKL \nonumber \\
# &- \expertsKL := \furtherBound,
# \end{align}
# #+END_EXPORT
# # #+BEGIN_EXPORT latex
# # \begin{align} \label{eq-lower-bound-further}
# # \mathcal{L}_{1}
# # \geq \sum_{\numData=1}^\NumData &\E_{\expertsInducingVariational \gatingsInducingVariational}
# # \left[ \E_{\singleLatentGatingsGivenInducing \singleLatentExpertGivenInducing} \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \right] \nonumber \\
# # &- \gatingsKL - \expertsKL \\
# # = \sum_{\numData=1}^\NumData &\E_{\gatingsVariational \expertsVariational}
# # \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \nonumber \\
# # &- \gatingsKL - \expertsKL := \mathcal{L}_{2},
# # \end{align}
# # #+END_EXPORT
# where $\gatingsVariational$ and $\expertsVariational$ represents the variational posteriors given by,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-variational-posteriors}
# \expertsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{ \expertInducingVariational} \left[ \singleLatentExpertGivenInducing \right] \\
# \gatingsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{\gatingInducingVariational} \left[ \singleLatentGatingGivenInducing \right].
# \end{align}
# #+END_EXPORT
As each GP's inducing variables are Normally distributed, the functional form of the
variational posteriors are given by,
#+BEGIN_EXPORT latex
\begin{align}
\label{eq--variational-posteriors-functional-experts}
\expertsVariational &= \prod_{\modeInd=1}^\ModeInd \mathcal{N} \left( \mode{\latentFunc}(\singleInput) \mid
\mode{\mathbf{A}} \mode{\mathbf{m}},
\expertKernelnn
+ \mode{\mathbf{A}}
(\mode{\mathbf{S}} - \expertKernelMM)
\mode{\mathbf{A}}^T
\right) \\
\gatingsVariational &= \prod_{\modeInd=1}^\ModeInd \mathcal{N} \left( \mode{\gatingFunc}(\singleInput) \mid
\mode{\hat{\mathbf{A}}} \mode{\hat{\mathbf{m}}},
\gatingKernelnn
+ \mode{\hat{\mathbf{A}}}
(\mode{\hat{\mathbf{S}}} - \gatingKernelMM)
\mode{\hat{\mathbf{A}}}^T
\right), \label{eq--variational-posteriors-functional-gating}
\end{align}
#+END_EXPORT
# $\mode{\mathbf{A}} = \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}$ and
where
$\mode{\mathbf{A}} = \expertKernelnM \expertKernelMM^{-1}$ and
$\mode{\hat{\mathbf{A}}} = \gatingKernelnM \gatingKernelMM^{-1}$.
Importantly, these variational posteriors marginalise the inducing variables in closed-form,
with Gaussian convolutions.
$\furtherBound$ removes $\ModeInd$ of the undesirable approximate $M$ dimensional integrals from
cref:eq-lower-bound-tight and $\furtherBoundTwo$ removes $\ModeInd^2$.
The variational expectation in cref:eq-lower-bound-further still requires approximation,
however, the integrals are now only one dimensional.
These integrals are approximated with Gibbs sampling and
in practice only single samples are used because the added stochasticity helps the optimisation.

The tight lower bound $\tightBound$ is the most accurate lower bound but it is also the most
computationally expensive.
The further $\furtherBound$ and the further^2 $\furtherBoundTwo$ lower bounds
have lower computational complexity at the cost of being looser bounds.
The performance of these bounds is evaluated in cref:sec-mcycle-results.

*** Optimisation
#+BEGIN_EXPORT latex
\renewcommand{\expertSampleInd}{\ensuremath{s}}
\renewcommand{\ExpertSampleInd}{\ensuremath{S}}
\renewcommand{\gatingSampleInd}{\ensuremath{\hat{s}}}
\renewcommand{\GatingSampleInd}{\ensuremath{\hat{S}}}
\renewcommand{\batchSampleInd}{\ensuremath{i}}

\newcommand{\singleExpertLikelihoodSample}{\ensuremath{p\left(\y_{\batchSampleInd} \mid \expertInducingOutput^{(\expertSampleInd)}\right)}}
%\newcommand{\singleGatingLikelihoodSample}{\ensuremath{\Pr\left(\alpha_\batchSampleInd = \modeInd \mid \gatingsInducingOutput^{(\gatingSampleInd)} \right)}}
\renewcommand{\singleGatingLikelihoodSample}{\ensuremath{\Pr\left(\alpha_\batchSampleInd = \modeInd \mid \GatingFunc(\x_{\batchSampleInd})^{(\gatingSampleInd)} \right)}}

%\newcommand{\singleExpertLikelihoodSample}{\ensuremath{p\left(\y_{\batchSampleInd} \mid \mode{f}(\x_{\batchSampleInd})^{(\expertSampleInd)}\right)}}
\newcommand{\singleGatingLikelihoodSample}{\ensuremath{\Pr\left(\alpha_\batchSampleInd = \modeInd \mid \GatingFunc_{\batchSampleInd}^{(\gatingSampleInd)} \right)}}
%\newcommand{\gatingsVariationalSample}{\ensuremath{q(\GatingFunc(\x_{\batchSampleInd-1}))}}
%\newcommand{\expertVariationalSample}{\ensuremath{q(\mode{\latentFunc}(\x_{\batchSampleInd-1}))}}

\newcommand{\gatingsVariationalSample}{\ensuremath{q(\GatingFunc(\x_{\batchSampleInd}))}}
#+END_EXPORT
\todo{add unbiased estimate of ELBO}
The bounds in  cref:eq-lower-bound-further,eq-lower-bound-tight,eq-lower-bound-further-2
meet the necessary conditions to perform stochastic
gradient methods on $\expertsInducingVariational$ and $\gatingsInducingVariational$.
Firstly, they contain a sum of $\NumData$ terms corresponding to input-output pairs, enabling optimisation
with mini-batches.
Secondly, the expectations over the log-likelihood are calculated using Monte Carlo samples.

*Stochastic optimisation*
At each iteration $j$, a random subset of $\NumData_b$ data points are sampled from the data set $\mathcal{D}$,
to get a minibatch $\mathcal{D}_j = \{\x_i, \y_i\}_{i=1}^{\NumData_b}$.
The further lower bound $\furtherLowerBound$ is then approximated by,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-approx-lower-bound-further}
# \hat{\mathcal{L}}_{2} = \frac{\NumData}{\NumData_b} \sum_{\singleInput, \singleOutput \in \mathcal{D}_i}
# &\E_{\gatingsVariational \expertsVariational}
# \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \\
# &- \gatingsKL - \expertsKL.
# \end{align}
# #+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-approx-lower-bound-further}
\hat{\mathcal{L}}_{\text{further}} = \frac{\NumData}{\NumData_b}
&\sum_{\x_{\batchSampleInd}, \y_{\batchSampleInd} \in \mathcal{D}_j}
\left(
\frac{1}{\ExpertSampleInd} \sum_{\expertSampleInd=1}^{\ExpertSampleInd}
\frac{1}{\GatingSampleInd} \sum_{\gatingSampleInd=1}^{\GatingSampleInd} \nonumber
\text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihoodSample \singleExpertLikelihoodSample \right)  \\
- &\gatingsKL \nonumber \\
- &\expertsKL,
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \renewcommand{\singleExpertLikelihoodSample}{\ensuremath{p\left(\y_{\batchSampleInd} \mid \mode{f}(\x_{\batchSampleInd-1})^{(\expertSampleInd)}\right)}}
# \renewcommand{\singleGatingLikelihoodSample}{\ensuremath{\Pr\left(\alpha_\batchSampleInd = \modeInd \mid \GatingFunc(\x_{\batchSampleInd})^{(\gatingSampleInd)} \right)}}
# \newcommand{\expertVariationalSample}{\ensuremath{q(\mode{\latentFunc}(\x_{\batchSampleInd-1}))}}
# \small
# %\sum_{\singleInput, \singleOutput \in \mathcal{D}_i}
# \begin{align} \label{eq-approx-lower-bound-further}
# \hat{\mathcal{L}}_{2} &= \frac{\NumData}{\NumData_b}
# \sum_{d_n \in \mathcal{D}_i}
# \left(
# \frac{1}{\ExpertSampleInd} \sum_{\expertSampleInd=1}^{\ExpertSampleInd}
# \frac{1}{\GatingSampleInd} \sum_{\gatingSampleInd=1}^{\GatingSampleInd} \nonumber
# \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihoodSample \singleExpertLikelihoodSample \right)  \\
# &- \gatingsKL - \expertsKL, \\
# \text{where} \quad &\mode{\latentFunc}(\x_{\batchSampleInd-1})^{(\expertSampleInd)} \sim \expertVariationalSample, \\
# &\mathbf{\gatingFunc}(\x_{\batchSampleInd-1})^{(\gatingSampleInd)} \sim \gatingsVariationalSample
# \end{align}
# \normalsize
# #+END_EXPORT
where $\expertInducingOutput^{(\expertSampleInd)} &\sim \expertInducingVariational$
and
$\mathbf{\gatingFunc}(\x_{\batchSampleInd})^{(\gatingSampleInd)} &\sim \gatingsVariationalSample$
denote samples from the variational posteriors.
The variational distributions over the inducing variables are represented using the mean vector $\mode{\mathbf{m}}$
and the lower triangular $\mode{\mathbf{L}}$ of the covariance matrix
$\mode{\mathbf{S}} = \mode{\mathbf{L}} \mode{\mathbf{L}}^T$.
A downside to this formulation is that $(\ModeInd^2)(M-1)M/2 + \ModeInd^2 M$ extra parameters need to be optimised.
In the two expert case this reduces to $(\ModeInd+1)(M-1)M/2 + (\ModeInd+1) M$ extra parameters.
Optimising the inducing inputs ($\gatingInducingInput$ and $\expertsInducingInput$) introduces a further
$\ModeInd^2\NumInducing\InputDim$ optimisation parameters.
The inducing inputs $\gatingInducingInput,\{\expertInducingInput\}_{\modeInd=1}^{\ModeInd}$,
kernel hyperparameters and noise variances, are treated as
variational hyperparameters and optimised alongside the variational parameters, using stochastic gradient descent
e.g. Adam citep:kingmaAdam2017.

# Note that the augmented model captures the dependencies in the joint distribution of the data through the
# inducing variables, but as $M \ll \NumData$ these have a much lower computational burden.

# ${\E_{\gatingsVariational \expertsInducingVariational} \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertGivenInducing \right]}$,

*Computational complexity* Assuming that each expert has the same number of inducing points $\NumInducing$,
the cost of computing the KL divergences and their derivatives is $\mathcal{O}\left( \ModeInd \NumInducing^{3} \right)$.
The cost of computing the expected likelihood term is dependent on the batch size $\NumData_b$.
For each data point in the minibatch, each of the $\ModeInd$ gating function variational posteriors has complexity
$\mathcal{O}\left( \NumInducing^{2} \right)$ to evaluate.
For each data point, only a single sample is drawn from each of these distributions.
Sampling each expert's inducing variable distribution $\expertInducingVariational$
has complexity $\mathcal{O}(\NumInducing^2)$ because
the covariance is represented as the lower triangular (via the cholesky decomposition).
In addition to this sampling, calculating each experts conditional $\singleExpertGivenInducing$ given these samples
has complexity $\mathcal{O}(\NumInducing^2)$.
\todo{How to combine all of these complexities?}


# For each data point in the minibatch, each of the $\ModeInd$ gating function GPs and $\ModeInd$ expert GPs has complexity
# $\mathcal{O}\left( \NumInducing^{2} \right)$ to evaluate.


# The gating network's variational posterior has complexity
# $\mathcal{O}\left( \ModeInd \NumInducing^{2} \right)$ to evaluate.
# Drawing a single sample from this

# Each expert's variational posterior $\expertInducingVariational$ is represented using a cholesky decomposition
# so sampling from all of them has complexity $\mathcal{O}\left( \ModeInd \NumInducing^{2} \right)$.

# # The cost of computing the expected likelihood term
# # is dependent on the batch size $\NumData_b$ and has complexity
# # $\mathcal{O}\left( \NumData_{b} \ModeInd^2 \NumInducing^{2} \right)$ to evaluate.


# When the number of inducing points $\NumInducing$ is smaller than the batch size, most of the cost will arise from
# computing the expected likelihood term.
# The bound has complexity
# $\max\left(\mathcal{O}\left( \ModeInd \NumInducing^{3} \right),\mathcal{O}\left( \NumData_{b} \ModeInd \NumInducing^{2} \right)\right)$.
# \todo{check complexities. What is complexity of approximating M dimensional integral with gibbs sampling?}

*** Predictions
#+BEGIN_EXPORT latex
\renewcommand{\testInput}{\ensuremath{\mathbf{X}^*}}
\renewcommand{\testOutput}{\ensuremath{\mathbf{y}^*}}
\renewcommand{\NumTest}{\ensuremath{\NumData^*}}
\renewcommand{\singleTestInput}{\ensuremath{\mathbf{x}_n^*}}
\renewcommand{\singleTestOutput}{\ensuremath{y_n^*}}
\newcommand{\testModeVarK}{\ensuremath{\bm\modeVar^* = \modeInd}}
\newcommand{\singleTestModeVar}{\ensuremath{\modeVar_n^*}}
\newcommand{\singleTestModeVarK}{\ensuremath{\singleTestModeVar = \modeInd}}

%\newcommand{\predictivePosterior}{\ensuremath{p(\testOutput \mid \testInput, \allOutput, \allInput)}}
%\newcommand{\predictiveProb}{\ensuremath{\Pr(\singleTestModeVarK \mid \singleTestInput, \allOutput, \allInput)}}
%\newcommand{\predictiveExpert}{\ensuremath{p(\singleTestOutput \mid \singleTestModeVarK, \singleTestInput, \allOutput, \allInput)}}
\newcommand{\predictivePosterior}{\ensuremath{p(\testOutput \mid \allOutput)}}
\newcommand{\predictiveProb}{\ensuremath{\Pr(\singleTestModeVarK \mid \allOutput)}}
\newcommand{\predictiveProbBernoulli}{\ensuremath{\Pr(\singleTestModeVar=1 \mid \allOutput)}}
\newcommand{\predictiveExpert}{\ensuremath{p(\singleTestOutput \mid \singleTestModeVarK, \allOutput)}}

\newcommand{\approxPredictiveProb}{\ensuremath{q(\singleTestModeVarK)}}
\newcommand{\approxPredictiveExpert}{\ensuremath{q(\singleTestOutput \mid \singleTestModeVarK)}}

\newcommand{\predictiveExpertLikelihood}{\ensuremath{p(\singleTestOutput \mid \mode{f}(\singleTestInput))}}
\newcommand{\predictiveGatingLikelihood}{\ensuremath{\Pr(\singleTestModeVarK \mid \GatingFunc(\singleTestInput))}}
\newcommand{\predictiveGatingLikelihoodBernoulli}{\ensuremath{\Pr(\modeVar_n^*=1 \mid \modei{\gatingFunc}{1}(\singleTestInput))}}

%\renewcommand{\expertPosterior}{\ensuremath{p(\mode{\latentFunc}(\singleTestInput) \mid \singleTestInput, \allOutput, \allInput)}}
%\renewcommand{\gatingPosterior}{\ensuremath{p(\GatingFunc(\singleTestInput) \mid \singleTestInput, \allOutput, \allInput)}}
\renewcommand{\expertPosterior}{\ensuremath{p(\mode{\latentFunc}(\singleTestInput) \mid \allOutput)}}
\renewcommand{\gatingPosterior}{\ensuremath{p(\GatingFunc(\singleTestInput) \mid \allOutput)}}

\renewcommand{\expertVariationalPosterior}{\ensuremath{q(\mode{\latentFunc}(\singleTestInput)}}
\renewcommand{\gatingVariationalPosterior}{\ensuremath{q(\GatingFunc(\singleTestInput)}}
\renewcommand{\gatingVariationalPosteriorBernoulli}{\ensuremath{q(\modei{\gatingFunc}{1}(\singleTestInput))}}
#+END_EXPORT
For a given set of test inputs $\testInput \in \R^{\NumTest \times \InputDim$,
this model makes probabilistic predictions following
a mixture of $\ModeInd$ Gaussians.
Making predictions with this model involves calculating a density over the output for each expert and combining
them using the probabilities obtained from the gating network, i.e. marginalising the expert indicator variable,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-predictive-posterior}
\predictivePosterior &= \prod_{\numData=1}^{\NumTest} \sum_{\modeInd=1}^\ModeInd \underbrace{\predictiveProb}_{\text{gating network posterior}} \underbrace{\predictiveExpert}_{\text{expert } \ModeInd \text{ posterior}} \\
&\approx \prod_{\numData=1}^{\NumTest} \sum_{\modeInd=1}^\ModeInd \approxPredictiveProb \approxPredictiveExpert
\end{align}
#+END_EXPORT

*Experts*
The experts make predictions at new test locations by integrating over their latent function posteriors,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-prediction}
\predictiveExpert
&= \int \underbrace{\predictiveExpertLikelihood}_{\text{likelihood}}
\underbrace{\expertPosterior}_{\text{posterior}} \text{d} \mode{\latentFunc}(\singleTestInput) \nonumber \\
&\approx \int \underbrace{\predictiveExpertLikelihood}_{\text{likelihood}}
\underbrace{\expertVariationalPosterior}_{\text{approx posterior}} \text{d} \mode{\latentFunc}(\singleTestInput)
\coloneqq \approxPredictiveExpert.
\end{align}
#+END_EXPORT
However, the experts' true posteriors $\expertPosterior$ are not known and have been approximated.
Each expert's approximate posterior is given by
$q(\mode{\latentFunc}(\allInputK), \expertInducingOutput) = p(\mode{\latentFunc}(\allInputK) \mid \expertInducingOutput) q(\expertInducingOutput)$.
To make a prediction at a set of test locations $\testInput$, we substitute our approximate posterior
into the standard probabilistic rule,
# *Experts* Each expert's predictive distribution over the output $\predictiveExpert$,
# is obtained by marginalising its predictive posterior,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-predictive-expert}
%\predictiveExpert &=
\underbrace{\expertPosterior}_{\text{posterior}} &=
\int p(\mode{\latentFunc}(\singleTestInput) \mid \mode{\latentFunc}(\allInputK), \expertInducingOutput)
p(\mode{\latentFunc}(\allInputK), \expertInducingOutput \mid \allOutput)
\text{d} \mode{\latentFunc}(\allInputK) \text{d} \expertInducingOutput \nonumber \\
&\approx \int p(\mode{\latentFunc}(\singleTestInput) \mid \mode{\latentFunc}(\allInputK), \expertInducingOutput)
p(\mode{\latentFunc}(\allInputK) \mid \expertInducingOutput) q(\expertInducingOutput)
\text{d} \mode{\latentFunc}(\allInputK) \text{d} \expertInducingOutput \nonumber \\
&= \int p(\mode{\latentFunc}(\singleTestInput) \mid \expertInducingOutput)
\expertInducingVariational
\text{d} \expertInducingOutput \nonumber \\
&= \mathcal{N} \left( \mode{\latentFunc}(\singleTestInput) \mid
\mode{\mathbf{A}} \mode{\mathbf{m}},
\expertKernelss
+ \mode{\mathbf{A}}
(\mode{\mathbf{S}} - \expertKernelMM)
\mode{\mathbf{A}}^T \right) \coloneqq \underbrace{\expertVariationalPosterior}_{\text{approx posterior}},
%\E_{\predictiveExpertPrior} \left[ \predictiveExpertLikelihood \right],
\end{align}
#+END_EXPORT
where $\mode{\mathbf{A}} = \expertKernelsM \expertKernelMM^{-1}$.
This integral has complexity $\mathcal{O}(\NumInducing^2)$.


*Gating network* The mixing probabilities associated with the gating network are obtained
by integrating the gating network's posterior through the gating likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-prediction}
\predictiveProb
&= \int \underbrace{\predictiveGatingLikelihood}_{\text{likelihood}}
\underbrace{\gatingPosterior}_{\text{posterior}} \text{d} \GatingFunc(\singleTestInput) \nonumber \\
&\approx \int \underbrace{\predictiveGatingLikelihood}_{\text{likelihood}}
\underbrace{\gatingVariationalPosterior}_{\text{approx posterior}} \text{d} \GatingFunc(\singleTestInput)
\coloneqq \approxPredictiveProb.
\end{align}
#+END_EXPORT
Again, the gating network's true posterior $\gatingPosterior$ has been approximated,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-predictive-gating}
\underbrace{\gatingPosterior}_{\text{posterior}}
&\approx \int p(\GatingFunc(\singleTestInput) \mid \gatingInducingOutput)
\gatingInducingVariational \text{d} \gatingInducingOutput \nonumber \\
&= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N} \left( \GatingFunc(\singleTestInput) \mid
\mode{\hat{\mathbf{A}}} \mode{\hat{\mathbf{m}}},
\gatingKernelss
+ \mode{\hat{\mathbf{A}}}
(\mode{\hat{\mathbf{S}}} - \gatingKernelMM)
\mode{\hat{\mathbf{A}}}^T \right) \coloneqq \underbrace{\gatingVariationalPosterior}_{\text{approx posterior}},
\end{align}
#+END_EXPORT
where $\mode{\hat{\mathbf{A}}} = \gatingKernelsM \gatingKernelMM^{-1}$.
In the general case, when $\ModeInd > 2$, the gating likelihood is the softmax,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-likelihood-softmax}
\underbrace{\predictiveGatingLikelihood}_{\text{likelihood}} = \text{softmax}(\GatingFunc(\singleTestInput)),
\end{align}
#+END_EXPORT
so cref:eq-gating-prediction is approximated with Monte Carlo quadrature.
In the two expert case there is only a single gating function, $\gatingFunc_1$, as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\Pr(\singleTestModeVar=2 \mid \modei{\gatingFunc}{1}(\singleTestInput)) = 1 - \Pr(\singleTestModeVar=1 \mid \modei{\gatingFunc}{1}(\singleTestInput)).
\end{align}
#+END_EXPORT
In this case, the gating likelihood is the Gaussian cdf,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-likelihood-bernoulli}
\underbrace{\predictiveGatingLikelihoodBernoulli}_{\text{likelihood}} = \Phi(\gatingFunc_1(\singleTestInput)),
\end{align}
#+END_EXPORT
so cref:eq-gating-prediction can be calculated in closed-form with,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-predictive-gating-bernoulli}
\predictiveProbBernoulli &=
\int \gatingVariationalPosteriorBernoulli \Phi\left(\modei{\gatingFunc}{1}(\singleTestInput)\right) \text{d} \modei{\gatingFunc}{1}(\singleTestInput) \nonumber \\
&= \Phi \left(\frac{\mu_{h*}}{\sqrt{1 + \sigma^2_{h_*} }}\right).
\end{align}
%\mathcal{N}\left(\gatingFunc(\x_*) \mid \mu_h, \sigma^2_h \right) \text{d} \gatingFunc(\x_*) \\
#+END_EXPORT
where $\mu_{h_*}$ and $\sigma^2_{h_*}$ are the mean and variance of the variational posterior
$\gatingVariationalPosteriorBernoulli$ at $\singleTestInput$.



# # $\predictiveGatingLikelihood$,
# # *Gating Network* The mixing probabilities associated with the gating network are obtained
# # by taking the expectation of the gating likelihood $\predictiveGatingLikelihood$,
# # under the predictive posterior of the gating network,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-predictive-gating}
# \predictiveProb &= \int \predictiveGatingPrior \predictiveGatingLikelihood \text{d} \GatingFunc(\x_*),
# %\predictiveProb &= \E_{\predictiveGatingPrior} \left[ \predictiveGatingLikelihood \right],
# \end{align}
# #+END_EXPORT
# where the predictive posterior $\predictiveGatingPrior$ is given by
# cref:eq--variational-posteriors-functional-gating.

# In the general case, when $\ModeInd > 2$, the gating likelihood is the softmax,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-gating-likelihood-softmax}
# \predictiveGatingLikelihood = \text{softmax}(\GatingFunc(\x_*)),
# \end{align}
# #+END_EXPORT
# so cref:eq-predictive-gating is approximated with Monte Carlo quadrature.
# In the two expert case there is only a single gating function, $\gatingFunc_1$, as,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# ${\Pr(\singleModeVar=2 \mid \modei{\gatingFunc}{1}(\singleInput)) = 1 - \Pr(\singleModeVar=1 \mid \modei{\gatingFunc}{1}(\singleInput))}$.
# \end{align}
# #+END_EXPORT
# In this case, the gating likelihood is the Gaussian cdf,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-gating-likelihood-bernoulli}
# \predictiveGatingLikelihoodBernoulli = \Phi(\gatingFunc_1(\x_*)),
# \end{align}
# #+END_EXPORT
# so cref:eq-predictive-gating can be calculated in closed-form with,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-predictive-gating-bernoulli}
# \Pr(\modeVar_*=1 \mid \x_*) &=
# \E_{\predictiveGatingPriorBernoulli} \left[ \Phi\left(\gatingFunc(\x_*)\right) \right] \\
# &= \Phi \left(\frac{\mu_{h*}}{\sqrt{1 + \sigma^2_{h_*} }}\right).
# \end{align}
# %\mathcal{N}\left(\gatingFunc(\x_*) \mid \mu_h, \sigma^2_h \right) \text{d} \gatingFunc(\x_*) \\
# #+END_EXPORT
# where $\mu_{h_*}$ and $\sigma^2_{h_*}$ are the mean and variance of the predictive posterior
# $\predictiveGatingPriorBernoulli$ at $\x_*$.

*** sparse graphical model old :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\fkn$};
      %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
      \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\hkn$};

      \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\uFk$};
      \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\Hku$};
      \node[const, left=of uk, xshift=0.4cm] (zk) {$\zFk$};
      \node[const, right=of uh, xshift=-0.4cm] (zh) {$\zHk$};

      \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\expertParamsK$};
      \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\gatingParamsK$};

      \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (uk)--(f);
      \draw[post] (uh)--(h);
      \draw[post] (zk)--(uk);
      \draw[post] (zh)--(uh);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a) (f) (h)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(zh) (uh) (h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    }
  \caption{Graphical model of the transition dynamics where the state difference $\singleOutput$
is generated by pushing the state and control $\singleInput$ through the latent process.}
\label{fig-graphical-model-sparse}
\end{figure}
#+END_EXPORT



#+BEGIN_EXPORT latex
\todo[inline]{
\begin{itemize}
\item We want $\pFGivenFu$ outside the log (and not just $\qFu$) so we can analytically integrate $\qFu$ (like in Eq. \ref{eq-experts-var-dist})??
\item In the ICRA paper I had only $\qFu$ outside the log so we had to approximate the M-dimensional integral over $\qFu$ with samples.
\end{itemize}
}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo[inline]{
This is the bound I had coded up previously,
\begin{align} \label{eq-expert-bound-old}
\mathcal{L}_{\text{old}} =
&\sum_{\numData}^{\NumData} E_{\prod_{\modeInd=1}^{\ModeInd} \qFku} \text{log} \sum_{k=1}^{K} \PrA \left[ \pykGivenFku \right] \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right]
\end{align}
This bound is valid but it is nicer to move the expectations over $\pFk$ outside the log, like in Eq. \ref{eq-experts-bound-4-fact}.
}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo[inline]{
This is another bound I implemented,
\begin{align} \label{eq-expert-bound-old}
\mathcal{L}_{\text{old}} =
&\sum_{\numData}^{\NumData} \text{log} \sum_{k=1}^{K} \PrA \E_{\prod_{\modeInd=1}^{\ModeInd} \qFku} \left[ \pykGivenFku \right] \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right]
\end{align}
\begin{itemize}
\item THIS IS WRONG as the expectation over $\qFku$ should be outside the log.
\item which is why I used to approximate it with single samples....
\end{itemize}
}
#+END_EXPORT

\todo[inline]{Need to extend bound for the gating network}

** Evaluation of Model and Approximate Inference
*** Intro :ignore:
# As a mixture of experts method we aim to improve on standard GP regression
# with the ability to model non-stationary functions and multimodal distributions.
# This work also proposes to address their inherent identifiability issues, by adopting a gating network
# that we can place informative priors on.
# As such, the method is tested on two data sets,
# 1) *Artificial data set* to demonstrate how the method improves identifiability,
# 2) *Motorcycle data set* cite:Silverman1985 to provide a comparison to other MoGPE methods and to,
#    - thoroughly compare the different ELBO's in Section ref:sec-inference,
#    - evaluate the impact of the number of inducing points $M$ and the batch size $\NumData_b$.

# This work also proposes to address their inherent identifiability issues, by adopting a gating network
# that can encode domain knowledge through informative priors.
# The method is then tested on an artificial data set to demonstrate its power at improving identifiability.

As a mixture of experts method, our model aims to improve on standard \acrshort{gp} regression
with the ability to model non-stationary functions and multimodal distributions over the output variable.
With this in mind, the model and approximate inference scheme are evaluated on two data sets.
Following other \acrshort{mogpe} work, they are first tested on the motorcycle data set citep:Silverman1985.
Although this data set does not represent state transitions from a dynamical system,
it does contain non-stationary points and heterogeneous noise,
making it interesting to study from the \acrshort{mogpe} perspective.
Secondly, they are tested on the illustrative example from cref:illustrative_example.
That is, a data set collected onboard a DJI Tello quadcopter flying in an environment subject to two dynamics modes.
# Secondly, they are tested on a data set collected onboard a DJI Tello quadcopter flying in the Bristol Robotics
# Laboratory.
*** Experiments
#+BEGIN_EXPORT latex
\newcommand{\numTest}{\ensuremath{n}}
\newcommand{\NumTest}{\ensuremath{N}}
%\newcommand{\testSingleInput}{\ensuremath{\x_{\numTest}}}
%\newcommand{\testSingleOutput}{\ensuremath{\y_{\numTest}}}
%\newcommand{\allTestInput}{\ensuremath{\allInput_*}}
%\newcommand{\allTestOutput}{\ensuremath{\allOutput_*}}
\newcommand{\predictedSingleInput}{\ensuremath{\testSingleInput^*}}
\newcommand{\predictedSingleOutput}{\ensuremath{\testSingleOutput^*}}
\renewcommand{\predictedSingleInput}{\ensuremath{\testSingleInput^*}}
\renewcommand{\predictedSingleInput}{\ensuremath{\hat{\x}^*_{\numTest}}}
\renewcommand{\predictedSingleOutput}{\ensuremath{\hat{\y}^*_{\numTest}}}
#+END_EXPORT

Each experiment was carried out on a system with an Intel Core i9 CPU at 2.4GHz with 16GB DDR4 RAM.
All data sets were split into test and training sets with $70\%$ for training and $30\%$ for testing.
In order to evaluate and compare the full predictive posteriors the Negative Log Predictive Probability (NLPP)
is computed on the test set.
The models are also compared using the Root Mean Squared Error (RMSE) and the Mean Absolute Error (MAE).
Given a test data set
$(\testInput, \testOutput) = \{(\singleTestInput, \singleTestOutput)\}_{\numTest=1}^{\NumTest}$,
they are calculated as follows,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-scores}
\text{RMSE} &= \sqrt{\frac{1}{\NumTest} \sum_{\numTest=1}^{\NumTest} (\predictedSingleOutput - \singleTestOutput)^2} \\
\text{MAE} &= \frac{1}{\NumTest} \sum_{\numTest=1}^{\NumTest} | \predictedSingleOutput - \singleTestOutput | \\
\text{NLPP} &= \frac{1}{\NumTest} \sum_{\numTest=1}^{\NumTest} - \log (\singleTestOutput \mid \singleTestInput, \mathcal{D}, \expertParams, \gatingParams)
\end{align}
#+END_EXPORT
where $\predictedSingleOutput$ is the models prediction at $\singleTestInput$.
Note that all figures in this section show models that were trained on the full data set, i.e. no test/train split.

*** Evaluation on Motorcycle Data Set label:sec-mcycle-results
**** intro :ignore:
The Motorcycle data set
(discussed in cite:Silverman1985) contains 133 data points
($\allInput \in \inputDomain \subseteq \R^{133 \times 1}$ and
$\allOutput \in \outputDomain \subseteq \R^{133 \times 1}$)
and input dependent noise.
The data set represents motorcycle impact data -- time (ms) vs acceleration (g).
The data set is represented by the black crosses in cref:fig-y-mcycle-two-experts.


# The model and inference are evaluated by instantiating the model with both two $\ModeInd=2$
# and three $\ModeInd=3$ experts.
To test the performance of our method (\acrshort{mosvgpe}), the model is
instantiated with $\ModeInd=2$ and $\ModeInd=3$ experts.
All experiments on the Motorcycle data set use $\NumInducing=32$ inducing points for
all GPs and are trained for $25,000$ iterations
with Adam citep:kingmaAdam2017, with a learning rate of $0.01$ and a batch size of $\NumData_b=16$.
The results are compared against a Gaussian process (GP) and a sparse variational Gaussian
process (SVGP),
which use Squared Exponential (SE) kernels with automatic relevance determination (ARD) and
a Gaussian likelihood.
# The method is compared to a Sparse Variational Gaussian Process (SVGP) trained with the same
# number of inducing points and training parameters.

cref:tab-mcycle-metrics summarises the results for the three ELBOs
($\tightBound$, $\furtherBound$, $\furtherBoundTwo$)
and compares them to a standard GP regression model
and a SVGP method instantiated with $\NumInducing=16$ and $\NumInducing=32$ inducing points.
Both methods use Gaussian likelihoods and optimise their hyperparameters, noise variances
(and inducing inputs in the SVGP case) using their well known
objectives -- the marginal likelihood and evidence lower bound.

***** Results table :ignore:

#+Name: tab-mcycle-metrics
#+Caption: Results on the Motorcycle data set cite:Silverman1985 with different instantiations of our model (\acrshort{mosvgpe}).
#+Caption: Comparison of the root mean squared error (RMSE) mean absolute error (MAE)
#+Caption: and negative log predictive probability (NLPP) on the test data set.
#+Caption: Results for a Gaussian process (GP) and a sparse variational Gaussian process (SVGP) with
#+Caption: $\NumInducing=16$ and $\NumInducing=32$ inducing points are shown for comparison.
#+Caption: All models were instantiated with Squared Exponential kernels and were
#+Caption: trainind for $25,000$ iterations.
#+Caption: The GP's hyperparamters were optimised using SciPy's citep:2020SciPy-NMeth L-BFGS-B optimiser.
#+Caption: The SVGP and \acrshort{mosvgpe} models were trained with Adam citep:kingmaAdam2017 using a learning rate
#+Caption: of $0.01$ and a minibatch size of $\NumData_b=16$.
#+Caption: The \acrshort{mosvgpe} expertiments used $\NumInducing=32$ inducing points for each expert GP
#+Caption: and each gating function GP.
|-----------------------------------+-------------------+-------------------+-------------------|
|                                   | RMSE              | NLPP              | MAE               |
|-----------------------------------+-------------------+-------------------+-------------------|
| GP                                | $\mathbf{0.4357}$ | $0.9886$          | $0.3242$          |
|-----------------------------------+-------------------+-------------------+-------------------|
| SVGP $(M=16)$                     | $0.4427$          | $0.9762$          | $0.3257$          |
| SVGP $(M=32)$                     | $0.4437$          | $0.9832$          | $0.3271$          |
|-----------------------------------+-------------------+-------------------+-------------------|
| \acrshort{mosvgpe} $(k=2, \tightBound)$      | $0.4442$          | $0.4863$          | $0.3260$          |
| \acrshort{mosvgpe} $(k=2, \furtherBound)$    | $0.4590$          | $0.5073$          | $0.3355}$         |
| \acrshort{mosvgpe} $(k=2, \furtherBoundTwo)$ | $0.4472$          | $0.5271$          | $\mathbf{0.3218}$ |
| \acrshort{mosvgpe} $(k=3, \tightBound)$      | $0.4569$          | $\mathbf{0.2634}$ | $0.3301$          |
| \acrshort{mosvgpe} $(k=3 ,\furtherBound)$    | $0.4866$          | $0.2695$          | $0.3449$          |
| \acrshort{mosvgpe} $(k=3 ,\furtherBoundTwo)$ | $0.4575$          | $0.5467$          | $0.3270$          |

***** After results table :ignore:
The NLPP indicates the probability of the data given the
parameters which are not marginalised, e.g. hyperparameters and inducing inputs.
Following Bayesian model selection, it is known that lower values indicate higher performing models, i.e.
predictive posteriors that more accurately match the distribution of the data.
The predictive posterior is most accurate when \acrshort{mosvgpe} is instantiated with three experts $\ModeInd=3$
and trained using the tight lower bound $\tightBound$.
In both the two and three expert experiments,
the tight lower bound $\tightBound$ achieved better NLPP than both of the further/further^2
lower bounds,  $\furtherBound$ and $\furtherBoundTwo$.
This is expected as it is a tighter bound.
As both of the further/further^2 lower bounds offer improved computational properties,
it is interesting to compare their performance.
The NLPP scores for the further lower bound $\furtherBound$ are almost equal to the tight lower bound $\tightBound$.
In contrast, the NLPP score in the three expert experiment for the further^2 lower bound $\furtherBoundTwo$ is
significantly worse.
This indicates that valuable information is lost in this bound.
This was expected as this bound corresponds to a further likelihood approximation,
which mixes the experts' noise models as opposed to their full SVGP models.


# It is worth noting here that the tight lower bound $\tightBound$ takes
# longer to compute than the further lower bound $\furtherBound$.
# \todo{add something on further bound being better computationally????}

With regards to the accuracy of the predictive means,
the standard GP regression model achieved the best RMSE, followed by the SVGP models and then the
\acrshort{mosvgpe} models.
It is worth noting that all of the RMSE and MAE scores are very similar.
Although adding more experts to the \acrshort{mosvgpe} model appears to learn more accurate predictive posteriors, the
predictive means appear to deteriorate ever so slightly (indicated by higher RMSE/MAE values).
This is most likely due to bias at the boundaries between the experts,
resulting from the mixing behaviour arising from our GP-based gating network.
If the gating functions do not have extremely low lengthscales then they will not be able to immediately switch
from one expert to another.
Although this appears to negatively impact performance here, it should be noted that in theory
the GP-based gating network can offer superior generalisation and identifiability.

**** two expert y fig :ignore:
#+BEGIN_EXPORT latex
%\begin{figure}[t!]
\begin{figure}[hbt!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_means.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
\subcaption{$\furtherBound$, poseterior mean and data set}
\label{fig-y-means-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L3/y_means.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
\subcaption{$\furtherBoundTwo$, poseterior mean and data set}
\label{fig-y-means-mcycle-two-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_samples.pdf}
\subcaption{$\furtherBound$, posterior samples}
\label{fig-y-samples-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L3/y_samples.pdf}
\subcaption{$\furtherBoundTwo$, posterior samples}
\label{fig-y-samples-mcycle-two-experts-further}
\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L1/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-two-experts}
%\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-three-experts}
%\end{minipage}
\caption{Visualisation of the model instantiated with $\ModeInd=2$ experts, after training on the motorcycle data set \citep{Silverman1985} with $\furtherBound$ (left column) and with $\furtherBoundTwo$ (right column).  (\subref{fig-y-means-mcycle-two-experts-tight}-\subref{fig-y-means-mcycle-two-experts-further}) show the data set (black crosses) and the posterior means associated with the \acrshort{mosvgpe} (black solid line) and the SVGP (red dashed line) for comparison. (\subref{fig-y-samples-mcycle-two-experts-tight}-\subref{fig-y-samples-mcycle-two-experts-further}) show samples from the \acrshort{mosvgpe} posterior where colour indicates the underlying expert (cyan $K=1$, magenta $\ModeInd=2$) and the red lines show the $\pm 2$ standard deviation error ($95\%$ confidence interval) of the SVGP posterior. All experiments were trained with $25,000$ iterations of Adam \citep{kingmaAdam2017} using a learning rate of $0.01$ and a minibatch size of $\NumData_b=16$. All GPs use Squared Exponential kernels and $M=32$ inducing points.}
\label{fig-y-mcycle-two-experts}
\end{figure}
#+END_EXPORT

# #+caption: \textbf{Motorcycle data} The left plot shows the observations (red crosses) together with the means of our model (black solid line) and a SVGP with noise (blue dashed line). It also shows probabilities assocaited with the gating network for $k=1$ (green line) and $k=2$ (blue line) on the right axis. The right plot shows 100 samples (blue) from the predictive posterior at 200 evenly spaced input locations as well as the $\pm 2$ std error (95% confidence interval) of the SVGP (solid red line). Both models use squared exponential covariance functions for all GPs.

**** Two Experts
The two further lower bounds ($\furtherBound$ and $\furtherBoundTwo$),
derived in cref:sec-inference, are compared by training each instantiation of
the model using the same model and training parameters.
They are compared by instantiating the model with two experts $\ModeInd=2$
and comparing their performance.
The results are shown in cref:fig-y-mcycle-two-experts,fig-latent-mcycle-two-experts,
where cref:fig-y-mcycle-two-experts visualises the predictive posteriors and
cref:fig-latent-mcycle-two-experts visualises the posteriors over the latent variables.
The left column shows results for $\furtherBound$ and the right column shows results for $\furtherBoundTwo$.
This layout is used in
cref:fig-y-mcycle-two-experts,fig-latent-mcycle-two-experts,fig-y-mcycle-three-experts,fig-latent-mcycle-three-experts.

# For each lower bound, cref:fig-y-mcycle-two-experts visualises the predictive mean (top row) and
# predictive density (bottom row) and compares them to a Sparse Variational Gaussian Process (SVGP).
# cref:fig-latent-mcycle-two-experts then visualises the posteriors over the latent variables associated with
# each model.

# for the tight bound $\tightBound$ (ref:fig-y-means-two-experts-tight)
# and the further bound $\furtherBound$ (ref:fig-y-means-two-experts-further) respectively.
cref:fig-y-means-mcycle-two-experts-tight,fig-y-means-mcycle-two-experts-further
compare the posterior means (black solid line) to the SVGP's posterior mean (red dashed line) and
cref:fig-y-samples-mcycle-two-experts-tight,fig-y-samples-mcycle-two-experts-further
compare the posterior densities to the SVGP.
The red lines show plus or minus two standard deviations of the SVGP's posterior variance.
As the \acrshort{mosvgpe} posterior is a Gaussian mixture, it is visualised by drawing samples
from its posterior, i.e. sample a mode indicator variable $\modeVar_*$ and then draw a sample
from the corresponding expert.

*Predictive posteriors* Both \acrshort{mosvgpe} results are capable of modelling the non-stationarity
at $x \approx -0.7$ better than the sparse variational Gaussian process (SVGP).
\todo{quantify this with local RMSE?}
At this non-stationary point there are clearly two modes in the \acrshort{mosvgpe} predictive distributions,
indicated by the overlap in samples from each expert
in cref:fig-y-samples-mcycle-two-experts-tight,fig-y-samples-mcycle-two-experts-further.
It is clear that the SVGP has explained the observations by increasing its single
noise variance term. In contrast, both of the \acrshort{mosvgpe} results have been able to learn
two noise variances and these reflect the noise in the observations much better.
This is indicated by expert one learning a low noise variance and expert two a high noise
variance (similar to the SVGP's noise variance).
\todo{Add values for noise variances}

*Latent variables* More insight into this behaviour can be obtained by considering the latent variables.
Figure ref:fig-latent-mcycle-two-experts shows the posteriors over the latent variables where
cref:fig-expert-gps-mcycle-two-experts-tight,fig-expert-gps-mcycle-two-experts-further
show the GP posteriors over each expert's latent function $q(\mode{\latentFunc}(\x_*))$.
cref:fig-gating-gps-mcycle-two-experts-tight,fig-gating-gps-mcycle-two-experts-further
show the GP posteriors over the latent gating functions $q(\mode{\gatingFunc}(\x_*))$
and cref:fig-mixing-probs-mcycle-two-experts-tight,fig-mixing-probs-mcycle-two-experts-further
show the mixing probabilities associated with the probability mass function over the expert
indicator variable $\modeVar$.
# These figures highlight differences between the two lower bounds,
# in particular, how they represent the uncertainty in the gating network.
# As our variational inference scheme couples the learning of the experts and the gating network
# it is interesting to see their impact on the posteriors over the latent variables.

The lengthscale of the gating network kernel governs how fast the model can shift responsibility from
expert one to expert two.
For both lower bounds
the distribution over the expert indicator variable tends to a uniform distribution (maximum entropy)
at $x \geq 1.5$.
Optimising with both bounds resulted in expert one learning a
long lengthscale to fit the horizontal line from $-2$ to $-1$ and
expert two learning a shorter lengthscale function to fit the wiggly section from $-0.5$ to $1.2$.
The noise variance inferred by expert one is larger for $\furtherBoundTwo$ than for $\furtherBound$.
The uncertainty in the experts' latent functions is also higher for $\furtherBoundTwo$.
This is because $\furtherBoundTwo$ is attempting to fit both experts to the entire data set and only
mixes their noise models.
In contrast, $\furtherBound$ fits each expert only in the regions where the gating network has
assigned it responsibility.

# This is in contrast to the tight lower bound $\tightBound$, whose distribution over the expert indicator variable
# assigns responsibility to expert two and represents the uncertainty in the
# GP posterior over the gating functions, instead of the posterior over the mode indicator variable.

# \todo{to make this point I need to need to train bounds with some missing data and see what happens?}
# The gating network serves as a probabilistic decision boundary that is dependent on both the mean and
# the variance of the GPs over the gating functions $\gatingFunc$.
# As such, the model loses a degree of freedom, w.r.t. interpretability, making it difficult to
# interpret the meaning of $\Pr(\alpha_*=k \mid \mathbf{x}_*, \mathcal{D}, \bm\phi)= 0.5$.
# It could mean that it is confident that the observations were generated by a 50:50 mixture of the two experts.
# Alternatively, the variance of $p(h_*  \mid  \mathbf{x}_*, \mathcal{D}, \bm\phi)$
# may be high indicating that the gating function can not be confident making a prediction at $\mathbf{x}_*$.
# It could also indicate that the model is confident that neither expert explains the data well
# and so the optimisation has set the mixing probability to $0.5$ to prevent either experts
# "fit" degrading.
# For example, if the model needs a third expert.


# This is due to the gating network sharing the responsibility at $x \geq 1.5$ and expert one increasing its
# noise variance to help explain away the data.

**** two expert latent fig :ignore:
#+BEGIN_EXPORT latex
%\begin{figure}[t!]
\begin{figure}[hbt!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/experts_f.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
\subcaption{$\furtherBound$, experts' GP posteriors}
\label{fig-expert-gps-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L3/experts_f.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
\subcaption{$\furtherBoundTwo$, experts' GP posteriors}
\label{fig-expert-gps-mcycle-two-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/gating_gps.pdf}
%\subcaption{Gating function posteriors $\predictiveGatingPrior$ with $\ModeInd=2$.}
\subcaption{$\furtherBound$, gating network's GP posteriors}
\label{fig-gating-gps-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L3/gating_gps.pdf}
%\subcaption{Gating function posteriors $\predictiveGatingPrior$ with $\ModeInd=3$.}
\subcaption{$\furtherBoundTwo$, gating network's GP posteriors}
\label{fig-gating-gps-mcycle-two-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/gating_mixing_probs.pdf}
%\subcaption{Gating network's predictive mixing probabilities $\predictiveProb$ with $\ModeInd=2$.}
\subcaption{$\furtherBound$, mixing probabilities}
\label{fig-mixing-probs-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L3/gating_mixing_probs.pdf}
%\subcaption{Gating network's predictive mixing probabilities $\predictiveProb$ with $\ModeInd=3$.}
\subcaption{$\furtherBoundTwo$, mixing probabilities}
\label{fig-mixing-probs-mcycle-two-experts-further}
\end{minipage}
\caption{Visualisation of the model's latent variables (instantiated with $\ModeInd=2$ experts), after training on the
Motorcycle data set \citep{Silverman1985}, with $\furtherBound$ (left column)
and with $\furtherBoundTwo$ (right column).
(\subref{fig-expert-gps-mcycle-two-experts-tight}-\subref{fig-expert-gps-mcycle-two-experts-further}) show the GP
posteriors associated with the experts' latent functions $q(\mode{\latentFunc}(\x_*))$,
where the solid lines show the mean and the shaded regions show the $95\%$ confidence intervals, i.e. $\pm2\sigma$.
The gating network's GP posteriors $q(h_{k}(\x_*))$ are shown in
(\subref{fig-gating-gps-mcycle-two-experts-tight}-\subref{fig-gating-gps-mcycle-two-experts-further}) and
their associated mixing probabilities $\approxPredictiveProb$ in
(\subref{fig-mixing-probs-mcycle-two-experts-tight}-\subref{fig-mixing-probs-mcycle-two-experts-further}).}
\label{fig-latent-mcycle-two-experts}
\end{figure}
\clearpage
#+END_EXPORT

**** three expert y fig :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/y_means.pdf}
\subcaption{$\furtherBound$, poseterior mean and data set}
\label{fig-y-means-three-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L3/y_means.pdf}
\subcaption{$\furtherBoundTwo$, poseterior mean and data set}
\label{fig-y-means-three-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/y_samples.pdf}
\subcaption{$\furtherBound$, posterior samples}
\label{fig-y-samples-mcycle-three-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L3/y_samples.pdf}
\subcaption{$\furtherBoundTwo$, posterior samples}
\label{fig-y-samples-mcycle-three-experts-further}
\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L1/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-two-experts}
%\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-three-experts}
%\end{minipage}
\caption{Visualisation of the model instantiated with $\ModeInd=3$ experts, after training on the motorcycle data set \citep{Silverman1985} with $\furtherBound$ (left column) and with $\furtherBoundTwo$ (right column).  (\subref{fig-y-means-three-experts-tight}-\subref{fig-y-means-three-experts-further}) show the data set (black crosses) and the posterior means associated with the \acrshort{mosvgpe} (black solid line) and the SVGP (red dashed line) for comparison. (\subref{fig-y-samples-mcycle-three-experts-tight}-\subref{fig-y-samples-mcycle-three-experts-further}) show samples from the \acrshort{mosvgpe} posterior where colour indicates the underlying expert (cyan $K=1$, magenta $\ModeInd=2$, yellow $\ModeInd=3$) and the red lines show the $\pm 2$ standard deviation error ($95\%$ confidence interval) of the SVGP posterior. All experiments were trained with $25,000$ iterations of Adam \citep{kingmaAdam2017} using a learning rate of $0.01$ and a minibatch size of $\NumData_b=16$. All GPs use Squared Exponential kernels and $M=32$ inducing points.}
\label{fig-y-mcycle-three-experts}
\end{figure}
#+END_EXPORT


# #+BEGIN_EXPORT latex
# \begin{figure}[t!]
# \centering
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/two-experts/y_samples.pdf}
# \subcaption{}
# \label{fig-y-samples-mcycle-two-experts}
# \end{minipage}
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/three-experts/y_samples.pdf}
# \subcaption{}
# \label{fig-y-samples-mcycle-three-experts}
# \end{minipage}
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/two-experts/y_samples_density.pdf}
# \subcaption{}
# \label{fig-y-samples-mcycle-two-experts}
# \end{minipage}
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/three-experts/y_samples_density.pdf}
# \subcaption{}
# \label{fig-y-samples-mcycle-three-experts}
# \end{minipage}
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/two-experts/y_means.pdf}
# %\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
# \subcaption{}
# \label{fig-gating-gps-mcycle-two-experts}
# \end{minipage}
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/three-experts/y_means.pdf}
# %\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
# \subcaption{}
# \label{fig-gating-gps-mcycle-three-experts}
# \end{minipage}
# \caption{Visualisation of the model after training on the motorcycle data set with two experts (left column) and  three experts (right column). (a-b) show the GP posteriros associated with the gating funcions, $q(h_{k}(\state_*))$, where the solind lines show the means $\E[h_{k}(\state_*)]$ and the shaded regions show the 95\% confidence interval, i.e. $\pm2\simga$. (c-d) show the predictive mixing probabilities $\predictiveProb$.}
# \label{fig-gating-network-mcycle-subset}
# \end{figure}
# #+END_EXPORT

**** Three Experts
# Given that the two lower bounds lead to different ways for explaining the data
# at $x \geq 1.5$, it is interesting to consider adding a third expert.
The model was then instantiated with three experts $\ModeInd=3$ and trained
following the same procedure as the two experts' experiments,
i.e. with the same initial model and training parameters.
The results are show in cref:fig-y-mcycle-three-experts,
where the top row visualises the predictive mean and
the bottom row the predictive density, for $\furtherBound$ (left column) and $\furtherBoundTwo$ (right column).
cref:fig-latent-mcycle-three-experts then visualises the posteriors over the latent variables associated with
each model/bound combination.

From cref:tab-mcycle-metrics, it is clear that the predictive posterior associated with $\furtherBound$
is the most accurate as it obtained the highest NLPP score.
As expected, the two lower bounds explain the data completely differently.
Instantiating the model with three experts $\ModeInd=3$ and training with $\furtherBound$,
leads to the extra expert fitting to the data at $x \geq 1.5$ and the gating network assigning responsibility to it
in this region.
In contrast, instantiating the model with three experts $\ModeInd=3$ and training with
$\furtherBoundTwo$, results in the gating network never using the extra expert.
Similar to the two expert case, the distribution over the expert indicator variable at $x \geq 1.5$
tends to a uniform distribution (maximum entropy).

In cref:fig-expert-gps-mcycle-three-experts-tight the third expert's posterior returns to the prior at
$x \geq 1.5$.
This demonstrates that not only is the gating network turning the experts "on" and "off" in different regions
but the model is also exhibiting data assignment behaviour.
That is, each expert appears to only be fitting to the observations in the regions where the gating network
has assigned it responsibility.
In our case, this behaviour is achieved via the inducing variables capturing the joint distribution over the
experts and the set of assignments, i.e. implicitly assigning data points to experts.

# This demonstrates the flexibility of \acrshort{mosvgpe} to assign data points "softly" via the inducing variables.

**** three expert latent fig :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[hbt!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/experts_f.pdf}
\subcaption{$\furtherBound$, experts' GP posteriors}
\label{fig-expert-gps-mcycle-three-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L3/experts_f.pdf}
\subcaption{$\furtherBoundTwo$, experts' GP posteriors}
\label{fig-expert-gps-mcycle-three-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/gating_gps.pdf}
\subcaption{$\furtherBound$, gating network's GP posteriors}
\label{fig-gating-gps-mcycle-three-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L3/gating_gps.pdf}
\subcaption{$\furtherBoundTwo$, gating network's GP posteriors}
\label{fig-gating-gps-mcycle-three-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/gating_mixing_probs.pdf}
\subcaption{$\furtherBound$, mixing probabilities}
\label{fig-mixing-probs-mcycle-three-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L3/gating_mixing_probs.pdf}
%\subcaption{Gating network's predictive mixing probabilities $\predictiveProb$ with $\ModeInd=3$.}
\subcaption{$\furtherBoundTwo$, mixing probabilities}
\label{fig-mixing-probs-mcycle-three-experts-further}
\end{minipage}
\caption{Visualisation of the model's latent variables (instantiated with $\ModeInd=3$ experts), after training on the
Motorcycle data set \citep{Silverman1985}, with $\furtherBound$ (left column)
and with $\furtherBoundTwo$ (right column).
(\subref{fig-expert-gps-mcycle-three-experts-tight}-\subref{fig-expert-gps-mcycle-three-experts-further}) show the GP
posteriors associated with the experts' latent functions $q(\mode{\latentFunc}(\state_*))$,
where the solid lines show the mean and the shaded regions show the $95\%$ confidence intervals, i.e. $\pm2\sigma$.
The gating network's GP posteriors $q(h_{k}(\state_*))$ are shown in
(\subref{fig-gating-gps-mcycle-three-experts-tight}-\subref{fig-gating-gps-mcycle-three-experts-further}) and
their associated mixing probabilities $\approxPredictiveProb$ in
(\subref{fig-mixing-probs-mcycle-three-experts-tight}-\subref{fig-mixing-probs-mcycle-three-experts-further}).}
\label{fig-latent-mcycle-three-experts}
\end{figure}
\clearpage
#+END_EXPORT

**** Batch Size vs Number of Inducing Points
# This section provides a brief evaluation on the effect of the number of inducing points and batch size.

*Batch size*
One of the main benefits of the variational inference scheme presented in this chapter, is that the bound can be
calculated with minibatches of a data set $\dataset$ and used as the objective for stochastic gradient descent.
Decreasing the batch size increases the stochasticity in the bound and leads to convergence in less evaluations of
the ELBO.
This is evident in cref:fig-mcycle-training-curve, which compares the negative ELBO for different
numbers of inducing points and batch sizes.
Further to this, computing the ELBO is less computationally demanding for smaller batch sizes.
However, if the batch size is made too small, then the optimisation can become unstable and prevent the optimiser
from finding a good solution.
The (blue) learning curve for batch size $N_b=32$ in cref:fig-mcycle-training-curve-133 shows an example of this behaviour.
In this case, the learning rate had to be made smaller, leading to slower convergence.
This is shown by the orange learning curve, which has not been able to reach the same negative ELBO as the other batch
sizes.
This is most likely due to the lower learning rate.
This interplay between the batch size and learning rate is well known in machine learning.
# Setting the batch size and learning rate is known to be awkward due to their interplay.

*Number of inducing points*
Our variational inference scheme models the joint distribution over the data and assignments via the inducing variables
($\expertsInducingOutput$ and $\gatingsInducingOutput$).
In practice, the number of inducing points should be less than the number
of data points $(\NumInducing \ll \NumData)$, to obtain improved computational performance.
The learning curves in cref:fig-mcycle-training-curve visualise the learning process for different numbers
of inducing points $\NumInducing$.
Best performance is obtained when the model is instantiated with $\NumInducing=133$ inducing inputs,
i.e. a one-to-one correspondence between inducing inputs and
data inputs, $\expertInducingInput=\gatingInducingInput=\allInput$.
As the number of inducing points decreases the model is still able to recover the same negative ELBO.

\todo{add results for num inducing points leading to worse performance e.g. M=8}

*Evidence lower bounds*
The tight lower bound $\tightBound$ and further lower bound $\furtherBound$ recovered similar results in
all experiments.
This indicates that $\furtherBound$ does not loosen the bound to a point where it loses valuable information.
In contrast, $\furtherBoundTwo$ is not able to recover the same results.
This was expected as $\furtherBoundTwo$ corresponds to a further likelihood approximation, where
the experts' noise models are mixed instead of their full SVGPs.
$\furtherBound$ offers a rich ELBO for optimising \acrshort{mosvgpe} that achieves similar results to $\tightBound$,
whilst having lower computational complexity per evaluation.
For this reason, the remainder of this dissertation uses $\furtherBound$ for all experiments.

**** two vs three expert y fig :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_means.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-y-means-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/y_means.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-y-means-mcycle-two-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_samples.pdf}
\subcaption{}
\label{fig-y-samples-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/y_samples.pdf}
\subcaption{}
\label{fig-y-samples-mcycle-two-experts-further}
\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L1/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-two-experts}
%\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-three-experts}
%\end{minipage}
\caption{Visualisation of the model instantiated with $\ModeInd=2$ experts, after training on the motorcycle data set \citep{Silverman1985} with $\tightBound$ (left column) and with $\furtherBound$ (right column).  \subref{fig-y-means-mcycle-two-experts-tight}-\subref{fig-y-means-mcycle-two-experts-further} show the data set (black crosses) and the posterior means associated with the \acrshort{mosvgpe} (black solid line) and a SVGP (red dashed line) for comparison. \subref{fig-y-samples-mcycle-two-experts-tight}-\subref{fig-y-samples-mcycle-two-experts-further} show samples from the \acrshort{mosvgpe} posterior where colour indicates the underlying expert (cyan $K=1$, magenta $\ModeInd=2$) and the red lines show the $\pm 2$ standard deviation error ($95\%$ confidence interval) of the SVGP posterior. All experiments were trained with $25,000$ iterations of Adam \citep{kingmaAdam2017} using a learning rate of $0.01$ and a minibatch size of $\NumData_b=16$. All GPs use Squared Exponential kernels and $M=32$ inducing points.}
\label{fig-y-mcycle-two-vs-three-experts}
\end{figure}
#+END_EXPORT

# #+caption: \textbf{Motorcycle data} The left plot shows the observations (red crosses) together with the means of our model (black solid line) and a SVGP with noise (blue dashed line). It also shows probabilities assocaited with the gating network for $k=1$ (green line) and $k=2$ (blue line) on the right axis. The right plot shows 100 samples (blue) from the predictive posterior at 200 evenly spaced input locations as well as the $\pm 2$ std error (95% confidence interval) of the SVGP (solid red line). Both models use squared exponential covariance functions for all GPs.

**** two vs three expert latent fig :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/experts_f.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-gating-gps-mcycle-two-experts}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/experts_f.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-gating-gps-mcycle-three-experts}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/gating_gps.pdf}
%\subcaption{Gating function posteriors $\predictiveGatingPrior$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-gating-gps-mcycle-two-experts}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/gating_gps.pdf}
%\subcaption{Gating function posteriors $\predictiveGatingPrior$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-gating-gps-mcycle-three-experts}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/gating_mixing_probs.pdf}
%\subcaption{Gating network's predictive mixing probabilities $\predictiveProb$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-gating-mixing-probs-mcycle-two-experts}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/gating_mixing_probs.pdf}
%\subcaption{Gating network's predictive mixing probabilities $\predictiveProb$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-gating-mixing-probs-mycle-three-experts}
\end{minipage}
\caption{Visualisation of the model after training on the motorcycle data set with two experts (left column) and  three experts (right column). (a-b) show the GP posteriros associated with the gating funcions, $q(h_{k}(\state_*))$, where the solind lines show the means $\E[h_{k}(\state_*)]$ and the shaded regions show the 95\% confidence interval, i.e. $\pm2\simga$. (c-d) show the predictive mixing probabilities $\predictiveProb$.}
\label{fig-gating-network-mcycle-subset}
\end{figure}
#+END_EXPORT

**** training loss fig :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/training_curves/training_loss_num_ind_133.png}
\subcaption{}
\label{fig-mcycle-training-curve-133}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/training_curves/training_loss_num_ind_64.png}
\subcaption{}
\label{fig-mcycle-training-curve-64}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/training_curves/training_loss_num_ind_32.png}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-mcycle-training-curve-32}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/training_curves/training_loss_num_ind_16.png}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-mcycle-training-curve-16}
\end{minipage}
\caption{Training curves showing the negative ELBO (- $\furtherBound$) vs step when training
\acrshort{mosvgpe} (instantiated with two experts $\ModeInd=2$), on the
Motorcycle data set \citep{Silverman1985} with different numbers of inducing points $M$ and different
batch sizes $\NumData_b$. All experiments used the Adam optimiser \citep{kingmaAdam2017} with a learning rate
of $0.01$ and Squared Exponential kernels for all GPs.}
\label{fig-mcycle-training-curve}
\end{figure}
#+END_EXPORT

**** End :ignore:
\newpage

*** Evaluation on Velocity Controlled Quadcopter label:sec-brl-experiment
**** intro :ignore:
# The goal of this dissertation is to control a DJI Tello quadcopter in an indoor environment
# subject to two modes of operation characterised by turbulence.
In order to verify that \acrshort{mosvgpe} works on real-world systems,
it was tested on a real-world quadcopter data set following the illustrative example detailed
in cref:illustrative_example.
The data set was collected at the Bristol Robotics Laboratory using
a velocity controlled DJI Tello quadcopter and a Vicon tracking system.
A high turbulence dynamics mode was induced by placing a desktop fan at the right side of a room.
cref:fig-quadcopter-environment shows a diagram of the environment.
The data set represents samples from a dynamical system with constant controls, i.e.
$\Delta\state_{\timeInd} = \latentFunc(\state_{\timeInd-1};\control_{\timeInd-1}=\control_*)$.
# The resulting data set has two-dimensional inputs and two-dimensional outputs, making it easy to visualise
# the different components of the model.
#+BEGIN_EXPORT latex
\begin{figure}[h]
\centering
\begin{minipage}[r]{0.55\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/brl-quadcopter-domain-figure.png}
\subcaption{Diagram showing a top down view of the environment (a room in the Bristol Robotics Laboratory).}
\label{fig-quadcopter-environment}
\todo{add better diagram of environment}
\end{minipage}
\begin{minipage}[r]{0.44\columnwidth}
%\includegraphics[width=\textwidth]{./images/quiver_step_20_direction_down.png}
\includegraphics[width=\textwidth]{./images/model/quadcopter/subset-10/dataset_quiver.pdf}
\subcaption{Quiver plot showing the data set of state transitions from 9 trajectories flown 7 times.}
\label{fig-quiver}
\end{minipage}
\caption{Illustration of (\subref{fig-quadcopter-environment}) the environment (a room in the Bristol Robotics Laboratory) and (\subref{fig-quiver}) the data set of state transitions. A turbulent dynamics mode is induced by a desk top fan at the right hand side of the room and a subset of the enivornment has not been observed.}
\end{figure}
#+END_EXPORT



*Environment* The environment is modelled with two dimensions
(the $x$ and $y$ coordinates), which is a realistic assumption,
as altitude control can be achieved with a separate controller.
The state space is then the 2D coordinates $\state = [x, y]$ and the control is simply the velocity
$\control = [\dot{x}, \dot{y}]$.

*Data collection* The Vicon system provided access to the true position of the quadcopter at all times, which
enabled pre-planned trajectories to be flown, using a simple PID controller on
feedback from the Vicon system.
To simplify data collection,
nine trajectories from $y=2$ to $y=-3$, with different initial $x$ locations,
were used as target trajectories to be tracked by the PID controller.
Each trajectory was repeated 7 times to capture the variability (process noise) in the dynamics.
# To simplify data collection and visualisation, the controls were kept constant during data
# collection, reducing the dynamics to
# ${\Delta \state_\numData= f(\state_{\numData-1}; \control=\control_{\text{fixed}}) + \epsilon}$.

# To simplify data collection and visualisation, the controls were kept constant during data
# collection, reducing the dynamics to
# ${\Delta \state_\numData= f(\state_{\numData-1}; \control=\control_{\text{fixed}}) + \epsilon}$.
# Data from multiple trajectories following the same target trajectory were collected to
# capture the variability (process noise) in the dynamics.

# Nine trajectories (with different starting states but the same target velocity) were used as
# target trajectories
# and repeated
# multiple times
# from multiple trajectories following the same target trajectory were collected to
# capture the variability (process noise) in the dynamics.

*Data processing* The Vicon stream recorded data at 100Hz, which was then down sampled to
give a time step of $\Delta t = 0.1s$.
This reduced the size of the data set and left reasonable lengthscales.
The data set consists of $\NumData=2081$ state transitions.
\todo{update data set size}
cref:fig-quiver visualises the state transition data set as a quiver plot.
# cref:fig-quiver is a quiver plot showing the resulting data set.

**** Results
The model was instantiated with two experts, with the goal of each expert learning a separate dynamics mode and the
gating network learning a representation of how the underlying dynamics modes vary over the state space.
The model was trained using the model and training parameters in cref:tab-params-quadcopter.

# #+NAME: fig-gating-mixing-probs-quadcopter-subset
# #+ATTR_LATEX: :width 1\textwidth :placement [hbt!] :center t
# #+caption: Gating network's predictive mixing probabilties $\predictiveProb$ after training on the quadcopter data set.
# [[file:./images/model/quadcopter/subset/gating_mixing_probs.pdf]]
# #+NAME: fig-gating-gps-quadcopter-subset
# #+ATTR_LATEX: :width 1\textwidth :placement [hbt!] :center t
# #+caption: Gating network's predictive posterior $\predictiveGatingPrior$ after training on the quadcopter data set. Each row corresponds to a single GP posterior, $q(h_{k}(\state_*))$, corresponding to expert $k$. The mean $\E[h_{k}(\state_*)]$ is on the left and the variance $\V[h_{k}(\x_*)]$ is on the right.
# [[file:./images/model/quadcopter/subset/gating_gps.pdf]]

#+NAME: fig-y-mm-quadcopter-subset
#+ATTR_LATEX: :width 1\textwidth :placement [hbt!] :center t
#+caption: Moment matched predictive posterior $p(\Delta \state_* \mid \x_*)$ after training on the quadcopter data set. Each row corresponds to an output dimension $d$ where the left plot shows the moment matched mean $\E[\Delta \state_d]$ and the right plot shows the moment matched variance $\V[\Delta \state_d]$.
[[file:./images/model/quadcopter/subset-10/y_moment_matched.pdf]]

#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{1.0\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/quadcopter/subset-10/gating_mixing_probs.pdf}
\subcaption{Posterior over mode indicator variable.}
\label{fig-gating-mixing-probs-quadcopter-subset}
\end{minipage}
\begin{minipage}[r]{1.0\textwidth}
\includegraphics[width=\textwidth]{./images/model/quadcopter/subset-10/gating_gps.pdf}
\subcaption{GP posteriors over gating functions.}
\label{fig-gating-gps-quadcopter-subset}
\end{minipage}
\caption{Visualisation of the gating network after training on the quadcopter data set. The plots in (\subref{fig-gating-mixing-probs-quadcopter-subset}) show the predictive mixing probabilities $\predictiveProb$ for Expert 1 (left) and Expert 2 (right). The plots in (\subref{fig-gating-gps-quadcopter-subset}) show the predictive GP posteriors $q(h_{k}(\singleTestInput))$ associated with Expert 1 (top) and Expert 2 (bottom). The left hand plots show the means and the right hand plots show the variances.}
\label{fig-gating-network-quadcopter-subset}
\end{figure}
#+END_EXPORT

#+NAME: fig-experts-f-quadcopter-subset
#+ATTR_LATEX: :width 1\textwidth :placement [hbt!] :center t
#+caption: Visualisation of the experts' predictive posteriors $\predictiveExpertsPrior$ after training on the quadcopter data set. Each row corresponds to a single GP posterior, $q(f_{kd}(\singleTestInput))$, corresponding to dimension $d$ of expert $\modeInd$. The mean $\E[f_{kd}(\singleTestInput)]$ is on the left and the variance $\V[f_{kd}(\x_*)]$ is on the right. The noise variances learned by Expert 1 and Expert 2 were $\Sigma_1 = \diag\left([0.0063, 0.0259]]\right)$ and $\Sigma_2 = \diag\left([0.0874, 0.0432]\right)$ respectively.
[[file:./images/model/quadcopter/subset-10/experts_f.pdf]]


# #+NAME: fig-experts-y-quadcopter-subset
# #+ATTR_LATEX: :width 1\textwidth :placement [hbt!] :center t
# #+caption: Experts' predictive posterior over the output $q(\Delta\state_{kd} \mid \state_*)$ after training on the quadcopter data set. Each row corresponds to a single GP posterior, $q(\Delta\state_{kd} \mid \state_*)$, corresponding to dimension $d$ of expert $k$. The mean $\E[\Delta\state_{kd}\mid \state_*]$ is on the left and the variance $\V[\Delta\state_{kd} \mid \x_*]$ is on the right.
# [[file:./images/model/quadcopter/subset/experts_y.pdf]]

At a new input location $\singleTestInput$ the density over the output,
$p(\singleTestOutput \mid \singleTestInput)$,
follows a mixture of $\ModeInd$ Gaussians.
Visualising a mixture of two Gaussians with a two-dimensional input space and a two-dimensional output space
requires the components and mixing probabilities to be visualised separately.
To aid with visualisation, Figure ref:fig-y-mm-quadcopter-subset shows the predictive density
approximated as a unimodal Gaussian density (via moment matching), where each row corresponds to an
output dimension.
The predictive mean is fairly constant over the domain, except for the region in front of the fan, where it is
higher.
This result makes sense as the data set was assumed to be collected with constant controls.
The region with high predictive mean in front of the fan, is modelling the drift arising from the fan blowing the quadcopter in the negative $x$ direction.
The right hand plots of Figure ref:fig-y-mm-quadcopter-subset
show the predictive variance. It is high where there are no training observations,
indicating that the method has successfully represented the model's /epistemic uncertainty/.
It is also high in the region in front of the fan, showing that the model has successfully inferred
the high process noise, associated with the turbulence induced by the fan.
Let us now visualise the individual experts and the gating network separately.
# As the data set was assumed to have constant controls, this result aligns with our knowledge of the environment.
# That is, the dynamics should be constant
# dynamics should be constant

*Gating network*
Figure ref:fig-gating-network-quadcopter-subset shows the gating network after training on the
data set.
Figure ref:fig-gating-mixing-probs-quadcopter-subset indicates that the model has assigned responsibility
to Expert 2 in front of the fan, as its mixing probability
$\Pr(\singleTestModeVar=1 \mid \singleTestInput)$ is high in this region.
This implies that Expert 2 represents the turbulent dynamics mode in front of the fan.
Figure ref:fig-gating-gps-quadcopter-subset shows the GP posteriors associated with the gating functions.
The mean of the gating function associated with Expert 1 $\E[h_{1}(\singleTestInput)]$ is
high in the low-turbulence regions and low in the high-turbulence region in front of the fan.
The posterior variance associated with the gating function GPs is high in the region with no training
observations. This is a desirable behaviour because it is modelling the /epistemic uncertainty/.
These results demonstrate that the gating network infers important information regarding how the
system switches between dynamics modes over the input space.
# The mean also tends to zero where the model has had no training observations.

*Identifiability*
These results show that the GP-based gating network is capable of turning a single expert
on in multiple regions of the input space.
This is a desirable behaviour as it has enabled only two underlying dynamics modes to be identified.
In contrast, other MoGPE methods may have assigned an extra expert to one of the regions modelled by Expert 1.
In particular, the regions at $y>0$ and $y<-1$ may have been assigned to separate experts.

*Latent spaces for control*
The gating network consists of two spaces which are rich with information regarding how the
system switches between dynamics modes.
Firstly, the pmf over the expert indicator variable.
Secondly, the GP posteriors over the gating functions.
It is worth noting that all MoGPE methods obtain a pmf over the expert indicator variable and
this space suffers from interpretability issues.
Consider the meaning of the mixing probabilities tending to a uniform
distribution ${\Pr(\singleTestModeVarK \mid \singleTestInput)=0.5}$.
This corresponds to maximum entropy for a categorical distribution and could mean two different things.
It could mean that,
1) the model has training data in this region, so can confidently predict, but is unsure which expert is responsible,
   - perhaps the observations do not belong to any expert and an extra expert is required,
2) the model does not have training data in this region, so cannot confidently predict which expert is responsible.
# 2) the model has training data in this region, so can confidently predict and is confident that the observations are generated by an equal mixture of the experts,
# It could mean that the model is confident that the observations are generated by an equal mixture of the experts,
# or it could mean that the gating network is not confident making predictions in this region.
This interpretability issue is overcome by our GP-based gating network, as these two cases are modelled differently.
Either the gating function(s) are all equal and their posterior variance(s) are low, implying that the gating network
can predict confidently but is unsure which expert is responsible.
This could mean that an extra expert is responsible for predicting at this location.
Alternatively, the gating functions' posterior variance(s) could be high, implying
that the model is not confident in predicting which expert is responsible at the given input location.
Importantly, the GP posteriors associated with our gating network, not only infer information regarding the
mode switching, but also model the gating network's /epistemic uncertainty/.
These GP posteriors provide convenient latent spaces for control and
are exploited by the trajectory optimisation algorithms presented later in this dissertation.

*Experts*
Figure ref:fig-experts-f-quadcopter-subset shows the predictive posteriors $q(\latentFunc_{kd}(\singleTestInput))$
associated with each dimension $d$ of each expert $\modeInd$.
The method has successfully learned a factorised representation of the underlying dynamics, where
Expert 1 has learned a dynamics mode with low process noise
$\Sigma_1 = \diag\left([0.0063, 0.0259]]\right)$
and Expert 2 a mode with high process noise
$\Sigma_2 = \diag\left([0.0874, 0.0432]\right)$.
Expert 2 has also clearly learned the drift induced by the fan, indicated by the dark red region at $y=0$
in the two bottom left plots of Figure ref:fig-experts-f-quadcopter-subset.
It has also learned the control response of the PID controller correcting for the deviation
from the reference trajectory, indicated by the white region below $y=0$.
The control response is an artifact of the data collection process.
It is clear that Expert 2 has learned both the drift and process noise terms
associated with the turbulent dynamics mode.

Both experts were initialised with independent inducing inputs, $\expertInducingInput$, providing the model
flexibility to "soft" partition the data set.
That is, each expert has the freedom to set its inducing inputs, $\expertInducingInput$,
to support only a subset of the data set.
The posterior (co)variance associated with each expert represents their /epistemic uncertainty/.
The top right plot in Figure ref:fig-experts-f-quadcopter-subset shows the posterior variance associated with
the $x$ dimension of Expert 1.
The posterior variance is high in front of the fan because the gating network has assigned responsibility to the
other expert in this region.
It is also high in the region where the model has had no training observations, as would be expected.
However, the posterior variance associated with the $y$ dimension of Expert 1, is not high in this region.
This is due to the lengthscale of the second output dimension allowing Expert 1 to confidently extrapolate.
\todo{is this because of the lengthscale or is it due to gating network}

The bottom right two plots in Figure ref:fig-experts-f-quadcopter-subset show the posterior variance
associated with the $x$ and $y$ dimensions of Expert 2.
The posterior variance is high everywhere except for the region in front of the fan.
Again, this is due to the gating network assigning responsibility to the other expert outside of the region in
front of the fan.
It is clear from these results that the likelihood approximation in cref:eq-likelihood-approximation, combined with our gating network
and variational inference scheme, are capable of modelling the assignment of observations to experts via
the inducing points.




# *Gating Network*
# Figure ref:fig-gating-network-quadcopter-subset shows the gating network after training on the
# data set.
# Figure ref:fig-gating-mixing-probs-quadcopter-subset indicates that the model has assigned responsibility
# to expert 2 in front of the fan as its mixing probability
# $\Pr(\singleTestModeVar=1 \mid \singleTestInput)$ is high in front of the fan, i.e. the high-turbulence region.
# The mixing probabilities tend to a uniform distribution
# ${\Pr(\singleTestModeVarK \mid \singleTestInput)=0.5}$
# in the region with no training observations.
# This corresponds to maximum entropy for categorical distributions and is a desirable behaviour.
# However, it is worth noting that the mixing probabilities can tend to a uniform distribution for multiples reasons.
# It could mean that the model is confident that the observations are generated by an equal mixture of the experts
# i.e. the gating function(s) are all equal and their posterior variance(s) are low.
# Alternatively, it could mean that the model is not confident in predicting which expert is responsible at the given
# input location i.e. the variance(s) of the GP posterior(s) associated with the gating function(s) could be high.


# # It is worth noting that the mixing probabilities lose a degree of freedom which makes it difficult to
# # interpret the meaning of $\Pr(\modeVar_*=\modeInd \mid \state_*= 0.5$.
# # It could mean that it is confident that the observations were generated by a 50:50 mixture of the two experts
# # i.e. the gating functions are all equal to zero and the posterior variance is low.
# # Alternatively, the variance of the gating function GP posterior could be high, resulting in the probability tending to
# # maximum entropy.

# Figure ref:fig-gating-gps-quadcopter-subset shows the GP posteriors associated with the gating functions.
# The mean of the gating function associated with expert 1 $\E[h_{1}(\singleTestInput)]$ is
# high in the low-turbulence regions and low in front of the fan, i.e. the high-turbulence mode.
# The mean also tends to zero where the model has had no training observations.
# The posterior variance is also high in this region, indicating that the gating
# network GPs have successfully modelled the /epistemic uncertainty/.
# Exploiting a GP-based gating network has clearly provided a principled approach to distinguishing
# between the different reasons the mixing probabilities may tend to a uniform distribution.





# The posterior variance is high where the model has not observed
# the system, indicating that the gating network GPs successfully infer the epistemic uncertainty when using the
# inference scheme in Section ref:sec-inference.
# Formulating the gating network based on GPs has clearly provided a principled approach to distinguishing
# between the different reasons the mixing probabilities may tend to a uniform distribution.

# The predictive posterior is mixture of $\ModeInd$ Gaussian resulting from combining the experts according to
# the gating network.

*** Evaluation on Simulated Quadcopter Data Set :noexport:
#+BEGIN_EXPORT latex
\newcommand{\roll}{\ensuremath{\phi}}
\newcommand{\pitch}{\ensuremath{\theta}}
\newcommand{\yaw}{\ensuremath{\psi}}

\newcommand{\world}{\ensuremath{w}}
\newcommand{\body}{\ensuremath{b}}
\newcommand{\intermediate}{\ensuremath{c}}
\renewcommand{\frame}{\ensuremath{F}}
\newcommand{\worldFrame}{\ensuremath{\frame_\world}}
\newcommand{\bodyFrame}{\ensuremath{\frame_\body}}
\newcommand{\intermediateFrame}{\ensuremath{\frame_\intermediate}}

\newcommand{\inertiaMatrix}{\ensuremath{J}}
\newcommand{\inertiaZ}{\ensuremath{I_{zz}}}
\newcommand{\zw}{\ensuremath{\mathbf{z}_\world}}

\newcommand{\positions}{\ensuremath{\mathbf{h}}}
\newcommand{\velocity}{\ensuremath{v}}
\newcommand{\velocityx}{\ensuremath{\dot{x}}}
\newcommand{\velocityy}{\ensuremath{\dot{y}}}
\newcommand{\Velocity}{\ensuremath{\mathbf{\velocity}}}

\newcommand{\rotationMatrix}{\ensuremath{\mathbf{R}}}
\newcommand{\angularVelocity}{\ensuremath{\bm\omega}}
\newcommand{\angularVelocityMatrix}{\ensuremath{\bm\Omega}}

\renewcommand{\thrust}{\ensuremath{f_{x}}}
\renewcommand{\Thrust}{\ensuremath{\mathbf{f_{x}}}}
\renewcommand{\torque}{\ensuremath{\tau_z}}

\newcommand{\quadcopterDynamics}{\ensuremath{f_{\text{quad}}}}
#+END_EXPORT
The quadcopter frames and Euler angles (roll $\roll$, pitch $\pitch$ and yaw $\yaw$) are shown
in Figure ref:fig-quadcopter-frames.
The subscripts $\world$, $\body$ and $\intermediate$ are used to denote the world
$\worldFrame$, body $\bodyFrame$ and intermediate $\intermediateFrame$ (after yaw angle rotation) frames respectively.
The the 3D nonlinear quadcopter dynamics are described with the quadcopter model from cite:wangSafe2018,zhouVector2014.

The 2D nonlinear quadcopter dynamics are based on the
state vector is given by $\state = [x, y, \velocityx, \velocityy, \yaw]$
where $\positions = [x, y]$ is the Euclidean coordinates relative to the world frame $\worldFrame$,
$\Velocity = [\velocityx, \velocityy]$ is the velocity in the world frame $\worldFrame$ and
$\yaw$ is the yaw angle, i.e. the angle around the $z$ axis.
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadcopter-dynamics}
\dot{\state} &= \quadcopterDynamics(\state, \control)
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadcopter-dynamics}
\rotationMatrix_{\body,\world} =
\begin{bmatrix}
\cos{\yaw} & -\sin(\yaw) \\
\sin{\yaw} & \cos(\yaw)
\end{bmatrix}
\end{align}
#+END_EXPORT
where,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadcopter-dynamics}
\dot{\state}_{1:2} &= \dot{\positions} = \state_{3:4} \\
\dot{\state}_{3:4} &= \dot{\Velocity} = \frac{1}{m} \rotationMatrix \Thrust \\
\dot{\state}_{5} &= \dot{\yaw} = \frac{1}{\inertiaZ} \torque
\end{align}
#+END_EXPORT
where $\Thrust = [\thrust, 0]^T$ is the total thrust force in the quadcopters from the rotors and $\torque$ is the torque on the quadcopter
around the $z$ axis of the world frame $\worldFrame$.
The thrust and torque are realistic controls for a 2D quadcopter system and gives the
control vector \control = [\thrust, \torque].

#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadcopter-dynamics}
\dot{\Velocity} &= g \zw + \frac{1}{m} \thrust \rotationMatrix \zw \\
\dot{\rotationMatrix} &= \rotationMatrix \angularVelocityMatrix \\
\dot{\angularVelocity} &= \inertiaMatrix^{-1} \torque - \inertiaMatrix^{-1} \angularVelocityMatrix \inertiaMatrix \angularVelocity \\
\dot{\positions} &= \velocity
\end{align}
#+END_EXPORT
where $g=9.81ms^{-2}$ is the acceleration due to gravity, $m$ is the mass, $\zw=[0,0,1]^T$,
$\positions=[x, y, z]^T$ is the position of the quadcopter in the world frame $\worldFrame$,
$\Velocity=[\velocity_x, \velocity_y, \velocity_z]^T$ is the
and velocity of the quadcopter in the world frame $\worldFrame$ respectively.
The angular velocity of the quadcopter in the body frame $\bodyFrame$ is denoted $\angularVelocity=[p, q ,r]^T$
and in tensor form $\angularVelocityMatrix=[[0,-p,q];[r,0,-p];[-q,p,0]]$

$\state=[x, y, z, \velocity_x, \velocity_y, \velocity_z, \roll, \pitch, \yaw, \dot{\roll}, \dot{\pitch}, \dot{\yaw}]$

$\state=[x, y, z, \velocity_x, \velocity_y, \velocity_z, \roll, \pitch, \yaw]^T$
$\state=[\ddot{x}, \ddot{y}, \ddot{z}, \dot{\roll}, \dot{\pitch}, \dot{\yaw}]^T$

\newpage
*** Point Mass 2D Dynamics :noexport:
#+BEGIN_EXPORT latex
\newcommand{\roll}{\ensuremath{\phi}}
\newcommand{\pitch}{\ensuremath{\theta}}
\newcommand{\yaw}{\ensuremath{\psi}}

\newcommand{\world}{\ensuremath{w}}
\newcommand{\body}{\ensuremath{b}}
\newcommand{\intermediate}{\ensuremath{c}}
\renewcommand{\frame}{\ensuremath{F}}
\newcommand{\worldFrame}{\ensuremath{\frame_\world}}
\newcommand{\bodyFrame}{\ensuremath{\frame_\body}}
\newcommand{\intermediateFrame}{\ensuremath{\frame_\intermediate}}

\newcommand{\inertiaMatrix}{\ensuremath{J}}
\newcommand{\inertiaZ}{\ensuremath{I_{zz}}}
\newcommand{\zw}{\ensuremath{\mathbf{z}_\world}}

\newcommand{\positions}{\ensuremath{\mathbf{h}}}
\newcommand{\velocity}{\ensuremath{v}}
\newcommand{\velocityx}{\ensuremath{\dot{x}}}
\newcommand{\velocityy}{\ensuremath{\dot{y}}}
\newcommand{\Velocity}{\ensuremath{\mathbf{\velocity}}}

\newcommand{\rotationMatrix}{\ensuremath{\mathbf{R}}}
\newcommand{\angularVelocity}{\ensuremath{\bm\omega}}
\newcommand{\angularVelocityMatrix}{\ensuremath{\bm\Omega}}

\renewcommand{\thrust}{\ensuremath{f_{x}}}
\renewcommand{\Thrust}{\ensuremath{\mathbf{f_{x}}}}
\renewcommand{\torque}{\ensuremath{\tau_z}}

\newcommand{\quadcopterDynamics}{\ensuremath{f_{\text{quad}}}}
#+END_EXPORT
cite:williamsAdvancing
cite:watsonStochastic2020
cite:watsonAdvancing2021

cite:levineVariational2013


cite:bhardwajDifferentiable2020

cite:mukadamContinuoustime2018

The dynamics of a point mass in 2D can represented with the
state vector $\state = [x, y, \velocityx, \velocityy, \yaw]$,
where $\positions = [x, y]$ denotes the 2D Euclidean coordinates relative to the world frame $\worldFrame$,
$\Velocity = [\velocityx, \velocityy]$ is the velocity in the world frame $\worldFrame$ and
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{tikzpicture}
% body frame
\draw[thick,->] (0,0) -- (1.5,1.5) node[anchor=south] {$x$};
\draw[thick,->] (0,0) -- (1.5,-1.5) node[anchor=north] {$y$};

% world frame
\draw[thick,->] (-3,-3) -- (3,-3) node[anchor=north west] {$x$};
\draw[thick,->] (-3,-3) -- (-3,3) node[anchor=south east] {$y$};
\foreach \x in {-3, -2, -1, 0,1,2,3}
   \draw (\x cm,-3) -- (\x cm,-3) node[anchor=north] {$\x$};
\foreach \y in {-3, -2, -1, 0,1,2,3}
    \draw (-3,\y cm) -- (-3,\y cm) node[anchor=east] {$\y$};
\end{tikzpicture}
\end{figure}
#+END_EXPORT


The point mass can apply a force along its $x$ axis (known as the thrust vector), which is denoted
$\Thrust = [\thrust, 0]^T$.
It can also rotate itself by applying a torque $\torque$ around its $z$ axis.
The resulting control vector for the 2D point mass is given by $\control = [\thrust, \torque]$.
The nonlinear dynamics of the system are given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadcopter-dynamics}
\dot{\state} &=
\begin{bmatrix}
    \dot{\state}_{1} \\
    \dot{\state}_{2} \\
    \dot{\state}_{3} \\
    \dot{\state}_{4} \\
    \dot{\state}_{5}
\end{bmatrix}
=
\begin{bmatrix}
    \state_{3} \\
    \state_{4} \\
    \frac{1}{m} \thrust \cos{(\yaw)} \\
    \frac{1}{m} \thrust \sin{(\yaw)} \\
    \frac{1}{\inertiaZ} \torque
\end{bmatrix}
\end{align}
#+END_EXPORT
where $m$ is the mass and $\inertiaZ$ is the moment of inertia around the vertical $z$ axis.
Defining the rotation matrix from the body frame $\bodyFrame$ to the world frame $\worldFrame$ as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadcopter-dynamics}
\rotationMatrix_{\body,\world} =
\begin{bmatrix}
\cos{\yaw} & -\sin(\yaw) \\
\sin{\yaw} & \cos(\yaw)
\end{bmatrix}
\end{align}
#+END_EXPORT
the dynamics can be calculated as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadcopter-dynamics}
\dot{\state}_{1:2} &= \dot{\positions} = \state_{3:4} \\
\dot{\state}_{3:4} &= \dot{\Velocity} = \frac{1}{m} \rotationMatrix \Thrust \\
\dot{\state}_{5} &= \dot{\yaw} = \frac{1}{\inertiaZ} \torque
\end{align}
#+END_EXPORT

The unknown dynamics can be modelled by placing GP priors on the accelerations ($\dot{\dot{x}}, \dot{\dot{y}}$) and
the angular (yaw) velocity $\dot{\yaw}$,
#+BEGIN_EXPORT latex
\renewcommand{\input}{\ensuremath{\hat{\state}}}
\begin{align} \label{eq-quadcopter-dynamics}
\dot{\state} &=
\begin{bmatrix}
    \dot{\state}_{1} \\
    \dot{\state}_{2} \\
    \dot{\state}_{3} \\
    \dot{\state}_{4} \\
    \dot{\state}_{5}
\end{bmatrix}
=
\begin{bmatrix}
    \state_{3} \\
    \state_{4} \\
    \frac{1}{m} \thrust \cos{(\yaw)} + \mathcal{N} \left( \mu_3(\input),  k_3(\input, \input) \right) \\
    \frac{1}{m} \thrust \sin{(\yaw)} + \mathcal{N} \left( \mu_4(\input),  k_4(\input, \input) \right) \\
    \frac{1}{\inertiaZ} \torque + \mathcal{N} \left( \mu_5(\input),  k_5(\input, \input) \right)
\end{bmatrix}
\end{align}
#+END_EXPORT


\newpage
** Remarks

*Implicit data assignment*
It is worth noting that in contrast to other MoGPE methods, this model does not directly assign observations to experts.
However, after augmenting each expert with separate inducing points,
the model has the flexibility to loosely /partition/ the data set.
Just as sparse GP methods can be viewed as methods that parameterise a full nonparametric GP,
our approach can be viewed as parameterising the nonparametric Mixture of Gaussian Process Experts.
Conveniently, our parameterisation, in particular the likelihood approximation in cref:eq-likelihood-approximation,
deals with the issue of marginalising exponentially many sets of assignments of observations to experts.
Evident from the results in this chapter, this likelihood approximation appears to retain important information
regarding the assigning of observations to experts, whilst efficiently marginalising the expert indicator variable.
It is also worth noting that the number of inducing points $\NumInducing$ associated with each expert,
could be set by considering the number of data points believed to belong to a particular expert.
Currently, each expert's inducing inputs are initialised by randomly sampling a subset of the data inputs.
Future work could explore different techniques for initialising each expert's inducing inputs.

*Full Bayesian treatment of inducing inputs*
Common practice in sparse GP methods is to jointly optimise the hyperparameters and the inducing inputs.
Optimising only some of the parameters, instead of marginalising all of them, is known as Type-II maximum likelihood.
In Bayesian model selection, it is well known that Type-II maximum likelihood can lead to overfitting
if the number of parameters being optimised is large.
In the case of inducing inputs, there can often be beyond hundreds or thousands that need to be optimised.
Further to this, cite:rossiSparse2021 show that optimising the inducing inputs
relies on being able to optimise both the prior and the posterior, therefore contradicting Bayesian inference.
Our variational inference scheme follows common practice and optimises the inducing inputs
jointly with the hyperparameters.
In some instances, we observe that optimising the inducing inputs leads to them taking values far away from the
training data.
Often this can be avoided by simply sampling the inducing inputs from
the training inputs and fixing them, i.e. not optimising them.
This often leads to better NLPP scores as well.
This observation highlights that a full Bayesian treatment of the inducing inputs is an interesting direction for
future work.
However, specifying priors and performing /efficient/ posterior inference over the inducing inputs
is a challenging problem.


*Latent spaces for control*
In conventional \acrshort{mogpe} methods, the epistemic uncertainty associated with the
gating network is not decoupled from the probability mass function over the expert indicator variable.
This is because an uncertain gating network is represented by a uniform distribution over the expert indicator variable.
In contrast, \acrshort{mosvgpe} models the uncertainty associated with each gating function via
their GPs' posterior covariance.
Further to this, formulating the gating network GPs
with differentiable mean and covariance functions, enables techniques from Riemannian geometry
to be deployed on the gating functions citep:carmoRiemannian1992.
The power of the \acrshort{gp}-based gating network will become apparent
when its latent /geometry/ is leveraged for control in cref:chap-traj-opt-geometry.
# In particular, cref:chap-traj-opt-geometry is interested in finding length minimising trajectories on the
# gating functions' GP posteriors, aka geodesic trajectories citep:tosiMetrics2014.

** Conclusion
This chapter has presented a method for learning representations of multimodal dynamical systems using
a \acrshort{mogpe} method.
Motivated by correctly identify the underlying dynamics modes and inferring latent structure that can
be exploited for control,
this work formulated a gating network based on input-dependent gating functions.
This aids the inherent identifiability issues associated with mixture models
as it can be used to constrain the set of admissible functions through the placement of informative
GP priors on the gating functions.
Further to this, the GP posteriors over the gating functions provide convenient latent spaces for control.
This is because they are rich with information regarding the separation of the underlying dynamics modes
and also model the /epistemic uncertainty/ associated with the gating network.
# Motivated by learning latent spaces for control
# and ensuring that the true underlying dynamics modes are identified,
# this work formulated a gating network based on input-dependent gating functions.
# As we shall see in cref:chap-traj-opt-geometry, the GP posteriors over the gating functions provide convenient
# latent spaces for control.




The variational inference scheme presented in this chapter addresses the issue of marginalising
every possible set of assignments of observations to experts
-- of which there are $\ModeInd^{\NumData}$ possibilities
-- in the MoGPE marginal likelihood.
It overcomes the issue of assigning observations to experts by augmenting each expert GP
with a set of inducing points.
These inducing points are assumed to be a sufficient statistic for the joint distribution
over every possible set of assignments to experts.
This induces a factorisation over data which
is used to derive three ELBOs that provide a coupling between the
optimisation of the experts and the gating network, by efficiently marginalising the expert indicator variable for single
data points.
The ELBOs are compared on the Motorcycle data set citep:Silverman1985.
The $\furtherBound$ bound provides the best performance as it balances the accuracy offered by the tight bound $\tightBound$,
with the computational improvements offered by further bounding the GPs.
The results demonstrate that the variational inference scheme principally handles uncertainty whilst
providing scalability via stochastic variational inference.
The method is further evaluated on a real-world quadcopter example demonstrating that
it can successfully learn a factorised representation of a real-world, multimodal, robotic system.

* A Geometric Take on Model-Based Control in Multimodal Dynamical Systems label:chap-traj-opt-geometry
** Maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\nominalStateTraj}{\ensuremath{\stateTraj_*}}
\newcommand{\nominalControlTraj}{\ensuremath{\controlTraj_*}}
\newcommand{\fixedControl}{\ensuremath{\control_{*}}}
\newcommand{\velocity}{\ensuremath{v}}

\newcommand{\trajectory}{\ensuremath{\bar{\state}}}
\newcommand{\stateControlTraj}{\ensuremath{\bm\tau}}
\newcommand{\jacTraj}{\ensuremath{\bar{\mathbf{J}}}}
\renewcommand{\modeVarTraj}{\ensuremath{\modeVar_{0:\TimeInd}=\desiredMode}}

\renewcommand{\stateDiffTraj}{\ensuremath{\Delta \stateTraj}}

%\renewcommand{\modeInd}{\ensuremath{\modeVar}}

\newcommand{\desiredMode}{\ensuremath{\modeInd^{*}}}
\renewcommand{\modeDes}[1]{\ensuremath{#1_{\desiredMode}}}
\newcommand{\desiredGatingFunction}{\ensuremath{\modeDes{\gatingFunc}}}
%\newcommand{\desiredDynamicsFunc}{\ensuremath{\mode{\latentFunc}}}
\newcommand{\desiredDynamicsFunc}{\ensuremath{\modeDes{\latentFunc}}}
%\newcommand{\desiredDynamicsFunc}{\ensuremath{\latentFunc_{\modeVar_{\timeInd}}}}
\newcommand{\desiredStateDomain}{\ensuremath{\modeDes{\stateDomain}}}
%\newcommand{\desiredStateDomain}{\ensuremath{\mode{\stateDomain}}}

%\newcommand{\controlledDynamicsFunc}{\ensuremath{\modeDes{\latentFunc}}}
\newcommand{\controlledDynamicsFunc}{\ensuremath{\latentFunc_{\controlTraj}}}

\newcommand{\valueFunc}{\ensuremath{V}}

\renewcommand{\controlledPolicyDist}{\ensuremath{q_\policy}}

\renewcommand{\satisfactionProb}{\ensuremath{p_{\modeVar}}}
#+END_EXPORT
# *** Geometry Maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\manifold}{\ensuremath{\mathcal{M}}}
\newcommand{\manifoldFunction}{\ensuremath{h}}
\newcommand{\manifoldDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\manifoldCodomain}{\ensuremath{\mathcal{Z}}}
\newcommand{\ManifoldDim}{\ensuremath{D}}
\newcommand{\manifoldDim}{\ensuremath{d}}
\newcommand{\manifoldDomainDim}{\ensuremath{d_{\manifoldDomain}}}
\newcommand{\manifoldCodomainDim}{\ensuremath{d_{\manifoldCodomain}}}
\newcommand{\manifoldInput}{\ensuremath{\mathbf{x}}}

% \newcommand{\jacobian}{\ensuremath{\mathbf{J}_{\mathbf{x}_t}}}
\newcommand{\jacobian}{\ensuremath{\mathbf{J}(\state(t))}}
\newcommand{\metricTensor}{\ensuremath{\mathbf{G}}}
\newcommand{\metricTensorTraj}{\ensuremath{\bar{\mathbf{G}}}}

\newcommand{\geodesicFunction}{\ensuremath{f_G}}

%\newcommand{\gatingDomain}{\ensuremath{\hat{\mathcal{X}}}}
%\newcommand{\gatingCodomain}{\ensuremath{\mathcal{A}}}
\newcommand{\gatingDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\gatingCodomain}{\ensuremath{\mathcal{Z}}}

\newcommand{\desiredManifold}{\ensuremath{\mathcal{M}_{k^*}}}
%\newcommand{\desiredMetricTensor}{\ensuremath{\mathbf{G}_{k^*}}}
\newcommand{\desiredMetricTensor}{\ensuremath{\mathbf{G}}}
%\newcommand{\desiredJacobian}{\ensuremath{\mathbf{J}_{k^*}(\state(t))}}
\newcommand{\desiredJacobian}{\ensuremath{\mathbf{J}}}
%\newcommand{\GatingDim}{\ensuremath{D_{x+u}}}
\newcommand{\GatingDim}{\ensuremath{D}}
\newcommand{\gatingDim}{\ensuremath{d}}

% Manfiold kernels
\renewcommand{\manifoldKernelMM}{\ensuremath{\mathbf{K}_{\NumInducing \NumInducing}}}
\newcommand{\jacManifoldKernelsM}{\ensuremath{\partial \mathbf{K}_{* \NumInducing}}}
\newcommand{\jacManifoldKernelMs}{\ensuremath{\partial \mathbf{K}_{\NumInducing *}}}
\newcommand{\hessManifoldKernel}{\ensuremath{\partial^2 \mathbf{K}_{**}}}
\renewcommand{\manifoldKernelNN}{\ensuremath{\mathbf{K}_{\NumData \NumData}}}
\newcommand{\jacManifoldKernelsN}{\ensuremath{\partial \mathbf{K}_{* \NumData}}}
\newcommand{\jacManifoldKernelNs}{\ensuremath{\partial \mathbf{K}_{\NumData *}}}
\newcommand{\hessManifoldKerneldd}{\ensuremath{\partial^2 k(\cdot, \cdot')}}
\newcommand{\jacManifoldKerneldN}{\ensuremath{\partial \mathbf{K}_{\cdot \NumData}}}
\newcommand{\jacManifoldKernelNd}{\ensuremath{\partial \mathbf{K}_{\NumData \cdot}}}

\newcommand{\manifoldInducingInput}{\ensuremath{\bm\xi}}
%\newcommand{\manifoldInducingOutput}{\ensuremath{\mathbf{u}}}
\newcommand{\manifoldInducingOutput}{\ensuremath{\manifoldFunction(\manifoldInducingInput)}}
\newcommand{\manifoldInducingVariational}{\ensuremath{q(\mathbf{u})}}
\newcommand{\manifoldInducingOutputMean}{\ensuremath{\mathbf{m}}}
\newcommand{\manifoldInducingOutputCov}{\ensuremath{\mathbf{S}}}
\newcommand{\manifoldMeanFunc}{\ensuremath{\mu}}


%\newcommand{\manifoldFunc}{\ensuremath{\mathbf{h}}}
%\newcommand{\desiredMeanFunc}{\ensuremath{\mu}}
\renewcommand{\muJac}{\ensuremath{\bm\mu_{\mathbf{J}}}}
\renewcommand{\covJac}{\ensuremath{\bm\Sigma_{\mathbf{J}}}}
\renewcommand{\testInput}{\ensuremath{\mathbf{x}_*}}

\newcommand{\stateDiff}{\ensuremath{\Delta \state}}

\renewcommand{\stateCostMatrix}{\ensuremath{\mathbf{Q}}}
\renewcommand{\controlCostMatrix}{\ensuremath{\mathbf{R}}}
\renewcommand{\terminalStateCostMatrix}{\ensuremath{\mathbf{H}}}
\renewcommand{\approxExpectedCost}{\ensuremath{J(\stateTraj, \controlTraj)}}

\renewcommand{\terminalState}{\ensuremath{\state_{\TimeInd}}}

\newcommand{\stateMean}{\ensuremath{\bm\mu_{\state_\timeInd}}}
\newcommand{\stateCov}{\ensuremath{\bm\Sigma_{\state_\timeInd}}}
\newcommand{\terminalStateMean}{\ensuremath{\bm\mu_{\state_\TimeInd}}}
\newcommand{\terminalStateCov}{\ensuremath{\bm\Sigma_{\state_\TimeInd}}}
\newcommand{\controlMean}{\ensuremath{\bm\mu_{\control_\timeInd}}}
\newcommand{\controlCov}{\ensuremath{\bm\Sigma_{\control_\timeInd}}}
\newcommand{\stateDiff}{\ensuremath{\Delta \state}}
\newcommand{\stateDiffMean}{\ensuremath{\bm\mu_{\stateDiff_\timeInd}}}
\newcommand{\stateDiffCov}{\ensuremath{\bm\Sigma_{\stateDiff_\timeInd}}}

\renewcommand{\transitionDistK}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \modeVar_{\timeInd}=\modeInd)}}
#+END_EXPORT
** Intro :ignore:
This chapter is concerned with controlling /unknown/ or /partially unknown/, multimodal dynamical systems,
given a historical data set of state transitions $\dataset$, sampled from the system.
In particular, it is concerned with /mode constrained/ trajectory optimisation, which attempts to
drive a system from an initial state $\state_0$ -- in a desired dynamics mode -- to a target state $\state_f$,
whilst remaining in the desired dynamics mode.

The \acrshort{mosvgpe} method from cref:chap-dynamics was intentionally formulated with latent variables
to represent the mode switching behaviour and its associated uncertainty.
This chapter unleashes the power of these latent variables by making decisions under their uncertainty.
The methods presented in this chapter are model-based control techniques that leverage
a single-step predictive dynamics model learned using the \acrshort{mosvgpe} method from cref:chap-dynamics.

This chapter introduces two different approaches to performing /mode constrained/ trajectory optimisation.
They both exploit concepts from Riemannian geometry -- extended to probabilistic manifolds -- to
encode mode remaining behaviour.
The first approach is an indirect control method that projects the trajectory optimisation problem onto
an \acrfull{ode} that implicitly encodes the mode remaining behaviour.
The second approach is a direct control method that resembles standard Gaussian process control methods
with the mode remaining behaviour encoded via a geometric objective function.

The remainder of this chapter is organised as follows.
cref:sec-problem-statement formally states the problem
and cref:sec-geometry-recap introduces the relevant background on Riemannian geometry and its extension to
probabilistic manifolds.
cref:sec-traj-opt-collocation,sec-traj-opt-energy
detail the two trajectory optimisation algorithms.
The methods are then evaluated using the illustrative example from cref:illustrative_example.
The work in this chapter is implemented in JAX and TensorFlow and is available on GitHub[fn::Code accompanying
cref:sec-traj-opt-collocation can be found @
[[https://github.com/aidanscannell/trajectory-optimisation-in-learned-multimodal-dynamical-systems][\icon{\faGithub}/aidanscannell/trajectory-optimisation-in-learned-multimodal-dynamical-systems]]
and code accompanying cref:sec-traj-opt-energy can be found @ [[https://github.com/aidanscannell/ModeOpt][\icon{\faGithub}/aidanscannell/ModeOpt]].]
citep:jax2018github.

# This section first recaps the relevant concepts from Riemannian geometry.
# It then details two control methods that exploit the geometry of the \acrshort{mosvgpe} model to obtain mode remaining behaviour.
# The first approach is an indirect control method based on differential flatness and collocation.
# The second method embeds the mode remaining behaviour into a more conventional Gaussian process
# control framework.
# Finally, this section evaluates the two approaches using the illustrative example from cref:illustrative_example.


\todo{cite ICRA paper}

** Problem Statement label:sec-problem-statement :noexport:
# #+BEGIN_EXPORT latex
# \begin{align}
# \mode{\latentFunc} : \stateDomain \times \controlDomain \rightarrow \stateDomain \quad \text{if} \quad \modeVar=\modeInd,
# \end{align}
# #+END_EXPORT
# $\stateDomain = \bigcup_{\modeInd \in \modeVarDomain} \mode{\stateDomain} = \{ \state \subseteq \stateDomain \mid \modeInd \in \modeVarDomain \}$.
# $\stateDomain = \bigcup_{\modeInd=1}^{\ModeInd} \mode{\stateDomain}$.
# This chapter considers multimodal dynamical systems where the dynamics modes are defined by disjoint
# state domains.
This chapter considers nonlinear, stochastic, multimodal dynamical systems,
#+BEGIN_EXPORT latex
\begin{subequations}
\begin{align}
\state_{\timeInd+1} &= \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \bm\epsilon \\
&= \mode{\dynamicsFunc}(\state_\timeInd, \control_\timeInd) + \mode{\bm\epsilon}
\quad \text{if} \quad \modeVar_{\timeInd}=\modeInd \nonumber \\
\mode{\bm\epsilon} &\sim \mathcal{N}(\mathbf{0}, \Sigma_{\mode{\bm\epsilon}}),
\end{align}
\end{subequations}
#+END_EXPORT
where the dynamics modes are defined by disjoint state domains.
That is, each dynamics mode is defined by its state domain
$\mode{\stateDomain} = \{ \state \in \stateDomain \mid \modeVar = \modeInd \}$, with
$\stateDomain_{i} \cap \stateDomain_{j} = \emptyset$ for $i \neq j$.
Each mode's dynamics are then given by,
#+BEGIN_EXPORT latex
\begin{align}
\mode{\latentFunc}: \mode{\stateDomain} \times \controlDomain \rightarrow \stateDomain.
\end{align}
#+END_EXPORT
Notice that each mode's transition dynamics are free to leave their state space $\mode{\stateDomain}$
and enter another mode.
Ideally, this work seeks to enforce the controlled system to remain in a given mode at every time step.
#+BEGIN_EXPORT latex
\begin{definition}[Mode Remaining] \label{def-mode-remaining}
Let $\modeInd$ denote a dynamics mode, defined by its state domain $\mode{\stateDomain} \subseteq \stateDomain$.
Given an inital state $\state_0 \in \mode{\stateDomain}$, and a sequence of controls $\controlTraj=\control_{0:\TimeInd-1}$,
the controlled system is said to be mode-remaining iff:
\begin{align} \label{eq-mode-remaining-def}
\dynamicsFunc(\state_{\timeInd}, \control_{\timeInd}) &\in \mode{\stateDomain} \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd]
\end{align}
\end{definition}
#+END_EXPORT

Given a desired dynamics mode $\desiredMode$, this work seeks to find a control
trajectory $\controlTraj = \control_{0:\TimeInd-1}$,
to navigate from an initial state $\state_{0} \in \desiredStateDomain$, to a
target state $\targetState \in \desiredStateDomain$,
over a horizon $\TimeInd$,
whilst minimising a cost function, $\costFunc: \stateDomain \times \controlDomain \rightarrow \R$
and keeping the system in the desired dynamics mode $\desiredMode$.
Formally, this problem is given by,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-mode-soc-problem}
\begin{align}
\min_{\controlTraj} &\E\Bigg[
\sum_{\timeInd=0}^{\TimeInd} \costFunc(\state_\timeInd, \control_\timeInd) \Bigg] \\
\text{s.t.} \quad &\state_{\timeInd+1} = \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \epsilon,
\quad \epsilon \sim \mathcal{N}(\mathbf{0}, \Sigma_{\epsilon}) \\
&\text{\cref{eq-mode-remaining-def}} \\
%\state_{\timeInd} &\in \desiredStateDomain \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] \\
&\state_\TimeInd = \targetState,
\end{align}
\end{subequations}
#+END_EXPORT
To simplify notation, the state and control trajectories are denoted
as $\stateTraj = \state_{1:T}$ and $\controlTraj = \control_{0:T-1}$ respectively.
The novelty of this problem arises from the mode remaining behaviour.
To the best of our knowledge, there is no prior work addressing how to find
trajectories that remain in a desired dynamics mode,
when knowledge of how the system switches between the dynamics modes is /not known a priori/.



# where the expectation is taken w.r.t. the distribution over state
# trajectories,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-trajectory-dist}
# %\controlledPolicyDist(\stateTraj \mid \state_0, \controlTraj) =
# p(\stateTraj \mid \state_0, \controlTraj) =
# \prod_{\timeInd=0}^{\TimeInd-1}
# p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd).
# \end{align}
# #+END_EXPORT



Given this learned dynamics model, it is assumed that a desired dynamics mode $\desiredMode$ can easily be identified.
This is a realistic assumption as the parameters associated with each dynamics GP can be used to identify
different behaviours.
For example, the noise variance associated with each mode's dynamics GP models its process noise.
If it is desirable to avoid modes with high process noise, then they can easily be identified by their noise
variances.
#+BEGIN_EXPORT latex
\begin{assumption} \label{}
A desired dynamics mode $\desiredMode$ is known.
\end{assumption}
#+END_EXPORT
Given a learned dynamics model and a desired dynamics mode $\desiredMode$,
the goals of the trajectory optimisation in this chapter can be summarised as follows,
# - Goal 1 :: Remain in a desired dynamics mode $\desiredMode$,
# - Goal 2 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/.
# - Goal 1 :: Remain in the operable, desired dynamics mode $\desiredMode$,
# - Goal 2 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/,
#   - in the desired dynamic mode,
#   - in the gating network.
- Goal 1 :: Navigate to a target state $\targetState$,
- Goal 2 :: Remain in the operable, desired dynamics mode $\desiredMode$,
  + Goal 2.1 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/, in the desired dynamics mode and in the gating network.
Goal 2.1 arises due to learning the dynamics model from observations.
The learned model may not be able to confidently predict which mode governs the dynamics at a given region.
This is due to lack of training observations and is known as /epistemic uncertainty/.
It is desirable to avoid entering these regions as it may result in the system leaving the desired dynamics mode.




This chapter assumes prior access to the environment, enabling a data set of state transitions to be collected.
This data set is used to learn a factorised representation of the underlying dynamics modes
with the \acrshort{mosvgpe} method from cref:chap-dynamics.
#+BEGIN_EXPORT latex
\begin{assumption} \label{}
Access to a single-step dynamics model learned using
the \acrshort{mosvgpe} method from \cref{chap-dynamics} and a data set $\dataset$ of state transitions
from the system.
\end{assumption}
#+END_EXPORT
Deploying model-based control in a learned dynamics model makes
it impossible to find trajectories that are guaranteed to be Mode Remaining (cref:def-mode-remaining).
This work relaxes the requirement to finding mode remaining trajectories with high probability.
#+BEGIN_EXPORT latex
\begin{definition}[$\delta$-mode remaining] \label{def-delta-mode-remaining}
Let $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ denote a multimodal dynamical system
and $\desiredMode$ a desired dynamics mode defined by its state domain $\desiredStateDomain \subseteq \stateDomain$.
Given an initial state $\state_0 \in \desiredStateDomain$ and $\delta \in (0,1]$,
a system is said to be $\delta$-mode remaining under the controls $\controlTraj$ iff:
\begin{align}
\Pr( \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] : \dynamicsFunc(\state_{\timeInd},\control_{\timeInd}) &\in \desiredStateDomain,
\control_{\timeInd} \in \controlDomain) \geq 1 - \delta
\end{align}
\end{definition}
#+END_EXPORT

# Given this learned dynamics model, it is assumed that a desired dynamics mode $\desiredMode$ can easily be identified.
# This is a realistic assumption as the parameters associated with each dynamics GP can be used to identify
# different behaviours.
# For example, the noise variance associated with each mode's dynamics GP models its process noise.
# If it is desirable to avoid modes with high process noise, then they can easily be identified by their noise
# variances.
# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# A desired dynamics mode $\desiredMode$ is known.
# \end{assumption}
# #+END_EXPORT
# Given a learned dynamics model and a desired dynamics mode $\desiredMode$,
# the goals of the trajectory optimisation in this chapter can be summarised as follows,
# # - Goal 1 :: Remain in a desired dynamics mode $\desiredMode$,
# # - Goal 2 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/.
# # - Goal 1 :: Remain in the operable, desired dynamics mode $\desiredMode$,
# # - Goal 2 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/,
# #   - in the desired dynamic mode,
# #   - in the gating network.
# - Goal 1 :: Navigate to a target state $\targetState$,
# - Goal 2 :: Remain in the operable, desired dynamics mode $\desiredMode$,
#   + Goal 2.1 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/, in the desired dynamics mode and in the gating network.
# Goal 2.1 arises due to learning the dynamics model from observations.
# The learned model may not be able to confidently predict which mode governs the dynamics at a given region.
# This is due to lack of training observations and is known as /epistemic uncertainty/.
# It is desirable to avoid entering these regions as it may result in the system leaving the desired dynamics mode.



# Given a desired dynamics mode $\desiredMode$, this work seeks to find a control
# trajectory $\controlTraj = \control_{0:\TimeInd-1}$,
# to navigate from an initial state $\state_{0} \in \desiredStateDomain$, to a
# target state $\targetState \in \desiredStateDomain$,
# over a horizon $\TimeInd$,
# whilst minimising a cost function, $\costFunc: \stateDomain \times \controlDomain \rightarrow \R$
# and keeping the system in the desired dynamics mode $\desiredMode$.

# where the expectation is taken w.r.t. the distribution over state-control
# trajectories under controller $\policy$,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-trajectory-dist}
# \controlledPolicyDist(\stateTraj \mid \state_0, \controlTraj) =
# \prod_{\timeInd=0}^{\TimeInd-1}
# p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)
# \policy(\control_\timeInd).
# \end{align}
# #+END_EXPORT




# #+BEGIN_EXPORT latex
# \begin{remark}[Epistemic Uncertainty]
# %Given that the underlying dynamics modes and how the dynamics switches between them, are
# %\textit{not fully known a priori}, these methods should \textit{air on the side of caution}.
# Due to lack of training observations, the learned model may be uncertain which mode governs the
# dynamics at a particular region. This is known as epistemic uncertainty.
# In a risk-averse setting, trajectories should also avoid entering these regions of high epistemic uncertainty.
# \end{remark}
# #+END_EXPORT

# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# The system has previously been observed to obtain a data set $\dataset$ of state transitions sampled at constant frequency.
# \end{assumption}
# #+END_EXPORT
# Given this learned dynamics model, it is assumed that a desired dynamics mode $\desiredMode$ can easily be identified.
# This is a realistic assumption as the parameters associated with each dynamics GP represent different
# characteristics of the system.
# For example, the noise variance associated with each mode's dynamics GP models the process noise that the mode
# is subject to.
# A high noise variance can therefore be used to identify dynamics modes that are undesirable.


# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# A desired dynamics mode $\desiredMode$  is known.
# Moreover, its transition dynamics are given by
# $\desiredDynamicsFunc: \desiredStateDomain \times \controlDomain \rightarrow \stateDomain$.
# \end{assumption}
# #+END_EXPORT
# the \acrshort{mosvgpe} method from cref:chap-dynamics to learn a factorised
# representation of the underlying dynamics modes.

# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# Further to this, it is assumed that the mode switching behaviour is governed by the state space, i.e.
# each dynamics modes is governed by its state domain
# $\stateDomain_{\modeInd} = \{ \state \in \stateDomain \mid \modeVar = \modeInd \}$,
# giving each mode's dynamics,
# $\mode{\latentFunc}: \mode{\stateDomain} \times \controlDomain \rightarrow \stateDomain$.

# The system switches between its dynamics modes over the systems state space, i.e.
# each dynamics modes is governed by its state domain
# $\stateDomain_{\modeInd} = \{ \state \in \stateDomain \mid \modeVar = \modeInd \}$,
# giving each mode's dynamics,
# $\mode{\latentFunc}: \mode{\stateDomain} \times \controlDomain \rightarrow \stateDomain$.
# \end{assumption}
# #+END_EXPORT

# and used to train a single-step dynamics model using
# the \acrshort{mosvgpe} method from \cref{chap-dynamics}.

# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# A data set $\dataset$ of state transitions have previously been sampled from the
# system (at constant frequency) and used to train a single-step dynamics model using
# the \acrshort{mosvgpe} method from \cref{chap-dynamics}.
# \end{assumption}
# #+END_EXPORT
# Further to this, it assumes that a desired dynamics mode $\desiredMode$ is /known a priori/.
# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# Let $\desiredMode$ denote the desired dynamics mode governed
# by its state domain
# $\desiredStateDomain = \{ \state \in \stateDomain \mid \modeVar = \desiredMode \}$.
# The desired mode's transition dynamics are given by
# $\desiredDynamicsFunc: \desiredStateDomain \times \controlDomain \rightarrow \stateDomain$,
# with continuous states $\state \in \stateDomain \subseteq \R^\StateDim$ and controls
# $\control \in \controlDomain \subseteq \R^\ControlDim$.
# \end{assumption}
# #+END_EXPORT

# The desired mode's transition dynamics are given by
# $\desiredDynamicsFunc: \desiredStateDomain \times \controlDomain \rightarrow \stateDomain$,
# with continuous states $\state \in \stateDomain \subseteq \R^\StateDim$ and controls
# $\control \in \controlDomain \subseteq \R^\ControlDim$,
# where $\desiredStateDomain = \{ \state \in \stateDomain \mid \modeVar = \desiredMode \}$
# denotes the subset of the state space associated with the desired dynamics mode.
# Note that the desired mode's transition dynamics $\desiredDynamicsFunc$
# are free to leave the desired mode's state space $\desiredStateDomain$.






\todo{cite active learning paper and multimodal mogpe paper?}
# The most similar works are cite:schreiterSafe2015 where active learning is gu



# Of particular interest in this chapter is the GP posterior over the desired mode's gating function.
# cref:fig-traj-opt-gating-network-gp visualises this GP posterior after training on the historical data set of state
# transitions from the illustrative example in cref:illustrative_example.
# One approach to encoding mode remaining behaviour is to consider the geometry of the latent gating functions.

** Problem Statement label:sec-problem-statement
# #+BEGIN_EXPORT latex
# \begin{align}
# \mode{\latentFunc} : \stateDomain \times \controlDomain \rightarrow \stateDomain \quad \text{if} \quad \modeVar=\modeInd,
# \end{align}
# #+END_EXPORT
# $\stateDomain = \bigcup_{\modeInd \in \modeVarDomain} \mode{\stateDomain} = \{ \state \subseteq \stateDomain \mid \modeInd \in \modeVarDomain \}$.
# $\stateDomain = \bigcup_{\modeInd=1}^{\ModeInd} \mode{\stateDomain}$.
# This chapter considers multimodal dynamical systems where the dynamics modes are defined by disjoint
# state domains.
This chapter considers nonlinear, stochastic, multimodal dynamical systems,
#+BEGIN_EXPORT latex
\begin{subequations}
\begin{align}
\state_{\timeInd+1} &= \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \bm\epsilon \\
&= \mode{\dynamicsFunc}(\state_\timeInd, \control_\timeInd) + \mode{\bm\epsilon}
\quad \text{if} \quad \modeVar_{\timeInd}=\modeInd \nonumber \\
\mode{\bm\epsilon} &\sim \mathcal{N}(\mathbf{0}, \Sigma_{\mode{\bm\epsilon}}),
\end{align}
\end{subequations}
#+END_EXPORT
where the dynamics modes are defined by disjoint state domains.
That is, each dynamics mode is defined by its state domain
$\mode{\stateDomain} = \{ \state \in \stateDomain \mid \modeVar = \modeInd \}$, with
$\stateDomain_{i} \cap \stateDomain_{j} = \emptyset$ for $i \neq j$.
Each mode's dynamics are then given by,
#+BEGIN_EXPORT latex
\begin{align}
\mode{\latentFunc}: \mode{\stateDomain} \times \controlDomain \rightarrow \stateDomain.
\end{align}
#+END_EXPORT
Notice that each mode's transition dynamics are free to leave their state space $\mode{\stateDomain}$
and enter another mode.
Ideally, this work seeks to enforce the controlled system to remain in a given mode at every time step.
#+BEGIN_EXPORT latex
\begin{definition}[Mode Remaining] \label{def-mode-remaining}
Let $\modeInd$ denote a dynamics mode, defined by its state domain $\mode{\stateDomain} \subseteq \stateDomain$.
Given an inital state $\state_0 \in \mode{\stateDomain}$, and a sequence of controls $\controlTraj=\control_{0:\TimeInd-1}$,
the controlled system is said to be mode-remaining iff:
\begin{align} \label{eq-mode-remaining-def}
\dynamicsFunc(\state_{\timeInd}, \control_{\timeInd}) &\in \mode{\stateDomain} \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd]
\end{align}
\end{definition}
#+END_EXPORT
Given a desired dynamics mode $\desiredMode$, this work seeks to find a control
trajectory $\controlTraj = \control_{0:\TimeInd-1}$,
to navigate from an initial state $\state_{0} \in \desiredStateDomain$, to a
target state $\targetState \in \desiredStateDomain$,
over a horizon $\TimeInd$,
whilst minimising a cost function, $\costFunc: \stateDomain \times \controlDomain \rightarrow \R$
and keeping the system in the desired dynamics mode $\desiredMode$.
Formally, this problem is given by,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-mode-soc-problem}
\begin{align}
\min_{\controlTraj} &\E\Bigg[
\sum_{\timeInd=0}^{\TimeInd} \costFunc(\state_\timeInd, \control_\timeInd) \Bigg] \\
\text{s.t.} \quad &\state_{\timeInd+1} = \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \epsilon,
\quad \epsilon \sim \mathcal{N}(\mathbf{0}, \Sigma_{\epsilon}) \\
&\text{\cref{eq-mode-remaining-def}} \\
%\state_{\timeInd} &\in \desiredStateDomain \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] \\
&\state_\TimeInd = \targetState,
\end{align}
\end{subequations}
#+END_EXPORT
where the expectation is taken w.r.t. the distribution over state
trajectories,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-trajectory-dist}
%\controlledPolicyDist(\stateTraj \mid \state_0, \controlTraj) =
p(\stateTraj \mid \state_0, \controlTraj) =
\prod_{\timeInd=0}^{\TimeInd-1}
p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd).
\end{align}
#+END_EXPORT
To simplify notation, the state and control trajectories are denoted
as $\stateTraj = \state_{1:T}$ and $\controlTraj = \control_{0:T-1}$ respectively.
The novelty of this problem arises from the mode remaining behaviour.
To the best of our knowledge, there is no prior work addressing how to find
trajectories that remain in a desired dynamics mode,
when knowledge of how the system switches between the dynamics modes is /not known a priori/.

# where the expectation is taken w.r.t. the distribution over state-control
# trajectories under controller $\policy$,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-trajectory-dist}
# \controlledPolicyDist(\stateTraj \mid \state_0, \controlTraj) =
# \prod_{\timeInd=0}^{\TimeInd-1}
# p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)
# \policy(\control_\timeInd).
# \end{align}
# #+END_EXPORT


This chapter assumes prior access to the environment, enabling a data set of state transitions to be collected.
This data set is used to learn a factorised representation of the underlying dynamics modes
with the \acrshort{mosvgpe} method from cref:chap-dynamics.
# This method correctly identifies the underlying dynamics modes and provides informative latent spaces that
# can be used to encode mode remaining behaviour into control strategies.
# In particular, the GP-based gating network infers informative latent structure.
# cref:fig-traj-opt-gating-network-gp shows the GP posterior associated with the desired mode's gating function
# after training
# \acrshort{mosvgpe} on the historical data set of state transitions from the quadcopter experiment in cref:illustrative_example.
#+BEGIN_EXPORT latex
\begin{assumption} \label{}
A single-step dynamics model has been learned using
the \acrshort{mosvgpe} method from \cref{chap-dynamics} and a data set $\dataset$ of state transitions
from the system.
\end{assumption}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# A data set $\dataset$ of state transitions have previously been sampled from the
# system (at constant frequency) and used to train a single-step dynamics model using
# the \acrshort{mosvgpe} method from \cref{chap-dynamics}.
# \end{assumption}
# #+END_EXPORT
# As the true underlying dynamics modes and how the system switches between them are /not fully known a priori/,
# it is impossible to develop algorithms that can find trajectories that are guaranteed to be

# As the control algorithms presented in this chapter leverage this learned dynamics model,
# it is impossible to find trajectories that are guaranteed to be Mode Remaining (cref:def-mode-remaining).
Leveraging a learned dynamics model in this way makes
it impossible to find trajectories that are guaranteed to be Mode Remaining (cref:def-mode-remaining).
This work relaxes the requirement to finding mode remaining trajectories with high probability.
#+BEGIN_EXPORT latex
\begin{definition}[$\delta$-mode remaining] \label{def-delta-mode-remaining}
Let $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ denote a multimodal dynamical system
and $\desiredMode$ a desired dynamics mode defined by its state domain $\desiredStateDomain \subseteq \stateDomain$.
Given an initial state $\state_0 \in \desiredStateDomain$ and $\delta \in (0,1]$,
a system is said to be $\delta$-mode remaining under the controls $\controlTraj$ iff:
\begin{align}
\Pr( \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] : \dynamicsFunc(\state_{\timeInd},\control_{\timeInd}) &\in \desiredStateDomain,
\control_{\timeInd} \in \controlDomain) \geq 1 - \delta
\end{align}
\end{definition}
#+END_EXPORT

Given this learned dynamics model, it is assumed that a desired dynamics mode $\desiredMode$ can easily be identified.
This is a realistic assumption as the parameters associated with each dynamics GP can be used to identify
different behaviours.
For example, the noise variance associated with each mode's dynamics GP models its process noise.
If it is desirable to avoid modes with high process noise, then they can easily be identified by their noise
variances.
#+BEGIN_EXPORT latex
\begin{assumption} \label{}
A desired dynamics mode $\desiredMode$ is known.
\end{assumption}
#+END_EXPORT
Given a learned dynamics model and a desired dynamics mode $\desiredMode$,
the goals of the trajectory optimisation in this chapter can be summarised as follows,
# - Goal 1 :: Remain in a desired dynamics mode $\desiredMode$,
# - Goal 2 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/.
# - Goal 1 :: Remain in the operable, desired dynamics mode $\desiredMode$,
# - Goal 2 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/,
#   - in the desired dynamic mode,
#   - in the gating network.
- Goal 1 :: Navigate to a target state $\targetState$,
- Goal 2 :: Remain in the operable, desired dynamics mode $\desiredMode$,
- Goal 3 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/, in the desired dynamics mode and in the gating network.
Goal 3 arises due to learning the dynamics model from observations.
The learned model may not be able to confidently predict which mode governs the dynamics at a given region.
This is due to lack of training observations and is known as /epistemic uncertainty/.
It is desirable to avoid entering these regions as it may result in the system leaving the desired dynamics mode.

# #+BEGIN_EXPORT latex
# \begin{remark}[Epistemic Uncertainty]
# %Given that the underlying dynamics modes and how the dynamics switches between them, are
# %\textit{not fully known a priori}, these methods should \textit{air on the side of caution}.
# Due to lack of training observations, the learned model may be uncertain which mode governs the
# dynamics at a particular region. This is known as epistemic uncertainty.
# In a risk-averse setting, trajectories should also avoid entering these regions of high epistemic uncertainty.
# \end{remark}
# #+END_EXPORT

# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# The system has previously been observed to obtain a data set $\dataset$ of state transitions sampled at constant frequency.
# \end{assumption}
# #+END_EXPORT
# Given this learned dynamics model, it is assumed that a desired dynamics mode $\desiredMode$ can easily be identified.
# This is a realistic assumption as the parameters associated with each dynamics GP represent different
# characteristics of the system.
# For example, the noise variance associated with each mode's dynamics GP models the process noise that the mode
# is subject to.
# A high noise variance can therefore be used to identify dynamics modes that are undesirable.


# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# A desired dynamics mode $\desiredMode$  is known.
# Moreover, its transition dynamics are given by
# $\desiredDynamicsFunc: \desiredStateDomain \times \controlDomain \rightarrow \stateDomain$.
# \end{assumption}
# #+END_EXPORT
# the \acrshort{mosvgpe} method from cref:chap-dynamics to learn a factorised
# representation of the underlying dynamics modes.

# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# Further to this, it is assumed that the mode switching behaviour is governed by the state space, i.e.
# each dynamics modes is governed by its state domain
# $\stateDomain_{\modeInd} = \{ \state \in \stateDomain \mid \modeVar = \modeInd \}$,
# giving each mode's dynamics,
# $\mode{\latentFunc}: \mode{\stateDomain} \times \controlDomain \rightarrow \stateDomain$.

# The system switches between its dynamics modes over the systems state space, i.e.
# each dynamics modes is governed by its state domain
# $\stateDomain_{\modeInd} = \{ \state \in \stateDomain \mid \modeVar = \modeInd \}$,
# giving each mode's dynamics,
# $\mode{\latentFunc}: \mode{\stateDomain} \times \controlDomain \rightarrow \stateDomain$.
# \end{assumption}
# #+END_EXPORT

# and used to train a single-step dynamics model using
# the \acrshort{mosvgpe} method from \cref{chap-dynamics}.

# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# A data set $\dataset$ of state transitions have previously been sampled from the
# system (at constant frequency) and used to train a single-step dynamics model using
# the \acrshort{mosvgpe} method from \cref{chap-dynamics}.
# \end{assumption}
# #+END_EXPORT
# Further to this, it assumes that a desired dynamics mode $\desiredMode$ is /known a priori/.
# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# Let $\desiredMode$ denote the desired dynamics mode governed
# by its state domain
# $\desiredStateDomain = \{ \state \in \stateDomain \mid \modeVar = \desiredMode \}$.
# The desired mode's transition dynamics are given by
# $\desiredDynamicsFunc: \desiredStateDomain \times \controlDomain \rightarrow \stateDomain$,
# with continuous states $\state \in \stateDomain \subseteq \R^\StateDim$ and controls
# $\control \in \controlDomain \subseteq \R^\ControlDim$.
# \end{assumption}
# #+END_EXPORT

# The desired mode's transition dynamics are given by
# $\desiredDynamicsFunc: \desiredStateDomain \times \controlDomain \rightarrow \stateDomain$,
# with continuous states $\state \in \stateDomain \subseteq \R^\StateDim$ and controls
# $\control \in \controlDomain \subseteq \R^\ControlDim$,
# where $\desiredStateDomain = \{ \state \in \stateDomain \mid \modeVar = \desiredMode \}$
# denotes the subset of the state space associated with the desired dynamics mode.
# Note that the desired mode's transition dynamics $\desiredDynamicsFunc$
# are free to leave the desired mode's state space $\desiredStateDomain$.






\todo{cite active learning paper and multimodal mogpe paper?}
# The most similar works are cite:schreiterSafe2015 where active learning is gu



# Of particular interest in this chapter is the GP posterior over the desired mode's gating function.
# cref:fig-traj-opt-gating-network-gp visualises this GP posterior after training on the historical data set of state
# transitions from the illustrative example in cref:illustrative_example.
# One approach to encoding mode remaining behaviour is to consider the geometry of the latent gating functions.

** Concepts from Riemannian Geometry label:sec-geometry-recap
*** intro :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
%\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/mosvgpe/desired_gating_gp_no_traj.pdf}
\includegraphics[width=\textwidth]{./images/mode-opt/env/scenario_7/mosvgpe/desired_gating_gp_no_obs.pdf}
\caption{
Visualisation of the GP posterior mean (left) and posterior variance (right), associated with the
desired mode's gating function after training \acrshort{mosvgpe}
on the historical data set of state transitions from the 2D quadcopter environment
in the illustrative example from \cref{illustrative_example}.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line)
and a subset of the environment which has not been observed (hashed box).}
\label{fig-traj-opt-gating-network-gp}
\end{figure}
#+END_EXPORT
# cref:fig-traj-opt-gating-network-gp shows the GP posterior associated with the desired mode's gating function
# after training
# \acrshort{mosvgpe} on the historical data set of state transitions from the quadcopter experiment in cref:illustrative_example.
The \acrshort{mosvgpe} model correctly identifies the underlying dynamics modes and infers informative
latent spaces that can be used to encode mode remaining behaviour.
cref:fig-traj-opt-gating-network-gp shows the GP posterior associated with the desired mode's gating function
after training
\acrshort{mosvgpe} on the historical data set of state transitions from the quadcopter experiment in cref:illustrative_example.
The work in this chapter is based on the observation that
goals 1 and 2 can be encoded as finding length minimising trajectories on the desired mode's gating function.
Intuitively, the length of a trajectory from $\mathbf{x}_0$ to $\mathbf{x}_f$ on the
manifold given by the desired mode's gating function,
increases when it passes over the contours; analogous to climbing a hill.
Given appropriate scaling of the gating function, shortest trajectories between two locations are
those that attempt to follow the contours, and as a result,
remain in a single mode by not climbing up or down any hills.
The relevant concepts from Riemannian geometry, that are needed to formalise this intuition, are now introduced.

# Now that the relevant concepts from Riemannian geometry have been introduced, the objective function
# for the first trajectory optimisation can be detailed.
# The geometry of the latent gating functions can be used to formulate a cost term
# that favours trajectories remaining in a preferred dynamics mode.
# One approach is to obtaining mode remaining behaviour is to consider the geometry of the latent gating functions.
# Let us quickly recap how to calculate lengths on Riemannian manifolds.

#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Riemannian manifolds}
In this dissertation if suffices to consider manifolds $\manifold$ defined by a mapping,
%#+BEGIN_EXPORT latex
\begin{equation} \label{eq-manifold-function}
\manifoldFunction : \manifoldDomain \rightarrow \manifoldCodomain,
\end{equation}
%#+END_EXPORT
where $\manifoldDomain$ and $\manifoldCodomain$ are open subsets of Euclidean spaces.
The manifold $\manifold$ is given by $\manifold = \manifoldFunction(\manifoldDomain)$ and is said to
be immersed in the ambient space $\manifoldCodomain$.
The dimensionality of the surface is denoted $\manifoldDomainDim = \text{dim}(\manifoldDomain)$
whilst $\manifoldCodomainDim = \text{dim}(\manifoldCodomain)$ denotes the dimensionality of the ambient space.
Riemannian manifolds can intuitively be seen as $\manifoldCodomainDim\text{-dimensional}$
curved surfaces with a smoothly
varying positive-definite inner product, governed by the Riemannian metric $\metricTensor$ \citep{carmoRiemannian1992}.
\begin{definition}[Riemannian Metric]
A Riemannian metric $\metricTensor$,
on a manifold $\manifold$, is a smooth function
$\metricTensor : \manifoldDomain \rightarrow \R^{\manifoldDomainDim \times \manifoldDomainDim}$
that assigns a symmetric positive definite matrix to any point in $\manifoldDomain$.
%A Riemannian metric $\metricTensor$ on a
%manifold $\manifold$ is a symmetric and positive definite matrix which defines
%a smoothly varying inner product
%$\langle \dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \rangle_{\mathbf{x}} = \dot{\mathbf{x}}_a^T \mathbf{G}(\mathbf{x}) \dot{\mathbf{x}}_b$
%in the tangent space $T_{\mathbf{x}}\manifold$, for each point $\mathbf{x} \in \manifold$ and
%$\dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \in T_{\mathbf{x}}\manifold$.
\end{definition}
Intuitively, the metric forms a local inner product in $\manifoldDomain$ that informs how to measure lengths
on the manifold $\manifold$, locally in $\manifoldDomain$.
Riemannian manifolds locally resemble Euclidean spaces and have
globally defined differentiable structure.
\end{myquote}
#+END_EXPORT

*** Lengths in Euclidean Spaces :ignore:
\newline
*Lengths in Euclidean spaces*
The $l^2$ norm (aka Euclidean norm) provides an intuitive notion for the length of a
vector $\manifoldInput \in \manifoldDomain \subseteq \R^{\manifoldDomainDim}$
in a Euclidean space.
Overloading notation, a continuous time trajectory is also denoted $\trajectory$ and corresponds to
${\trajectory: [t_0, t_f] \rightarrow \manifoldDomain$.
Under the $l^2$ norm, the length of a trajectory $\trajectory$ is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-euclidean-length}
\text{Length}\left(\trajectory\right)
&= \int_{t_0}^{t_f}\left\|\dot{\manifoldInput}(t)\right\|_2 \mathrm{d}t,
\end{align}
#+END_EXPORT
where Newton's notation has been used to denote differentiation with respect to time $t$.
As a norm can be expressed for any space endowed with an inner product, it is possible to
calculate lengths of trajectories on manifolds endowed with an inner product.
# Just as the dot product is the inner product of the Euclidean space, it is possible to
# define inner products for Riemannian spaces (also known as Riemannian manifolds).
# Remembering that a norm can be expressed for any space endowed with an inner product,
# provides us with the tools to for calculating lengths on manifolds.

# Under the $l^2$ norm, the length of a trajectory
# ${\trajectory = \{\manifoldInput(t) \in \manifoldDomain \mid \forall t \in [t_0, t_f]\}}$ is given by,

*** Lengths on Riemannian Manifolds :ignore:

*Lengths on Riemannian manifolds*
The length of a trajectory $\trajectory$ on a manifold $\manifold$, can be calculated
by mapping it through the function $\manifoldFunction$ and
using cref:eq-euclidean-length,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-length}
\text{Length}\left(\manifoldFunction(\trajectory)\right)
&= \int_{t_0}^{t_f}\left\|\dot{\manifoldFunction}(\manifoldInput(t))\right\|_2 \mathrm{d}t.
\end{align}
#+END_EXPORT
Applying the chain-rule allows cref:eq-manifold-length to be expressed in terms of the Jacobian and the
velocity,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-length-chain}
\text{Length}\left(\manifoldFunction(\trajectory)\right)
&= \int_{t_0}^{t_f}\left\|\jacobian \dot{\manifoldInput}(t)\right\|_2 \mathrm{d}t, \\
\jacobian &= \frac{\partial \manifoldFunction}{\partial \manifoldInput(t)} \in \R^{1 \times \manifoldDomainDim}.
\end{align}
#+END_EXPORT
This implies that the length of a trajectory on the manifold $\manifold$,
can be calculated in the input space $\manifoldDomain$,
using a locally defined norm,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-norm}
\left\|\jacobian \dot{\manifoldInput}(t)\right\|_2
&= \sqrt{ \left( \jacobian \dot{\manifoldInput}(t) \right)^T
\jacobian \dot{\manifoldInput}(t)} \nonumber \\
&= \sqrt{\dot{\mathbf{x}}^T(t) \metricTensor_{\mathbf{x}_t} \dot{\manifoldInput}(t)}
\defeq \left\|\dot{\manifoldInput}(t)\right\|_{\metricTensor_{\manifoldInput_t}},
%= \left\|\dot{\mathbf{x}}(t)\right\|_{\mathbf{G}(\mathbf{x}(t))}.
\end{align}
#+END_EXPORT
where $\metricTensor_{\mathbf{x}_t} = \jacobian^T \jacobian}$
is a symmetric positive definite matrix
(akin to a local Mahalanobis distance measure), known as the natural Riemannian metric.
The length of a trajectory on a manifold $\manifold$, endowed with the metric $\metricTensor$,
can then be calculated with,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-length-G}
\text{Length}(\manifoldFunction(\trajectory))
&= \int_{t_0}^{t_f}\left\|\dot{\manifoldInput}(t)\right\|_{\metricTensor_{\mathbf{x}_t}} \mathrm{d}t.
\end{align}
#+END_EXPORT

cref:fig-traj-opt-gating-network-gp shows the GP posterior over the desired mode's gating function,
$\desiredGatingFunction : \stateDomain \times \controlDomain \rightarrow \gatingCodomain$.
Consider finding
length minimising trajectories on the manifold $\desiredManifold=\desiredGatingFunction(\gatingDomain)$
associated with the desired mode's gating function, where the metric is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-desired-metric-tensor}
\desiredMetricTensor &= \desiredJacobian^T \desiredJacobian, \\
\desiredJacobian &= \frac{\partial \desiredGatingFunction}{\partial \manifoldInput(t)} \in \R^{1 \times \GatingDim}.
\end{align}
#+END_EXPORT
These trajectories will attempt to remain in the desired dynamics mode $\desiredMode$, encoding Goals 1 and 2.
However, length minimising trajectories subject to this metric do not encode Goal 3.
That is, they will not avoid regions of the learned dynamics with high /epistemic uncertainty/.
However, Goal 3 can be encoded by observing that the metric tensor is actually stochastic and extending the concept
of length minimising trajectories to probabilistic manifolds.
# For notational conciseness, only a single gating function
# $\manifoldFuncion$ is used to denote the desired mode's gating function in this chapter.

# Note, that the gating functions are assumed to only depend on the state,
# $\desiredGatingFunction : \gatingDomain \rightarrow \gatingCodomain$,
# where $\gatingDomain \in \R^{\GatingDim}$ and $\gatingCodomain \in \R$,
# so the metric is given by,

# That is, trajectories minimising cref:eq-manifold-length-G,
# subject to the metric $\desiredMetricTensor$,

# In this work the gating functions are assumed to only depend on the state,
# $\desiredGatingFunction : \gatingDomain \rightarrow \gatingCodomain$,
# where $\gatingDomain \in \R^{\GatingDim}$ and $\gatingCodomain \in \R$.
# That is, trajectories minimising cref:eq-manifold-length-G,
# subject to the metric $\desiredMetricTensor$,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-desired-metric-tensor}
# \desiredMetricTensor &= \desiredJacobian^T \desiredJacobian, \\
# \desiredJacobian &= \frac{\partial \desiredGatingFunction}{\partial \manifoldInput(t)} \in \R^{1 \times \GatingDim}.
# \end{align}
# #+END_EXPORT

# These trajectories will attempt to remain in the desired dynamics mode $\desiredMode$, i.e. encode Goals 1 and 2.
# However, this metric does encode Goal 3.
# We now address Goal 3 by observing that the metric tensor is actually stochastic.
# For notational conciseness, only a single gating function
# $\manifoldFuncion$ is used to denote the desired mode's gating function in this chapter.

*** Probabilistic Geometries label:sec-prob-geo
**** Extension to Probabilistic Geometries :ignore:
\newline

Following cite:tosiMetrics2014 this work formulates a metric tensor that captures the variance in the manifold
via a probability distribution.
First note that as the differential operator is linear, the derivative of a GP is also a GP,
assuming that the mean and covariance functions are differentiable.
#+BEGIN_EXPORT latex
\begin{assumption}[Differentiable Gaussian Process] \label{}
Let $\mu : \stateDomain \rightarrow \R$ and $k: \stateDomain \times \stateDomain \rightarrow \R$
denote the mean and covariance functions associated with a Gaussian process.
The Gaussian process is differentiable iff
$\exists \frac{\partial \mu(\state)}{\partial \state}, \frac{\partial^2 k(\state, \state')}{\partial \state \partial \state'} \quad \forall \state, \state' \in \stateDomain$.
\end{assumption}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo{should there be minus mean function in jac?}
\begin{myquote}
\textbf{Gaussian Process Jacobain}
As the differential operator is linear, a function $\manifoldFunction: \gatingDomain \rightarrow  \R$ distributed as a GP,
\begin{align} \label{eq-jacobian-gp-random-var}
\manifoldFunction(\cdot) \sim \mathcal{GP}\left(\mu(\cdot), k(\cdot, \cdot)\right),
\end{align}
%\begin{align} \label{eq-jacobian-gp-random-var}
%\manifoldFunction(\allInput) \sim \mathcal{GP}\left(\mu(\allInput), k(\allInput, \allInput)\right),
%\end{align}
where $\mu$ and $k$ represent the mean and covariance functions,
is jointly Gaussian with its Jacobian
at a new input location $\testInput \in \R^{1 \times \GatingDim}$,
\begin{align} \label{eq-jacobian-random-var} \testJac = \Jac(\testInput) = \frac{\partial \manifoldFunction}{ \partial \testInput} \in \R^{\GatingDim},
\end{align}
assuming that the mean and covariance functions are differentiable.
As such, the conditional distribution over the Jacobian
$\testJac$ can easily be obtained using the properties of multivariate Normals
and is given by,
%\begin{align} \label{eq-predictive-jacobian-dist}
%\Jac(\cdot)
%&\sim \mathcal{GP}\left(
%\underbrace{\partial \mu(\cdot) + \jacManifoldKerneldN \manifoldKernelNN^{-1} \manifoldFunction(\allInput)}_{\muJac},
%\underbrace{\hessManifoldKerneldd - \jacManifoldKerneldN \manifoldKernelNN^{-1} \jacManifoldKernelNd}_{\covJac}
%\right),
%\end{align}
\begin{align} \label{eq-predictive-jacobian-dist}
\testJac
&\sim \mathcal{N}\left(
\underbrace{\frac{\partial \mu}{\partial \testInput} + \jacManifoldKernelsN \manifoldKernelNN^{-1} \manifoldFunction(\allInput)}_{\muJac},
\underbrace{\hessManifoldKernel - \jacManifoldKernelsN \manifoldKernelNN^{-1} \jacManifoldKernelNs}_{\covJac}
\right),
\end{align}
%\begin{align} \label{eq-predictive-jacobian-dist}
%p\left(\testJac | \manifoldFunction(\allInput), \testInput, \allInput \right)
%&= \mathcal{N}\left(\testJac \mid \muJac, \covJac \right) \\
%\muJac &= \frac{\partial \mu}{\partial \testInput} + \jacManifoldKernelsM \manifoldKernelNN^{-1}
%\left(\manifoldFunction(\allInput) - \manifoldMeanFunc(\allInput) \right) \\
%\muJac &= \frac{\partial \mu}{\partial \testInput} + \jacManifoldKernelsM \manifoldKernelNN^{-1}
%\manifoldFunction(\allInput) \\
%\covJac &= \hessManifoldKernel - \jacManifoldKernelsN \manifoldKernelNN^{-1} \jacManifoldKernelNs
%\end{align}
where the covariance matrices are given by,
\begin{align} \label{eq}
\manifoldKernelNN &= k\left( \allInput, \allInput \right) \in \R^{\NumData \times \NumData} \\
\jacManifoldKernelsN&= \frac{\partial k\left(\testInput, \allInput\right)}{\partial \testInput} \in \R^{\GatingDim \times \NumData} \\
\hessManifoldKernel &= \frac{\partial^2 k\left(\testInput, \testInput \right)}{\partial \testInput \partial \testInput} \in \R^{\GatingDim \times \GatingDim}.
\end{align}
Eq. \ref{eq-predictive-jacobian-dist} is a $\GatingDim\text{-dimensional}$ multivariate Normal distribution.
\end{myquote}
#+END_EXPORT

Therefore, the metric tensor $\desiredMetricTensor$ in cref:eq-desired-metric-tensor is the outer product of
two Normally distributed random variables.
As such, the metric tensor $\desiredMetricTensor$ is also a random variable, following
a non-central Wishart distribution citep:andersonNonCentral1946,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-metric-dist}
  \desiredMetricTensor \sim
  \mathcal{W}_{\GatingDim}\left(P, \covJac, \mathbb{E}\left[\Jac^{T}\right] \mathbb{E}[\Jac]\right),
\end{align}
#+END_EXPORT
where $P$ is the number of degrees of freedom (always one in our case) and
$\E\left[\desiredJacobian\right]$ and $\covJac$ are the mean and covariance matrices
associated with the GP over the Jacobian.
The expected value of the metric tensor in cref:eq-metric-dist is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-metric}
  \E[\desiredMetricTensor] = \E[\mathbf{J}^T] \E[\mathbf{J}] + \covJac.
\end{align}
#+END_EXPORT
Importantly, this expected metric tensor includes a covariance term $\covJac$,
which implies that lengths on the manifold increase in areas of high covariance.
This is a desirable behaviour because it encourages length minimising trajectories
to avoid regions of the learned dynamics with high epistemic uncertainty,
encoding Goal 3.
To aid with user control the metric tensor in cref:eq-expected-metric is modified with
a weighting parameter $\lambda$ that enables the relevance of the covariance term to be adjusted,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-metric-weighting}
  \tilde{\mathbf{G}} = \E[\mathbf{J}^T] \E[\mathbf{J}] + \lambda \mathbf{\Sigma}_J.
\end{align}
#+END_EXPORT
Setting $\lambda$ to be small should find trajectories that prioritise staying in the desired mode,
whereas selecting a large $\lambda$ should find trajectories that prioritise avoiding regions
of the dynamics with high epistemic uncertainty.


The model in Chapter ref:chap-dynamics is built upon sparse GP approximations,
so the Jacobian in cref:eq-predictive-jacobian-dist must be extended for such approximations.


#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Sparse Gaussian Process Jacobian}
To obtain the distribution over the Jacobian
in the sparse variational Gaussian process setting, we first condition on the inducing variables
$\manifoldInducingOutput  \in \R^{\NumInducing \times 1}$,
%\begin{equation} \label{eq-}
%\manifoldInducingOutput \sim \mathcal{GP}\left(\mu(\manifoldInducingInput),
%k(\manifoldInducingInput, \manifoldInducingInput)\right)
%\end{equation}
\begin{equation} \label{eq-}
\testJac \mid \manifoldInducingOutput \sim \mathcal{N}\left(
\frac{\partial \mu}{\partial \testInput} + \jacManifoldKernelsM \manifoldKernelMM^{-1}
\manifoldInducingOutput,
\hessManifoldKernel - \jacManifoldKernelsM \manifoldKernelMM^{-1}
\jacManifoldKernelMs \right).
\end{equation}
where the inducing variables' density is from the prior in \cref{eq-jacobian-gp-random-var},
so the covariance matrices are given by,
\begin{align} \label{eq-}
\manifoldKernelMM &= k\left( \manifoldInducingInput, \manifoldInducingInput \right) \in \R^{\NumInducing \times \NumInducing} \\
\jacManifoldKernelsM&= \frac{\partial k\left(\testInput, \manifoldInducingInput \right)}{\partial \testInput} \in \R^{\GatingDim \times \NumInducing}.
\end{align}
The distribution over the Jacobian is then obtained by marginalising the inducing variables
with respect to their variational density,
\begin{equation} \label{eq-}
\manifoldInducingOutput \sim \mathcal{N}(\manifoldInducingOutputMean, \manifoldInducingOutputCov).
\end{equation}
The distribution over the Jacobian is then obtained via a Gaussian convolution,
\begin{align}
%q(\testJac \mid \testInput)
\testJac &\sim \mathcal{N}\left(
\underbrace{\frac{\partial \mu}{\partial \testInput} + \jacManifoldKernelsM \manifoldKernelMM^{-1}
\manifoldInducingOutputMean}_{\muJac},
\underbrace{\hessManifoldKernel - \jacManifoldKernelsM \manifoldKernelMM^{-1}
\left( \manifoldKernelMM - \manifoldInducingOutputCov \right) \manifoldKernelMM^{-1} \jacManifoldKernelMs}_{\covJac}
\right).
\end{align}
%The distribution over the Jacobian is approximated as follows,
%\begin{align}
%p(\testJac \mid \testInput, \allOutput, \allInput) &=
%\int \int p(\testJac \mid \testInput, \manifoldFunction(\allInput), \manifoldInducingOutput)
%p(\manifoldFunction(\allInput), \manifoldInducingOutput \mid \allOutput, \allInput)
%\text{d} \manifoldFunction(\allInput) \text{d} \manifoldInducingOutput \nonumber \\
%&\approx \int \int p(\testJac \mid \testInput, \manifoldFunction(\allInput), \manifoldInducingOutput)
%p(\manifoldFunction(\allInput) \mid \manifoldInducingOutput) \manifoldInducingVariational
%\text{d} \manifoldFunction(\allInput) \text{d} \manifoldInducingOutput \nonumber \\
%&= \int  p(\testJac \mid \testInput, \manifoldInducingOutput)
%\manifoldInducingVariational
%\text{d} \manifoldInducingOutput
%\coloneqq q(\testJac \mid \testInput)
%\end{align}
%where the mean and covariance are given by,
%\begin{align} q(\testJac \mid \testInput) &= \mathcal{GP}\left(\muJac, \covJac \right) \\
%\muJac &= \frac{\partial \mu}{\partial \testInput} + \jacManifoldKernelsM \manifoldKernelMM^{-1}
%\left( \manifoldInducingOutputMean - \manifoldMeanFunc(\allInput) \right), \\
%\covJac &= \hessManifoldKernel - \jacManifoldKernelsM \manifoldKernelMM^{-1}
%\left( \manifoldKernelMM - \manifoldInducingOutputCov \right) \manifoldKernelMM^{-1} \jacManifoldKernelMs \right).
%\end{align}
\end{myquote}
#+END_EXPORT

** Indirect Control via Latent Geodesics label:sec-traj-opt-collocation
*** intro :ignore:
This section presents an indirect trajectory optimisation algorithm that exploits the fact that
length minimising trajectories on the manifold endowed with the expected metric from cref:eq-expected-metric,
encodes all of the goals.
Shortest lengths on a manifold are known as geodesics, so we refer to shortest trajectories as geodesic trajectories.
This algorithm exploits the fact that geodesic trajectories are solutions to a 2^{nd} order \acrshort{ode} and projects
the optimisation onto this \acrshort{ode}.
# #+BEGIN_EXPORT latex
# \begin{remark}
# \end{remark}
# #+END_EXPORT

# The first control algorithm presented in this chapter is an indirect trajectory optimisation algorithm that
# exploits the fact that
# length minimising trajectories, on the manifold endowed with the expected metric from cref:eq-expected-metric,
# encodes both of our goals.
# Shortest lengths on a manifold are known as geodesics, so we refer to shortest
# trajectories as geodesic trajectories.

*** Geodesics :ignore:
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Geodesics}
Given the method for calculating lengths on Riemannian manifolds in \cref{eq-manifold-length},
it is trivial to define the notion of a shortest trajectory, or geodesic, as a length minimising trajectory.
Formally, a geodesic is defined as follows,
\begin{definition}[Geodesic]
Given two points $\manifoldInput_0, \manifoldInput_f \in
\manifold$, a Geodesic is a length minimising trajectory (curve)
$\trajectory_g$ connecting the points, such that,
\begin{subequations} \label{eq-geodesic}
\begin{align}
  \trajectory_{g} =\arg &\min_{\trajectory} \operatorname{Length}(\trajectory) \\
\text{s.t.} \quad  \manifoldInput(t_0)&=\manifoldInput_{0} \\
  \manifoldInput(t_f)&=\manifoldInput_{f}.
\end{align}
\end{subequations}
\end{definition}
\end{myquote}
#+END_EXPORT
*Geodesic \acrshort{ode}* An important observation from cite:carmoRiemannian1992, is that geodesics
satisfy a continuous-time $2^{\text{nd}}$ order \acrshort{ode}, given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-2ode}
 \ddot{\manifoldInput}(t)
&= \geodesicFunction(t, \manifoldInput, \dot{\manifoldInput}) \nonumber \\
&=-\frac{1}{2} \metricTensor^{-1}(\manifoldInput(t))\left[
\frac{\partial \operatorname{vec}[\metricTensor(\manifoldInput(t))]}{\partial \manifoldInput(t)}
\right]^{T}\left(\dot{\manifoldInput}(t) \otimes \dot{\manifoldInput}(t)\right),
\end{align}
#+END_EXPORT
where $\operatorname{vec}[\metricTensor(\manifoldInput(t)])$ stacks the columns of $\metricTensor(\manifoldInput(t))$
and $\otimes$ denotes the Kronecker product.
The implication of cref:eq-geodesic,eq-2ode, is that trajectories that are solutions
to the $2^{\text{nd}}$ order \acrshort{ode} in cref:eq-2ode, implicitly minimise the objective
in cref:eq-manifold-length-G.
Given this observation, computing geodesics involves finding a solution to cref:eq-2ode
with $\manifoldInput(t_0) = \manifoldInput_0$ and $\manifoldInput(t_f) = \manifoldInput_f$.
This is a boundary value problem (BVP) with a smooth solution so it can be solved using
any BVP solver, e.g. (multiple) shooting and collocation methods.
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-length-objective}
# \text{Length(\gatingFunc(\stateTraj))}
# = \int_{t_0}^{t_f}\left\|\dot{\manifoldInput}(t)\right\|_{\metricTensor(\mathbf{x}(t))} \mathrm{d}t.
# \end{align}
# #+END_EXPORT

*** Implicit Trajectory Optimisation
Solving the 2^{nd} order \acrshort{ode} in cref:eq-2ode with the expected metric from cref:eq-expected-metric-weighting,
is equivalent to solving our trajectory optimisation problem subject to the same boundary conditions.
This resembles an indirect control method as it is based on an observation that the
necessary conditions for optimality are encoded via the geodesic \acrshort{ode}.
However, it is worth noting that solutions to the geodesic \acrshort{ode} are not guaranteed to satisfy the
dynamics constraints.


# The original trajectory optimisation problem can be converted to finding $\state(t)$
# for $t \in [t_0, t_f]$ subject to the boundary conditions and the geodesic \acrshort{ode},
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-diff-flat-ode}
# \ddot{\mathbf{x}}
# &= f_G(t, \mathbf{x}(t), \dot{\mathbf{x}}(t)).
# \end{align}
# #+END_EXPORT
*Collocation*
Since neither $\dot{\mathbf{x}}(t_0)$ nor $\dot{\mathbf{x}}(t_f)$ are known, cref:eq-2ode cannot
be solved with simple forward-backward integration.
Instead, the problem is transcribed using collocation.
Collocation methods are used to transcribe continuous-time trajectory optimisation problems into
nonlinear programs, i.e. constrained parameter optimisation citep:kellyIntroduction2017,fahrooDirect2000.
The expected metric in cref:eq-expected-metric is substituted into cref:eq-2ode and solved via collocation.
This work implements a simple Hermite-Simpson collocation method that enforces the state
derivative interpolated by the polynomials to equal the geodesic \acrshort{ode} $f_G$
at the midpoints between a set of $I$ collocation points
$\{\state_i\}_{i=1}^I$. This is achieved through the collocation defects,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-defect}
\Delta_{i+\frac{1}{2}} &= \ddot{\state}_{i+\frac{1}{2}} - f_G(t_{i+\frac{1}{2}}, \state_{i+\frac{1}{2}}, \dot{\state}_{i+\frac{1}{2}})
\end{align}
#+END_EXPORT
where $\ddot{\state}_{i+\frac{1}{2}}, \dot{\state}_{i+\frac{1}{2}}, \state_{i+\frac{1}{2}}$
are obtained by interpolating between $\state_i$ and $\state_{i+1}$.
cref:eq-defect defines a set of constraints that ensure trajectories are solutions
to the geodesic \acrshort{ode} $f_G$.
The nonlinear program that this method solves is given by,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-collocation-problem}
\begin{align}
\min_{\state(t), \dot{\state}(t)}& \int_{t_0}^{t_f}
\costFunc(\state(\timeInd), \dot{\state}(\timeInd)) \text{d}t \\
&\text{s.t. }\text{\cref{eq-defect}}  \\
\mathbf{x}&(t_0) = \mathbf{x}_0 \\
\mathbf{x}&(t_f) = \targetState.
\end{align}
\end{subequations}
#+END_EXPORT
In practice, a quadratic cost function is used to regularise the state derivative $\dot{\state}$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-cost-state-derivative}
\costFunc(\state(t), \dot{\state}(t))
&= \dot{\state}(\timeInd)^T \controlCostMatrix \dot{\state}(\timeInd)
= \left\| \dot{\state}(\timeInd) \right\|_{\controlCostMatrix},
\end{align}
#+END_EXPORT
where $\controlCostMatrix$ is a user defined, real symmetric positive definite weight matrix.
It is solved using Sequential Least Squares Programming (SLSQP) in SciPy citep:2020SciPy-NMeth.

*Latent variable controls*
This nonlinear program returns a continuous-time state trajectory, however, it does not return the
control trajectory.
Inspired by differential flatness, the control trajectory is recovered from the state trajectory by performing
inference in the probabilistic dynamics model.
The state difference outputs $\stateDiffTraj = \{\stateDiff_{\timeInd}\}_{\timeInd=1}^{\TimeInd}$ are first
calculated from the state trajectory $\stateTraj = \{\state_{\timeInd}\}_{\timeInd=0}^{\TimeInd}$.
The control trajectory $\controlTraj$ is then inferred from the state trajectory by extending
the ELBO for the desired mode's SVGP expert with latent variable inputs.
Following cite:hensmanGaussian2013, the ELBO for a single SVGP expert is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-control-elbo-svgp}
\log p(\stateDiffTraj \mid \stateTraj, \controlTraj) \geq
\E_{q(\mode{\latentFunc}(\stateTraj, \controlTraj))} \left[ \nonumber
&\log p(\stateDiffTraj \mid \mode{\latentFunc}(\stateTraj, \controlTraj)) \right] \\
&- \text{KL}\left(q(\mode{\latentFunc}(\expertInducingInput)) \mid p(\mode{\latentFunc}(\expertInducingInput))) \right)
\coloneqq \mathcal{L}_{\text{SVGP}},
\end{align}
#+END_EXPORT
where the variational posterior is given by
$q(\mode{\latentFunc}(\stateTraj, \controlTraj)) &= \int p(\mode{\latentFunc}(\stateTraj, \controlTraj) \mid \mode{\latentFunc}(\expertInducingInput)) q(\mode{\latentFunc}(\expertInducingInput)) \text{d} \mode{\latentFunc}(\expertInducingInput)$.
The control inputs $\controlTraj$ are recovered by treating them as latent variables and extending the lower bound to,
#+BEGIN_EXPORT latex
\begin{align}
\log p(\stateDiffTraj \mid \stateTraj) &= \log \int p(\stateDiffTraj \mid \stateTraj, \controlTraj) p(\controlTraj) \text{d}\controlTraj \\
&\geq \E_{q(\controlTraj)} \left[ \mathcal{L}_{\text{SVGP}} + \log p(\controlTraj) - \log q(\controlTraj) \right],
\label{eq-control-elbo}
\end{align}
#+END_EXPORT
where each time step of the latent control trajectory is assumed to be Normally distributed,
#+BEGIN_EXPORT latex
\begin{align}
p(\controlTraj) = \prod_{\timeInd=0}^{\TimeInd-1} \mathcal{N}(\control_{\timeInd} \mid \mathbf{0}, \mathbf{I}),
\end{align}
#+END_EXPORT
and its variational posterior is given by,
#+BEGIN_EXPORT latex
\begin{align}
q(\controlTraj) = \prod_{\timeInd=0}^{\TimeInd-1} \mathcal{N}(\control_{\timeInd} \mid \mathbf{m}_{\timeInd}, \mathbf{S}_{\timeInd}).
\end{align}
#+END_EXPORT
The posterior over the latent control trajectory $q(\controlTraj) \approx p(\controlTraj \mid \stateTraj, \stateDiffTraj)$
is obtained by finding the variational parameters
$\{\mathbf{m}_{\timeInd}, \mathbf{S}_{\timeInd}\}_{\timeInd=0}^{\TimeInd-1}$ that maximise the ELBO in
cref:eq-control-elbo.

# This objective regularises the control trajectory under the L2 norm.
# The meaning of minimum control effort depends upon the specific problem.
# It may refer to minimising the total expenditure of fuel, or to minimising the energy dissipated from the system.
# Regardless of the specific interpretation, it is usually desirable to regularise the controls.

\todo{Lagrance multipliers to make TO unconstrained? Feels unnecessary to include...}

# *** Results label:sec-traj-opt-results
# #+NAME: fig-geometric-traj-opt-over-svgp
# #+ATTR_LATEX: :width 1.1\textwidth :placement [!t] :center t
# #+caption: Contour plots showing the GP posterior mean (left) and variance (right) over the gating function associated with dynamics mode 1 after training on a subset of the quadcopter data set. The initial and optimised trajectories are overlayed to show the influence of the GP's mean and variance on the trajectory optimisation with different $\lambda$ settings.
# [[file:./images/geometric-traj-opt-over-svgp.pdf]]

*** Remarks :ignore:
Although this method provides an elegant solution to finding trajectories that satisfy Goals 1, 2 and 3,
it is not without its limitations.
First of all, this approach does not necessarily find trajectories that satisfy the dynamics constraints,
as it projects the problem onto the geodesic \acrshort{ode}.
#+BEGIN_EXPORT latex
\begin{remark}
Dynamics constraints not guaranteed to be satisfied.
\end{remark}
#+END_EXPORT
Secondly, it does not consider the full distribution over state-control trajectories.
Without the inclusion of the full probabilistic dynamics model, it is impossible
to consider the full distribution over state-control trajectories.
Although propagating uncertainty through a single dynamics GP is straightforward,
handling the collocation constraints is not.
This is because the geodesic \acrshort{ode} will become a Stochastic Differential Equation (SDE).
#+BEGIN_EXPORT latex
\begin{remark}
Ignores much of the stochasticity inherent in the problem.
\end{remark}
#+END_EXPORT

The goal of this chapter is to find trajectories that are $\delta-\text{mode remaining}$ (cref:def-delta-mode-remaining).
However, this method is not able to provide such guarantees as it ignores the state uncertainty accumulated
over a trajectory.
Consider calculating the probability that a single time step remains in the desired mode,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-mode-chance-constraint-integral}
\Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{0}, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd-1}=\desiredMode) =
&\int \underbrace{\Pr(\modeVar_{\timeInd} = \desiredMode \mid \GatingFunc(\state_{\timeInd}))}_{\text{Bernoulli/softmax likelihood}} \nonumber \\
&\int \underbrace{q(\GatingFunc(\state_{\timeInd}) \mid \state_{\timeInd})}_{\text{approx posterior}}
\underbrace{p(\state_\timeInd \mid \state_{0}, \control_{0:\timeInd-1}, \bm\modeVar_{0:\timeInd-1}=\desiredMode)}_{\text{state dist}}
\text{d} \state_{\timeInd}
\text{d} \GatingFunc(\state_{\timeInd})
\end{align}
\normalsize
#+END_EXPORT
where $q(\GatingFunc(\state_{\timeInd}) \mid \state_{\timeInd})$ is the approximate posterior over the
gating functions from cref:eq-predictive-gating.
Note the dependence on the state input $\state_{\timeInd}$ is reintroduced here,
as it becomes a random variable when making multi-step predictions.
This probability will decrease as the uncertainty in the state increases.
For example, when a trajectory passes through regions of the desired dynamics GP with high uncertainty.
This is implied by the marginalisation over $\state_\timeInd$.
It will also decrease when the trajectory passes through regions of the state space where the gating functions
are uncertain, indicated by the marginalisation over $\GatingFunc(\state_{\timeInd})$.
The method presented in this section does not quantify the state uncertainty over a trajectory and at
best can calculate the
following probability,
$\Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{0:\timeInd}, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd-1}=\desiredMode)$.
As such, this method cannot validate that trajectories are $\delta-\text{mode remaining}$.
#+BEGIN_EXPORT latex
\begin{remark}
Cannot validate that trajectories are $\delta-\text{mode remaining}$.
\end{remark}
#+END_EXPORT
# The trajectory optimisation algorithm presented in the next section attempts to address these issues.


# Firstly, this approach does do not necessarily find trajectories that satisfy the dynamics constraints,
# as it projects the problem onto the geodesic \acrshort{ode}.
# Secondly, this algorithm ignores much of the stochasticity inherent in the problem,
# as it does not consider the full distribution over state-control trajectories.
# Without the inclusion of the full probabilistic dynamics model, it is impossible
# to consider the full distribution over state-control trajectories.
# Although propagating uncertainty through a single GP dynamics mode is straight forward,
# how to handle the collocation constraints is not immediately clear.
# This is because the geodesic \acrshort{ode} will become a Stochastic Differential Equation (SDE).

# Perhaps the biggest limitation of this method, is its dependence on the
# differential flatness property, which is used to obtain the controls from the
# optimised state trajectory and geodesic \acrshort{ode}.
# #+BEGIN_EXPORT latex
# \begin{remark}
# Requires the state and control to be expressable in terms of a flat output.
# %Only applicable in systems where the state and control can be expressed in terms of a flat output.
# \end{remark}
# #+END_EXPORT
# This dependence makes this algorithm only applicable in systems where the state and control can be expressed in terms
# of a flat output.




# Thirdly, this algorithm does not consider the full stochastic optimal control problem in cref:mode-soc-problem.
# In reality, the geodesic \acrshort{ode} in cref:eq-2ode is stochastic because both the state and Jacobian are random
# variables.
# This algorithm assumes that the goals are encoded via the expected metric in cref:eq-expected-metric
# such that the full stochastic problem can be simplified to a deterministic \acrshort{ode}.
# In practice, this assumption appears to achieve both of our goals whilst offering an easier problem to solve.
** Direct Control via Riemannian Energy label:sec-traj-opt-energy
*** intro :ignore:
This section details a direct control approach which embeds the mode remaining behaviour directly into the
SOC problem in cref:eq-mode-soc-problem, via a geometric objective function.
In contrast to the previous approach, this method:
1) enforces the dynamics constraints,
2) principally handles the uncertainty associated with the dynamics,
3) can validate that trajectories are $\delta-\text{mode remaining}$.
This approach is a shooting method that enforces the dynamics constraints through simulation, i.e.
the state trajectory is enforced to match the integral of the dynamics with respect to time.

Similar to the previous approach, this method builds on the observation that
length minimising trajectories on the Riemannian manifold $\manifold$,
associated with the desired mode's gating function $\manifoldFunction$, encodes the goals.
Further to this, this method exploits the fact that length minimising trajectories on a Riemannian manifold $\manifold$,
are also energy minimising trajectories citep:carmoRiemannian1992.
As such, mode remaining behaviour can be encoded through the minimisation of
Riemannian energy. Formally, this is given by,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-mode-soc-problem-geometry}
\begin{align}
\min_{\controlTraj} \E\big[ c&(\stateTraj, \controlTraj)) \big] \label{eq-mode-soc-problem-geometry-cost} \\
\text{s.t.} \quad \state_{\timeInd+1} &= \dynamicsFunc(\state_\timeInd, \control_\timeInd) + {\epsilon},
\quad {\epsilon} \sim \mathcal{N}(\mathbf{0}, \Sigma_{\epsilon})
%\state_\TimeInd &= \targetState,
\end{align}
\end{subequations}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{subequations} \label{eq-mode-soc-problem-geometry}
# \begin{align}
# \min_{\controlTraj} \E\big[ \text{Energy}&(\manifoldFunction(\stateTraj)) \big] \label{eq-mode-soc-problem-geometry-energy} \\
# \text{s.t.} \quad \state_{\timeInd+1} &= \dynamicsFunc(\state_\timeInd, \control_\timeInd) + {\epsilon},
# \quad {\epsilon} \sim \mathcal{N}(\mathbf{0}, \Sigma_{\epsilon}) \\
# \state_0 &= \state_0 \\
# \state_\TimeInd &= \targetState
# \end{align}
# \end{subequations}
# #+END_EXPORT
where the mode remaining behaviour and the terminal state boundary condition are encoded via the
cost function,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-cost-with-energy}
\costFunc(\stateTraj, \controlTraj) &=
\underbrace{\text{Energy}(\manifoldFunction(\stateTraj))}_{\text{Riemannian energy}} +
\underbrace{(\terminalState - \targetState)^T \terminalStateCostMatrix (\terminalState - \targetState)}_{\text{terminal cost}}
+ \sum_{\timeInd=0}^{\TimeInd-1}
\underbrace{\control_{\timeInd}^T \controlCostMatrix \control_{\timeInd}}_{\text{control cost}}.
%\underbrace{\| \terminalState - \targetState \|_\terminalStateCostMatrix}_{\text{terminal state cost}} +
%\underbrace{\sum_{\timeInd=1}^{\TimeInd}
%\| \stateDiff_\timeInd \|_{\metricTensor_{\state_{\timeInd}}}}_{\text{Riemannian energy}} +
%+ \underbrace{\sum_{\timeInd=0}^{\TimeInd-1}
%\| \control_\timeInd \|_\controlCostMatrix}_{\text{control cost}}
\end{align}
#+END_EXPORT
where $\terminalStateCostMatrix$
and $\controlCostMatrix$ are user defined,
real, symmetric, positive semi-definite and positive definite matrices respectively.
The energy of a trajectory on a Riemannian manifold, endowed with the metric $\metricTensor$, is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-trajectory-energy}
\text{Energy}(\manifoldFunction(\trajectory)) = \sum_{\timeInd=1}^{\TimeInd}
\stateDiff_{\timeInd}^T \metricTensor_{\state_{\timeInd}} \stateDiff_{\timeInd},
\end{align}
#+END_EXPORT
where $\stateDiff_{\timeInd} = \state_{\timeInd} - \state_{\timeInd-1$ is the state difference.
#+BEGIN_EXPORT latex
\begin{remark} \label{}
In contrast to the collocation solver in \cref{sec-traj-opt-collocation},
the terminal state boundary condition is encoded via the cost function, instead of being enforced by the solver.
\end{remark}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-trajectory-energy}
# \text{Energy}(\manifoldFunction(\trajectory))
# = \int \dot{\state}(\timeInd) \metricTensor(\state(\timeInd)) \dot{\state}(\timeInd) \text{d} \timeInd.
# \end{align}
# #+END_EXPORT

This may seem like an easy optimisation problem,
however, calculating the expected cost in cref:eq-mode-soc-problem-geometry-cost is not trivial.
Given a starting state $\state_0$ and a control trajectory $\controlTraj$,
the expectation in cref:eq-mode-soc-problem-geometry-cost is taken with respect to the joint
state-metric distribution over a trajectory, $p(\stateTraj, \metricTensorTraj, \mid \state_0, \controlTraj)$.
Calculating this expectation is difficult as multi-step predictions in the MoSVSGPE dynamics model
cannot be calculated in closed-from.

This work adopts a two-stage approximation to obtain a closed-form expression for the expected cost.
First, multi-step dynamics predictions are approximated to obtain Normally distributed states at
each time step.
Given Normally distributed states, calculating the expected terminal and control cost terms in
cref:eq-quadratic-cost-with-energy is trivial.
However, the expected Riemannian energy in cref:eq-quadratic-cost-with-energy has no closed-form expression,
due to the metric's $\metricTensor$ dependence on the state.
The second stage approximates the calculation of the expected Riemannain energy under Normally distributed states.

# The following issues arise when calculating this expectation:
# 1) Calculating multi-step predictions in the MoSVSGPE dynamics model cannot be calculated in closed-from,
#    - This is because the state difference after the first time step is a Gaussian mixture and
#      propagating Gaussian mixtures through Gaussian processes has no closed-form solution.
# 2) Unlike the quadratic terminal/control costs  in cref:eq-quadratic-cost-with-energy, the expected Riemannian energy in cref:eq-trajectory-energy has no closed-form expression under Normally distributed states.
# This work adopts a two-stage approximation to obtain a closed-form expression for the expected cost.
# First, multi-step state predictions under the dynamics are approximated to obtain Normally distributed at
# each time step.
# Given Normally distributed states, calculating the expected terminal/control cost terms in
# cref:eq-quadratic-cost-with-energy is trivial.
# However, the expected Riemannian energy in cref:eq-quadratic-cost-with-energy has no closed-form expression,
# due to the metric's $\metricTensor$ dependence on the state, via the Jacobian.
# The second stage approximates the calculation of the expected Riemannain energy under Normally distributed states.

# 1) Simulating the \acrshort{mosvgpe} dynamics model to obtain multi-step predictions is not trivial,

# This work obtains a closed-form expression for the expected cost  by first simplifying the .

*** Approximate Inference for Dynamics Predictions label:sec-dynamics-predictions
#+BEGIN_EXPORT latex
\renewcommand{\singleInput}{\ensuremath{\state_\timeInd, \control_\timeInd}}
\renewcommand{\singleInput}{\ensuremath{\hat{\state}_\timeInd}}
\renewcommand{\singleOutput}{\ensuremath{\Delta\state_{\timeInd+1}}}

\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput)}}
\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput)}}
%\newcommand{\expertInducingVariational}{\ensuremath{q(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\expertInducingVariational}{\ensuremath{q(\expertInducingOutput)}}
\newcommand{\expertsInducingVariational}{\ensuremath{q(\expertsInducingOutput)}}
\newcommand{\expertsVariational}{\ensuremath{q(\LatentFunc_\numData)}}
%\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \expertInducingOutput)}}
%\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))}}
\renewcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \singleInput, \expertInducingOutput)}}

\renewcommand{\expertVariational}{\ensuremath{q(\mode{\latentFunc}(\singleInput) \mid \singleInput)}}

\newcommand{\singleInputMean}{\ensuremath{\hat{\bm\mu}}}
\newcommand{\singleInputCov}{\ensuremath{\hat{\bm\Sigma}}}
#+END_EXPORT
# *Approximate Inference for Dynamics Predictions*
Multi-step predictions in the \acrshort{mosvgpe} dynamics model have no closed-form solution  because the state
difference after the first time step is a Gaussian mixture, and
propagating Gaussian mixtures through Gaussian processes has no closed-form solution.
Further to this, constructing approximate closed-form solutions is difficult,
due to the exponential growth in the number of Gaussian components.
#+BEGIN_EXPORT latex
\begin{myquote} \label{}
Consider assuming each of the dynamics modes to be independent.
Recursively propagating the Gaussian components associated with the state, through
all of the modes, over a trajectory of length $\TimeInd$, would lead to the final state consisting of
$\ModeInd^{\TimeInd}$ Gaussian components.
\end{myquote}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{assumption}[Mode Remaining Dynamics] \label{}
# Let $\desiredMode$ denote a desired dynamics mode, defined by its state domain $\desiredStateDomain \subseteq \stateDomain$.
# Given an inital state $\state_0 \in \desiredStateDomain$, and a sequence of controls $\controlTraj$,
# the controlled system remains in the desired mode
# $\dynamicsFunc(\state_{\timeInd}, \control_{\timeInd}) &\in \desiredStateDomain \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd]$.
# \end{assumption}
# #+END_EXPORT
This work sidesteps this issue and obtains closed-form multi-step predictions
by enforcing that the controlled system remains in the desired dynamics mode.
Multi-step predictions can then be calculated in closed-form by cascading single-step predictions
using the desired dynamics GP, whose transition density is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-dynamics-gp}
\transitionDistK
&= \mathcal{N} \left( \mode{\latentFunc}(\singleInput) \mid
\mode{\mathbf{A}} \mode{\mathbf{m}},
\expertKernel(\singleInput, \singleInput)
+ (\mode{\mathbf{S}} - \expertKernel(\expertInducingInput,\expertInducingInput))
\mode{\mathbf{A}}^T
\right)
\end{align}
#+END_EXPORT
where
$\mode{\mathbf{A}} = \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}$.
Cascading single-step predictions requires recursively mapping uncertain state-control inputs through
the desired mode's dynamics GP, i.e. recursively calculating the following integral,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-state-unc-prop}
p(\state_{\timeInd+1} \mid \state_0, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd}=\desiredMode)
&= \int \transitionDistK p(\state_\timeInd \mid \state_{0}, \control_{0:\timeInd-1}, \bm\modeVar_{0:\timeInd-1}=\desiredMode) \text{d}\state_{\timeInd}
\end{align}
#+END_EXPORT
with $p(\state_0) = \delta(\state_0)$.
Approximate closed-form solutions exist for propagating Normally distributed states and controls
through GP models
citep:girardGaussian2003,kussGaussian2006,quinonero-candelaPropagation2003.
\todo{correct girard citation}
This work exploits the moment-matching approximation
implemented in the =uncertain_conditional= from GPflow citep:GPflow2017.
For more details on the approximation see Section 7.2.1 of cite:kussGaussian2006.

# However, this assumption should be enforced, i.e. the controlled system should remain in the desired mode.
# As the dynamics model is learned from observations, this work relaxes the requirement to finding
# mode remaining trajectories with high probability.
# #+BEGIN_EXPORT latex
# \begin{definition}[$\delta$-mode remaining] \label{def-delta-mode-remaining}
# Let $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ denote a multimodal dynamical system
# and $\desiredMode$ a desired dynamics mode defined by its state domain $\desiredStateDomain \subseteq \stateDomain$.
# Given an initial state $\state_0 \in \desiredStateDomain$ and $\delta \in (0,1]$,
# a system is said to be $\delta$-mode remaining under the controls $\controlTraj$ iff:
# \begin{align}
# \Pr( \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] : \dynamicsFunc(\state_{\timeInd},\control_{\timeInd}) &\in \desiredStateDomain,
# \control_{\timeInd} \in \controlDomain) \geq 1 - \delta
# \end{align}
# \end{definition}
# #+END_EXPORT
*$\delta-\text{mode remaining}$ chance constraints*
Enforcing the controlled system to remain in the desired dynamics mode simplifies
calculating multi-step predictions and the expected cost in cref:eq-quadratic-cost-with-energy.
As the dynamics model is learned from observations, this work relaxes the requirement to ensuring that trajectories
are $\delta-\text{mode remaining}$ (cref:def-delta-mode-remaining).
The conditions to be $\delta-\text{mode remaining}$ can be enforced with chance constraints,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mode-chance-constraint}
\Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{0}, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd-1}=\desiredMode)
\geq 1-\delta \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd].
%\geq \satisfactionProb \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd].
\end{align}
#+END_EXPORT
These constraints enforce the system to remain in the desired dynamics mode with satisfaction probability
$\satisfactionProb=1-\delta$, at each time step.
As the \acrshort{mosvgpe} model assumes that the mode indicator variable $\modeVar$,
depends on the state via the gating function, this probability is calculated as follows,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-mode-chance-constraint-integral}
\Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{0}, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd-1}=\desiredMode) =
&\int \underbrace{\Pr(\modeVar_{\timeInd} = \desiredMode \mid \GatingFunc(\state_{\timeInd}))}_{\text{Bernoulli/softmax likelihood}} \nonumber \\
&\int \underbrace{q(\GatingFunc(\state_{\timeInd}) \mid \state_{\timeInd})}_{\text{approx posterior}}
\underbrace{p(\state_\timeInd \mid \state_{0}, \control_{0:\timeInd-1}, \bm\modeVar_{0:\timeInd-1}=\desiredMode)}_{\text{state dist}}
\text{d} \state_{\timeInd}
\text{d} \GatingFunc(\state_{\timeInd})
\end{align}
\normalsize
#+END_EXPORT
where $q(\GatingFunc(\state_{\timeInd}) \mid \state_{\timeInd})$ is the approximate posterior over the
gating functions from cref:eq-predictive-gating.
# Note the dependence on the state input $\state_{\timeInd}$ is reintroduced here,
# as it becomes a random variable when making multi-step predictions.
# This probability will decrease as the uncertainty in the state increases.
# For example, when a trajectory passes through regions of the desired dynamics GP with high uncertainty.
# This is implied by the marginalisation over $\state_\timeInd$.
# It will also decrease when the trajectory passes through regions of the state space where the gating functions
# are uncertain, indicated by the marginalisation over $\GatingFunc(\state_{\timeInd})$.

*** Approximate Riemannian Energy
Given this approach for simulating the \acrshort{mosvgpe} dynamics model, the state at each time step is Normally distributed.
Unlike the terminal and control cost terms in cref:eq-quadratic-cost-with-energy, the expected Riemannian
energy,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-trajectory-riemannian-energy}
\E_{\stateTraj, \jacTraj} \left[ \text{Energy}(\stateTraj) \right]
= \sum_{\timeInd=1}^\TimeInd
\E_{\stateDist}\left[ \stateDiff_\timeInd^T
\E_{\Jac_{\state_{\timeInd}} \mid \state_{\timeInd}}\left[ \Jac_{\state_\timeInd} \Jac_{\state_\timeInd}^T \right]
 \stateDiff_\timeInd \right],
\end{align}
#+END_EXPORT
has no closed-form expression under Normally distributed states.
This is because the metric tensor $\metricTensor$ depends on the Jacobian, which depends on the state.
However, it is possible to approximate the expected energy to obtain a closed-form expression.

The distribution over the Jacobian when the input location in Normally distributed
$\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)$ can be calculated in closed-form when using the
Squared Exponential kernel.
However, this work simplifies the problem and calculates the Jacobian at the state mean of each time step
along a trajectory, i.e.
$\Jac_{\state_{\timeInd}} \approx \frac{\partial \manifoldFunction(\state_{\timeInd})}{\partial \stateMean}$.
The distribution over the Jacobian given deterministic inputs can be calculated using cref:eq-predictive-jacobian-dist.

Approximating the Jacobian to be independent of the state enables the expected metric tensor to be calculated in
closed-form with cref:eq-expected-metric-weighting.
Given this approximation, the Riemannian energy retains a quadratic form, so the expectation with respect to
$\stateDiff_{\timeInd} \sim \mathcal{N}(\stateDiffMean, \stateDiffCov)$ can be calculated with,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-approximate-trajectory-riemannian-energy}
\E \left[ \text{Energy}(\manifoldFunction(\stateTraj)) \right] &\approx
\sum_{\timeInd=1}^\TimeInd
\E_{\stateDiff_{\timeInd}}\left[ \stateDiff_\timeInd^T
\E_{\Jac_{\state_{\timeInd}}}\left[ \metricTensor_{\state_{\timeInd}} \right]
 \stateDiff_\timeInd \right] \nonumber \\
&= \sum_{\timeInd=1}^\TimeInd \stateDiffMean^T (\muJac \muJac^T + \covJac) \stateDiffMean
+ \text{tr}\left(
\left(\muJac \muJac^T + \covJac \right)
\stateDiffCov \right)
\end{align}
#+END_EXPORT
# It's a bit hacky but we could assumed the Jacobian distribution is given by marginalising the state
# and moment matching,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# p(\mathbf{J}_{\state_{\timeInd}}) =
# \int p(\mathbf{J}_{\state_{\timeInd}} \mid \state_\timeInd)
# \mathcal{N}(\state_{\timeInd} \mid \stateMean, \stateCov)
# \text{d}\state_{\timeInd}
# \approx \mathcal{N}(\mathbf{J}_{\state_{\timeInd}} \mid \muJac, \covJac)
# \end{align}
# #+END_EXPORT
#+BEGIN_EXPORT latex
\begin{myquote} \label{}
The expected metric tensor encourages trajectories to 1) follow contours on the desired mode's gating function
and 2) avoid entering regions where it is uncertain which mode governs the dynamics.
The expectation over the state difference then encourages trajectories to remain in regions of the desired dynamics mode with
low uncertainty.
\end{myquote}
#+END_EXPORT
Given this approximation for the expected Riemannian energy, the expected cost in cref:eq-mode-soc-problem-geometry
can be calculated in closed-form with,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-quadratic-cost-with-energy}
\approxExpectedCost =
&\underbrace{
(\terminalStateMean - \targetState)^T \terminalStateCostMatrix (\terminalStateMean - \targetState)
+\text{tr}(\terminalStateCostMatrix \terminalStateCov)
}_{\text{expected terminal cost}} \\
&+ \underbrace{\sum_{\timeInd=1}^{\TimeInd} \stateDiffMean^T (\muJac \muJac^T + \covJac) \stateDiffMean
+ \text{tr}\left( \left(\muJac \muJac^T + \covJac \right)
\stateDiffCov \right)}_{\text{expected Riemannian energy}} \\
&+ \underbrace{\sum_{\timeInd=0}^{\TimeInd-1}
 \controlMean^T \controlCostMatrix \controlMean +
\text{tr}(\controlCostMatrix \controlCov) }_{\text{expected control cost}}.
\end{align}
#+END_EXPORT
This work then approximately solves the problem in cref:eq-mode-soc-problem-geometry by solving,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-mode-soc-problem-geometry-approx}
\begin{align}
\min_{\controlTraj} &\approxExpectedCost \\
\text{s.t.}& \quad \text{\cref{eq-state-unc-prop,eq-mode-chance-constraint}},
\end{align}
\end{subequations}
#+END_EXPORT
using Sequential Least Squares Programming (SLSQP) in SciPy citep:2020SciPy-NMeth.
This method obtains closed-form expressions for the expected cost in cref:eq-mode-soc-problem-geometry
by constraining the system to be $\delta$ -mode remaining (cref:def-delta-mode-remaining).
# Given this constraints on the dynamics, the mode remaining

*** Practical Implementation
An alternative approach to obtain mode remaining behaviour is to optimise subject to the chance constraints
in cref:eq-mode-chance-constraint alone, i.e. without the Riemannian energy cost term.
However, this constrained optimisation is often not able to converge in practice.
Experiments and intuition indicate that the geometry of the gating functions provide a much
better optimisation landscape.
This is because the gating functions vary gradually over the state domain, whilst the mixing probability changes
abruptly at the boundaries between dynamics modes.

Therefore in practice, the optimisation in cref:eq-mode-soc-problem-geometry-approx
is performed unconstrained, i.e. without enforcing the chance constraints at every iteration.
Instead, the chance constraints are used to validate trajectories found by the unconstrained
optimiser, before deploying them in the environment.
In most experiments, this strategy was far superior than constraining the optimisation at every iteration.

# Instead, the trajectory returned from the unconstrained optimisation is checked to ensure that
# it satisfies the chance constraints.

*** Cost Functions :ignore:noexport:
*Cost Functions*
This work primarily focuses on quadratic costs, as they are ubiquitous in control and lead
to closed-form expectations under Normally distributed states
${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$
and controls
${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$.
This work seeks to find trajectories between a start state $\state_0$ and a target state $\targetState$, at
time $\TimeInd$.
It is common to minimise the deviation of the final state $\state_\TimeInd$
from the desired target state $\targetState$.
It is also common to find trajectories that minimise the expenditure of control effort.
As such, this work adopts the following cost function,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-cost-control}
\costFunc(\stateTraj, \controlTraj) &=
\underbrace{(\terminalState - \targetState)^T \terminalStateCostMatrix (\terminalState - \targetState)}_{\text{terminal cost}}
+ \sum_{\timeInd=0}^{\TimeInd-1}
\underbrace{\control_{\timeInd}^T \controlCostMatrix \control_{\timeInd}}_{\text{control cost}}
\nonumber \\
&=
 \| \terminalState - \targetState \|_\terminalStateCostMatrix
+ \sum_{\timeInd=0}^{\TimeInd-1}
\| \control_\timeInd \|_\controlCostMatrix
\end{align}
#+END_EXPORT
where $\terminalStateCostMatrix$
and $\controlCostMatrix$ are user defined,
real, symmetric, positive semi-definite and positive definite matrices respectively.
Importantly, the expected value of this cost under Normally distributed states and controls can be
calculated in closed form with,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-quadratic-cost}
\E_{\stateTraj, \controlTraj} \left[ \costFunc(\stateTraj, \controlTraj) \right] =
\text{tr}(\terminalStateCostMatrix \terminalStateCov) +
(\terminalStateMean - \targetState)^T \terminalStateCostMatrix (\terminalStateMean - \targetState)
+ \sum_{\timeInd=1}^{\TimeInd-1}
\text{tr}(\controlCostMatrix \controlCov )
+ \controlMean^T \controlCostMatrix \controlMean.
\end{align}
#+END_EXPORT

** Quadcopter Experiments label:sec-traj-opt-results
*** intro :ignore:
The trajectory optimisation algorithms presented in this chapter are evaluated
on the illustrative example from cref:illustrative_example, i.e.
flying a velocity controlled quadcopter from an initial state $\state_0$, to a target
state $\state_{f}$, whilst avoiding the turbulent dynamics mode.
See cref:fig-real-world-schematic for a schematic of the environment and details of the problem.
The turbulent dynamics mode is subject to higher drift in the negative $x$ direction, due to the
wind field created by the fan.
It is also subject to higher diffusion (aka process noise) resulting from the turbulence induced by the fan.
Although the exact turbulent dynamics are not known, they are believed to be difficult to control.
This is due to the high process noise which may lead to catastrophic failure.
It is therefore desirable to find trajectories that avoid entering this turbulent dynamics mode.

#+BEGIN_EXPORT latex
\begin{figure}[h!]
\centering
\includegraphics[width=0.7\textwidth]{./images/quadcopter-domain-collocation-ppt.png}
\caption{Diagram showing the real-world quadcopter navigation problem.
It shows the desired dynamics mode in blue (Mode 1) and
the turbulent dynamics mode induced by the fan at the right hand side of the room, in green (Mode 2).
The white box indicates a region of the environment which was not observed.
The goal is to navigate from the start state $\state_0$, to the target state $\targetState$, whilst remaining
in the desired dynamics mode.}
\label{fig-real-world-schematic}
\end{figure}
#+END_EXPORT
The state-space of the velocity controlled quadcopter example consists of the 2D Cartesian coordinates $\state = (x, y)$.
The controls consist of the speed in each direction, given by $\control = (\velocity_x, \velocity_y)$.

*** Real-World Quadcopter Experiments label:sec-traj-opt-results-collocation
**** intro :ignore:
The indirect control via latent geodesics method presented in cref:sec-traj-opt-collocation is evaluated
on the real-world quadcopter control problem detailed in cref:sec-brl-experiment.
However, a different subset of the environment was not observed
and the model was trained using an old variational inference scheme, not the one presented in cref:chap-dynamics.
cref:fig-real-world-schematic shows the environment and details the quadcopter navigation problem.
The controls were kept constant during data collection, reducing the dynamics to
$\Delta \state_{\timeInd+1} = \dynamicsFunc(\state_{\timeInd}; \control_{\timeInd}=\fixedControl)$.
See cref:sec-brl-experiment for more details on data collection and processing.
# #+BEGIN_EXPORT latex
# \begin{figure}[h!]
# \centering
# \includegraphics[width=0.7\textwidth]{./images/quadcopter_bimodal_domain.pdf}
# \caption{Diagram showing the real-world quadcopter navigation problem.
# It shows (Mode 1) the turbulent dynamics mode (blue) induced by
# the fan at the right hand side of the room and (Mode 2) the desired dynamics mode (red) everywhere else.
# The hashed box indicates a region of the environment which was not observed.
# The goal is to navigate from the start state $\state_0$, to the target state $\targetState$, whilst remaining
# in the desired dynamics mode.}
# \label{fig-real-world-schematic}
# \end{figure}
# #+END_EXPORT

**** results figs :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[!t]
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=0.7\textwidth]{./images/geometric-traj-opt-over-prob.pdf}
\subcaption{Desired mode's mixing probability.}
\label{fig-geometric-traj-opt-over-prob}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/geometric-traj-opt-over-svgp.pdf}
\subcaption{GP posterior mean (left) and variance (right) over the desired mode's gating function.}
\label{fig-geometric-traj-opt-over-svgp}
\end{minipage}
\caption{\textbf{Indirect geodesic control} Trajectory optimisation results after solving the nonlinear
program in \cref{eq-collocation-problem}
using the desired mode's ($\modeVar=1$) gating function from the \acrshort{mosvgpe} (with $\ModeInd=2$ experts)
after training on the real-world velocity controlled quadcopter data set.
The intial (cyan) and optimised trajectories' -- for two settings of $\lambda$ -- are
overlayed on the desired mode's (\subref{fig-geometric-traj-opt-over-prob}) mixing probability and
(\subref{fig-geometric-traj-opt-over-svgp}) gating function GP posterior.}
\label{fig-geometric-traj-opt}
\end{figure}
#+END_EXPORT
**** Model Learning
The model from cref:chap-dynamics was instantiated with $\ModeInd=2$ experts and trained on
the data collected from the velocity controlled quadcopter experiment.
Each mode's dynamics \acrshort{gp} used a Squared Exponential kernel with \acrfull{ard} and a constant mean function.
The gating network used a single gating function with a Bernoulli likelihood.
Its \acrshort{gp} prior utilised a Squared Exponential kernel with \acrshort{ard} and a zero mean function.

cref:fig-geometric-traj-opt shows the gating network posterior where the model has clearly learned
two dynamics modes, characterised by the drift and process noise induced by the fan.
Mode 1 represents the operable dynamics mode whilst Mode 2 represents the inoperable turbulent dynamics mode.
This is illustrated in cref:fig-geometric-traj-opt-over-prob which shows the probability that
the desired mode ($\modeVar = 1$) governs the dynamics over the domain.
cref:fig-geometric-traj-opt-over-svgp shows the \acrshort{gp} posterior mean (left) and variance (right) of the
gating function $\gatingFunc_1$ associated with the desired dynamics mode.
The mean is high where the model believes the desired mode is responsible for predicting, low where it
believes another mode is responsible and zero where it is uncertain.
The variance (right) has also clearly captured information regrading the epistemic uncertainty,
i.e. where the model is uncertain which mode governs the dynamics.

**** Indirect Control via Latent Geodesics :ignore:
\newline

#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/geometric-traj-opt-prob-vs-time.pdf}
\subcaption{Desired mode's (\modeVar=1) mixing probability over the trajectories.}
\label{fig-mixing_prob_vs_time}
\end{minipage}
\begin{minipage}[r]{0.49\columnwidth}
\includegraphics[width=\textwidth]{./images/geometric-traj-opt-epistemic-vs-time.pdf}
%\label{}
\subcaption{Posterior variance associated with the desired mode's (\modeVar=1) gating function over the trajectories.}
\label{fig-epistemic_var_vs_time}
\end{minipage}
\caption{\textbf{Indirect geodesic control} Comparision of the intial and optimised trajectories' performance -- for two
settings of $\lambda$ -- at a) staying in the desired mode and b) avoiding regions of the gating network
with high epistemic uncertainty.}
\label{fig-metric-vs-time}
\end{figure}
#+END_EXPORT
# The indirect geodesic control method projects the trajectory optimisation onto the gating function associated with
# the desired mode.
# These experiments adopted a quadratic cost function to regularise the controls,
# Experiments with the indirect geodesic control method adopted a quadratic cost function to regularise the controls,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-quadratic-cost-control}
# \costFunc(\stateTraj, \controlTraj)
# &= \sum_{\timeInd=0}^{\TimeInd-1} \control_{\timeInd}^T \controlCostMatrix \control_{\timeInd}
# = \sum_{\timeInd=0}^{\TimeInd-1} \left\| \control_\timeInd \right\|_{\controlCostMatrix},
# \end{align}
# #+END_EXPORT
# where $\controlCostMatrix$ is a user defined, real symmetric positive definite weight matrix.
# It is common to use this cost function as it encodes minimising the expenditure of control effort.
# For example, minimising the total expenditure of fuel, or minimising the energy dissipated from the system.
**** Table :ignore:
#+LABEL: tab-results
#+CAPTION: *Indirect geodesic control* Comparison of performance with different settings of $\lambda$.
#+CAPTION: The performance measures are summed over collocation points.
#+attr_latex: :placement [!b]
|-------------------------+--------------------------------------------------+----------------------------------------------------------|
| Trajectory              | Mixing Probability                               | Epistemic Uncertainty                                    |
|                         | $\sum_{i=1}^{I} \Pr(\modeVar_i=1 \mid \state_i)$ | $\sum_{i=1}^{I} \mathbb{V}[\gatingFunc^{(1)}(\state_i)]$ |
|-------------------------+--------------------------------------------------+----------------------------------------------------------|
| Initial                 | $7.480$                                          | $1.345$                                                  |
| Optimised $\lambda=20$  | $6.091$                                          | $\mathbf{1.274}$                                         |
| Optimised $\lambda=0.5$ | $\mathbf{8.118}$                                 | $1.437}$                                                 |

**** After Table :ignore:
\newline

The initial (cyan) trajectory in cref:fig-geometric-traj-opt-over-svgp was initialised as a straight line with
$10$ collocation points indicated by the crosses.
The collocation solver guarantees that trajectories end at the target state.
However, trajectories are not guaranteed to remain in the desired dynamics mode, nor are they guaranteed to
satisfy the systems dynamics.
cref:tab-results compares the initial trajectory to the results obtained
with two settings of the $\lambda$ parameter from cref:eq-expected-metric-weighting.
It is clear from cref:fig-geometric-traj-opt,fig-metric-vs-time
that for $\lambda=0.5$,
trajectories favour remaining in the desired mode at the cost of entering regions of the gating network
with high epistemic uncertainty.
For $\lambda=20$, the trajectory initially remains in the desired mode
but then sacrifices entering the turbulent mode in favour of avoiding the area of high epistemic
uncertainty.
These results align with the intended behaviour of the user tunable $\lambda$ parameter.
However, in this experiment, increasing the relevance of the covariance term in the expected Riemannian
metric has no benefit.
This is because it pushes the trajectory into the turbulent dynamics mode.


These experiments do not recover the controls using cref:eq-control-elbo because
a full transition dynamics model has not been learned.
That is, the dynamics model does not depend on the controls so the control trajectory cannot be recovered.
In cref:sec-traj-opt-results-energy the full method is tested in a simulated environment representing
this

Further to this, the mode remaining behaviour is sensitive to the tolerance on the collocation constraints.
Setting the tolerance too small can result in the solver failing to converge, whilst setting the tolerance too
large often omits any mode remaining behaviour.
*** Simulated Quadcopter Experiments label:sec-traj-opt-results-energy
**** maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\deltaTime}{\ensuremath{\Delta \timeInd}}
\newcommand{\env}[1]{\ensuremath{\hat{#1}}}
\newcommand{\modeProbTraj}{\ensuremath{\Pr(\allModeVarK \mid \stateTraj)}}

\newcommand{\windDrift}[1]{\ensuremath{\bm\omega_{#1}}}
\newcommand{\windTurbulence}[1]{\ensuremath{\bm\epsilon_{#1}}}
\newcommand{\windTurbulenceNoise}[1]{\ensuremath{\bm\Sigma_{\windTurbulence{#1}}}}
#+END_EXPORT
**** intro :ignore:
# The constant control data set in cref:sec-brl-experiment cannot be used for the
# method in cref:sec-traj-opt-energy as it requires a data set with controls.
Due to COVID-19 and limited access to the laboratory, the direct control method in cref:sec-traj-opt-energy
was tested in two simulated environments.
Both control methods were tested in the simulated environments so that they could be compared.
To aid comparison with the real-world experiments, the layout of Environment 1 was kept consistent
with the real-world experiments.


# This is because the data set in cref:sec-brl-experiment was collected with constant controls and
# the method in cref:sec-traj-opt-energy requires a data set with controls.

# To aid with comparison, the two control methods are tested and evaluated in a simulated environment.
# The simulation environment represents the velocity controlled quadcopter in an environment subject to spatially
# varying wind, constituting both drift and diffusion components.

**** Simulator Setup
The simulated environments have two dynamics modes, $\modeVar \in \{1, 2\}$, whose
transition dynamics are given by simple Newtonian dynamics (velocity times time).
The modes are induced by different wind fields which are characterised by their
drift $\windDrift{\modeInd}$ and process noise $\windTurbulence{\modeInd}$ terms.
Each mode's dynamics are given by,
#+BEGIN_EXPORT latex
\begin{align}
\mode{\dynamicsFunc}(\state_\timeInd, \control_\timeInd; \Delta\timeInd=0.25)
&= \control_{\timeInd} \times \Delta\timeInd + \windDrift{\modeInd} + \windTurbulence{\modeInd} \\
%\quad \text{if} \quad \modeVar_\timeInd = \modeInd \\
\windTurbulence{\modeInd} &\sim \mathcal{N}(\bm0, \windTurbulenceNoise{\modeInd}).
\end{align}
#+END_EXPORT
The state domain is constrained to $x, y \in [-3, 3]$ and min/max controls are implemented by constraining the
control domain to $\velocity_x, \velocity_y \in [-5, 5]$.

In both environments, Mode 1 represents the operable, desired dynamics mode and Mode 2 represents the
turbulent dynamics mode.
This results from Mode 2 having much higher drift and process noise than Mode 1.
cref:fig-dataset-scenario illustrates the two environments, where the goal is to navigate from the initial state
$\state_0$, to the target state $\targetState$, given the state transition data sets indicated by the quiver plots.

# The modes have the same $y$ drift but Mode 2 has 100 times higher drift in the negative $x$.
# Further to this, Mode 2 is subject to much higher process noise.
# As such, Mode 1 represents the operable, desired dynamics mode and Mode 2 represents the turbulent dynamics mode.

# with
# $\windDrift{1} = \{0.02, 0.4\}$, $\windDrift{2} = \{-2.0, -0.4\}$,
# $\windTurbulenceNoise{1} = \diag\left([0.0001, 0.0002]\right)$ and
# $\windTurbulenceNoise{2} = \diag\left([0.2, 0.05]\right)$.
# The modes have the same $y$ drift but Mode 2 has 100 times higher drift in the negative $x$.
# Further to this, Mode 2 is subject to much higher process noise.
# As such, Mode 1 represents the operable, desired dynamics mode and Mode 2 represents the turbulent dynamics mode.

# The mode indicator variable $\modeVar \in \{1, 2\}$, determines where each mode governs the dynamics.
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=0.7\textwidth]{./images/mode-opt/env/scenario_7/env_with_dataset_start_end_pos.pdf}
\subcaption{\textbf{Environment 1} - $\windDrift{1} = \{0.02, -0.4\}$, $\windDrift{2} = \{-2.0, -0.4\}$,
$\windTurbulenceNoise{1} = \diag\left([0.0002, 0.0001]\right)$ and
$\windTurbulenceNoise{2} = \diag\left([0.2, 0.05]\right)$}
\label{fig-dataset-scenario-7}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=0.7\textwidth]{./images/mode-opt/env/scenario_5/env_with_dataset_start_end_pos.pdf}
\subcaption{\textbf{Environment 2} - $\windDrift{1} = \{0.4, 0.02\}$, $\windDrift{2} = \{0.4, -2.0\}$,
$\windTurbulenceNoise{1} = \diag\left([0.0001, 0.0002]\right)$ and
$\windTurbulenceNoise{2} = \diag\left([0.05, 0.2]\right)$}
\label{fig-dataset-scenario-5}
\end{minipage}
\caption{Visualisation of two multimodal environments ($\ModeInd=2$), with mode boundary's indicated
by the purple lines.
A state transition data set has been sampled from each environment and is visualised by the quiver plots.
The initial $\state_0$ and target states $\targetState$ for the trajectory optimisation are overlayed.}
\label{fig-dataset-scenario}
\end{figure}
#+END_EXPORT
$4000$ state transitions were sampled from each environment with $\Delta \timeInd = 0.25s$.
A subset of the state transitions were then removed to induce a region of high epistemic uncertainty in the
learned dynamics model. This enable the control methods ability to avoid regions of high epistemic uncertainty
to be tested.
cref:fig-dataset-scenario shows the state transition data sets and also indicates the separation of the modes.

# The simulated environment is instantiated with the gating mask in cref:fig-dataset-scenrio-7,
# where the red region indicates the desired (operable) dynamics mode $\modeVar=1$ and the white region
# indicates the turbulent dynamics mode $\modeVar=2$.
# #+BEGIN_EXPORT latex
# \begin{figure}[h!]
# \centering
# \includegraphics[width=0.9\textwidth]{./images/mode-opt/env/scenario_7/env_with_dataset_start_end_pos.pdf}
# \caption{\textbf{State transition data set} visualised as a quiver plot with
# the start/end states and the mode boundary overlayed.}
# \label{fig-dataset-scenrio-7}
# \end{figure}
# #+END_EXPORT

**** Model Learning
# To begin with, $\NumData$ state transitions were sampled from the simulator with $\Delta \timeInd = 0.25s$.
Following the experiments in cref:sec-traj-opt-results-collocation,
the model from cref:chap-dynamics was instantiated with $\ModeInd=2$ experts, one to
represent the desired dynamics mode and one to represent the turbulent dynamics mode.
Each mode's dynamics GP used a Squared Exponential kernel with \acrshort{ard} and a zero mean function.
The gating network used a single gating function with
a Squared Exponential kernel with \acrshort{ard} and a zero mean function.
In contrast to the real-world experiments, the simulated experiments presented here learn the full transition
dynamics model, i.e. they do not assume constant controls.

**** Performance Indicators
This section evaluates the performance of trajectories using four performance indicators.
1. Probability of remaining in the desired mode with the model's uncertainty marginalised,
    calculated using cref:eq-mode-chance-constraint-integral.
    #+BEGIN_EXPORT latex
    \begin{align} \label{eq-mode-probability-all-unc}
    \sum_{\timeInd=1}^{\TimeInd}
    \Pr(\modeVar_\timeInd=\desiredMode \mid \state_0, \control_{0:\timeInd}, \modeVar_{0:\timeInd-1}=\desiredMode).
    \end{align}
    #+END_EXPORT
    This probability will decrease when a trajectory leaves the desired mode AND when it
    passes through regions of the learned dynamics with high uncertainty.
2. Probability of remaining in the desired mode without the model's uncertainty marginalised,
    #+BEGIN_EXPORT latex
    \begin{align} \label{eq-mode-probability-no-unc}
    \sum_{\timeInd=0}^{\TimeInd} \Pr(\modeVar_\timeInd=\desiredMode \mid \gatingFunc(\state_{0:\timeInd}), \state_{0:\timeInd}, \control_{0:\timeInd}, \modeVar_{0:\timeInd-1}=\desiredMode)
    \end{align}
    #+END_EXPORT
    This probability will ONLY decrease when a trajectory leaves the desired mode.
    # #+BEGIN_EXPORT latex
    # \begin{align} \label{eq-mode-probability-all-unc}
    # \sum_{\timeInd=0}^{\TimeInd} \Pr(\modeVar_\timeInd=\desiredMode \mid \state_0, \control_{0:\timeInd})
    # \end{align}
    # #+END_EXPORT
   # - The indirect control method calculates the probability of being in the desired dynamics mode under the
   #   MoSVGPE model,
   #  #+BEGIN_EXPORT latex
   #  \begin{align}
   #  \sum_{\timeInd=1}^{\TimeInd}
   #   \Pr(\modeVar_\timeInd=\desiredMode \mid \state_{0:\timeInd}, \control_{0:\timeInd}, \modeVar_{0:\timeInd-1}=\desiredMode)
   #  \end{align}
   #  #+END_EXPORT
   # - The direct control method also calculates the probability of being in the desired dynamics mode
   #   but it also marginalises the state uncertainty along the trajectory,
3. The state variance accumulated from cascading single-step predictions,
   #+BEGIN_EXPORT latex
    \begin{align} \label{eq-metric-state-var}
    \sum_{\timeInd=1}^{\TimeInd} \mathbb{V}[\state_{\timeInd}]
    \end{align}
   #+END_EXPORT
    This will increase when a trajectory passes through regions of the desired dynamics mode with high uncertainty.
4. The gating function variance accumulated from cascading single-step predictions,
   #+BEGIN_EXPORT latex
    \begin{align} \label{eq-metric-gating-var}
    \sum_{\timeInd=1}^{\TimeInd}
    \mathbb{V}[\gatingFunc_{\desiredMode}(\state_{\timeInd})]
    \end{align}
   #+END_EXPORT
    This will increase when a trajectory passes through regions of the gating network with high uncertainty.
Intuitively, the goal is to maximise the probability of being in the desired mode,
whilst minimising the variance accumulated over a trajectory, i.e. maximise cref:eq-metric-prob-unc.
To test each methods ability at avoiding regions of high epistemic uncertainty,
a subset of the environment was intentionally not observed.

# This section evaluates the performance of the two algorithms at achieving the goals:
# - Goal 1 :: Remain in the operable, desired dynamics mode $\desiredMode$,
# - Goal 2 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/,
#   - in the desired dynamic mode.
#   - in the gating network,

# The indirect geodesic control method from cref:sec-traj-opt-collocation is evaluated on the real-world
# quadcopter data set detailed in cref:sec-brl-experiment.
# Due to coronavirus and limited access to the laboratory, the direct control method in cref:sec-traj-opt-energy
# is tested in a simulated version of the illustrative example from cref:illustrative_example.
# This is because the data set in cref:sec-brl-experiment was collected with constant controls and
# the method in cref:sec-traj-opt-energy requires a data set with controls.
# Nevertheless, results on the real-world quadcopter are included to demonstrate the applicability of this method
# to real-world robotic systems.

\todo{Add section looking at GP Jacobian and metric tensor for one of the experiments?}

**** Environment 1
***** Blah :ignore:
The two control methods were first tested in a simulated representation of the real-world environment, named
Environment 1.
cref:fig-dataset-scenario-7 illustrates the environment and shows the state transition data set that was used
to train the model.
Three settings of the tuneable $\lambda$ parameter --
which determines the relevance of the covariance term in the expected Riemannian metric -- were tested for
each method.
All experiments in Environment 1 used a $\TimeInd=20$ step horizon.
cref:fig-geodesic-low-traj-opt-7,fig-geodesic-traj-opt-7,fig-geodesic-high-traj-opt-7
show the results using the indirect geodesic control method in cref:sec-traj-opt-collocation and
cref:fig-riemannian-energy-low-traj-opt-7,fig-riemannian-energy-traj-opt-7,fig-riemannian-energy-high-traj-opt-7
show the results using the direct control method in cref:sec-traj-opt-energy.
The top row of each figure shows the optimised trajectories overlayed on each mode's mixing probability.
This is useful for seeing how well trajectories remain in regions of the learned dynamics model with high
probability of being in the desired mode.
The bottom row of each figure shows the trajectories overlayed on the gating function's posterior
mean (left) and posterior variance (right).
As the geometry-based methods in this chapter are based on finding shortest trajectories on this manifold,
the posterior mean plot is useful for observing contour following behaviour.
Similarly, as the methods should find trajectories that avoid regions of the gating network with
high epistemic uncertainty, overlaying the trajectories on the posterior variance is useful for seeing
this behaviour.

To further help visualise the results, the optimised controls are rolled out in
the desired mode's dynamics GP (magenta) and in the environment (cyan).
Deviation between these two trajectories is undesirable as it indicates that the controls,
1. drive the system out of the desired mode, or,
2. drive the system into regions of the learned model with high epistemic uncertainty.
For the indirect geodesic control experiments, the state trajectory found by the collocation solver
is also overlayed (yellow).
cref:tab-results-sim-envs summarises the all of the results for the simulated experiments.

# In the indirect geodesic control experiments, the yellow trajectory represents the state trajectory found by the
# collocation solver on the geodesic ODE.

***** Table :ignore:

#+begin_table
#+LATEX: \caption{\textbf{Results in simulated environments} Comparison of the direct geodesic control method from \cref{sec-traj-opt-collocation} and the  indirect control method from \cref{sec-traj-opt-energy}, in the two simulated environments, when using different settings of $\lambda$. The performance measures are summed over the collocation points for the indirect geodesic control method and over each time step for the direct control method. The "Mode prob" column calculates the probability at each time step without marginalising the state and gating function uncertainty whereas the "Mode prob with uncertainty" marginalises both the state and gating function uncertainty at each time step.}
#+LATEX: \label{tab-results-sim-envs}
#+LATEX: \resizebox{\textwidth}{!}{
#+ATTR_LATEX: :center t :placement [!b] :align clcccccc
|-----+---------------------------+-----------+---------------------------------+----------------------------------+--------------------------+------------------------------------------------------+--------------------------------------------------+---|
| Env | Method                    | $\lambda$ | Mode prob                       | Mode prob with uncertainty       | State uncertainty        | Gating uncertainty                                   | Riemannian energy                                |   |
|     |                           |           | cref:eq-mode-probability-no-unc | cref:eq-mode-probability-all-unc | $\mathbb{V}[\stateTraj]$ | $\mathbb{V}[\gatingFunc_{\desiredMode}(\stateTraj)]$ | cref:eq-approximate-trajectory-riemannian-energy |   |
|-----+---------------------------+-----------+---------------------------------+----------------------------------+--------------------------+------------------------------------------------------+--------------------------------------------------+---|
|     | Direct                    | $0.5$     | $16.96$                         | $16.22$                          | $8.69$                   | $346.62$                                             | $238.29$                                         |   |
|     | Direct                    | $1.0$     | $\mathbf{18.51}$                | $\mathbf{16.98}$                 | $8.70$                   | $367.16$                                             | $\mathbf{222.07}$                                |   |
|   1 | Direct                    | $20.0$    | $12.64$                         | $13.00$                          | $8.69$                   | $\mathbf{157.83}$                                    | $349.08$                                         |   |
|     | Indirect                  | $0.5$     | $20.91$                         | $16.77$                          | $8.70$                   | $404.51$                                             | $180.10$                                         |   |
|     | Indirect                  | $1.0$     | $20.98$                         | $18.10$                          | $8.72$                   | $388.51$                                             | $173.84$                                         |   |
|     | Indirect                  | $20.0$    | $12.64$                         | $13.00$                          | $\mathbf{8.69}$          | $157.83$                                             | $349.08$                                         |   |
|     | Inference                 | N/A       | $20.94$                         | $19.99$                          | $8.74$                   | $208.62$                                             | $287.52$                                         |   |
|     | Inference (deterministic) | N/A       | $20.96$                         | $19.58$                          | $8.73$                   | $258.27$                                             | $237.32$                                         |   |
|-----+---------------------------+-----------+---------------------------------+----------------------------------+--------------------------+------------------------------------------------------+--------------------------------------------------+---|
|     | Indirect                  | $0.01$    | $20.98$                         | $20.80$                          | $1.79$                   | $497.47$                                             | $275.00$                                         |   |
|     | Indirect                  | $0.5$     | $20.96$                         | $20.34$                          | $1.42$                   | $367.47$                                             | $182.06$                                         |   |
|   2 | Indirect                  | $1.0$     | $20.98$                         | $20.68$                          | $1.28$                   | $334.47$                                             | $144.16$                                         |   |
|     | Indirect                  | $5.0$     | $20.81$                         | $20.13$                          | $1.37$                   | $374.13$                                             | $134.33$                                         |   |
|     | Inference                 | N/A       | $20.98$                         | $20.72$                          | $1.75$                   | $596.67$                                             | $442.88$                                         |   |
|     | Inference (deterministic) | N/A       | $20.98$                         | $20.61$                          | $1.56$                   | $644.35$                                             | $534.39$                                         |   |
#+LATEX: }
#+end_table

***** Geodesic collocation 0.5 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/geodesic-collocation-low/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-geodesic-low-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/geodesic-collocation-low/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-geodesic-low-traj-opt-over-svgp-7}
\end{minipage}
\caption{\textbf{Indirect geodesic control} $\bm\lambda\mathbf{=0.5}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$, using the indirect geodesic control method from \cref{sec-traj-opt-collocation}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The optimised state trajectory from the collocation solver is indicated in yellow.
The trajectories are overlayed on (\subref{fig-geodesic-low-traj-opt-over-prob-7})
each mode's mixing probability
and (\subref{fig-geodesic-low-traj-opt-over-svgp-7})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.}
\label{fig-geodesic-low-traj-opt-7}
\end{figure}
#+END_EXPORT

***** Geodesic collocation 1.0 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/geodesic-collocation/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-geodesic-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/geodesic-collocation/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-geodesic-traj-opt-over-svgp-7}
\end{minipage}
\caption{\textbf{Indirect geodesic control} $\bm\lambda\mathbf{=1.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$, using the indirect geodesic control method from \cref{sec-traj-opt-collocation}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The optimised state trajectory from the collocation solver is indicated in yellow.
The trajectories are overlayed on (\subref{fig-geodesic-traj-opt-over-prob-7})
each mode's mixing probability
and (\subref{fig-geodesic-traj-opt-over-svgp-7})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.}
\label{fig-geodesic-traj-opt-7}
\end{figure}
#+END_EXPORT

***** Geodesic collocation 20 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/geodesic-collocation-high/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-geodesic-high-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/geodesic-collocation-high/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-geodesic-high-traj-opt-over-svgp-7}
\end{minipage}
\caption{\textbf{Indirect geodesic control} $\bm\lambda\mathbf{=20.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$, using the indirect geodesic control method from \cref{sec-traj-opt-collocation}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The optimised state trajectory from the collocation solver is indicated in yellow.
The trajectories are overlayed on (\subref{fig-geodesic-high-traj-opt-over-prob-7})
each mode's mixing probability
and (\subref{fig-geodesic-high-traj-opt-over-svgp-7})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.}
\label{fig-geodesic-high-traj-opt-7}
\end{figure}
#+END_EXPORT

***** Riemannian energy 0.5 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/riemannian-energy-low/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/riemannian-energy-low/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-traj-opt-over-svgp-7}
\end{minipage}
\caption{\textbf{Riemannian Energy} $\bm\lambda\mathbf{=0.5}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the Riemannian energy cost function from \cref{eq-approximate-trajectory-riemannian-energy}
with $\lambda=0.5$
and deterministic controls (no maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-riemannian-energy-low-traj-opt-over-prob-7})
each mode's mixing probability
and (\subref{fig-riemannian-energy-low-traj-opt-over-svgp-7})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-riemannian-energy-low-traj-opt-7}
\end{figure}
#+END_EXPORT

***** Riemannian energy 1 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/riemannian-energy/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-riemannian-energy-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/riemannian-energy/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-riemannian-energy-traj-opt-over-svgp-7}
\end{minipage}
\caption{\textbf{Riemannian Energy} $\bm\lambda\mathbf{=1.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the approximate Riemannian energy cost function from \cref{eq-approximate-trajectory-riemannian-energy}
with $\lambda=1.0$.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-riemannian-energy-traj-opt-over-prob-7})
each mode's mixing probability
and (\subref{fig-riemannian-energy-traj-opt-over-svgp-7})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-riemannian-energy-traj-opt-7}
\end{figure}
#+END_EXPORT

***** Riemannian energy 20 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/riemannian-energy-high/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/riemannian-energy-high/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-riemannian-energy-high-traj-opt-over-svgp-7}
\end{minipage}
\caption{\textbf{Riemannian Energy} $\bm\lambda\mathbf{=20.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the Riemannian energy cost function from \cref{eq-approximate-trajectory-riemannian-energy}
with $\lambda=20.0$
and deterministic controls (no maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-riemannian-energy-high-traj-opt-over-prob-7})
each mode's mixing probability
and (\subref{fig-riemannian-energy-high-traj-opt-over-svgp-7})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-riemannian-energy-high-traj-opt-7}
\end{figure}
#+END_EXPORT

***** target state :ignore:
\newline

*Target state*
The methods are first evaluated at their ability to navigate to the target state.
All experiments in Environment 1 were able to find trajectories that navigate to the target state under the
desired mode's GP dynamics.
This is indicated by the magenta trajectory in
cref:fig-riemannian-energy-low-traj-opt-7,fig-riemannian-energy-traj-opt-7,fig-riemannian-energy-high-traj-opt-7,fig-geodesic-low-traj-opt-7,fig-geodesic-traj-opt-7,fig-geodesic-high-traj-opt-7
successfully navigating to the target state $\targetState$.
The state trajectory found by the collocation solver is guaranteed to satisfy the boundary conditions
in the indirect geodesic control experiments
(cref:fig-geodesic-low-traj-opt-7,fig-geodesic-traj-opt-7,fig-geodesic-high-traj-opt-7).
This is because it is a property of the collocation solver.
The inference strategy has then successfully recovered controls that drive the system along the
collocation solver's state trajectory (yellow),
indicated by the dynamics trajectory (magenta) following the collocation trajectory (yellow).

All of the direct control experiments shown in
cref:fig-riemannian-energy-low-traj-opt-7,fig-riemannian-energy-traj-opt-7,fig-riemannian-energy-high-traj-opt-7
were also able to find trajectories that navigate to the target state under the learned dynamics (magenta).
Although the direct control method does not guarantee that trajectories will satisfy the boundary conditions,
in practice, setting the terminal state cost matrix $\terminalStateCostMatrix$ to be very high,
appears to work well.

Although all of the dynamics trajectories (magenta) navigate to the target state, not all of
their corresponding environment trajectories (cyan) do.
This is due to the trajectories in
cref:fig-riemannian-energy-high-traj-opt-7,fig-geodesic-low-traj-opt-7,fig-geodesic-traj-opt-7,fig-geodesic-high-traj-opt-7
leaving the desired dynamics mode and becoming subject to the turbulent dynamics.

***** mode remaining :ignore:
\newline

*Mode remaining*
The experiments are now evaluated at their ability to remain in the desired dynamics mode.
None of the indirect geodesic control experiments
(cref:fig-geodesic-low-traj-opt-7,fig-geodesic-traj-opt-7,fig-geodesic-high-traj-opt-7)
were able to successfully remain in the desired dynamics mode.
This is shown by the dynamics trajectories (magenta) passing over the mode boundary into the turbulent dynamics mode.
This is further emphasised by the environment trajectory (cyan) deviating from the
dynamics trajectory (magenta) when it passes into the turbulent dynamics mode.
This is due to the strong wind field blowing the quadcopter in the negative $x$ direction.
Although none of the indirect geodesic control experiments successfully remained in the desired dynamics mode,
they have exhibited mode remaining behaviour.
That is, the trajectories with $\lambda=0.5$ in cref:fig-geodesic-low-traj-opt-7
and with $\lambda=1.0$ in cref:fig-geodesic-traj-opt-7,
navigate to the left and almost reach the mode boundary.
# This is further indicated in cref:tab-results-sim-envs, where the probability of remaining in the desired
# dynamics mode is higher than the experiments with $\lambda=20.0$.

In practice, setting the lower and upper bounds on the collocation constraints in SciPy, is extremely fiddly,
and impacts how much mode remaining behaviour is exhibited.
Setting the bounds too loose results in trajectories not satisfying the geodesic ODE and
setting them to tight leads to the constrained optimisation exiting with an error.
These issues could potentially be resolved by casting the constrained optimisation into an unconstrained optimisation
using Lagrange multipliers. However, this is left for future work.

The two direct control experiments in cref:fig-riemannian-energy-low-traj-opt-7,fig-riemannian-energy-traj-opt-7,
with $\lambda =0.5$ and $\lambda=1.0$ respectively,
were able to successfully remain in the desired dynamics mode, and thus,
navigate to the target state in the environment (cyan).
This is demonstrated by their dynamics trajectories (magenta) and their environment trajectories
(cyan), not passing over the mode boundary into the turbulent dynamics mode.
These results are confirmed by cref:tab-results-sim-envs where the trajectory found with $\lambda=1.0$ obtained
the highest probability of remaining in the desired dynamics mode for Environment 1.
Interestingly, the trajectory found with $\lambda=0.5$ obtained the second highest probability
when not considering the epistemic from the learned dynamics model (cref:eq-mode-probability-no-unc),
but only the third highest when considering the epistemic uncertainty (cref:eq-mode-probability-all-unc).
This is due to the lower relevance of the covariance term in the expected Riemannian metric
allowing the trajectory to pass
through regions of the gating network with high epistemic uncertainty.
This can be seen in the bottom right plot of cref:fig-riemannian-energy-low-traj-opt-7 where the trajectory
passes straight over the region of highest gating function variance in the middle of the environment.
This is quantified in cref:tab-results-sim-envs as it is the trajectory with the highest accumulated gating
variance.
From visual inspection, the trajectory found with $\lambda=1.0$ has more clearance from the turbulent dynamics
mode.
This is due to the higher relevance of the covariance term pushing the trajectory away from the region of high
epistemic uncertainty in the gating network.

The trajectory found with $\lambda=20.0$ (cref:fig-riemannian-energy-high-traj-opt-7) did not successfully remain
in the desired dynamics mode.
This indicates that setting the relevance of the covariance term too high can have a negative impact on performance.
This is likely due to the optimisation landscape favouring trajectories that
avoid the region of high posterior variance in the gating network more than following the contours of the
gating function.
# This is most likely due to the trajectory optimiser finding a local optima when $\lambda=20.0$.

# This is because for $\lambda=20.0$ (cref:fig-riemannian-energy-high-traj-opt-7) the loss landscape favours
# avoiding the region of high posterior variance in the gating network, over remaining the desired dynamics mode.
# In this setting, increasing $\lambda$ has a negative impact on performance

# Two of the direct control experiments (with $\lambda =0.5$ and $\lambda=1.0$),
# (cref:fig-riemannian-energy-low-traj-opt-7,fig-riemannian-energy-traj-opt-7)
# were able to successfully navigate to the target state in the environment.
# This is because for $\lambda=20.0$ (cref:fig-riemannian-energy-high-traj-opt-7) the loss landscape favours
# avoiding the region of high posterior variance in the gating network, over remaining the desired dynamics mode.
# In this setting, increasing $\lambda$ has a negative impact on performance

***** epistemic uncertainty :ignore:
\newline

*Epistemic uncertainty*
Finally, the methods are evaluated at their ability to avoid regions of the dynamics with high epistemic uncertainty.
cref:tab-results-sim-envs shows the state variance and the gating function variance accumulated over
each trajectory.
In all experiments, the state variance accumulated over the trajectory (from cascading single-step predictions
via moment matching) were fairly similar.
This is due to the dynamics of the simulator being simple enough that the desired mode's GP
can confidently interpolate into the turbulent dynamics mode and the region which has not been observed.
In the latter case, it is up to the gating network to model the epistemic uncertainty arising from limited
training data.
# In this case, the dynamics model explains epistemic uncertainty is explained by the gating network
# In this case, the complexity of the dynamics arises from the multmodaility which the model explains by
# assigning the second dynamics mode to a second expert.
# regions of the

In the indirect geodesic control experiments with $\lambda=1.0$ and $\lambda=0.5$, the trajectories navigate
directly over the region of high posterior variance associated with the gating function.
This can be seen in the bottom right plots of cref:fig-geodesic-low-traj-opt-7,fig-geodesic-traj-opt-7.
In contrast, the indirect geodesic control experiment with $\lambda=20.0$ (in cref:fig-geodesic-high-traj-opt-7)
avoided this region.
However, this uncertainty avoiding behaviour came at the cost of passing straight through the turbulent
dynamics mode.

Similarly for the direct control experiments, setting $\lambda=20.0$ resulted in the trajectory passing
straight through the turbulent dynamics mode.
However, the benefit of the covariance term in the expected Riemannian metric
is demonstrated in the results with $\lambda=0.5$ in cref:fig-riemannian-energy-low-traj-opt-7 and with
$\lambda=1.0$ in cref:fig-riemannian-energy-traj-opt-7.
As mentioned earlier, this resulted in the trajectory having more clearance from the turbulent dynamics mode,
because the covariance term pushed the trajectory to the left of the region of high variance in the gating
network.
This is confirmed in cref:tab-results-sim-envs where trajectory found with $\lambda=1.0$ accumulated
less gating function variance than the trajectory found with $\lambda=1.0$.

These results indicate that the covariance term -- in the expected Riemannain metric -- plays an important role
at keeping trajectories in the desired dynamics mode.
They also indicate that the interplay between the contour following term and the covariance term,
alter the optimisation landscape in a complex manner.
As such, it is not immediately clear how the value of $\lambda$ will alter it,
making it hard to set $\lambda$.
Especially in this scenario where the inoperable dynamics mode intersects a region of high epistemic
uncertainty.
In this case, the effect of adjusting the tunable $\lambda$ parameter is complex enough that it may
be best to just leave it at $\lambda=1.0$.
It is worth noting here that the experiments in Environment 2 do demonstrate a
scenario where it is beneficial to adjust the $\lambda$ parameter.

*Overall* Visual inspection combined with the results in cref:tab-results-sim-envs,
indicate that the indirect control method with $\lambda=1.0$ was the highest performing experiment in Environment 1.

# In cref:fig-riemannian-energy-low-traj-opt-7 the optimised trajectory only just remained in the desired mode.
# That is, the trajectory passes very close to the boundary between the dynamics modes.
# This is because with $\lambda=0.5$ the method is starting to ignore the uncertainty in the gating network.
# In contrast, the optimised trajectory with $\lambda=1.0$ follows the contours of the gating function's
# posterior variance (cref:fig-riemannian-energy-low-traj-opt-over-svgp-7 right).
# As a result, the trajectory is remaining in the desired mode by following the contours of the
# desired mode's gating function
# (cref:fig-riemannian-energy-low-traj-opt-over-svgp-7 left)
# and avoiding regions of the gating network which it cannot predict confidently
# (cref:fig-riemannian-energy-low-traj-opt-over-svgp-7 right).


# This is because it obtained the highest probability of remaining in the desired mode over the trajectory.
# That is, it exhibited the best mode remaining behaviour under the \acrshort{mosvgpe} dynamics model.
# Further to this, it obtained the lowest accumulation of gating function variance over the trajectory,
# indicating that it introduced the least amount of epistemic uncertainty from the gating network.
# This is an interesting result as I originally hypothesised that the gating function variance would
# decrease with higher $\lambda$ values.
# This is most likely due to the trajectory optimiser finding a local optima when $\lambda=20.0$.
# In contrast, the loss landscape non

**** Environment 2
***** Riemannian energy 0.01 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy-low/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy-low/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Riemannian Energy} $\bm\lambda\mathbf{=0.01}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the Riemannian energy cost function from \cref{eq-approximate-trajectory-riemannian-energy}
with $\lambda=0.01$
and deterministic controls (no maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-riemannian-energy-low-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-riemannian-energy-low-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-riemannian-energy-low-traj-opt-5}
\end{figure}
#+END_EXPORT

***** Riemannian energy 0.5 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy-low-2/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-2-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy-low-2/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-2-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Riemannian Energy} $\bm\lambda\mathbf{=0.5}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the Riemannian energy cost function from \cref{eq-approximate-trajectory-riemannian-energy}
with $\lambda=0.5$
and deterministic controls (no maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-riemannian-energy-low-2-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-riemannian-energy-low-2-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-riemannian-energy-low-2-traj-opt-5}
\end{figure}
#+END_EXPORT

***** Riemannian energy 1 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-riemannian-energy-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-riemannian-energy-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Riemannian Energy} $\bm\lambda\mathbf{=1.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the approximate Riemannian energy cost function from \cref{eq-approximate-trajectory-riemannian-energy}
with $\lambda=1.0$.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-riemannian-energy-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-riemannian-energy-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-riemannian-energy-traj-opt-5}
\end{figure}
#+END_EXPORT

***** Riemannian energy 5 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy-high/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy-high/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-riemannian-energy-high-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Riemannian Energy} $\bm\lambda\mathbf{=5.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the Riemannian energy cost function from \cref{eq-approximate-trajectory-riemannian-energy}
with $\lambda=5.0$
and deterministic controls (no maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-riemannian-energy-high-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-riemannian-energy-high-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-riemannian-energy-high-traj-opt-5}
\end{figure}
#+END_EXPORT

***** Geodesic collocation 0.5 :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/geodesic-collocation-low/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-geodesic-low-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/geodesic-collocation-low/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-geodesic-low-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Indirect geodesic control} $\bm\lambda\mathbf{=0.5}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$, using the indirect geodesic control method from \cref{sec-traj-opt-collocation}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The optimised state trajectory from the collocation solver is indicated in yellow.
The trajectories are overlayed on (\subref{fig-geodesic-low-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-geodesic-low-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.}
\label{fig-geodesic-low-traj-opt-5}
\end{figure}
#+END_EXPORT

***** Geodesic collocation 1.0 :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/geodesic-collocation/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-geodesic-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/geodesic-collocation/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-geodesic-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Indirect geodesic control} $\bm\lambda\mathbf{=1.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$, using the indirect geodesic control method from \cref{sec-traj-opt-collocation}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The optimised state trajectory from the collocation solver is indicated in yellow.
The trajectories are overlayed on (\subref{fig-geodesic-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-geodesic-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.}
\label{fig-geodesic-traj-opt-5}
\end{figure}
#+END_EXPORT

***** Geodesic collocation 30 :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/geodesic-collocation-high/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-geodesic-high-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/geodesic-collocation-high/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-geodesic-high-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Indirect geodesic control} $\bm\lambda\mathbf{=20.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$, using the indirect geodesic control method from \cref{sec-traj-opt-collocation}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The optimised state trajectory from the collocation solver is indicated in yellow.
The trajectories are overlayed on (\subref{fig-geodesic-high-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-geodesic-high-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.}
\label{fig-geodesic-high-traj-opt-5}
\end{figure}
#+END_EXPORT

***** target state :ignore:
\newline

The direct control method was tested in a second simulated environment.
In Environment 2, the turbulent dynamics mode and the unobserved regions are in different locations.
Notice from cref:fig-dataset-scenario-5 that they do not overlap.
The indirect geodesic control method struggled to find solutions in this environment.
For this reason, it has been left out.
Following the experiments in Environment 1, four different settings of the $\lambda$ parameter were tested.
\todo[inline]{add results for geodesic collocation not working}


*Target state*
The magenta trajectories in
cref:fig-riemannian-energy-low-2-traj-opt-5,fig-riemannian-energy-low-traj-opt-5,fig-riemannian-energy-traj-opt-5,fig-riemannian-energy-high-traj-opt-5
indicate that all of the direct control experiments in Environment 2 were able to find trajectories
that navigate to the target state $\targetState$ under the desired mode's GP dynamics.
Further to this, the cyan trajectories indicate that all of the experiments found controls that
successfully navigate to the target state in environment.
Similarly to the results for Environment 1, setting the terminal state cost matrix $\terminalStateCostMatrix$
to be very high, ensured trajectories successfully reached the target state.
# All of their corresponding environment trajectories (cyan) also navigate to the target state $\targetState$.
# The small error between the environment trajectories (cyan) and dynamics trajectories (magenta) is due to
# the accumulation of process noise.

***** mode remaining :ignore:
\newline

*Mode remaining*
All of the experiments  were able to successfully remain in the desired dynamics mode.
This is indicated by none of the dynamics trajectories (magenta) passing over the mode boundary into
the turbulent dynamics mode.
cref:tab-results-sim-envs shows that the
trajectories found with $\lambda=0.01$ and $\lambda=1.0$ in
cref:fig-riemannian-energy-low-traj-opt-5
and
cref:fig-riemannian-energy-traj-opt-5
respectively, obtained the highest probability of remaining in the desired dynamics mode.
From visual inspection of
cref:fig-riemannian-energy-low-2-traj-opt-5,fig-riemannian-energy-low-traj-opt-5,fig-riemannian-energy-traj-opt-5,fig-riemannian-energy-high-traj-opt-5
it is clear that the trajectory found with $\lambda=0.01$ achieved a lower probability
because it has the most clearance from the desired dynamics mode.
Interestingly, although the trajectory found with $\lambda=1.0$
passed through less regions of high epistemic uncertainty, indicated by lower
state and gating variance in cref:tab-results-sim-envs,
it obtained a lower probability of remaining in the desired dynamics mode
when considering the epistemic uncertainty (cref:eq-mode-probability-all-unc).
Further to this, the trajectory found with $\lambda=0.01$ obtained the lowest probability even though it
accumulated the most state and gating variance in comparison to the other $\lambda$ values.
This indicates that avoiding regions of high epistemic uncertainty does not necessarily increase the
probability of remaining in the desired dynamics mode under cref:eq-mode-probability-all-unc.
This is because the mode probability is calculated using both the gating function's mean and variance.
In this case, the influence of the gating function mean has a higher impact on
the mode probability than the variance.
# Intuitively, this suggests that if the model's interpolation/extrapolation believes a region belongs
# Although the model is uncertain, if its interpolation/extrapolation suggest that the


The trajectory found with $\lambda=5.0$ obtained the lowest probability of remaining in the desired dynamics mode.
Increasing the relevance of the covariance term in the expected Riemannian metric
is equivalent to decreasing the relevance of the mode remaining term.
As a result, the optimisation has favoured avoiding regions of high epistemic uncertainty
over remaining in the desired dynamics mode.
Again, the low probability of remaining in the desired dynamics mode
when considering the epistemic uncertainty (cref:eq-mode-probability-all-unc), indicates that avoiding regions
of high epistemic uncertainty does not necessarily lead to higher probabilities.
# Quantitatively, this indicates that adjusting $\lambda$ can have a negative impact on performance.
# This highlights that the mode probability is
# further indicates that although

# The left hand plot of cref:fig-riemannian-energy-low-2-traj-opt-over-svgp-5 shows the
# contour following that achieves the mode remaining behaviour.
The second half of the trajectories in left hand plots of
cref:fig-riemannian-energy-low-2-traj-opt-over-svgp-5,fig-riemannian-energy-low-traj-opt-over-svgp-5,fig-riemannian-energy-traj-opt-over-svgp-5,fig-riemannian-energy-high-traj-opt-over-svgp-5
show the contour following that achieves the mode remaining behaviour.
As the value of $\lambda$ increases, the trajectories exhibit less contour following on the gating function's
mean.
This is because increasing the relevance of the covariance term in the Riemannian metric tensor
alters the optimisation landscape, which in turn reduces the contour following behaviour.

# It is worth noting that the trajectory found with $\lambda=0.01$ does not follow a smooth state trajectory.
# This is shown by the sixth and seventh time steps changing direction abruptly.
# Although this behaviour can be alleviated by increasing the control regularisation,
# complex interactions between the control cost term and the Riemannian energy cost term make it difficult to set.

***** epistemic uncertainty :ignore:
\newline


*Epistemic uncertainty*
In contrast to the experiments in Environment 2, the state variance accumulated over each trajectory does vary
between experiments.
However, the results in  cref:tab-results-sim-envs suggest that the state variance does not have a large impact
on the mode remaining behaviour.
This is indicated by no correlation between the mode probability and the state variance,
which is likely due to the state variance being extremely low.
The trajectory found with $\lambda=1.0$ accumulated the least state and gating function variance.
In contrast, the result for $\lambda=0.01$ accumulated the highest state and gating variance, whilst also
obtaining the highest probability of remaining in the desired dynamics mode.
As mentioned earlier, this result suggests that avoiding regions of high epistemic uncertainty is not the most
important factor for obtaining the highest probability of remaining in the desired dynamics mode.

It is worth noting that the quantitative performance measures do not tell the full story.
Consider the case where a second region belonging to the turbulent dynamics mode exists inside the
region that has not been observed.
In this case, the trajectory found with $\lambda=5.0$ is most likely to avoid entering the turbulent dynamics mode.
This is because it avoids the region of high epistemic uncertainty associated with no observations.
The probability of remaining in the desired dynamics mode does not capture this notion.
There are two reasons for this.
Firstly, the gating network is overconfident in this region due to learning a long lengthscale.
This could be overcome by fixing the lengthscale a priori.
There is a second issue due to the fundamental modelling approximation induced via GPs.
The GP-based gating network is not aware that the region with no observations is so large that it could fit
another turbulent dynamics mode inside it.
This is due to the Squared Exponential kernel function relying on point-wise calculations to build the covariance
matrix.
Future work could explore kernel functions that capture structure at a mode level (shape etc),
as opposed to simple point-wise structure.
# \todo[inline]{is this spatial awareness? What to call this? More than point-wise structure,
# Such a kernel could learn the shape of a dynamics mode, for example, the turbulent region induced by a fan.
# Such a kernel would be aware if a region is large enough for another
# This problem could be
# That is, the kernel function is not able to detect if

\todo[inline]{Carl you mentioned something about the kernel using pointwise calculations which make it impossible to model}
# This overconfidence arises due to the gating network learning a long lengthscale.
# One approach to mitigate this behaviour, is to fix the lengthscale.
# However, it may not be obvious how to set this value a priori.

# This overconfidence in the gating network arises from the gating network evaluating the covariance between
# each new point
# the


The results for $\lambda=0.01$, $\lambda=0.5$ and $\lambda=1.0$ in cref:tab-results-sim-envs,
follow the results for Environment 1, suggesting
that increasing $\lambda$ leads to more uncertainty avoiding behaviour in the gating network.
The result for $\lambda=5.0$ in cref:tab-results-sim-envs does not follow this trend.
However, visual inspection of the right hand plots of
cref:fig-riemannian-energy-traj-opt-over-svgp-5,fig-riemannian-energy-high-traj-opt-over-svgp-5
show that the trajectories for $\lambda=1.0$ and $\lambda=5.0$, are initially similar whilst they
avoid the region with high epistemic uncertainty in the gating network.
The increased relevance of the covariance term with $\lambda=5.0$ also appears to have a negative impact on
the trajectories ability to remain in the desired dynamics mode.
This can be seen in cref:fig-riemannian-energy-high-traj-opt-over-svgp-5,
where the trajectory passes very close to the mode boundary.
Again, this further emphasises the difficulty in setting $\lambda$ due to complex interactions between
the mode remaining behaviour and the uncertainty avoiding behaviour.

*** Conclusion
Overall, the direct control approach from cref:sec-traj-opt-energy performed significantly better in the experiments.
Firstly, two of the indirect control experiments in Environment 1 were able to remain in the desired dynamics mode,
whilst the indirect geodesic control experiments could not.
Further to this, the direct control approach worked in Environment 2, where it was not possible to get the
indirect control method working.
Not only did the direct control approach perform better in the experiments, but it is also significantly easier
to configure.
That is, setting the cost weights and the optimisation parameters is straightforward.
In contrast, setting the parameters for the indirect control method is not, in particular,
the upper and lower bounds for the collocation constraints.

The quantitative results indicate that good performance is generally achieved with $\lambda=1.0$.
That is, not modifying the expected Riemannian metric tensor.
Although some benefits can be obtained via setting $\lambda$,
for example, encoding a notion of risk-sensitivity for avoiding the region with no observations,
it is not always clear how it should be set.
Further to this, in many realistic scenarios, the state-space will not be easy to visualise like in
the 2D quadcopter experiments.
As such, visual inspection of trajectories may not be possible.
For these reasons, I conclude that it is best to air on the side of caution when setting $\lambda$.
That is, $\lambda$ should be set to $1.0$ unless its effect is immediately clear.

** Discussion & Future Work
This section compares the two control methods presented in this chapter and suggests future
work to address their limitations.
cref:tab-geometry-control-comparison offers a succinct comparison of the two approaches.
# The main drawback of the indirect control method is that it does not enforce the dynamics constraints.

#+begin_table
#+LATEX: \caption{Comparison of the Indirect Control via Latent Geodesics method from \cref{sec-traj-opt-collocation} and the Direct Control via Riemannian Energy method from \cref{sec-traj-opt-energy}.}
#+LATEX: \label{tab-geometry-control-comparison}
#+LATEX: \resizebox{\textwidth}{!}{
#+ATTR_LATEX: :center t :placement [!t]
|                                                    | Indirect Control | Direct Control |
|----------------------------------------------------+------------------+----------------|
| Dynamics constraints guaranteed?                   | \times           | \checkmark     |
| Considers epistemic uncertainty in dynamics?       | \times           | \checkmark     |
| Considers epistemic uncertainty in gating network? | \checkmark       | \checkmark     |
| Can remain in *multiple* modes?                    | \times           | \times         |
| Boundary conditions guaranteed?                    | \checkmark       | \times         |
| $\delta-\text{mode remaining}$?                    | \times           | \checkmark     |
| Continuous time trajectory?                        | \checkmark       | \times         |
# | Decouples goals?                                   | \checkmark       | \checkmark     |
#+LATEX: }
#+end_table

*Dynamics constraints*
The direct control method ensures trajectories satisfy the learned dynamics.
It achieves this by enforcing the distribution over state trajectories,
to match the distribution from cascading single-step predictions through the desired mode's dynamics \acrshort{gp}.
This can be seen as a method for approximating the integration of the controlled dynamics with respect to time, whilst
considering the epistemic uncertainty associated with learning from observations.
The chance constraints ensure the controlled system is $\delta-\text{mode remaining}$, which
makes the approximate dynamics integration valid.
In contrast, the indirect control method does not find trajectories that are guaranteed to satisfy the dynamics
constraints.
This is a limiting factor that makes the direct control method more appealing.
Therefore, methods for incorporating the dynamics constraints into the indirect method are an interesting
direction for future work.
# However, future work could attempt to enforce the dynamics constraints in the indirect method.

# Future work could consider extending the indirect control method to enforce the dynamics constraints.
# Further to this,
# *Uncertainty aware*
*Decision-making under uncertainty*
The combination of the approximate dynamics integration and the chance constraints
in the direct control method, leads to a closed-form expression for the expected cost.
This expression considers the epistemic uncertainty associated with the learned dynamics model,
both in the desired dynamics mode and in the gating network.
In contrast, the indirect control method ignores the uncertainty accumulated by cascading single-step predictions through
the learned dynamics model.
That is, it considers the uncertainty associated with the gating network and ignores any uncertainty
in the desired dynamics mode.
Although this did not have a massive impact in the simulated quadcopter experiments, it may have a
larger impact in real-world systems with more complex dynamics modes.
That is, systems whose underlying dynamics result in each mode's dynamics GP being more uncertain.
In the simulated quadcopter experiments, the GPs representing the underlying dynamics modes were fairly certain --
due to simple dynamics -- and most of the epistemic uncertainty was captured by the gating network.
# It models the state at each time step to be deterministic and approximates the full geodesic \acrshort{sde}
# to be deterministic.
# As as result, the cost function will not keep the state trajectory in regions of low uncertainty.

*Uncertainty propagation*
This work only considered the moment matching approximation for propagating uncertainty through the probabilistic
dynamics model.
cite:chuaDeep2018 test different uncertainty propagation schemes in Bayesian neural networks.
It would be interesting to test sampling-based approaches for the direct control approach.
For example, to see the impact of calculating the expected Riemannian metric without approximations.
# In particular, calculating the expected Riemannian metric would require less assumptions.
# In particular, calculating the expected Riemannian metric would require less assumptions.

# Moment matching cannot represent multimodal distributions as it forces a unimodal distribution at each time step.
# cite:mcallisterImproving2016 hypothesise that this effect may be caused due to smoothing of the loss surface and
# implicitly penalising multimodal distributions, which often occur in uncontrolled systems.

# Note that the uncertainty in the gating network may be low in these regions.
# This is undesirable behaviour in a risk-averse setting.
# It approximates the geodesic \acrshort{sde} by replacing the random variable associated with the Riemannain metric
# tensor with its expected value.
# In combination with the deterministic state approximation, this reduces the geodesic \acrshort{sde} to an \acrshort{ode}.

# As as result, trajectories may pass through regions of the desired dynamics mode which are uncertain.
# Note that the uncertainty in the gating network may be low in these regions.
# This is undesirable behaviour in a risk-averse setting.


*Decouple goals*
Both approaches provide a mechanism to set the relevance of the covariance term, i.e. decouple the goals.
This is achieved by augmenting the expected Riemannian metric tensor with the user-tunable weight
parameter $\lambda$, that can be adjusted to determine the relevance of the covariance term.
The experiments show that although adjusting $\lambda$ can be beneficial in some scenarios, it is not necessarily straight
forward to set.
In particular, when regions of high epistemic uncertainty intersect with mode boundaries.
Therefore, care should be taken when setting $\lambda$.

*Multiple desired modes*
Although not tested, the approaches are theoretically sound and should be applicable in systems with more
than two dynamics modes.
However, it is interesting to consider if this is even necessary given the goals.
For example, in the quadcopter experiment, the transition dynamics model was intentionally instantiated
with two dynamics modes, even though there could be more in reality.
The desired dynamics mode was engineered to have a noise variance that was deemed operable.
The other dynamics mode was then used to explain away all of the inoperable modes.
In most scenarios, a similar approach could be followed.
Nevertheless, it is interesting to consider systems with more than two modes.
This is because the main goal is to avoid entering the inoperable dynamics mode, not just remain in a desired
dynamics mode.
As such, an interesting direction for future work is to develop control methods that can remain in a set of
dynamics modes, as opposed to just one.

*Mode remaining guarantee*
Neither method provides theoretical guarantees that the controlled system will remain in the desired dynamics mode.
However, the direct control approach does provide chance constraints that can be evaluated to check if a
trajectory is $\delta-\text{mode remaining}$.
Recent work by
cite:berkenkampSafe2019,berkenkampSafe2017,kollerLearningBased2018
consider the use of invariant sets to synthesise closed-loop controllers/policies, that are guaranteed to
remain in a given set of states.
Their work considers safe sets defined by Lyapanov stability but it would be interesting to extend their
concepts to synthesise $\delta-\text{mode remaining}$ controllers
$\pi_{\desiredMode} : \desiredStateDomain  \rightarrow \controlDomain$,
with theoretical guarantees of the form,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-invariant-set-controller}
\Pr( \dynamicsFunc(\state_\timeInd, \pi_{\desiredMode}(\state_\timeInd)) \in \desiredStateDomain, \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd]  ) \ge 1 - \delta.
\end{align}
#+END_EXPORT

*Continuous-time trajectories*
The direct control method required control regularisation to prevent state transitions that
"jump" over the undesired mode.
Although the discrete time steps of the trajectory appear to satisfy the constraints and minimise the cost,
in reality, the continuous-time trajectory passes through the undesired mode.
This is a general problem that arises when using discrete-time solvers.
In contrast, the indirect control method uses a collocation solver where the cost can be evaluated at arbitrary
points along the continuous-time trajectory.
An interesting direction for future work is to extend the direct control method to find trajectories in continuous-time.
# This is because they can be used to enforce the dynamics and interpolate the cost with higher resolution.
# Stochastic collocation methods may offer

# the single-step dynamics models time step.

# This is an artifact of solving the trajectory optimisation problem in discrete time and can be solved by decreasing
# the single-step dynamics models time step.
# Alternatively, an interesting direction for future work is to consider methods that optimise continuous time controls
# that can be used to interpolate cost over trajectory to get higher resolution.
# For example a Gaussian process over controls instead of Gaussian.
# Or stochastic collocation.
*State-control dependent modes*
This chapter assumed that the dynamics modes were separated according to their state domains $\mode{\stateDomain}$.
It would be interesting to extend this work to systems where the modes are governed by both state and control.
For example, flying at high speed through a wind field may be deemed an operable dynamics mode, whilst
flying at low speed may not.
# For example, flying at high speed through a wind field may belong to the operable dynamics mode with low drift and
# process noise.

*Real-time control*
Whilst real-time control requires efficient inference algorithms,
"offline" trajectory optimisation can trade in computational cost for greater accuracy.
This work is primarily interested in finding trajectories that attempt to remain in a
desired dynamics mode.
For the sake of simplicity, it has considered the "offline" trajectory optimisation setting.
The increased computational time may hinder
its suitability to obtain a closed-loop controller via MPC citep:eduardof.Model2007.
However, it can be used "offline" to generate reference trajectories for a tracking controller,
or for guided policy search in a model-based RL setting citep:levineGuided2013.
Alternatively, future work could investigate approximate inference methods for efficient state estimation
to aid with real-time control, e.g. iLQG/DDP.

*Infeasible trajectories*
It is worth noting that it might not be possible to find a trajectory
between a given start state $\state_0$, and a target state $\targetState$, that satisfies the chance constraints.
This may be due to either the desired dynamics mode being uncertain, or the gating network being uncertain.
In this scenario, it is desirable to explore the environment and update the learned dynamics mode with this new experience.
This will reduce the epistemic uncertainty in the model, increasing the likelihood of being able to find
trajectories that satisfy the chance constraints.
This motivates the work in cref:chap-active-learning, which addresses exploration of
multimodal dynamical systems, whilst attempting to remain in a desired dynamics mode.

** Conclusion
This chapter has shown how the geometry of the \acrshort{mosvgpe} gating network
infers valuable information regarding how a multimodal dynamical system switches between its underlying dynamics modes.
Moreover, it has shown how this latent geometry can be leveraged to encode mode remaining behaviour into
two different control strategies.
Both methods are evaluated on a quadcopter navigation problem.
Overall, the direct control approach offers superior performance, as it finds trajectories that do not violate
the dynamics constraints, whilst ensuring trajectories are $\delta-\text{mode remaining}$.

# Given a start and end state, both of the methods presented in this chapter can be tuned to find trajectories that either
# prioritise remaining in a desired dynamics mode, or prioritise avoiding regions of the learned
# dynamics model with high epistemic uncertainty.

# The direct control algorithm from cref:sec-traj-opt-energy and the mode chance constraints from
# cref:eq-mode-chance-constraint are used in cref:chap-active-learning

# The control algorithms find trajectories from an initial state
# $\state_0$, to a target state $\targetState$, whilst remaining in a desired dynamics mode.

# The methods leverage the \acrshort{mosvgpe} model from cref:chap-dynamics to first learn a single-step dynamics model.
# This chapter has shown that the geometry of the \acrshort{mosvgpe} gating network
# infers valuable information regarding how a system switches between its underlying modes.
# Moreover, it has shown how this latent geometry can be leveraged to encode both mode remaining
# and risk-sensitive behaviour into control strategies.

# This chapter has shown that the geometry of the gating network from the \acrshort{mosvgpe} model from cref:chap-dynamics
# infers valuable information regarding how a system switches between its underlying modes.
# Further to this, it has detailed two trajectory optimisation algorithms that encode the goals via this
# latent geometry.
# That is, the algorithms find trajectories that remain in a desired dynamics mode,
# whilst also avoiding regions of high epistemic uncertainty.

# This chapter has presented a method for performing trajectory optimisation in
# multimodal dynamical systems with the transition dynamics modelled as a
# \acrshort{mogpe} method.
# The trajectory optimisation is projected onto a probabilistic Riemannian
# manifold parameterised by the gating network of the \acrshort{mogpe} model.
# Given a start and end state, the trajectory optimisation can be tuned to find trajectories that either
# prioritise remaining in a desired dynamics mode or prioritise avoiding regions of the learned transition
# dynamics model with high epistemic uncertainty.

* Multimodal Model-Based Control Probabilistic Inference label:chap-traj-opt-inference
** Maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\startStateDist}{\ensuremath{p(\state_{1})}}
\newcommand{\transitionDist}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \modeVarK)}}
%\newcommand{\trajectoryDist}{\ensuremath{p(\stateControlTraj)}}
\renewcommand{\controlDist}{\ensuremath{\policy(\control_\timeInd \mid \state_\timeInd)}}


\renewcommand{\trajectoryVarDist}{\ensuremath{q(\stateTraj, \controlTraj)}}
\newcommand{\controlTrajVarDist}{\ensuremath{q(\control_{0:\TimeInd} \mid \state_{0:\TimeInd})}}
\newcommand{\controlVarDist}{\ensuremath{q(\control_{\timeInd})}}

\newcommand{\optimalVar}{\ensuremath{\mathcal{O}}}
\newcommand{\monotonicFunc}{\ensuremath{g}}
\newcommand{\temperature}{\ensuremath{\gamma}}

\newcommand{\modeProb}{\ensuremath{\Pr(\modeVar_\timeInd = \modeInd \mid \state_\timeInd, \control_\timeInd)}}
\renewcommand{\optimalProb}{\ensuremath{\Pr(\optimalVar_\timeInd = 1 \mid \state_\timeInd, \control_\timeInd)}}
\newcommand{\optimalDist}{\ensuremath{P(\optimalVar_\timeInd \mid \state_\timeInd, \control_\timeInd)}}

\newcommand{\terminalCostDist}{\ensuremath{\Pr(\optimalVar_\TimeInd=1 \mid \state_\TimeInd)}}

\renewcommand{\marginalLikelihood}{\ensuremath{p(\optimalVar_{0:\TimeInd} = 1, \modeVar_{0:\TimeInd}=\desiredMode \mid \state_0)}}
%\newcommand{\jointDist}{\ensuremath{p(\optimalVar_{1:\TimeInd}=1, \modeVar_{1:\TimeInd}=\modeDes{\modeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})}}
%\newcommand{\jointDist}{\ensuremath{p(\optimalVar_{1:\TimeInd}, \modeVar_{1:\TimeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})}}
\renewcommand{\jointDist}{\ensuremath{p(\optimalVar_{0:\TimeInd}=1, \stateTraj, \controlTraj \mid \state_0})}}

\renewcommand{\trajectoryDist}{\ensuremath{p(\state_{1:\TimeInd}, \control_{0:\TimeInd} \mid \state_0)}}


\newcommand{\objective}{\ensuremath{J_{\text{quadratic}}}}

\renewcommand{\priorPolicy}{\ensuremath{\policy_0}}
\renewcommand{\priorPolicyDist}{\ensuremath{p_{\policy_0}}}
#+END_EXPORT
** Intro :ignore:
# The methods leveraged the latent geometry of the \acrshort{mosvgpe} methods \acrshort{gp}-based gating network
# to encode the mode remaining behaviour.
cref:chap-traj-opt-geometry presented two methods for finding trajectories from an initial state $\state_0$ -- in
a desired dynamics mode -- to a target state $\state_f$, whilst remaining in the desired dynamics mode.
These methods encoded the mode remaining behaviour via the latent geometry of the gating network.
In contrast, this chapter unleashes the power of the probability mass function over the expert indicator variable.
As all \acrshort{mogpe} methods have a probability mass function over the expert indicator variable,
leveraging this latent space for control makes it applicable with a wider range of \acrshort{mogpe} dynamics models.

# Leveraging this latent space for control makes it applicable in a wider range of \acrshort{mogpe} models
# because all \acrshort{mogpe} methods have a probability mass function over the expert indicator variable.
# Whereas they do no necessarily have a \acrshort{gp}-based gating network.

# The approach presented in this chapter extends the control as inference framework.
# It encodes mode remaining behaviour by conditioning on the expert indicator variable and inferring the optimal controls.

This chapter is organised as follows.
cref:sec-inference-background first recaps the necessary background and related work.
cref:sec-traj-opt-inference then details the trajectory optimisation algorithm.
Finally, cref:sec-traj-opt-inference-results evaluates the method in the two simulated quadcopter environments.
The work in this chapter is implemented in TensorFlow and is available on GitHub
cite:jax2018github[fn:2:Code accompanying this Chapter can be found @ [[https://github.com/aidanscannell/ModeOpt][\icon{\faGithub}/aidanscannell/ModeOpt.]]]

# This chapter presents an another control method  for controlling /unknown/ or /partially unknown/,
# multimodal dynamical systems,
# given a historical data set of state transitions $\dataset$, sampled from the system.
# It finds trajectories from an initial state $\state_0$ -- in a desired dynamics mode -- to
# a target state $\state_f$, whilst remaining in the desired dynamics mode.

# This chapter formulates control in multimodal dynamical systems as probabilistic inference.
# The method is model-based and leverages the \acrshort{mosvgpe} method from cref:chap-dynamics to learn a
# cref:chap-traj-opt-geometry leveraged the geometry of the \acrshort{gp} posteriors in the
# \acrshort{mosvgpe} gating functions to encode mode remaining behaviour.
# The approach presented in this chapter unleashes the power of the probability mass function over the expert indicator
# variable, making the approach applicable in a wider range of \acrshort{mogpe} models.



# the power of the MoSVGPE method's GP-based gating network by
# The trajectory optimisation algorithm presented in cref:sec-traj-opt-geometric can be viewed as
# an indirect method.
# \todo{Fact check it as an indirect method?}
# The trajectory optimisation algorithm presented in this section is a direct method that considers the
# full stochastic nature of the learned dynamics model. In particular, it considers uncertainty propagation
# through the dynamics.
# Given the SOC problem in cref:eq-mode-soc-problem, the trajectory optimisation algorithm presented here
# builds off of the control-as-inference framework citep:toussaintProbabilistic2006,kappenOptimal2013,
# which frames optimal control as probabilistic inference.
# Translating optimal control to inference can be viewed as a subset of probabilistic numerics
# citep:hennigProbabilistic2015, where statistical methods are used to solve numerical problems,
# providing uncertainty quantification, regularisation and faster convergence.
# This is achieved by assuming access to a dynamics model and
# embedding the cost function into the likelihood.

# The approach presented in this chapter extends the control as inference framework to multimodal dynamical systems, which
# infers the optimal controls after conditioning on the goals.
# The mode remaining behaviour is encoded by further conditioning on the mode indicator variable.

** Problem Statement
This chapter shares the problem statement in cref:sec-problem-statement.
It is briefly recapped here for convenience but please refer to cref:sec-problem-statement for more details.

This chapter considers nonlinear, stochastic, multimodal dynamical systems,
#+BEGIN_EXPORT latex
\begin{subequations}
\begin{align}
\state_{\timeInd+1} &= \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \bm\epsilon \\
&= \mode{\dynamicsFunc}(\state_\timeInd, \control_\timeInd) + \mode{\bm\epsilon}
\quad \text{if} \quad \modeVar_{\timeInd}=\modeInd \nonumber \\
\mode{\bm\epsilon} &\sim \mathcal{N}(\mathbf{0}, \Sigma_{\mode{\bm\epsilon}}).
\end{align}
\end{subequations}
#+END_EXPORT
Given a learned representation of the dynamics and a desired dynamics mode $\desiredMode$,
the goals of the trajectory optimisation are to,
- Goal 1 :: Navigate to a target state $\targetState$,
- Goal 2 :: Remain in the operable, desired dynamics mode $\desiredMode$,
  + Goal 2.1 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/, in the desired dynamics mode and in the gating network.

# - Goal 1 :: Remain in the operable, desired dynamics mode $\desiredMode$,
# - Goal 2 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/,
#   - in the desired dynamic mode,
#   - in the gating network.
That is, given a desired dynamics mode $\desiredMode$, the goal is to find a control
trajectory $\controlTraj = \control_{0:\TimeInd-1}$,
to navigate from an initial state $\state_{0}$, to a target state $\targetState$, over a horizon $\TimeInd$,
whilst minimising a cost function, $\costFunc: \stateDomain \times \controlDomain \rightarrow \R$
and keeping the system in the desired dynamics mode $\desiredMode$.
Formally, this problem is given by,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-mode-soc-problem}
\begin{align}
\min_{\controlTraj} &\E\Bigg[
\sum_{\timeInd=0}^{\TimeInd} \costFunc(\state_\timeInd, \control_\timeInd) \Bigg] \\
\text{s.t.} \quad &\state_{\timeInd+1} = \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \epsilon,
\quad \epsilon \sim \mathcal{N}(\mathbf{0}, \Sigma_{\epsilon}) \\
&\text{\cref{eq-mode-remaining-def}} \\
%\state_{\timeInd} &\in \desiredStateDomain \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] \\
&\state_\TimeInd = \targetState,
\end{align}
\end{subequations}
#+END_EXPORT
where the expectation is taken w.r.t. the distribution over state-control trajectories,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-trajectory-dist-with-controls}
p(\stateTraj, \controlTraj \mid \state_0) =
\prod_{\timeInd=0}^{\TimeInd-1}
p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd).
\end{align}
#+END_EXPORT
In contrast to cref:chap-traj-opt-geometry, this chapter models the controls with probability distributions.
As such, the expectation is over $p(\stateTraj, \controlTraj \mid \state_0)$ instead of
$p(\stateTraj \mid \state_0, \controlTraj)$.

# This chapter unleashes the power of the probability mass function over the expert indicator variable
# (visualised in cref:fig-traj-opt-gating-network).
# #+BEGIN_EXPORT latex
# \begin{figure}[t!]
# \centering
# \includegraphics[width=1.0\textwidth]{./images/traj-opt/mode-remaining/mosvgpe/desired_prob_no_traj.pdf}
# \caption{
# Visualisation of the latent variable posteriors associated with the gating network after training the \acrshort{mosvgpe}
# (with $\ModeInd=2$ experts) on the historical data set of state transitions from the 2D quadcopter environment
# in the illustrative example from \cref{illustrative_example}.
# (\subref{eq-traj-opt-gating-network-over-prob}) shows the probability of the desired mode over the domain
# and (\subref{eq-traj-opt-gating-network-over-svgp}) shows the desired mode's gating function posterior
# mean (left) and posterior variance (right).}
# \label{eq-traj-opt-gating-network-prob}
# \end{figure}
# #+END_EXPORT
# This chapter unleashes the power of these latent variables by making decisions under their uncertainty.

** Problem Statement label:sec-problem-statement-inference-traj-opt :noexport:
This chapter considers nonlinear, stochastic, multimodal dynamical systems,
#+BEGIN_EXPORT latex
\begin{subequations}
\begin{align}
\state_{\timeInd+1} &= \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \bm\epsilon \\
&= \mode{\dynamicsFunc}(\state_\timeInd, \control_\timeInd) + \mode{\bm\epsilon}
\quad \text{if} \quad \modeVar_{\timeInd}=\modeInd \nonumber \\
\mode{\bm\epsilon} &\sim \mathcal{N}(\mathbf{0}, \Sigma_{\mode{\bm\epsilon}}),
\end{align}
\end{subequations}
#+END_EXPORT
where the dynamics modes are defined by disjoint state domains.
That is, each dynamics mode is defined by its state domain
$\mode{\stateDomain} = \{ \state \in \stateDomain \mid \modeVar = \modeInd \}$, with
$\stateDomain_{i} \cap \stateDomain_{j} = \emptyset$ for $i \neq j$.
Each mode's dynamics are then given by,
#+BEGIN_EXPORT latex
\begin{align}
\mode{\latentFunc}: \mode{\stateDomain} \times \controlDomain \rightarrow \stateDomain.
\end{align}
#+END_EXPORT
Notice that each mode's transition dynamics are free to leave their state space $\mode{\stateDomain}$
and enter another mode.
Ideally, this work seeks to enforce the controlled system to remain in a given mode at every time step.
#+BEGIN_EXPORT latex
\begin{definition}[Mode Remaining] \label{def-mode-remaining}
Let $\modeInd$ denote a dynamics mode, defined by its state domain $\mode{\stateDomain} \subseteq \stateDomain$.
Given an inital state $\state_0 \in \mode{\stateDomain}$, and a sequence of controls $\controlTraj=\control_{0:\TimeInd-1}$,
the controlled system is said to be mode-remaining iff:
\begin{align} \label{eq-mode-remaining-def}
\dynamicsFunc(\state_{\timeInd}, \control_{\timeInd}) &\in \mode{\stateDomain} \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd]
\end{align}
\end{definition}
#+END_EXPORT
Given a desired dynamics mode $\desiredMode$, this work seeks to find a control
trajectory $\controlTraj = \control_{0:\TimeInd-1}$,
to navigate from an initial state $\state_{0} \in \desiredStateDomain$, to a
target state $\targetState \in \desiredStateDomain$,
over a horizon $\TimeInd$,
whilst minimising a cost function, $\costFunc: \stateDomain \times \controlDomain \rightarrow \R$
and keeping the system in the desired dynamics mode $\desiredMode$.
Formally, this problem is given by,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-mode-soc-problem}
\begin{align}
\min_{\controlTraj} &\E\Bigg[
\sum_{\timeInd=0}^{\TimeInd} \costFunc(\state_\timeInd, \control_\timeInd) \Bigg] \\
\text{s.t.} \quad &\state_{\timeInd+1} = \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \epsilon,
\quad \epsilon \sim \mathcal{N}(\mathbf{0}, \Sigma_{\epsilon}) \\
&\text{\cref{eq-mode-remaining-def}} \\
%\state_{\timeInd} &\in \desiredStateDomain \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] \\
&\state_\TimeInd = \targetState,
\end{align}
\end{subequations}
#+END_EXPORT
where the expectation is taken w.r.t. the distribution over state
trajectories,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-trajectory-dist}
%\controlledPolicyDist(\stateTraj \mid \state_0, \controlTraj) =
p(\stateTraj \mid \state_0, \controlTraj) =
\prod_{\timeInd=0}^{\TimeInd-1}
p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd).
\end{align}
#+END_EXPORT
To simplify notation, the state and control trajectories are denoted
as $\stateTraj = \state_{1:T}$ and $\controlTraj = \control_{0:T-1}$ respectively.
The novelty of this problem arises from the mode remaining behaviour.
To the best of our knowledge, there is no prior work addressing how to find
trajectories that remain in a desired dynamics mode,
when knowledge of how the system switches between the dynamics modes is /not known a priori/.

# where the expectation is taken w.r.t. the distribution over state-control
# trajectories under controller $\policy$,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-trajectory-dist}
# \controlledPolicyDist(\stateTraj \mid \state_0, \controlTraj) =
# \prod_{\timeInd=0}^{\TimeInd-1}
# p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)
# \policy(\control_\timeInd).
# \end{align}
# #+END_EXPORT


This chapter assumes prior access to the environment, enabling a data set of state transitions to be collected.
This data set is used to learn a factorised representation of the underlying dynamics modes
with the \acrshort{mosvgpe} method from cref:chap-dynamics.
# This method correctly identifies the underlying dynamics modes and provides informative latent spaces that
# can be used to encode mode remaining behaviour into control strategies.
# In particular, the GP-based gating network infers informative latent structure.
# cref:fig-traj-opt-gating-network-gp shows the GP posterior associated with the desired mode's gating function
# after training
# \acrshort{mosvgpe} on the historical data set of state transitions from the quadcopter experiment in cref:illustrative_example.
#+BEGIN_EXPORT latex
\begin{assumption} \label{}
A single-step dynamics model has been learned using
the \acrshort{mosvgpe} method from \cref{chap-dynamics} and a data set $\dataset$ of state transitions
from the system.
\end{assumption}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# A data set $\dataset$ of state transitions have previously been sampled from the
# system (at constant frequency) and used to train a single-step dynamics model using
# the \acrshort{mosvgpe} method from \cref{chap-dynamics}.
# \end{assumption}
# #+END_EXPORT
# As the true underlying dynamics modes and how the system switches between them are /not fully known a priori/,
# it is impossible to develop algorithms that can find trajectories that are guaranteed to be
As the control algorithms presented in this chapter leverage this learned dynamics model,
it is impossible to find trajectories that are guaranteed to be Mode Remaining (cref:def-mode-remaining).
This work relaxes the requirement to finding mode remaining trajectories with high probability.
#+BEGIN_EXPORT latex
\begin{definition}[$\delta$-mode remaining] \label{def-delta-mode-remaining}
Let $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ denote a multimodal dynamical system
and $\desiredMode$ a desired dynamics mode defined by its state domain $\desiredStateDomain \subseteq \stateDomain$.
Given an initial state $\state_0 \in \desiredStateDomain$ and $\delta \in (0,1]$,
a system is said to be $\delta$-mode remaining under the controls $\controlTraj$ iff:
\begin{align}
\Pr( \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] : \dynamicsFunc(\state_{\timeInd},\control_{\timeInd}) &\in \desiredStateDomain,
\control_{\timeInd} \in \controlDomain) \geq 1 - \delta
\end{align}
\end{definition}
#+END_EXPORT

Given this learned dynamics model, it is assumed that a desired dynamics mode $\desiredMode$ can easily be identified.
This is a realistic assumption as the parameters associated with each dynamics GP can be used to identify
different behaviours.
For example, the noise variance associated with each mode's dynamics GP models its process noise.
If it is desirable to avoid modes with high process noise, then they can easily be identified by their noise
variances.
#+BEGIN_EXPORT latex
\begin{assumption} \label{}
A desired dynamics mode $\desiredMode$ is known.
\end{assumption}
#+END_EXPORT
Given a learned dynamics model and a desired dynamics mode $\desiredMode$,
the goals of the trajectory optimisation in this chapter can be summarised as follows,
# - Goal 1 :: Remain in a desired dynamics mode $\desiredMode$,
# - Goal 2 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/.
- Goal 1 :: Remain in the operable, desired dynamics mode $\desiredMode$,
- Goal 2 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/,
  - in the desired dynamic mode,
  - in the gating network.
The second goal arises due to learning the dynamics model from observations.
The learned model may not be able to confidently predict which mode governs the dynamics at a given region.
This is due to lack of training observations and is known as /epistemic uncertainty/.
It is desirable to avoid entering these regions as it may result in the system leaving the desired dynamics mode.

# #+BEGIN_EXPORT latex
# \begin{remark}[Epistemic Uncertainty]
# %Given that the underlying dynamics modes and how the dynamics switches between them, are
# %\textit{not fully known a priori}, these methods should \textit{air on the side of caution}.
# Due to lack of training observations, the learned model may be uncertain which mode governs the
# dynamics at a particular region. This is known as epistemic uncertainty.
# In a risk-averse setting, trajectories should also avoid entering these regions of high epistemic uncertainty.
# \end{remark}
# #+END_EXPORT

# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# The system has previously been observed to obtain a data set $\dataset$ of state transitions sampled at constant frequency.
# \end{assumption}
# #+END_EXPORT
# Given this learned dynamics model, it is assumed that a desired dynamics mode $\desiredMode$ can easily be identified.
# This is a realistic assumption as the parameters associated with each dynamics GP represent different
# characteristics of the system.
# For example, the noise variance associated with each mode's dynamics GP models the process noise that the mode
# is subject to.
# A high noise variance can therefore be used to identify dynamics modes that are undesirable.


# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# A desired dynamics mode $\desiredMode$  is known.
# Moreover, its transition dynamics are given by
# $\desiredDynamicsFunc: \desiredStateDomain \times \controlDomain \rightarrow \stateDomain$.
# \end{assumption}
# #+END_EXPORT
# the \acrshort{mosvgpe} method from cref:chap-dynamics to learn a factorised
# representation of the underlying dynamics modes.

# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# Further to this, it is assumed that the mode switching behaviour is governed by the state space, i.e.
# each dynamics modes is governed by its state domain
# $\stateDomain_{\modeInd} = \{ \state \in \stateDomain \mid \modeVar = \modeInd \}$,
# giving each mode's dynamics,
# $\mode{\latentFunc}: \mode{\stateDomain} \times \controlDomain \rightarrow \stateDomain$.

# The system switches between its dynamics modes over the systems state space, i.e.
# each dynamics modes is governed by its state domain
# $\stateDomain_{\modeInd} = \{ \state \in \stateDomain \mid \modeVar = \modeInd \}$,
# giving each mode's dynamics,
# $\mode{\latentFunc}: \mode{\stateDomain} \times \controlDomain \rightarrow \stateDomain$.
# \end{assumption}
# #+END_EXPORT

# and used to train a single-step dynamics model using
# the \acrshort{mosvgpe} method from \cref{chap-dynamics}.

# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# A data set $\dataset$ of state transitions have previously been sampled from the
# system (at constant frequency) and used to train a single-step dynamics model using
# the \acrshort{mosvgpe} method from \cref{chap-dynamics}.
# \end{assumption}
# #+END_EXPORT
# Further to this, it assumes that a desired dynamics mode $\desiredMode$ is /known a priori/.
# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# Let $\desiredMode$ denote the desired dynamics mode governed
# by its state domain
# $\desiredStateDomain = \{ \state \in \stateDomain \mid \modeVar = \desiredMode \}$.
# The desired mode's transition dynamics are given by
# $\desiredDynamicsFunc: \desiredStateDomain \times \controlDomain \rightarrow \stateDomain$,
# with continuous states $\state \in \stateDomain \subseteq \R^\StateDim$ and controls
# $\control \in \controlDomain \subseteq \R^\ControlDim$.
# \end{assumption}
# #+END_EXPORT

# The desired mode's transition dynamics are given by
# $\desiredDynamicsFunc: \desiredStateDomain \times \controlDomain \rightarrow \stateDomain$,
# with continuous states $\state \in \stateDomain \subseteq \R^\StateDim$ and controls
# $\control \in \controlDomain \subseteq \R^\ControlDim$,
# where $\desiredStateDomain = \{ \state \in \stateDomain \mid \modeVar = \desiredMode \}$
# denotes the subset of the state space associated with the desired dynamics mode.
# Note that the desired mode's transition dynamics $\desiredDynamicsFunc$
# are free to leave the desired mode's state space $\desiredStateDomain$.






\todo{cite active learning paper and multimodal mogpe paper?}
# The most similar works are cite:schreiterSafe2015 where active learning is gu



# Of particular interest in this chapter is the GP posterior over the desired mode's gating function.
# cref:fig-traj-opt-gating-network-gp visualises this GP posterior after training on the historical data set of state
# transitions from the illustrative example in cref:illustrative_example.
# One approach to encoding mode remaining behaviour is to consider the geometry of the latent gating functions.

** Problem Statement :ignore:noexport:

The novelty of our problem, however, arises from the mode remaining behaviour.
To the best of our knowledge, there is no prior work addressing how to find
trajectories that remain in a desired dynamics mode,
when knowledge of how the system switches between the dynamics modes is /not known a priori/.
This chapter explores different mechanisms for encoding mode remaining behaviour.

The \acrshort{mosvgpe} method from cref:chap-dynamics can be used to learn a factorised representation of the underlying
dynamics modes.
Conveniently, it was formulated with latent variables
to represent the mode indicator variable $\modeVar$ and our uncertainty in it.
Of particular interest are the posteriors' over the mode indicator variable and the gating functions
(visualised in cref:fig-traj-opt-gating-network).
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\includegraphics[width=1.0\textwidth]{./images/traj-opt/mode-remaining/mosvgpe/desired_prob_no_traj.pdf}
\caption{
Visualisation of the latent variable posteriors associated with the gating network after training the \acrshort{mosvgpe}
(with $\ModeInd=2$ experts) on the historical data set of state transitions from the 2D quadcopter environment
in the illustrative example from \cref{illustrative_example}.
(\subref{eq-traj-opt-gating-network-over-prob}) shows the probability of the desired mode over the domain
and (\subref{eq-traj-opt-gating-network-over-svgp}) shows the desired mode's gating function posterior
mean (left) and posterior variance (right).}
\label{eq-traj-opt-gating-network-prob}
\end{figure}
#+END_EXPORT
This chapter unleashes the power of these latent variables by making decisions under their uncertainty.

The trajectory optimisation algorithm presented in cref:sec-traj-opt-geometric can be viewed as
an indirect method.
\todo{Fact check it as an indirect method?}
The trajectory optimisation algorithm presented in this section is a direct method that considers the
full stochastic nature of the learned dynamics model. In particular, it considers uncertainty propagation
through the dynamics.
Given the SOC problem in cref:eq-mode-soc-problem, the trajectory optimisation algorithm presented here
builds off of the control-as-inference framework citep:toussaintProbabilistic2006,kappenOptimal2013,
which frames optimal control as probabilistic inference.
Translating optimal control to inference can be viewed as a subset of probabilistic numerics
citep:hennigProbabilistic2015, where statistical methods are used to solve numerical problems,
providing uncertainty quantification, regularisation and faster convergence.
This is achieved by assuming access to a dynamics model and
embedding the cost function into the likelihood.

** Background and Related Work label:sec-inference-background
*** Cost Functions as Likelihoods :ignore:
This section firsts recaps the control-as-inference framework.
To formulate optimal control as probabilistic inference
it is first embedded into a graphical model (see Figure ref:fig-basic-control-graphical-model).
The joint probability model (over a trajectory) is augmented with an
additional variable to encode the notion of cost (or reward)
over the trajectory (see Figure ref:fig-augmented-control-graphical-model).
The new variable is a Bernoulli random variable $\optimalVar_\timeInd \in \{0, 1\}$, that indicates if
time step $\timeInd$ is /optimal/ $\optimalVar_\timeInd=1$, or not /optimal/ $\optimalVar_\timeInd=0$.
The likelihood distribution can be formulated by mapping the negative
cost through a monotonically increasing function $\monotonicFunc$,
giving the likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-monotonicOptimalityLikelihood}
\optimalProb &\coloneqq \monotonicFunc( -\costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
\todo{compare different fmons?}
# #+BEGIN_EXPORT latex
# \todo[inline]{add detials on Cross Entropy Method (CEM) cite:chuaDeep2018 and other methods}
# \begin{myquote}
# As shown in \cite{okadaVariational2020}, the choice of monotonic function,
# $\monotonicFunc$,
# leads to the inference algorithm resembling different well-known algorithms.
# For example, selecting $\monotonicFunc$ to be the exponential function,
# \begin{align} \label{eq-}
# \optimalProb &\propto \text{exp}( -\costFunc(\state_\timeInd, \control_\timeInd))
# \end{align}
# leads to the algorithm resembling Model Predictive Path Integral (MPPI)
# \cite{williamsModel2017,williamsInformation2017}, whilst
# selecting $\monotonicFunc$ to as,
# \begin{align} \label{eq-}
# \optimalProb &\propto \mathbb{I}\left[( \costFunc(\state_\timeInd, \control_\timeInd)) \leq \costFunc_{\text{thd}} \right]
# \end{align}
# recovers an algorithm resembling the Cross Entropy Method (CEM).
# \end{myquote}
# #+END_EXPORT
A common (and convenient) approach is to formulate
the likelihood using an exponential transform of
the cost. This results in a Boltzmann distribution where the
inverse temperature, $\temperature$, is used to scale the cost,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-exponentialOptimalityLikelihood}
\optimalProb &\propto \exp ( -\temperature\costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
The resulting negative log-likelihood,
for a single state-control trajectory \stateTraj, \controlTraj,
is an affine transformation of the cost,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-negative-log-likelihood-cost}
-\log \Pr(\optimalVar_{0:\TimeInd} \mid \stateTraj, \controlTraj)
&=  \temperature\costFunc(\stateTraj, \controlTraj),
\end{align}
#+END_EXPORT
which preserves convexity.
When the inverse temperature parameter is set to 1, (i.e. $\temperature=1$) the Maximum Likelihood (ML)
trajectory coincides with classical optimal control citep:toussaintRobot2009.

**** Quadratic Cost Functions :noexport:
This work primarily focuses on quadratic costs, $\costFunc$, which are ubiquitous in control.

*Terminal Control Problems*
To find a trajectory between a start state, $\state_0$, and a target state, $\targetState$, at
time, $\TimeInd$, it is common to minimise the deviation of the final state, $\state_\TimeInd$,
from the desired target state $\targetState$.
To allow greater generality, a user defined, real symmetric positive definite matrix,
$\terminalStateCostMatrix$, can be introduced,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-terminal-cost}
J &= (\state_{\TimeInd} - \targetState)^T \terminalStateCostMatrix (\state_{\TimeInd} - \targetState) \nonumber \\
&= || \state_\TimeInd - \targetState ||_{\terminalStateCostMatrix}.
\end{align}
#+END_EXPORT
# When $\terminalStateCostMatrix$ is the identity matrix cref:eq-quadratic-terminal-cost-matrix-notation
# is recovered.
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-quadratic-terminal-cost-matrix-notation}
# J = (\state_{\TimeInd} - \targetState)^T(\state_{\TimeInd} - \targetState).
# \end{align}
# #+END_EXPORT
# This can also be written as,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-quadratic-terminal-cost-norm}
# J = || \state_\TimeInd - \targetState ||_2,
# \end{align}
# #+END_EXPORT
# where $|| \state_\TimeInd - \targetState ||_2$ denotes the L2 norm.

# *Minimum Control Effort Problems*
# It is also common to find trajectories that minimise the expenditure of control effort.
# The meaning of minimum control effort depends upon the specific problem.
# It may refer to minimising the total expenditure of fuel, or to minimising the energy dissipated from the system.
# Regardless of the specific interpretation, it is usually desirable to regularise the controls.
# Similar to the terminal cost, it is common to use a quadratic form,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-quadratic-cost-control}
# J &= \sum_{\timeInd=0}^{\TimeInd-1} \control_{\timeInd}^T \controlCostMatrix \control_{\timeInd} \nonumber \\
# &= || \control_\timeInd ||_\controlCostMatrix^2,
# \end{align}
# #+END_EXPORT
# where $\controlCostMatrix$ is a user defined, real symmetric positive definite weight matrix.
# This objective regularises the control trajectory under the L2 norm.
# It is worth noting that the control weight matrix can be a function of time, $\controlCostMatrix(t)$.

# #+BEGIN_EXPORT latex
# \todo{Add tracking/regulating state cost term? And length of trajectory with $\dot{x}^T\dot{x}$?}
# #+END_EXPORT

# *Tracking Problems*
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-quadratic-cost-state}
# J &= \state_{\timeInd}^T \stateCostMatrix \state_{\timeInd}
#   &= || \state_\timeInd ||_\stateCostMatrix^2
# \end{align}
# #+END_EXPORT
# where $\stateCostMatrix$ and $\controlCostMatrix$ are
# user defined, real symmetric positive semi-definite and real symmetric positive definite weight matrices respectively.
# It is worth noting that the state and control weight matrices can be functions of time, i.e.
# $\stateCostMatrix(t)$ and $\controlCostMatrix(t)$.
# This cost function regularises state
# This work seeks to control the terminal state and regularise the controls so combines these two objectives as
# follows,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-quadratic-cost-terminal-control}
# \objective =
# ||\state_{\TimeInd} - \targetState ||_{\terminalStateCostMatrix}
# + \sum_{\timeInd=0}^{\TimeInd-1}
# + ||\control_{\timeInd}||_{\controlCostMatrix}.
# \end{align}
# #+END_EXPORT
# Importantly, given a trajectory where each state and control are normally distributed,
# ${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$
# and
# ${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$,
# the expected cost for a trajectory can be calculated in closed-form,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-expected-quadratic-cost}
# %\E_{\stateTraj, \controlTraj} \left[ \costFunc_{\text{quadratic}}(\stateTraj, \controlTraj) \right]
# \E_{\stateTraj, \controlTraj} \left[ \objective \right] =
# \text{tr}(\terminalStateCostMatrix \terminalStateCov) +
# (\terminalStateMean - \targetState)^T \terminalStateCostMatrix (\terminalStateMean - \targetState)
# + \sum_{\timeInd=1}^{\TimeInd-1}
# %\text{tr}(\stateCostMatrix \stateCov_\timeInd)
# %+ \stateMean_{\timeInd}^T \stateCostMatrix \stateMean_{\timeInd}
# \text{tr}(\controlCostMatrix \controlCov )
# + \controlMean^T \controlCostMatrix \controlMean
# \end{align}
# #+END_EXPORT

It is worth noting that if a cost function is not quadratic then a quadratic approximation
can be obtained with a Taylor expansion.

*** graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \begin{subfigure}{.32\textwidth}
  \centering
   \resizebox{0.93\columnwidth}{!}{
 %  \resizebox{0.32\textwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_0$};
      \node[latent, right=of x1] (x2) {$\state_1$};
      \node[latent, right=of x2] (x3) {$\state_2$};
      \node[latent, right=of x3] (x4) {$\state_3$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_0$};
      \node[latent, right=of u1] (u2) {$\control_1$};
      \node[latent, right=of u2] (u3) {$\control_2$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);
    \end{tikzpicture}
    }
  \caption{Graphical model with states and controls.}
\label{fig-basic-control-graphical-model}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
  \centering
   \resizebox{0.93\columnwidth}{!}{
 %  \resizebox{0.32\textwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_0$};
      \node[latent, right=of x1] (x2) {$\state_1$};
      \node[latent, right=of x2] (x3) {$\state_2$};
      \node[latent, right=of x3] (x4) {$\state_3$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_0$};
      \node[latent, right=of u1] (u2) {$\control_1$};
      \node[latent, right=of u2] (u3) {$\control_2$};

      \node[obs, above=of u1] (o1) {$\optimalVar_0$};
      \node[obs, right=of o1] (o2) {$\optimalVar_1$};
      \node[obs, right=of o2] (o3) {$\optimalVar_2$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);
    \end{tikzpicture}
    }
  \caption{Graphical model with optimality variable.}
\label{fig-augmented-control-graphical-model}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
  \centering
   \resizebox{0.93\textwidth}{!}{
   %\resizebox{0.32\textwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_0$};
      \node[latent, right=of x1] (x2) {$\state_1$};
      \node[latent, right=of x2] (x3) {$\state_2$};
      \node[latent, right=of x3] (x4) {$\state_3$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_0$};
      \node[latent, right=of u1] (u2) {$\control_1$};
      \node[latent, right=of u2] (u3) {$\control_2$};

      \node[latent, above=of u1] (o1) {$\optimalVar_0$};
      \node[latent, right=of o1] (o2) {$\optimalVar_1$};
      \node[latent, right=of o2] (o3) {$\optimalVar_2$};

      \node[latent, below=of x1] (a1) {$\modeVar_0$};
      \node[latent, right=of a1] (a2) {$\modeVar_1$};
      \node[latent, right=of a2] (a3) {$\modeVar_2$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);

      \draw[->] (x1)--(a1);
      \draw[->] (u1) to [out=-90,in=30] (a1);
      \draw[->] (x2)--(a2);
      \draw[->] (u2) to [out=-90,in=30] (a2);
      \draw[->] (x3)--(a3);
      \draw[->] (u3) to [out=-90,in=30] (a3);
    \end{tikzpicture}
    }
  \caption{Graphical model with optimality and mode indicator variables.}
\label{fig-mode-augmented-control-graphical-model}
\end{subfigure}
\caption{Graphical models of control formulated as inference.}
\label{fig-control-graphical-model}
\end{figure}
#+END_EXPORT

*** Inference of Sequential Latent Variables
\newline
The joint probability for an optimal trajectory (i.e.
for $\optimalVar_\timeInd=1$ for all $\timeInd \in \{0,\ldots,\TimeInd\}$),
can be factorised using its Markovian structure,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-traj-opt-joint-dist}
\jointDist =
\underbrace{\terminalCostDist}_{\text{Terminal Cost}}
\prod_{\timeInd=0}^{\TimeInd-1}
\underbrace{\optimalProb}_{\text{Integral Cost}}
\underbrace{\transitionDist}_{\text{Dynamics}}
\underbrace{\controlDist}_{\text{Controller}}
\end{align}
\normalsize
#+END_EXPORT
where $\controlDist$ denotes the controller or policy.
cite:toussaintRobot2009 highlight that although the ML trajectory coincides with the classical
optimal trajectory, taking expectations over trajectories
i.e. calculating $\log p(\optimalVar_{0:\TimeInd}=1 \mid \state_0)$,
is not equivalent to expected cost minimisation.
cite:rawlikStochastic2013 extend the concepts from  cite:toussaintRobot2009 to show the general relation
to classical SOC.
They introduce a prior policy, $\priorPolicy$, and the distribution over state-control trajectories
under this policy is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-prior-trajectory-dist}
\priorPolicyDist(\stateTraj, \controlTraj) =
p(\stateTraj, \controlTraj \mid \optimalVar_{0:\TimeInd}=1, \state_0) =
Z^{-1}
\controlledPolicyDist(\stateTraj, \controlTraj) \prod_{\timeInd=0}^\TimeInd \exp \left( - \temperature
\costFunc(\state_\timeInd,\control_\timeInd) \right)
\end{align}
#+END_EXPORT
where the distribution over state-control trajectories under policy $\policy$, is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-trajectory-dist}
%p(\stateTraj, \controlTraj \mid \state_0) =
\controlledPolicyDist(\stateTraj, \controlTraj) =
\prod_{\timeInd=0}^{\TimeInd-1}
p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)
\policy(\control_\timeInd \mid \state_\timeInd).
\end{align}
#+END_EXPORT
This distribution $\priorPolicyDist(\stateTraj, \controlTraj)$ is conditioned on the
optimality variable but generated by a potentially uniform policy, $\priorPolicy$.
Intuitively $\controlledPolicyDist(\stateTraj, \controlTraj)$, is thought of as the controlled process, which
is not conditioned on optimality and $\priorPolicyDist(\stateTraj, \controlTraj)$
as the posterior process, conditioned on optimality
but generated by a potentially uniform policy, $\priorPolicy$.
The dual problem is then to find the control policy, $\policy$, where the controlled process,
$\controlledPolicyDist(\stateTraj, \controlTraj)$,
matches the posterior process, $\priorPolicyDist(\stateTraj, \controlTraj)$.
Given $\priorPolicy$ is an arbitrary stochastic policy and $\mathbb{D}$ is the set of deterministic
policies, the problem,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-kl-control}
\policy_* &= \argmin_{\policy \in \mathbb{D}}\text{KL}(\controlledPolicyDist \mid\mid \priorPolicyDist) \nonumber \\
&= \argmin_{\policy \in \mathbb{D}} Z + \temperature
\E_{\controlledPolicyDist(\stateTraj, \controlTraj)}
\left[ \costFunc(\stateTraj, \controlTraj) \right]
+ \E_{\controlledPolicyDist(\stateTraj)} \left[ \text{KL}(\policy \mid\mid \priorPolicy) \right] \nonumber \\
&= \argmin_{\policy \in \mathbb{D}}
\underbrace{Z}_{\text{constant}} +
\underbrace{\temperature \E_{\controlledPolicyDist(\stateTraj, \controlTraj)}
\left[ \costFunc(\stateTraj, \controlTraj) \right]}_{\text{expected costs}}
- \E_{\controlledPolicyDist(\stateTraj, \controlTraj)} \left[ \log \priorPolicy(\controlTraj \mid \stateTraj) \right]
- \underbrace{\E_{\controlledPolicyDist(\stateTraj)} \left[ H(\policy) \right]}_{\text{max entropy term}},
\end{align}
#+END_EXPORT
is equivalent to the SOC problem in cref:eq-mode-soc-problem, with a modified integral cost,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-cost-per-stage}
\hat{\costFunc}(\state_\timeInd, \control_\timeInd) = \costFunc(\state_\timeInd, \control_\timeInd)
- \frac{1}{\temperature} \log \priorPolicy(\control_\timeInd \mid \state_\timeInd).
\end{align}
#+END_EXPORT
The problem in cref:eq-kl-control finds trajectories that balance
minimising expected costs
$\E_{\controlledPolicyDist(\stateTraj, \controlTraj)} \left[ \costFunc(\stateTraj, \controlTraj) \right]$
and selecting a policy $\policy$ that is similar to the prior policy $\priorPolicy$.
If the prior policy $\priorPolicy$ is assumed to be uniform, then
$\E_{\controlledPolicyDist(\stateTraj, \controlTraj)} \left[ \log \priorPolicy(\controlTraj \mid \stateTraj) \right]$
becomes constant and the optimised policy, $\policy_*$, is a balance of minimising expected costs and
maximising the policy's entropy,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-kl-control-uniform}
\policy_* &= \argmin_{\policy \in \mathbb{D}}
\underbrace{\temperature \E_{\controlledPolicyDist(\stateTraj, \controlTraj)}
\left[ \costFunc(\stateTraj, \controlTraj) \right]}_{\text{expected costs}}
- \underbrace{\E_{\controlledPolicyDist(\stateTraj)} \left[ H(\policy) \right]}_{\text{max entropy term}}.
\end{align}
#+END_EXPORT

*Maximum Entropy Regularisation* Formulating trajectory optimisation in this way encodes the
maximum causal entropy principle, which is often used to achieve robustness,
in particular for inverse optimal control cite:ziebartModeling2010.
\todo{add something on ME being questionable for stochastic dynamics}
# It is important to note that the maximum entropy framework is problematic for stochastic dynamics.
# In essence, it assumes that the agent is able to control the dynamics as well as the controls in order
# to obtain optimal trajectories.

*Other Approaches* There are multiple approaches to performing inference in this graphical model.
Trading accuracy for computational complexity is often required for real-time control.
In this case, one approach would be to approximate the dynamics with linear or
quadratic approximations, as is done in iLQR/iLQG and DDP respectively.
Given linear dynamics,
the full graphical model in cref:eq-traj-opt-joint-dist can be
computed using approximate Gaussian message passing, for which effective methods exist cite:loeligerFactor2007.
The inference problem can then be solved using the
expectation maximisation algorithm for dynamical system estimation
citep:shumwayAPPROACH1982,ghahramaniLearning1999,schonSystem2011, with input estimation cite:watsonStochastic2021.

# However, inference in nonlinear SSMs is

** Mode Remaining Trajectory Optimisation as Variational Inference label:sec-traj-opt-inference
*** maths :ignore:
#+BEGIN_EXPORT latex
%\renewcommand{\optimalProb}{\ensuremath{\Pr(\optimalVar_\timeInd = 1, \modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd)}}
%\renewcommand{\modeProb}{\ensuremath{\Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd)}}
\renewcommand{\optimalVarTraj}{\ensuremath{\optimalVar_{0:\TimeInd}=1}}
\renewcommand{\modeVarTraj}{\ensuremath{\modeVar_{0:\TimeInd}=\desiredMode}}

\renewcommand{\modeVarK}{\ensuremath{\modeVar_{\timeInd} = \modeInd}}
\renewcommand{\modeProb}{\ensuremath{\Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd)}}
\renewcommand{\terminalCostDist}{\ensuremath{\Pr(\optimalVar_\TimeInd=1, \modeVar_\TimeInd=\desiredMode \mid \state_\TimeInd)}}
\renewcommand{\terminalModeProbDist}{\ensuremath{\Pr(\modeVar_\TimeInd=\desiredMode \mid \state_\TimeInd)}}
\renewcommand{\jointDist}{\ensuremath{p(\optimalVar_{0:\TimeInd}=1, \modeVar_{0:\TimeInd}=\desiredMode, \stateTraj, \controlTraj \mid \state_0})}}

\renewcommand{\transitionDistOptimal}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \optimalVarTraj, \modeVarTraj)}}
\renewcommand{\transitionDist}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)}}
\renewcommand{\transitionDistK}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \modeVarK)}}
#+END_EXPORT

*** intro :ignore:
# The resulting probabilistic graphical model
# can be solved efficiently using recursive Bayesian inference over time.

# *Augmented Graphical Model*
In order to find trajectories that remain in a desired
dynamics mode, this work further augments
the graphical model in cref:fig-control-graphical-model with the mode indicator variable $\modeVar \in \modeDomain$
from cref:eq-multimodal-dynamics-disc.
The resulting graphical model is shown in Figure ref:fig-mode-augmented-control-graphical-model,
where the evidence is that $\optimalVar_{\timeInd}=1$ and $\modeVar_{\timeInd}=\desiredMode$ for
all $\timeInd \in \{0,\ldots,\TimeInd\}$.
The joint probability model is then given by,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-traj-opt-joint-dist-mode}
\jointDist =
&\underbrace{\terminalCostDist}_{\text{terminal cost}} \nonumber \\
&\prod_{\timeInd=0}^{\TimeInd-1} \bigg[
\underbrace{\optimalProb}_{\text{integral cost}}
\underbrace{\modeProb}_{\text{mode remaining term}} \bigg] \nonumber \\
&\prod_{\timeInd=0}^{\TimeInd-1} \bigg[
\underbrace{\transitionDist}_{\text{dynamics}}
\underbrace{\controlDist}_{\text{controller}} \bigg].
\end{align}
\normalsize
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \small
# \begin{align} \label{eq-traj-opt-joint-dist-mode}
# \jointDist =
# &\underbrace{\terminalCostDist}_{\text{Terminal Cost}} \nonumber \\
# &\prod_{\timeInd=0}^{\TimeInd-1} \bigg[
# \underbrace{\optimalProb}_{\text{Integral Cost}}
# \underbrace{\modeProb}_{\text{Mode Remaining Term}} \nonumber \\
# &\underbrace{\transitionDist}_{\text{Dynamics}}
# \underbrace{\controlDist}_{\text{Controller}} \bigg].
# \end{align}
# \normalsize
# #+END_EXPORT
That is, the probability of observing a trajectory is given by taking the product of its probability of occurring
according to the dynamics, with the exponential of the negative cost and the probability of remaining
in the desired dynamics mode.
Given deterministic dynamics, the trajectory with highest probability will be that with the lowest
cost and highest probability of remaining in the desired dynamics mode.

This work draws on the connection between KL-divergence control citep:rawlikStochastic2013
and structured variational inference.
Whilst the derivation shown here differs from cite:rawlikStochastic2013, the underlying framework and
objective are the same.

# Also note that the posterior is conditioned on the initial state $\state_0$.


# Intuitively, the posterior of interest is th
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# p(\state_{1:\TimeInd}, \control_{1:\TimeInd} \mid \optimalVar_{1:\timeInd}=1, \state_1) =
# \trajectoryDist
# \left[
# \prod_{\timeInd=1}^\TimeInd
# p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \optimalVar_{1:\TimeInd})
# \policy(\control_\timeInd \mid \state_\timeInd)
# p(\optimalVar_\timeInd=1 \mid \state_\timeInd, \control_\timeInd)
# \end{align}
# #+END_EXPORT

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# p(\state_{1:\TimeInd}, \control_{1:\TimeInd}\mid \optimalVar_{1:\TimeInd}=1, \state_1)
# =
# \prod_{\timeInd=1}^\TimeInd
# p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)
# p(\optimalVar_\timeInd=1 \mid \state_\timeInd, \control_\timeInd)
# p(\control_\timeInd \mid \state_\timeInd)
# \prod_{\timeInd=1}^\TimeInd
# \text{exp}\left( -\temperature \integralCostFunc(\state_\timeInd, \control_\timeInd) \right)
# \end{align}
# #+END_EXPORT

# In the RL setting the policy is parameterised with parameters, $\theta$, which are optimised and used
# to predict the controls.
# In contrast, the control setting is usually interested in optimising the states and controls directly.

# The object of interest is the distribution over the controls, $\controlTraj$,
# conditioned on the initial state, $\state_0$, all future time steps being optimal and in the
# desired dynamics mode,

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-optimal-control-message-passing}
# p(\controlTraj \mid \state_{0}, \optimalVar_{0:\TimeInd}=1, \modeVar_{0:\TimeInd}=\desiredMode) &=
# \frac{
# p(\optimalVar_{0:\TimeInd}=1 \mid \stateTraj, \controlTraj)
# p(\modeVar_{0:\TimeInd}=\desiredMode \mid \stateTraj, \controlTraj)
# p(\stateTraj, \controlTraj \mid \optimalVar_{0:\TimeInd}=1, \modeVar_{0:\TimeInd}=\desiredMode, \state_0)
# }{}
# \end{align}
# #+END_EXPORT

*** Variational Inference :ignore:
# p(\stateTraj, \controlTraj \mid \)$
\newline

*Variational Inference*
In variational inference the goal is to approximate a distribution $p(\mathbf{y})$
with another, potentially simpler distribution $q(\mathbf{y})$.
Typically this distribution $q(\mathbf{y})$ is selected to be a product of conditional distributions connected in a
chain or tree, which lends itself to exact inference.
In this work, the goal is to approximate the intractable distribution over optimal trajectories,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\stateTraj, \controlTraj \mid \state_0, \optimalVarTraj, \modeVarTraj)
= Z^{-1}
\Bigg[ \prod_{\timeInd=0}^{\TimeInd-1}
&\underbrace{ \transitionDistOptimal}_{\text{optimal dynamics}} \nonumber \\
&\underbrace{ p(\control_\timeInd \mid \state_\timeInd, \optimalVarTraj, \modeVarTraj) }_{\text{optimal policy}}
\Bigg] \nonumber \\
\prod_{\timeInd=0}^{\TimeInd} \Bigg[
&\underbrace{\modeProb}_{\text{mode remaining}}
\underbrace{\exp \left( - \gamma  \costFunc(\state_\timeInd, \control_\timeInd) \right)}_{\text{cost}} \Bigg],
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# p(\stateTraj, \controlTraj \mid \state_0, \optimalVarTraj, \modeVarTraj)
# = Z^{-1}
# &\underbrace{\terminalModeProbDist}_{\text{Mode Remaining}}
# \underbrace{\exp \left( - \gamma  \terminalCostFunc(\state_\TimeInd, \control_\TimeInd) \right)}_{\text{Terminal Cost}}
# \nonumber \\
# \prod_{\timeInd=0}^{\TimeInd-1} \Bigg[
# &\underbrace{ \transitionDistOptimal}_{\text{Optimal Dynamics}} \nonumber \\
# &\underbrace{p(\control_\timeInd \mid \state_\timeInd, \optimalVarTraj, \modeVarTraj) }_{\text{Optimal Policy}} \nonumber \\
# &\underbrace{\modeProb}_{\text{Mode Remaining}}
# \underbrace{\exp \left( - \gamma  \costFunc(\state_\timeInd, \control_\timeInd) \right)}_{\text{Cost}} \Bigg],
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# p(\stateTraj, \controlTraj \mid \state_0, \optimalVar_{0:\TimeInd}=1, \modeVar_{0:\TimeInd}=\desiredMode)
# = \underbrace{\left[ \prod_{\timeInd=0}^{\TimeInd-1} \transitionDist \policy(\control_\timeInd \mid \state_\timeInd) \right]}_{\text{controlled dynamics}}
# \exp \left( \sum_{\timeInd=0}^\TimeInd \costFunc(\state_\timeInd, \control_\timeInd) \right),
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# q(\stateTraj, \controlTraj \mid \state_0)
# = \prod_{\timeInd=0}^{\TimeInd-1} q(\state_{\timeInd+1} \mid \state_{0})
# %= \prod_{\timeInd=0}^{\TimeInd-1} q(\state_{\timeInd+1} \mid \state_{\timeInd}, \control_\timeInd)
# \controlVarDist.
# \end{align}
# #+END_EXPORT
# where $Z=p(\optimalVarTraj, \modeVarTraj \mid \state_0)$.
with the variational distribution $q(\stateTraj, \controlTraj \mid \state_0)$.
As this method is using a learned representation of the transition dynamics,
it suffices to assume that the optimal dynamics are given by the desired mode's learned dynamics.
Calculating this distribution for a set of controls
$q(\controlTraj) = \prod_{\timeInd=0}^{\TimeInd-1} \controlVarDist$,
requires simulating the trajectory in the learned,
single-step dynamics model, i.e. making long-term predictions.

*** Approximate Inference for Dynamics Predictions :ignore:

*Approximate Inference for Dynamics Predictions*
Constructing approximate closed-form solutions based on the model in cref:chap-dynamics
is especially difficult, due to the exponential growth in the number of Gaussian components.
Similar to approach in cref:sec-traj-opt-energy this method obtains multi-step predictions by cascading single-step
predictions through the desired mode's dynamics GP using the moment matching approximation.
However, this approach extends the predictions to handle Normally distributed controls
$\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-state-control-unc-prop}
p(\state_{\timeInd+1} \mid \state_0, \modeVar_{0:\timeInd}=\desiredMode)
&= \int_{\controlDomain} \int_{\stateDomain}
\transitionDistK p(\state_\timeInd \mid \state_{0}, \modeVar_{0:\timeInd-1}=\desiredMode) \text{d}\state_{\timeInd}
\text{d}\control_{\timeInd}
\end{align}
#+END_EXPORT
Given this method for making long term predictions, the variational distribution over state-control trajectories is
given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
q(\stateTraj, \controlTraj \mid \state_0) = \prod_{\timeInd=0}^{\TimeInd-1}
p(\state_{\timeInd+1} \mid \state_0, \modeVar_{0:\timeInd}=\desiredMode)
) \controlVarDist.
\end{align}
#+END_EXPORT

The approach detailed in cref:sec-dynamics-predictions

cref:eq-single-dynamics-gp


The model in Chapter ref:chap-dynamics represents each mode's dynamics function as a sparse variational GP,
giving the transition density,

moment matching leads to closed-form gradients

*** Evidence Lower BOund
Variational inference seeks to optimise $q(\stateTraj, \controlTraj \mid \state_0)$
w.r.t. the variational lower bound, aka evidence lower bound (ELBO).
In this setup, the evidence is that $\optimalVar_{\timeInd}=1$ and $\modeVar_{\timeInd}=\desiredMode$ for
all $\timeInd \in \{0,\ldots,\TimeInd\}$.
Note that the posterior is conditioned on the initial state $\state_0$.
Given this, the ELBO is given by,
# but we drop the dependence from here on out for notational conciseness.
# #+BEGIN_EXPORT latex
# \small
# \begin{align} \label{eq-}
# \text{log} \marginalLikelihood
# &= \text{log} \E_{\trajectoryVarDist} \left[ \frac{\jointDist}{\trajectoryVarDist} \right] \nonumber \\
# &= \text{log} \E_{\trajectoryVarDist} \left[
# \frac{\prod_{\timeInd=0}^{\TimeInd-1} \optimalProb
# \Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd)
# \controlDist}{\prod_{\timeInd=0}^{\TimeInd-1}  \controlVarDist}
# \right] \nonumber \\
# &\geq \E_{\trajectoryVarDist} \left[ \sum_{\timeInd=0}^{\TimeInd} - \temperature \costFunc(\state_\timeInd, \control_\timeInd) \right]
# + \E_{\trajectoryVarDist} \left[
# \sum_{\timeInd=0}^{\TimeInd}
# \log \Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd) \right] \nonumber \\
# &\quad - \sum_{\timeInd=0}^{\TimeInd}
# \text{KL}\left(\controlVarDist \mid\mid \policy(\control_\timeInd \mid \state_\timeInd) \right)  \coloneqq \mathcal{L},
# \end{align}
# \normalsize
# #+END_EXPORT
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-}
\text{log} \marginalLikelihood
&= \text{log} \E_{\trajectoryVarDist} \left[ \frac{\jointDist}{\trajectoryVarDist} \right] \nonumber \\
&= \text{log} \E_{\trajectoryVarDist} \left[
\frac{\prod_{\timeInd=0}^{\TimeInd-1} \optimalProb
%\Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd)
\modeProb
\controlDist}{\prod_{\timeInd=0}^{\TimeInd-1}  \controlVarDist}
\right] \nonumber \\
&\geq \sum_{\timeInd=0}^{\TimeInd-1}
\underbrace{\E_{\controlVarDist q(\state_{\timeInd})}
\left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]}_{\text{expected cost}}
+ \sum_{\timeInd=0}^{\TimeInd-1}
\underbrace{\E_{\controlVarDist q(\state_{\timeInd})}
\left[ \log \modeProb\right]}_{\text{mode remaining term}} \nonumber \\
&\quad - \sum_{\timeInd=0}^{\TimeInd-1}
\text{KL}\left(\controlVarDist \mid\mid \policy(\control_\timeInd \mid \state_\timeInd) \right)  \coloneqq \mathcal{L},
\end{align}
\normalsize
#+END_EXPORT
where for notational conciseness the terminal values have been omitted.
To simplify inference, the variational dynamics
$q(\state_{\timeInd+1} \mid \state_{0})$
and the optimal dynamics ${\transitionDistOptimal}$ are assumed equal.
Assuming a uniform prior policy $\policy(\control_\timeInd \mid \state_\timeInd)$ leads to the $\text{KL}$
term reducing to an entropy term and a constant,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-traj-opt-elbo}
\mathcal{L} = &-\sum_{\timeInd=0}^{\TimeInd}
\underbrace{\E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]}_{\text{expected cost}}
+ \sum_{\timeInd=0}^{\TimeInd} \underbrace{\E_{\controlVarDist q(\state_{\timeInd})} \left[
\log \modeProb \right]}_{\text{mode remaining term}} \nonumber \\
&+ \sum_{\timeInd=0}^{\TimeInd-1} \cancel{\underbrace{\E_{\controlVarDist q(\state_{\timeInd})}
\left[\log\policy(\control_{\timeInd} \mid \state_\timeInd)\right]}_{\text{constant}}}
+ \sum_{\timeInd=0}^{\TimeInd-1} \underbrace{\mathcal{H}(\control_{\timeInd})}_{\text{entropy}}
\end{align}
#+END_EXPORT
The ELBO in cref:eq-traj-opt-elbo resembles the KL control objective in cref:eq-kl-control-uniform
#+BEGIN_EXPORT latex
\begin{align} \label{eq-traj-opt-elbo}
\policy_* = \argmin_{\controlTraj} &-\sum_{\timeInd=0}^{\TimeInd}
\underbrace{\E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]}_{\text{expected cost}}
+ \sum_{\timeInd=0}^{\TimeInd} \underbrace{\E_{\controlVarDist q(\state_{\timeInd})} \left[
\log \modeProb \right]}_{\text{mode remaining term}} \nonumber \\
&+ \sum_{\timeInd=0}^{\TimeInd-1} \cancel{\underbrace{\E_{\controlVarDist q(\state_{\timeInd})}
\left[\log\policy(\control_{\timeInd} \mid \state_\timeInd)\right]}_{\text{constant}}}
+ \sum_{\timeInd=0}^{\TimeInd-1} \underbrace{\mathcal{H}(\control_{\timeInd})}_{\text{entropy}}
\end{align}
#+END_EXPORT
which encodes maximum entropy control.
In practice, maximum entropy control is achieved by parameterising the control at each time step to
be Normally distributed ${\controlVarDist = \mathcal{N}(\control_{\timeInd} \mid \controlMean, \controlCov)}$.
The control dimensions are assumed independent so $\controlCov$ becomes diagonal.
The controls can also be assumed to be deterministic, i.e. to follow a dirac delta distribution
$\controlVarDist = \delta(\control_\timeInd)$,
which omits the maximum entropy behaviour.

In both of these settings, the variational parameters
$\controlMean$ (and $\controlCov$)
are optimised by repeatedly rolling out the control distributions in the
desired mode's GP dynamics model and optimising them using cref:eq-traj-opt-elbo (without the constant term).

#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-traj-opt-inference}
\begin{align}
\min_{\controlTraj} &-\sum_{\timeInd=0}^{\TimeInd}
\underbrace{\E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]}_{\text{expected cost}}
+ \sum_{\timeInd=0}^{\TimeInd} \underbrace{\E_{q(\state_{\timeInd})} \left[
\log \modeProb \right]}_{\text{mode remaining term}}
+ \sum_{\timeInd=0}^{\TimeInd-1} \underbrace{\mathcal{H}(\control_{\timeInd})}_{\text{entropy}} \\
\text{s.t.}& \quad \cref{eq-state-unc-prop}
\end{align}
\end{subequations}
#+END_EXPORT
where the
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-cost-control}
\costFunc(\stateTraj, \controlTraj) &=
\underbrace{(\terminalState - \targetState)^T \terminalStateCostMatrix (\terminalState - \targetState)}_{\text{terminal cost}}
+ \sum_{\timeInd=0}^{\TimeInd-1}
\underbrace{\control_{\timeInd}^T \controlCostMatrix \control_{\timeInd}}_{\text{control cost}}
%\nonumber \\
%&= \| \terminalState - \targetState \|_\terminalStateCostMatrix + \sum_{\timeInd=0}^{\TimeInd-1}
%\| \control_\timeInd \|_\controlCostMatrix
\end{align}
#+END_EXPORT
Importantly, the expected value of this cost under Normally distributed states and controls can be
calculated in closed form with,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-quadratic-cost}
\E_{\stateTraj, \controlTraj} \left[ \costFunc(\stateTraj, \controlTraj) \right] =
\text{tr}(\terminalStateCostMatrix \terminalStateCov) +
(\terminalStateMean - \targetState)^T \terminalStateCostMatrix (\terminalStateMean - \targetState)
+ \sum_{\timeInd=1}^{\TimeInd-1}
\text{tr}(\controlCostMatrix \controlCov )
+ \controlMean^T \controlCostMatrix \controlMean.
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{remark} \label{}
In contrast to the collocation solver in \cref{sec-traj-opt-collocation},
the terminal state boundary condition in \cref{eq-mode-soc-problem-geometry}
is encoded via the cost function, instead of being enforced by the solver.
\end{remark}
#+END_EXPORT

*** Cost Functions :ignore:
*Cost Functions*
This work primarily focuses on quadratic costs, as they are ubiquitous in control and lead
to closed-form expectations under Normally distributed states
${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$
and controls
${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$.
This work seeks to find trajectories between a start state $\state_0$ and a target state $\targetState$, at
time $\TimeInd$.
It is common to minimise the deviation of the final state $\state_\TimeInd$
from the desired target state $\targetState$.
It is also common to find trajectories that minimise the expenditure of control effort.
As such, this work adopts the following cost function,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-cost-control}
\costFunc(\stateTraj, \controlTraj) &=
\underbrace{(\terminalState - \targetState)^T \terminalStateCostMatrix (\terminalState - \targetState)}_{\text{terminal cost}}
+ \sum_{\timeInd=0}^{\TimeInd-1}
\underbrace{\control_{\timeInd}^T \controlCostMatrix \control_{\timeInd}}_{\text{control cost}}
\nonumber \\
&=
 \| \terminalState - \targetState \|_\terminalStateCostMatrix
+ \sum_{\timeInd=0}^{\TimeInd-1}
\| \control_\timeInd \|_\controlCostMatrix
\end{align}
#+END_EXPORT
where $\terminalStateCostMatrix$
and $\controlCostMatrix$ are user defined,
real, symmetric, positive semi-definite and positive definite matrices respectively.
Importantly, the expected value of this cost under Normally distributed states and controls can be
calculated in closed form with,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-quadratic-cost}
\E_{\stateTraj, \controlTraj} \left[ \costFunc(\stateTraj, \controlTraj) \right] =
\text{tr}(\terminalStateCostMatrix \terminalStateCov) +
(\terminalStateMean - \targetState)^T \terminalStateCostMatrix (\terminalStateMean - \targetState)
+ \sum_{\timeInd=1}^{\TimeInd-1}
\text{tr}(\controlCostMatrix \controlCov )
+ \controlMean^T \controlCostMatrix \controlMean.
\end{align}
#+END_EXPORT

*** Approximate Inference for Dynamics Predictions :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\singleInput}{\ensuremath{\state_\timeInd, \control_\timeInd}}
\renewcommand{\singleInput}{\ensuremath{\hat{\state}_\timeInd}}
\renewcommand{\singleOutput}{\ensuremath{\Delta\state_{\timeInd+1}}}

\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput)}}
\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput)}}
%\newcommand{\expertInducingVariational}{\ensuremath{q(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\expertInducingVariational}{\ensuremath{q(\expertInducingOutput)}}
\newcommand{\expertsInducingVariational}{\ensuremath{q(\expertsInducingOutput)}}
\newcommand{\expertsVariational}{\ensuremath{q(\LatentFunc_\numData)}}
%\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \expertInducingOutput)}}
%\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))}}
\renewcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \singleInput, \expertInducingOutput)}}

\renewcommand{\expertVariational}{\ensuremath{q(\mode{\latentFunc}(\singleInput) \mid \singleInput)}}

\newcommand{\singleInputMean}{\ensuremath{\hat{\bm\mu}}}
\newcommand{\singleInputCov}{\ensuremath{\hat{\bm\Sigma}}}
#+END_EXPORT
*Approximate Inference for Dynamics Predictions*
Fortunately, this work is interested in remaining in a single dynamics mode $\desiredMode$,
which simplifies making long-term predictions.
The model in Chapter ref:chap-dynamics represents each mode's dynamics function as a sparse variational GP,
giving the transition density as,
#+BEGIN_EXPORT latex
\begin{align}
\transitionDistOptimal &= \expertVariational \nonumber \\
&=
\mathcal{N} \left( \mode{\latentFunc}(\singleInput) \mid
\mode{\mathbf{A}} \mode{\mathbf{m}},
\expertKernel(\singleInput, \singleInput)
+ (\mode{\mathbf{S}} - \expertKernel(\expertInducingInput,\expertInducingInput))
\mode{\mathbf{A}}^T
\right)
%\nonumber \\
%\mode{\mathbf{A}} &=
%\expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}.
\end{align}
#+END_EXPORT
where $\mode{\mathbf{A}} = \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}$.
To obtain long-term predictions, we cascade one-step predictions.
This requires mapping uncertain state-control inputs through the desired mode's GP dynamics model.
This prediction problem corresponds to recursively calculating the following integral,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-state-unc-prop}
q(\state_{\timeInd+1} \mid \state_0) = \int \int
\expertVariational
q(\control_\timeInd) q(\state_\timeInd)
\text{d}\state_{\timeInd} \text{d}\control_{\timeInd}
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-state-unc-prop}
# q(\state_{\timeInd+1} \mid \state_0) = \int \int
# \transitionDistK
# q(\control_\timeInd) q(\state_\timeInd)
# \text{d}\state_{\timeInd} \text{d}\control_{\timeInd}
# \end{align}
# #+END_EXPORT
with $q(\state_0) = \delta(\state_0)$.
This work considers Normally distributed controls
${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$ and
deterministic controls.
Approximate closed-form solutions exist for propagating Normally distributed states and controls
through GP models
citep:girardGaussian2003,kussGaussian2006,quinonero-candelaPropagation2003.
\todo{correct girard citation}
This work exploits the moment-matching approximation
implemented in the =uncertain_conditional= from GPflow citep:GPflow2017.
For more details on the approximation see Section 7.2.1 of citep:kussGaussian2006.

Given this method for making long term predictions, the distribution over state-control trajectories is
given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
q(\stateTraj, \controlTraj \mid \state_0)
= \prod_{\timeInd=0}^{\TimeInd-1} q(\state_{\timeInd+1} \mid \state_0) \controlVarDist.
\end{align}
#+END_EXPORT
The variational lower bound is then given by sums over each time step,
#+BEGIN_EXPORT latex
%\small
\begin{align} \label{eq-traj-opt-elbo}
\mathcal{L} = &- \sum_{\timeInd=0}^{\TimeInd}
\underbrace{\E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]}_{\text{Expected cost}}
+ \sum_{\timeInd=0}^{\TimeInd} \underbrace{\E_{\controlVarDist q(\state_{\timeInd})} \left[
\log \Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd) \right]}_{\text{Mode remaining term}} \nonumber \\
&+ \sum_{\timeInd=0}^{\TimeInd} \underbrace{\E_{\controlVarDist q(\state_{\timeInd})}
\left[\log\policy(\control_{\timeInd} \mid \state_\timeInd)\right]}_{\text{Constant}}
+ \sum_{\timeInd=0}^{\TimeInd} \underbrace{\mathcal{H}(\control_{\timeInd})}_{\text{Entropy}}.
\end{align}
%\normalsize
#+END_EXPORT

# This work
# Consider the problem of predicting the next state $\state_{\timeInd+1}$
# given multivariate Normally distributed states
# ${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$
# and controls
# ${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$,
# where $\desiredDynamicsFunc \sim \mathcal{GP}$.



# Approximate closed-form solutions exist for propagating uncertain inputs through GP models
# citep:girardGaussian2003,kussGaussian2006,quinonero-candelaPropagation2003.
# \todo{correct girard citation}
# This work exploits the moment-matching approximation
# implemented in the =uncertain_conditional= from GPflow citep:GPflow2017.
# For more details on the approximation see Section 7.2.1 of citep:kussGaussian2006.
# \todo{Add moment matching figure?}
# This work propagates model uncertainty forwards by cascading such one-step predictions.
# This results in a factorised variational posterior,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# q(\stateTraj, \controlTraj \mid \state_0)
# = \prod_{\timeInd=0}^{\TimeInd-1} q(\state_{\timeInd+1} \mid \state_0) \controlVarDist.
# \end{align}
# #+END_EXPORT
# where each state and control are Normally
# distributed

*** Approximate Inference for Dynamics Predictions :noexport:
#+BEGIN_EXPORT latex
\renewcommand{\singleInput}{\ensuremath{\state_\timeInd, \control_\timeInd}}
\renewcommand{\singleInput}{\ensuremath{\hat{\state}_\timeInd}}
\renewcommand{\singleOutput}{\ensuremath{\Delta\state_{\timeInd+1}}}

\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput)}}
\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput)}}
%\newcommand{\expertInducingVariational}{\ensuremath{q(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\expertInducingVariational}{\ensuremath{q(\expertInducingOutput)}}
\newcommand{\expertsInducingVariational}{\ensuremath{q(\expertsInducingOutput)}}
\newcommand{\expertsVariational}{\ensuremath{q(\LatentFunc_\numData)}}
%\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \expertInducingOutput)}}
%\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))}}
\renewcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \singleInput, \expertInducingOutput)}}

\renewcommand{\expertVariational}{\ensuremath{q(\mode{\latentFunc}(\singleInput) \mid \singleInput)}}

\newcommand{\singleInputMean}{\ensuremath{\hat{\bm\mu}}}
\newcommand{\singleInputCov}{\ensuremath{\hat{\bm\Sigma}}}

%\newpage
#+END_EXPORT
The aformentioned trajectory optimisation technique relies on the ability to simulate the controlled
system using the learned dynamics model.
Recursively estimating the state of a nonlinear dynamical system is a common problem.
Exact Bayesian solutions in closed-form can only be obtained for a few special cases.
For example, the Kalman filter for linear Gaussian systems citep:kalmanNew1960 is exact.
In the nonlinear case, approximate methods are required to obtain efficient closed-form solutions.
\todo{cite Extended Kalman filter (EKF), Unscented Kalman filter (UKF) etc?}

Constructing approximate closed-form solutions based on the model in Chapter ref:chap-dynamics
is especially difficult, due to the exponential growth in the number of Gaussian components.
\marginpar{multimodal exponential growth}
Consider approximating the dynamics modes to be independent over a trajectory of length, $N$,
and recursively propagating each component through both modes.
This approximation would lead to the distribution over the final state consisting of
$\ModeInd^N$ Gaussian components.

# After the initial time step the state distribution will be a mixture of $\ModeInd$ modes.
# Assuming that the $\ModeInd$ components are independent and that each of the dynamics GPs are independent,
# propagating the components would result in a
# mode would result in the next state distribution containing $\ModInd^2$ modes.

# This is due to the explosion in the number of Gaussian components that would need to be modelled.

Luckily, this work is interested in remaining in a single dynamics mode $\desiredMode$,
which simplifies the problem.
Assuming that the controlled system remains in this desired dynamics mode,
the state trajectory can be simulated using only a single dynamics function,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\state_{\timeInd+1} &= \mode{\latentFunc}(\state_\timeInd, \control_\timeInd) + \state_\timeInd +  \mode{\epsilon}.
\end{align}
#+END_EXPORT
The model in Chapter ref:chap-dynamics represents each mode's dynamics function as a sparse variational GP,
giving the transition density as,
#+BEGIN_EXPORT latex
\begin{subequations}
\label{eq-sparse-gp-dynamics}
\begin{align}
p(\Delta\state_{\timeInd+1} \mid \singleInput, \modeVarK) &=
\E_{\expertVariational} \left[\singleExpertLikelihood \right] \label{eq-sparse-gp-dynamics-0} \\
\expertVariational &=  \E_{ \expertInducingVariational} \left[  \singleLatentExpertGivenInducing \right] \label{eq-sparse-gp-dynamics-1} \\
\expertInducingVariational &= \mathcal{N}\left( \expertInducingOutput \mid \mode{\mathbf{m}}, \mode{\mathbf{S}} \right)
\label{eq-sparse-gp-dynamics-2}
\end{align}
\end{subequations}
#+END_EXPORT
where the functional form of \expertVariational is given by,
#+BEGIN_EXPORT latex
\begin{align}
\expertVariational &=
\mathcal{N} \left( \mode{\latentFunc}(\singleInput) \mid
\mode{\mathbf{A}} \mode{\mathbf{m}},
\expertKernel(\singleInput, \singleInput)
+ (\mode{\mathbf{S}} - \expertKernel(\expertInducingInput,\expertInducingInput))
\mode{\mathbf{A}}^T
\right) \\
\mode{\mathbf{A}} &=
\expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}.
\end{align}
#+END_EXPORT
Remember that the variational posterior $\expertVariational$ is an approximation,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\expertVariational &\approx p(\mode{\latentFunc}(\singleInput) \mid \singleInput, \allInput, \allOutput),
\end{align}
#+END_EXPORT
that captures the joint distribution in the data through the inducing variables $\expertInducingOutput$.
Given a deterministic state-control input, $\singleInput$, cref:eq-sparse-gp-dynamics can be used
to calculate the density over the next state $\state_{\timeInd+1}$.
However, as both the state and control at a given time step could also be Gaussian distributed,
${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$,
and
${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$,
the joint distribution of the state and control, $p(\singleInput)$, is also Gaussian distributed,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\singleInput) = p(\state_{\timeInd}, \control_\timeInd) = p(\state_{\timeInd}) p(\control_\timeInd).
\end{align}
#+END_EXPORT

**** Predictions with Uncertain Inputs
Consider the problem of predicting the next state  $\state_{\timeInd+1}$
given multivariate Normally distributed states,
${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$,
and controls,
${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$,
where $\desiredDynamicsFunc \sim \mathcal{GP}$.
This prediction problem corresponds to calculating the following integral,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-state-unc-prop}
p(\state_{\timeInd+1}) = \int \int
p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd) p(\control_\timeInd) p(\state_\timeInd)
\text{d}\state_{\timeInd} \text{d}\control_{\timeInd}
\end{align}
#+END_EXPORT
Approximate closed-form solutions exist for propagating uncertain inputs through GP models
cite:girardGaussian2003,kussGaussian2006,quinonero-candelaPropagation2003.
\todo{correct girard citation}
This work exploits the moment-matching approximation
implemented in the =uncertain_conditional= from GPflow cite:GPflow2017.
For more details on the approximation see Section 7.2.1 of cite:kussGaussian2006.
\todo{Add moment matching figure?}
This work propagates model uncertainty forwards by cascading such one-step predictions.

# The transition density $\transitionDist$ can be obtained
# by recursive moment-matching
# This is the case when using a GP model to simulate more than a single time step.

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# q(\desiredDynamicsFunc(\singleInput) ) = \int  \underbrace{q(\desiredDynamicsFunc(\singleInput) \mid \singleInput)}_{\text{GP predictive dist}}
# p(\singleInput) \text{d}\singleInput.
# \end{align}
# #+END_EXPORT



# Importantly, given a trajectory where each state and control are normally distributed,
# ${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$
# and
# ${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$,
# the expected cost for a trajectory can be calculated in closed-form,

** Simulated Quadcopter Experiments label:sec-traj-opt-inference-results
#+BEGIN_EXPORT latex
\newcommand{\deltaTime}{\ensuremath{\Delta \timeInd}}
\newcommand{\env}[1]{\ensuremath{\hat{#1}}}
\newcommand{\modeProbTraj}{\ensuremath{\Pr(\allModeVarK \mid \stateTraj)}}
#+END_EXPORT
The control as inference algorithm presented in this section is evaluated in a simulated environment.
The simulation environment represents a velocity controlled quadcopter in an environment subject to spatially
varying wind, constituting both drift and diffusion components.

\todo{add details of sim here?}

*** Model Learning
*Data Collection* To begin with, $\NumData$ state transitions were sampled from the simulator.
In contrast to the algorithm presented in cref:sec-traj-opt-geometric, this method learns the full transition
dynamics model, i.e. it does not assume constant controls.
A subset of the environment was intentionally not observed.
This introduced a region of high epistemic uncertainty into the model, i.e. a region of the system where
the dynamics cannot be predict confidently.
This enabled the performance of the trajectory optimiser at achieving Goal 2 to be evaluated.

# *Dynamics Configuration* The model from cref:chap-dynamics was instantiated with $\ModeInd=2$ experts, one to
# represent the operable dynamics mode and one to represent the mode subject to high levels
# of wind and associated disturbances.
# Each mode was instantiated with a simple nominal dynamics function,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-nominal-dynamics-vc}
# \nominalDynamics(\state_\timeInd, \control_\timeInd) &= \control_\timeInd \deltaTime
# \end{align}
# #+END_EXPORT
# to encode our prior knowledge of the system -- distance = speed $\times$ time.
# The model is then tasked with learning the unknown errors associated with each dynamics mode.
# Namely, the drift and process noise induced by the wind.
*Dynamics Configuration* The model from cref:chap-dynamics was instantiated with $\ModeInd=2$ experts, one to
represent the operable dynamics mode and one to represent the turbulent dynamics mode subject to high levels
of wind and disturbances.
Each mode was instantiated with a Squared Exponential kernel with automatic relevance determination and
a zero mean function.
It was /known a priori/ that the dynamics modes vary spatially over the environment,
so the gating network was instantiated to depend only on the state
$\gatingFunc : \stateDomain \rightarrow \R$,
instead of both the state and control
$\gatingFunc : \stateDomain \times \controlDomain \rightarrow \R$.
Note that the relevant input dimensions can easily be set in =GPflow=
by setting the =active_dims= parameter associated with the kernel to only use specific dimensions of the input.
In this case, only the state dimensions, instead of the entire state-control input.

All experiments used a $\TimeInd=20$ step horizon and the controls were optimised using
SciPy's Sequential Least Squares Programming (SLSQP) cite:2020SciPy-NMeth.


*** Performance Metrics
In order to evaluate and compare different trajectories, a number of performance metrics were used.
Given that trajectories were optimised subject to the desired mode's GP dynamics, it makes sense to evaluate
trajectories subject to the GP's latent variables.
A trajectories performance at achieving Goal 1 can be quantified by the
probability of every time step remaining in the desired dynamics mode.
This is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mode-prob-trajectory}
\modeProbTraj &= \sum_{\timeInd=0}^{\TimeInd} \modeProb \\
\modeProbTraj &= \prod_{\timeInd=0}^{\TimeInd} \modeProb,
\end{align}
#+END_EXPORT
with $\allModeVarK = \{\modeVarK \mid \timeInd \in [0, \TimeInd] \cap \mathbb{Z} \}$.
Higher values indicate better performance as they correspond to trajectories that remain in the desired dynamics mode.
The performance of achieving Goal 2 can be quantified by summing the variance associated with the desired
mode's gating function over a trajectory,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-var-trajectory}
\sigma^2_{\stateTraj} = \sum_{\timeInd=0}^{\TimeInd} \V\left[\desiredGatingFunction(\state_\timeInd) \right].
\end{align}
#+END_EXPORT
Lower values indicate better performance as they correspond to trajectories that remain in regions of the
GP dynamics with low epistemic uncertainty.
As this work exploits the latent geometry of the gating network, it is also interesting to compare a trajectories
length and energy on the manifold endowed with the metric $\metricTensor$.
These are calculated as follows,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-length-trajectory}
\text{Length}(\desiredGatingFunction(\stateTraj))
&= \sum_{\timeInd=0}^{\TimeInd} \| \Delta \state_\timeInd \|_{\metricTensor(\state_\timeInd)} \\
\label{eq-energy-trajectory}
\text{Energy}(\desiredGatingFunction(\stateTraj)) &=
\sum_{\timeInd=0}^{\TimeInd} \Delta \state_\timeInd^T \metricTensor(\state_\timeInd) \Delta \state_\timeInd.
\end{align}
#+END_EXPORT
Based on the intuition provided in this chapter, trajectories with shorter length and lower energy
should correspond to better performance.

Trajectories optimised in the GP dynamics were also evaluated by
rolling them out in the (simulated) environment and comparing them.
Given a trajectory rolled out in the environment $$, with the state at time
$\timeInd$ denoted $\env{\state}_\timeInd$,
the Euclidean distance between states at the same time instance from the GP dynamics trajectory $\state_\timeInd$,
is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-state-trajectory-norm}
D(\stateTraj, \env{\stateTraj})
&= \sum_{\timeInd=0}^{\TimeInd} d(\state_\timeInd, \env{\state}_\timeInd) \\
&= \sum_{\timeInd=0}^{\TimeInd} \| \state_\timeInd - \env{\state}_\timeInd \|_2.
\end{align}
#+END_EXPORT
Intuitively, trajectories that leave the desired dynamics mode have larger values -- as they
are subject to different dynamics -- leading to different state trajectories $\stateTraj$.
# Intuitively, environment trajectories deviating from model trajectories indicated that they have
# left the desired dynamics mode.

*** Simulated Quadcopter Experiments
These figures are slices with controls equal zero

**** Baseline Prob :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/baseline-traj-opt/trajectories_over_desired_prob.pdf}
\subcaption{Trajectories overlayed on the gating network's mixing probabilities.}
\label{eq-traj-opt-over-prob-baseline}
\end{minipage}
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/baseline-traj-opt/trajectories_over_desired_gating_gp.pdf}
\subcaption{Trajectories overlayed on desired mode's gating function GP.}
\label{eq-traj-opt-over-svgp-baseline}
\end{minipage}
\caption{\label{fig-traj-opt-baseline}
This figure demonstrates the shortcomings of finding trajectories
from a start state $\state_0$, to a target state $\targetState$,
in the system in \cref{fig-point-mass-motivation},
without avoiding the turbulent dynamics mode.
To demonstrate the methods shortcomings, the optimised controls were rolled out in the desired mode's GP dynamics (magenta)
and in the environment (cyan).
(\subref{eq-traj-opt-over-prob-baseline}) shows the trajectories overlayed on the desired mode's mixing probability
and (\subref{eq-traj-opt-over-svgp-baseline}) shows them overlayed on the desired mode's gating function posterior
mean (left) and posterior variance (right).
It is clear that the trajectory resulting from rolling out the controls in the environment,
passes through the turbulent dynamics mode and as a result, deviates from the
trajectory rolled out in the desired mode's dynamics.}
\end{figure}
#+END_EXPORT
**** Discussion Baseline Prob :ignore:noexport:
The contour plots in cref:fig-traj-opt-baseline represent the gating network associated with the model from
cref:chap-dynamics after performing Bayesian inference, given a historical data set of state transitions
$\dataset$ from the environment in cref:fig-point-mass-motivation.
Control trajectories were found by minimising the expected cost under the state distribution
resulting from cascading single-step predictions through the desired mode's ($\modeInd=2$ in this case)
learned GP dynamics model.
The cost function consisted of a quadratic integral control cost and a terminal state cost.
This approach only considered the dynamics of the desired mode, leading to trajectories in the environment deviating
from those planned in the learned dynamics model.
The trajectories also pass through regions of the dynamics with high epistemic uncertainty, which is not desirable.
\todo{finish this para when have trajectory deviating at high epistemic uncertainty}

**** Discussion Baseline Prob :ignore:noexport:
cref:fig-traj-opt-baseline shows results for a baseline trajectory optimisation method, where the optimised trajectory
rolled out in the desired mode's GP dynamics (magenta), passes through the undesired dynamics mode
(mode one $\modeVar = 1$ in this case).
This trajectory rolled out in the simulated environment (cyan),
clearly deviates from the models predicted trajectory, when it passes through the undesired dynamics mode.
This trajectory was obtained from a standard form of control in GP dynamics models, with
no maximum entropy control or mode remaining behaviour.
It used deterministic controls and the cost function from cref:eq-quadratic-cost-control.
As such, the controls were optimised w.r.t. the expected cost in cref:eq-expected-quadratic-cost.

**** Mode Conditioning max entropy :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/mode-conditioning-max-entropy/trajectories_over_desired_prob.pdf}
\subcaption{}
\label{fig-mode-conditioning-max-entropy-traj-opt-over-prob}
\end{minipage}
\begin{minipage}[r]{0.99\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/mode-conditioning-max-entropy/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-mode-conditioning-max-entropy-traj-opt-over-svgp}
\end{minipage}
\caption{\textbf{Mode Conditioning ELBO with Maximum Entropy Control}
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the mode conditioning ELBO from \cref{eq-traj-opt-elbo} and Gaussian controls (maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-mode-conditioning-max-entropy-traj-opt-over-prob})
each mode's mixing probability
and (\subref{fig-mode-conditioning-max-entropy-traj-opt-over-svgp})
the posterior mean (left) and posterior variance (right) associated with desired mode's gating function.}
\label{fig-mode-conditioning-max-entropy-traj-opt}
\end{figure}
#+END_EXPORT

**** Mode Conditioning NO max entropy :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/mode-conditioning-no-max-entropy/trajectories_over_desired_prob.pdf}
\subcaption{}
\label{fig-mode-conditioning-no-max-entropy-traj-opt-over-prob}
\end{minipage}
\begin{minipage}[r]{0.99\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/mode-conditioning-no-max-entropy/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-mode-conditioning-no-max-entropy-traj-opt-over-svgp}
\end{minipage}
\caption{\textbf{Mode Conditioning ELBO without Maximum Entropy Control}
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the mode conditioning ELBO from \cref{eq-traj-opt-elbo} and deterministic controls (no maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-mode-conditioning-no-max-entropy-traj-opt-over-prob})
each mode's mixing probability
and (\subref{fig-mode-conditioning-no-max-entropy-traj-opt-over-svgp})
the posterior mean (left) and posterior variance (right) associated with desired mode's gating function.}
\label{fig-mode-conditioning-no-max-entropy-traj-opt}
\end{figure}
#+END_EXPORT

**** Mode Conditioning ELBO :ignore:
cref:fig-mode-conditioning-max-entropy-traj-opt
shows the results after performing trajectory optimisation with the mode conditioning ELBO in cref:eq-traj-opt-elbo.
The optimised trajectory avoids entering the turbulent dynamics mode successfully.
The control at each time step was initialised as a diagonal Gaussian, which lead to the
ELBO encoding maximum entropy control.
In contrast, cref:fig-mode-conditioning-no-max-entropy-traj-opt shows results when using deterministic
controls, i.e. removing the maximum entropy control.
Although subtle, the maximum entropy term appears to find trajectories with more clearance from
the turbulent dynamics mode.
This is most easily seen by observing that the line connecting the 7^{th} and 8^{th} time steps
in cref:fig-mode-conditioning-no-max-entropy-traj-opt, appears to pass through the turbulent dynamics mode,
whereas in cref:fig-mode-conditioning-no-max-entropy-traj-opt there is more clearance.

Before evaluating the performance of the mode conditioning ELBO with regards to Goal 2,
it is worth considering the influence of the expected cost term on the optimisation.
The expected cost increases when the state-control trajectory distribution has high variance.
This variance consists of the control variance (at each time step), which is being optimised, as well as
the state variance,
which is calculated by cascading single-step predictions through the desired mode's dynamics GP.
As such, the expected cost term in cref:eq-traj-opt-elbo will favour trajectories
that remain in regions of the desired mode's dynamics GP with low variance.
In systems with only a single dynamics mode, this may suffice for avoiding regions of the dynamics with
high epistemic uncertainty.
However, in multimodal systems, if one of the mode's dynamics are fairly simple, then its GP
may confidently interpolate/extrapolate into regions that belong to another mode.
In this case, the epistemic uncertainty is not represented in the dynamics GP, as it
arises from the mode assignment.
As such, this epistemic uncertainty should be represented in the gating network.
In the \acrshort{mosvgpe} model, this form of epistemic uncertainty is modelled by the gating function's posterior variance.

The trajectories found with the mode conditioning ELBO  in
cref:fig-mode-conditioning-max-entropy-traj-opt,fig-mode-conditioning-no-max-entropy-traj-opt
do not avoid the region of high posterior variance associated with the desired mode's gating function.
This is because the only term encoding this behaviour is the mode remaining term from
cref:eq-traj-opt-elbo, which is the mode probability obtained after marginalising the gating function.
Although, marginalisation is considered a principled way of handing uncertainty, in this scenario,
it can be conjectured otherwise.
This is because in the region with no observations, it is perfectly plausible that there
is another region belonging to the turbulent dynamics mode, or a different dynamics mode altogether.
In a risk-averse setting, it is desirable not to introduce this uncertainty into the trajectory,
i.e. avoid this region completely.
Therefore, it is beneficial to decouple this uncertainty avoiding behaviour from the mode remaining behaviour.

**** Mode Conditioning ELBO with Riemannian Energy

In some cases, the geometric approach may fail

**** Conditioning energy no max entropy :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/conditioning-energy-no-max-entropy/trajectories_over_desired_prob.pdf}
\subcaption{}
\label{fig-conditioning-energy-no-max-entropy-traj-opt-over-prob}
\end{minipage}
\begin{minipage}[r]{0.99\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/conditioning-energy-no-max-entropy/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-conditioning-energy-no-max-entropy-traj-opt-over-svgp}
\end{minipage}
\caption{\textbf{Mode Conditioning and Riemannian Energy} $\bm\lambda\mathbf{=1.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the mode conditioning ELBO from \cref{eq-traj-opt-elbo},
deterministic controls (no maximum entropy control) and
the Riemannian energy cost function from \cref{eq-approximate-trajectory-riemannian-energy}
with $\labmda=1.0$.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-conditioning-energy-no-max-entropy-traj-opt-over-prob})
each mode's mixing probability
and (\subref{fig-conditioning-energy-no-max-entropy-traj-opt-over-svgp})
the posterior mean (left) and posterior variance (right) associated with desired mode's gating function.}
\label{fig-conditioning-energy-no-max-entropy-traj-opt}
\end{figure}
#+END_EXPORT

**** Conditioning energy WITH max entropy :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/conditioning-energy/trajectories_over_desired_prob.pdf}
\subcaption{}
\label{eq-geometric-traj-opt-over-prob}
\end{minipage}
\begin{minipage}[r]{0.49\columnwidth}
    \begin{minipage}[r]{0.7\columnwidth}
        \centering
        %\includegraphics[width=\textwidth]{./images/geometric-traj-opt-prob-vs-time.pdf}
        \includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/conditioning-energy/mode_probs_vs_time.pdf}
        %\subcaption{Mode 1's mixing probability over the trajectories.}
        \subcaption{}
        \label{geometric-traj-opt-prob-vs-time}
    \end{minipage}
    \begin{minipage}[r]{0.7\columnwidth}
        %\includegraphics[width=\textwidth]{./images/geometric-traj-opt-epistemic-vs-time.pdf}
        \includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/conditioning-energy/variance_vs_time.pdf}
        \subcaption{}
        \label{geometric-traj-opt-epistemic-vs-time}
        %\subcaption{Posterior variance associated with mode 1's gating function over the trajectories.}
    \end{minipage}
\end{minipage}
\begin{minipage}[r]{0.99\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/conditioning-energy/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{eq-geometric-traj-opt-over-svgp}
\end{minipage}
\caption{\label{fig-metric-vs-time}Comparision of the intial and optimised trajectories' performance -- for two settings of $\lambda$ -- at 1) staying in the desired mode and 2) avoiding regions of high epistemic uncertainty.
(\subref{eq-geometric-traj-opt-over-prob}) shows the trajectories overlayed on the desired mode's mixing probability
and (\subref{eq-geometric-traj-opt-over-svgp}) shows them overlayed on the desired mode's gating function posterior
mean (left) and posterior variance (right).
(\subref{geometric-traj-opt-prob-vs-time}) and (\subref{geometric-traj-opt-epistemic-vs-time}) show the
probability of remaining in the desired mode and the desired mode's posterior variance
over the trajectories respectively.}
\end{figure}
#+END_EXPORT

** old :noexport:
#+BEGIN_EXPORT latex
\begin{align} \label{eq-optimal-control-message-passing}
p(\controlTraj \mid \state_{0}, \optimalVar_{0:\TimeInd}=1, \modelVar_{0:\TimeInd}=\desiredMode) &=
\frac{p(\optimalVar_{1:\TimeInd}=1, \control_{1:\TimeInd} \mid \state_\timeInd)}{
p(\optimalVar_{1:\TimeInd}=1 \mid \state_\timeInd)
} \\
&=
\frac{p(\optimalVar_{0}=1 \mid \state_0, \control_0)
\policy(\control_0 \mid \state_0)
\prod_{i=\timeInd+1}^{\TimeInd} \int p(\optimalVar_{i}=1 \mid \state_i, \control_i)
p(\state_i \mid \state_{i-1}, \control_{i-1})
\policy(\control_{i} \mid \state_i)
\text{d}\state_{i}}{
p(\optimalVar_{\TimeInd}=1 \mid \state_\TimeInd, \control_\TimeInd)
\prod_\timeInd^{\TimeInd-1}
\int p(\optimalVar_{\timeInd}=1 \mid \state_\timeInd, \control_\timeInd)
\transitionDist
\policy(\control_\timeInd \mid \state_\timeInd)
\text{d}\state_{\timeInd+1} \text{d}\control_{\timeInd+1}}. \\
&=
\frac{ \int \prod_{\timeInd=1}^{\TimeInd} p(\optimalVar_{\timeInd}=1 \mid \state_\timeInd, \control_\timeInd)
\transitionDist
\policy(\control_{\timeInd} \mid \state_\timeInd)
\text{d}\state_{\timeInd:\TimeInd}}{
\int \prod_{\timeInd=1}^{\TimeInd}
p(\optimalVar_{\timeInd}=1 \mid \state_\timeInd, \control_\timeInd)
\transitionDist
\policy(\control_\timeInd\mid \state_\timeInd)
\text{d}\state_{\timeInd+1:\TimeInd} \text{d}\control_{\timeInd:\TimeInd}}.
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-optimal-control-message-passing}
p(\control_{\timeInd} \mid \state_{\timeInd}, \optimalVar_{\timeInd:\TimeInd}=1) =
\frac{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)
\controlDist p(\state_\timeInd)}{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd)
p(\state_\timeInd)}.
\end{align}
#+END_EXPORT
This distribution represents the optimal policy and can be obtained using a standard sum-product inference
algorithm. See cite:levineReinforcement2018 for more details.
Intuitively, $p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)$
denotes the probability that a trajectory is optimal from $\timeInd$ to $\TimeInd$ given it begins in state
$\state_\timeInd$ with control $\control_\timeInd$.
Similarly $p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd)$
denotes the probability that the trajectory is optimal from $\timeInd$ to $\TimeInd$ given it begins in state
$\state_\timeInd$.
The state only message, $p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd)$, can be recovered
from the state-control message,
$p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)$,
by simply marginalising the control,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd)
&=
\int p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd)
p(\control_{\timeInd} \mid \state_{\timeInd})
\text{d}\control_{\timeInd}.
\end{align}
#+END_EXPORT
Computing the state-control distribution,
$p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)$,
requires a recursive backwards message passing algorithm
starting from the last time step $\timeInd=\TimeInd$ and passing information backwards in time to $\timeInd=1$.
The terminal term $p(\optimalVar_\TimeInd=1 \mid \state_\TimeInd, \control_\TimeInd)$ can be calculated
from the "cost" likelihood (cref:eq-monotonicOptimalityLikelihood) and the proceeding time steps
can be obtained via recursive message passing,
#+BEGIN_EXPORT latex
%\small
\begin{align*} \label{eq-}
p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd)
&=
\prod_{\timeInd}^{\TimeInd-1}
\int \int
\optimalProb p(\state_{\timeInd+1} \mid \state_{\timeInd}, \control_{\timeInd}) p(\control_{\timeInd+1} \mid \state_{\timeInd+1})
\text{d}\state_{\timeInd+1} \text{d}\control_{\timeInd+1}
\end{align*}
%\normalsize
#+END_EXPORT
Given these recursive messages the optimal policy
$p(\control_{\timeInd} \mid \state_{\timeInd}, \optimalVar_{\timeInd:\TimeInd}=1)$
can be calculated using cref:eq-optimal-control-message-passing.

This inference mirrors trajectory optimisation as it also has a forward pass that simulates the
current controller and a backward pass that updates the controller to improve the trajectory.
However, what does this inference procedure actually optimise?

*Maximum Entropy Regularisation* Formulating trajectory optimisation in this way encodes the
maximum causal entropy principle, which is often used to achieve robustness,
in particular for inverse optimal control cite:ziebartModeling2010.
It is important to note that the maximum entropy framework is problematic for stochastic dynamics.
In essence, it assumes that the agent is able to control the dynamics as well as the controls in order
to obtain optimal trajectories.

This inference procedure is obtained by marginalising the full trajectory distribution
and conditioning the policy on $\state_\timeInd$ at each time step.
The full trajectory distribution is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\stateControlTraj) = p(\state_1 \mid \optimalVar_{1:\TimeInd}) \prod_{\timeInd=1}^\TimeInd
p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \optimalVar_{1:\TimeInd})
p(\control_\timeInd \mid \state_\timeInd, \optimalVar_{1:\TimeInd})
\end{align}
#+END_EXPORT

In the case of stochastic dynamics the optimised trajectory distribution is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\stateControlTraj) = p(\state_1 \mid \optimalVar_{1:\TimeInd}) \prod_{\timeInd=1}^\TimeInd
p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \optimalVar_{1:\TimeInd})
p(\control_\timeInd \mid \state_\timeInd, \optimalVar_{1:\TimeInd})
\end{align}
#+END_EXPORT


#+BEGIN_EXPORT latex
\todo[inline]{Considering process noise in the dynamics separately from the epistemic (parametric) uncertainty,
\begin{align} \label{eq-}
p(\optimalVar_{1:\TimeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})
= \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \optimalDist
p(\state_{\timeInd+1} \mid \dynamicsFunc(\state_\timeInd)) p(\dynamicsFunc(\state_\timeInd) \mid \state_\timeInd, \control_\timeInd)
\controlDist
\end{align}
\begin{align} \label{eq-}
\optimalProb &= \monotonicFunc( -\costFunc(\state_\timeInd, \dynamicsFunc(\state_{\timeInd-1}), \control_\timeInd))
\end{align}
}
#+END_EXPORT

** Joint Probability Model :ignore:noexport:
\newline
The joint probability for an optimal trajectory, i.e.
for $\optimalVar_\timeInd=1$ and $\modeVarK$ for all $\timeInd \in \{1,\ldots,\TimeInd\}$, is given by,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-}
\jointDist =
& \startStateDist \underset{\text{Terminal Cost}}{\terminalCostDist} \\
&\prod_{\timeInd=1}^{\TimeInd-1}
\underset{\text{Cost}}{\optimalProb}
\underset{\text{Mode Constraint}}{\modeProb}
\underset{\text{Dynamics}}{\transitionDist}
\underset{\text{Controller}}{\controlDist}
\end{align}
\normalsize
#+END_EXPORT
where $\controlDist$ denotes the policy.
In the case of RL the policy is parameterised with parameters, $\theta$, but in control we are usually interested in
optimising the states and controls directly.
The object of interest in this model is the distribution over the control, $\control_\timeInd$,
conditioned on the state, $\state_\timeInd$, and all future time steps being optimal,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\control_{\timeInd} \mid \state_{\timeInd}, \optimalVar_{\timeInd:\TimeInd}=1, \modeVar_{\timeInd:\TimeInd}=\modeInd) =
\frac{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)
p(\modeVar_{\timeInd:\TimeInd}=\modeInd \mid \state_\timeInd, \control_\timeInd)
\controlDist p(\state_\timeInd)}{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd)
p(\modeVar_{\timeInd:\TimeInd}=\modeInd \mid \state_\timeInd)
p(\state_\timeInd)}.
\end{align}
#+END_EXPORT
The distributions $p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd)$,
$p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd)$,
$p(\modeVar_{\timeInd:\TimeInd}=\modeInd \mid \state_\timeInd, \control_\timeInd)$ and
$p(\modeVar_{\timeInd:\TimeInd}=\modeInd \mid \state_\timeInd)$
can be obtained via recursive message passing,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd) =
\prod_{\timeInd+1}^{\TimeInd-1}
\int \int \optimalProb p(\state_\timeInd \mid \state_{\timeInd-1}, \control_{\timeInd-1}) \controlDist \text{d}\state_{\timeInd} \text{d}\control_{\timeInd}.
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\todo[inline]{Considering process noise in the dynamics separately from the epistemic (parametric) uncertainty,
\begin{align} \label{eq-}
p(\optimalVar_{1:\TimeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})
= \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \optimalDist
p(\state_{\timeInd+1} \mid \dynamicsFunc(\state_\timeInd)) p(\dynamicsFunc(\state_\timeInd) \mid \state_\timeInd, \control_\timeInd)
\controlDist
\end{align}
\begin{align} \label{eq-}
\optimalProb &= \monotonicFunc( -\costFunc(\state_\timeInd, \dynamicsFunc(\state_{\timeInd-1}), \control_\timeInd)) \\
\end{align}
}
#+END_EXPORT

** graphical model :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[h!]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[latent, above=of u1] (o1) {$\optimalVar_1$};
      \node[latent, right=of o1] (o2) {$\optimalVar_2$};
      \node[latent, right=of o2] (o3) {$\optimalVar_3$};

      \node[latent, below=of x1] (a1) {$\modeVar_1$};
      \node[latent, right=of a1] (a2) {$\modeVar_2$};
      \node[latent, right=of a2] (a3) {$\modeVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);

      \draw[->] (x1)--(a1);
      \draw[->] (u1) to [out=-90,in=30] (a1);
      \draw[->] (x2)--(a2);
      \draw[->] (u2) to [out=-90,in=30] (a2);
      \draw[->] (x3)--(a3);
      \draw[->] (u3) to [out=-90,in=30] (a3);
    \end{tikzpicture}
    %}
\caption{Graphical model with optimality and mode indicator variables.}
\label{fig-constrained-control-graphical-model}
\end{figure}
#+END_EXPORT

** graphical model :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[obs, above=of u1] (o1) {$\optimalVar_1$};
      \node[obs, right=of o1] (o2) {$\optimalVar_2$};
      \node[obs, right=of o2] (o3) {$\optimalVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);
    \end{tikzpicture}
    %}
  \caption{Graphical model of control formulated as inference.}
\label{fig-control-graphical-model}
\end{figure}
#+END_EXPORT

** Discussion

control constraints to ensure controls stay in region that have been observed

Although the initial trajectory may transition between multiple dynamics modes, the lower bound favours
trajectories that remain in the desired dynamics mode.

# Whilst real-time control requires efficient inference algorithms,
# "offline" trajectory optimisation can trade in computational cost for greater accuracy.
# This work is primarily interested in finding trajectories that attempt to remain in a
# desired dynamics mode.
# For the sake of simplicity, it considers the "offline" trajectory optimisation setting.
# The increased computational time may hinder
# its suitability to obtain a closed-loop controller via MPC citep:eduardof.Model2007 .
# However, it can be used "offline" to generate reference trajectories for a tracking controller,
# or for guided policy search in a model-based RL setting citep:levineGuided2013.
# Alternatively, future work could investigate approximate inference methods for efficient state estimation
# to aid with real-time control,
# e.g. based on linear/quadratic approximations of the dynamics (iLQG/DDP).

** Comparison and Discussion

- Algorithm 2 can remain in a set of modes whereas Algorithm 1 can only remain in a single mode. \todo{could Algorithm 1 be modified to remain in a set of modes? Transforming gating functions?}
- Algorithm 2 more naturally handles chance constraints as it calculates
  the mode probabilities.
- Trajectories from Algorithm 2 will satisfy the dynamics whereas trajectories from Algorithm 1 may not.
  This is because the state and control trajectories obtained from
  Algorithm 1 are solutions to the geodesic \acrshort{ode} and not the dynamics.
- How to add constraints to Algorithm 1 e.g. for smoothness?
- Algorithm 2 doesn't decouple mean and variance and allow a $\lambda$ parameter.
- Algorithm 2 considers uncertainty in dynamics. Algorithm 1 approximates geodesic SDE to be deterministic.

The control methods required control regularisation to prevent the controls leading to state transitions that
"jump" over the undesired mode.
That is, trajectories whose discrete time steps appear to satisfy the constraints/cost but the actual continuos time
trajectory passes through the undesired mode.

Methods for learning continuous time controls that can be used to interpolate cost over trajectory
to get higher resolution.
For example a Gaussian process over controls instead of Gaussian.
Or stochastic collocation.


- Mixture models without a GP-based gating network
- Not dependent on differentiable mean and covariance functions for the gating function GP(s).
- Maximum entropy control
- Remain in a set of modes

** Conclusion
This chapter has presented a method for performing trajectory optimisation in
multimodal dynamical systems with the transition dynamics modelled as a
Mixtures of Gaussian Process Experts method.

Given a start and end state, the trajectory optimisation can be tuned to find trajectories that either
prioritise remaining in a desired dynamics mode or prioritise avoiding regions of the learned transition
dynamics model with high epistemic uncertainty.

* Mode Constrained Exploration for Model-Based Reinforcement Learning label:chap-active-learning
#+begin_export latex
\epigraph{Real knowledge is to know the extent of ones ignorance.}{\textit{Confucius (Philosopher, 551479BC).}}
#+end_export
** Intro :ignore:
#+BEGIN_EXPORT latex
\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{\MakeUppercase{\timeInd}}}
\newcommand{\stateTraj}{\ensuremath{\bar{\state}}}
\newcommand{\controlTraj}{\ensuremath{\bar{\control}}}
\newcommand{\nominalStateTraj}{\ensuremath{\stateTraj_*}}
\newcommand{\nominalControlTraj}{\ensuremath{\controlTraj_*}}

\newcommand{\utraj}{\ensuremath{\bar{\control}}}
\newcommand{\policies}{\ensuremath{\Pi}}
\newcommand{\policy}{\ensuremath{\pi_{\theta}}}


\renewcommand{\dataset}{\ensuremath{\mathcal{D}}}
\renewcommand{\params}{\ensuremath{\bm\theta}}
\renewcommand{\input}{\ensuremath{\hat{\state}}}
\renewcommand{\output}{\ensuremath{\modeVar}}
\renewcommand{\inputDomain}{\ensuremath{\controlDomain}}
\renewcommand{\outputDomain}{\ensuremath{\mathcal{A}}}

\newcommand{\outputGivenInputParams}{\ensuremath{p(\output \mid \input, \params)}}
\newcommand{\outputGivenInputData}{\ensuremath{p(\output \mid \input, \mathcal{D})}}

\newcommand{\gatingFunc}{\ensuremath{g}}
#+END_EXPORT
This chapter is concerned with exploration in multimodal dynamical systems
where the transition dynamics are /not fully known a priori/.
In particular, it is interested in exploring a single desired dynamics
mode whilst avoiding entering any of the other modes.
This is a challenging problem in systems where both the dynamical modes and
how they switch over the state-control space are /not known a priori/.
This is because the agent must observe regions outside of the desired dynamics mode
in order to know that a particular region does not belong to the desired mode.
Based on this logic, this chapter utilises the model from Chapter ref:chap-dynamics
to design an information theoretic trajectory optimisation algorithm
that

The trajectory optimiastion algorithm in this chapter is only concerned with
the aforementioned exploration and is not intended to run fast enough for
real-time MPC.
Although not explored here, the algorithm could be used for guided policy
search in a model-based reinforcement learning setting
cite:levineGuided2013,levineVariational2013,okadaVariational2020.
Alternatively, one could explore approximations enabling the approach to
work for real time MPC e.g. linearising the dynamics and/or a second order
Taylor expansion of the cost function.
\todo{should this be cost of value function??}

To the best of our knowledge, there is no previous work addressing exploration of
a single dynamics mode in multimodal dynamical systems.
cite:schreiterSafe2015 use a GP classifier to identify safe and unsafe regions
when learning GP dynamics models in an active learning setting.
However, they assume that they can directly observe whether a particular
data point from the environment belongs to either the safe or unsafe regions.
In contrast, this chapter is concerned with scenarios where the mode cannot be directly observed from the
environment, but instead, is inferred by a probabilistic dynamics model.

Utilising mutual information for active learning has been well motivated by, for example,
cite:krauseNearOptimal2008.
They highlight that mutual information may lead to a more accurate model than differential entropy.
It has also been shown that minimising the mutual information  is the same as minimising the expected posterior
uncertainty (conditional entropy) in the model cite:ertinMaximum2003.
** Problem Statement

#+BEGIN_EXPORT latex
\begin{theorem}[definition] \label{}
Given a desired dynamics mode $\desiredMode$ with state space
$\desiredStateDomain = \{ \state \in \stateDomain \mid \modeVar = \desiredMode \}$
controller $\policy_{\desiredMode}$ and the
The controller satisfies the constraints
\end{theorem}
#+END_EXPORT

** Bayesian Information Theoretic Active Learning
Following cite:houlsbyBayesian2011 this section introduces Bayesian
information theoretic active learning.
The Bayesian framework assumes that there are some latent parameters $\params$,
that control the dependence between the inputs $\input \in \inputDomain$
and the outputs $p(\output \mid \input, \params)$.
Given observations of the system
$\mathcal{D} = \{(\input_\timeInd, \output_\timeInd)\}_{\timeInd=1}^\TimeInd$,
it is assumed that the posterior over the parameters given the data
$p(\params \mid \dataset)$
has been inferred.
The goal of information theoretic active learning is to reduce the number of
possible hypothesis as fast as possible, i.e. minimise the uncertainty associated
with the parameters using Shannon's entropy cite:coverElements2006.
#+BEGIN_EXPORT latex
\newcommand{\crv}{\ensuremath{X}}
\newcommand{\density}{\ensuremath{p(\crv)}}
\begin{myquote}
\textbf{Differential Entropy}
Let $\crv$ be a continuos random variable, with a probability density
$\density$,
whose support is a set $\mathcal{X}$.
The differential entropy $H(\crv)$ is then defined as,
\begin{align} \label{eq-differential-entropy}
H(\crv) = - \int_{\mathcal{X}} \density \text{log} \density \text{d} \crv.
\end{align}
\end{myquote}
#+END_EXPORT
Data points $\dataset^*$ should thus be selected according to,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-posterior-entropy}
\text{arg}\underset{\dataset^*}{\min}
H(\params \mid \dataset^*)
= \text{arg}\underset{\dataset^*}{\min}
- \int p(\params \mid \dataset^*) \text{log} p(\params \mid \dataset^*) \text{d} \params.
\end{align}
#+END_EXPORT
In general, solving this problem is NP-hard,
so it is common to approximate it with a myopic greedy approximation.
cite:krauseNearOptimal2008,dasguptaAnalysis2005 show that a myopic policy can perform near-optimally.
The objective is then to seek the input $\input$ that maximises the decrease in
expected posterior entropy,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-decrease-entropy}
\text{arg}\underset{\control}{\max}
H(\params \mid \dataset)
- \E_{\output \sim \outputGivenInputData}
\left[ H(\params \mid \output, \input, \dataset) \right].
\end{align}
#+END_EXPORT
# The expectation requires the unseen output $\output$ and many works have
An important insight is that minimising cref:eq-expected-decrease-entropy
is equivalent to minimising the conditional mutual information between
the output and the parameters $I(\output, \params \mid \input, \dataset$.
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Mutual Information}
Given two sets of random variables, $\mathbf{X}$ and $\mathbf{F}$, with joint density $p(\mathbf{X}, \mathbf{F})$ the
mutual information \cite{coverElements2006} is given by,
\begin{align} \label{eq-mutual-information}
I(\mathbf{X};\mathbf{F}) = \int p(\mathbf{X},\mathbf{F}) \text{log}\frac{p(\mathbf{X},\mathbf{F})}{p(\mathbf{X})p(\mathbf{F})}
\text{d}\mathb{X} \text{d}\mathb{F},
\end{align}
#+END_EXPORT
and its well known relationship to differential entropy $H(\cdot)$ is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information-entropy}
I(\mathbf{X};\mathbf{F}) =  H(\mathbf{X}) - H(\mathbf{X} \mid \mathbf{F}).
\end{align}
\end{myquote}
#+END_EXPORT
An equivalent objective function can therefore be formulated in the output $\output$
space as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-output-space-entropy}
\text{arg}\underset{\input}{\max}
H(\output \mid \input, \dataset)
- \E_{\params \sim p(\params \mid\dataset)} \left[ H(\output \mid \input, \params) \right].
\end{align}
#+END_EXPORT
The entropy terms in cref:eq-output-space-entropy are now calculated
in a (usually) low dimensional output space.
In the binary classification setting, cref:eq-output-space-entropy simply
involves calculating the entropy of Bernoulli random variables.
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Entropy of Bernoulli Variable}
Consider a Bernoulli random varialbe $\output$ with probability $p$.
The entropy of such a Bernoulli random variable is given by,
\begin{align} \label{eq-entropy-bernoulli}
H(\alpha) &= h_{\text{Bern}}(p) \\
h_{\text{Bern}}(p) &= -p \text{log}p - (1-p) \text{log}(1-p),
\end{align}
\end{myquote}
#+END_EXPORT
Intuitively, the first term in cref:eq-output-space-entropy
($H(\output \mid \input, \dataset)$) seeks to
find the input $\input$ where the model is marginally most uncertain about
its corresponding output $\output$, whilst the
second term
$(- \E_{\params \sim p(\params \mid\dataset)} \left[ H(\output \mid \input, \params) \right])$
prefers inputs whose parameter settings are confident.
As highlighted by cite:houlsbyBayesian2011, this can be interpreted as finding the
input $\input$ for which the parameters under the posterior, disagree about
the output $\output$ the most.

*** Active Learning for GP Classification
cite:houlsbyBayesian2011 formulate Bayesian active learning in the GP
binary classification model,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-binary-gp-classification}
\gatingFunc &\sim \mathcal{GP}(\mu(\cdot), k(\cdot,\cdot)) \\
\output \mid \gatingFunc, \input &\sim \text{Bernoulli}(\Phi(\gatingFunc(\input)))
\end{align}
#+END_EXPORT
where $\Phi$ represents the Gaussian CDF and $\Phi(\gatingFunc(\input))$ is
the probability of the (Bernoulli) output variable
$\alpha$ cite:nickischApproximations2008.
In this context,  cref:eq-output-space-entropy can be re-written as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-output-space-entropy-gp-classification}
\text{arg}\underset{\input}{\max}
H(\output \mid \input, \dataset)
- \E_{\gatingFunc \sim p(\gatingFunc \mid\dataset)} \left[ H(\output \mid \input, \gatingFunc) \right].
\end{align}
#+END_EXPORT
Their approach introduces approximations to the two terms in
cref:eq-output-space-entropy-gp-classification.
They assume that the posterior over the latent function $\gatingFunc$
is approximated to be Gaussian,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-classification-posterior}
\gatingFunc \mid \input, \mathcal{D} &\sim \mathcal{N}(\gatingFunc(\input) \mid \mu_{\input,\dataset}, \sigma^2_{\input,\dataset}).
%\mu_{\input,\dataset} &= k(\input, \allInput) k(\allInput, \allInput)^{-1} \bm\alpha \\
%\sigma^2_{\input,\dataset}) &= k(\input, \input)
%- k(\input, \allInput) k(\allInput, \allInput)^{-1} k(\allInput, \input)
\end{align}
#+END_EXPORT
The $\overset{1}{\approx}$ symbol will be used to indicate when such an approximation
has been exploited.
The first term in cref:eq-output-space-entropy-gp-classification is approximated as follows,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-entropy--data}
H(\output \mid \input, \dataset) &\overset{1}{\approx}
h_{\text{Bern}} \left(\int \Phi(\gatingFunc(\input)) \mathcal{N}(\gatingFunc(\input) \mid \mu_{\input,\dataset}, \sigma^2_{\input,\dataset}) \text{d} \gatingFunc(\input) \right) \\
&= h_{\text{Bern}} \left( \Phi \left( \frac{\mu_{\input,\dataset}}{\sqrt{\sigma^2_{\input,\dataset} + 1}} \right) \right).
\end{align}
#+END_EXPORT
The second term is approximated as follows,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-entropy-expected-term}
\E_{\gatingFunc \sim p(\gatingFunc \mid \dataset)}
\left[H(\output \mid \input, \gatingFunc) \right]
&\overset{1}{\approx} \int h_{\text{Bern}}(\Phi(\gatingFunc(\input)))
\mathcal{N}(\gatingFunc(\input) \mid \mu_{\input,\dataset}, \sigma^2_{\input,\dataset})
\text{d} \gatingFunc(\input) \\
&\overset{2}{\approx}
\int \text{exp}\left( -\frac{\gatingFunc(\input)^2}{\pi \text{ln}2} \right)
\mathcal{N}(\gatingFunc(\input) \mid \mu_{\input,\dataset}, \sigma^2_{\input,\dataset})
\text{d} \gatingFunc(\input) \\
&= \frac{C}{\sqrt{\sigma_{\input,\dataset}^2 + C^2}}
\text{exp}\left( -\frac{\mu_{\input,\dataset}^2}{2\left( \sigma_{\input,\dataset}^2 + C^2 \right)} \right) \label{eq-entropy-expected-term-last}
\end{align}
#+END_EXPORT
where $C = \sqrt{\frac{\pi \text{ln}2}{2}}$.
This approximation exploits a Taylor expansion of $\text{ln} h_{\text{Bern}}(\Phi(\gatingFunc(\input)))$
(see supplementary material of cite:houlsbyBayesian2011)
allowing it to be represented up to $\mathcal{O}(\gatingFunc(\input)^4)$ by a squared
exponential curve ($\text{exp}(-\gatingFunc(\input)^2/\pi\text{ln}2)$).
This approximation is referred to as $\overset{2}{\approx}$.
A simple Gaussian convolution then gives the closed form expression in Eq.
ref:eq-entropy-expected-term-last.
The objective in cref:eq-output-space-entropy-gp-classification can then be approximated as,
#+BEGIN_EXPORT latex
\newcommand{\approxEntropy}{\ensuremath{\hat{H}}}
\begin{align} \label{eq-approximate-output-space-entropy-obj}
\text{arg}\max_{\input}
\approxEntropy(\output)
\end{align}
#+END_EXPORT
where,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-approximate-output-space-entropy}
\approxEntropy(\output)
\coloneqq
h_{\text{Bern}} \left( \Phi \left( \frac{\mu_{\input,\dataset}}{\sqrt{\sigma^2_{\input,\dataset} + 1}} \right) \right)
+ \frac{C}{\sqrt{\sigma_{\input,\dataset}^2 + C^2}}
\text{exp}\left( -\frac{\mu_{\input,\dataset}^2}{2\left( \sigma_{\input,\dataset}^2 + C^2 \right)} \right)
\end{align}
#+END_EXPORT
The objective is smooth and differentiable for most practically relevant kernels of $\input$, so
gradient-based optimisation can be used to find the maximally informative $\input$.


# #+BEGIN_EXPORT latex
# \begin{myquote}
# \textbf{Entropy of Bernoulli Variable}
# Consider the probit case, where the value of the output $\output$,
# given latent gating function $\gatingFunc(\input)$, takes a Bernoulli
# distribution with probability $\Phi(\gatingFunc(\input))$,
# where $\Phi$ represents the Gaussian CDF.
# The entropy of such a Bernoulli random variable is given by,
# \begin{align} \label{eq-entropy-bernoulli}
# H(\alpha \mid \control, \gatingFunc) &= H_{\mathcal{Bern}}(\Phi(\gatingFunc(\control))) \\
# H_{\mathcal{Bern}}(p) &= -p \text{log}p - (1-p) \text{log}(1-p),
# \end{align}
# where the probability $\Phi(\gatingFunc(\input))$ has been denoted $p$.
# \end{myquote}
# #+END_EXPORT

** Bayesian Information Theoretic Exploration Strategy
#+BEGIN_EXPORT latex
\newcommand{\posterior}{\ensuremath{p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \control_{\timeInd})}}
#+END_EXPORT

# The /static/ active learning problem addressed previously is fundamentally different
# to the problem in dynamical systems.
# In the /static/ problem, it is possible to query any point in the input domain.
# As such, it is open to a clean information-theoretic treatment.
# In the /dynamic/ problem, the system must be steered through the
# unknown dynamics $\dynamics$ by a sequence of controls $\controlTraj$.
In dynamical systems an arbitrary state $\state$ cannot be sampled, so instead, the dynamics must be steered
to $\state$ through the unknown dynamics $\dynamicsFunc$ through a sequence of controls $\control$.
Thus, there is information gain along the trajectory which must also be
considered.
As highlighted by cite:buisson-fenetActively2020, information gain in dynamical systems is therefore
fundamentally different to the /static/ problem addressed by cite:krauseNearOptimal2008
and cite:houlsbyBayesian2011.
The goal here is to pick the most informative control trajectory $\controlTraj$ whilst observing $\stateTraj$.

Recent work has addressed active learning in (unimodal) GP dynamics models
citep:buisson-fenetActively2020 and (unimodal)
GP state-space models citep:caponeLocalized2020.
cite:schreiterSafe2015 consider safe exploration for active learning where they distinguish safe and unsafe regions
with a binary GP classifier, which is learned separately to the dynamics model. Their exploration strategy considers
the differential entropy of the posterior GP associated with the dynamics model and they use the GP classifier to
define a set of constraints.
In contrast, this chapter exploits the coupled learning of the dynamics modes and the gating network (i.e. the classifier)
and projects the exploration strategy onto the posterior GP(s) associated with the gating network.
The coupled learning provides well-calibrated uncertainty estimates in the gating network, making it a
good choice for information-based exploration.

In order to calculate the approximate entropy objective in the output space $\approxEntropy(\output_{\timeInd})$
(cref:eq-approximate-output-space-entropy-obj)
at a given time step $\timeInd$, the posterior $\posterior$ over the gating function is required.
After the first time step this distribution is calculated as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-posterior-0}
p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \control_{\timeInd})&=
p(\state_0)
\int p(\gatingFunc(\state_{1}, \control_{1}) \mid \state_{0}, \control_{0})
p(\state_{1} \mid \state_{0}, \control_{0}) p(\control_{0}) \text{d}\state_0
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \control_{\timeInd})&=
p(\state_0) \prod_{\timeInd=1}^{\timeInd}
\int p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \state_{\timeInd}, \control_{\timeInd})
p(\state_{\timeInd} \mid \state_{\timeInd-1}, \control_{\timeInd-1}) p(\control_{\timeInd}) \text{d}\state_\timeInd
\end{align}
#+END_EXPORT


#+BEGIN_EXPORT latex
\newcommand{\gatingOutput}{\ensuremath{h(\singleInput)}}
\newcommand{\gatingTransition}{\ensuremath{p(\gatingOutput \mid \state_\timeInd, \control_\timeInd)}}
\begin{align} \label{eq-mutual-information}
\underset{\stateTraj \in \stateDomain^\TimeInd, \controlTraj \in \controlDomain^\TimeInd}{\text{argmax}} \approxEntropy(\stateTraj, \controlTraj) &=
\underset{\state \in \stateDomain, \control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd \approxEntropy_{\timeInd}(\state_{\timeInd}, \control_{\timeInd})
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-classification-posterior}
\gatingFunc \mid \input, \mathcal{D} &\sim \mathcal{N}(\gatingFunc(\input) \mid \mu_{\input,\dataset}, \sigma^2_{\input,\dataset}).
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\newcommand{\gatingOutput}{\ensuremath{h(\singleInput)}}
\newcommand{\gatingTransition}{\ensuremath{p(\gatingOutput \mid \state_\timeInd, \control_\timeInd)}}
\begin{align} \label{eq-mutual-information}
\underset{\stateTraj \in \stateDomain^\TimeInd, \controlTraj \in \controlDomain^\TimeInd}{\text{argmax}} H(\stateTraj, \controlTraj) &=
\underset{\state \in \stateDomain, \control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd H_{\timeInd}(\state_{\timeInd}, \control_{\timeInd}) \\
&= \underset{\state \in \stateDomain, \control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd - \int \gatingTransition
\text{log} \gatingTransition \text{d} \gatingOutput
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
\underset{\control \in \controlDomain}{\text{argmax}} H(\stateTraj, \controlTraj) &=
\underset{\control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd H_{\timeInd}(\state_{\timeInd}, \control_{\timeInd}) \\
&= \underset{\control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd - \int \gatingTransition
\text{log} \gatingTransition \text{d} \gatingOutput \\
\end{align}
#+END_EXPORT


The exploration strategy introduced in this chapter builds on the approximate
entropy objective in cref:eq-approximate-output-space-entropy.


is a selective
sampling approach based on the differential entropy of the GP posterior
over the desired modes gating function.
The
Considering a full discriminative model,
discover the dependence of the mode indicator variable $\modeVar \in \modeDomain$
on the state-control input $\input \in \inputDomain$





prediction with uncertain input


We consider a fully discriminative model where the goal of active learning is to discover the dependence of some variable y  Y on an input variable x  X . The key idea in active learning is that the learner chooses the input queries xi  X and observes the systems response yi, rather than passively receiving
(xiyi) pairs.




#+BEGIN_EXPORT latex
\begin{align} \label{eq--entropy-model}
\E_{\gatingFunc \sim p(\gatingFunc \mid \mathcal{D})} \left[ H(\alpha \mid \control, \gatingFunc) \right]
&\approx \int h \left( \Phi(\gatingFunc(\state, \control)) \mathcal{N}(\gatingFunc(\state, \control) \mid \mu_\gatingFunc, \sigma^2_\gatingFunc) \text{d} \gatingFunc(\state, \control)
\end{align}
#+END_EXPORT

** Mode Constraints
In order to ensure that the exploration strategy remains in the desired dynamics
mode this section introduces additional information to describe the discriminative
function when it gets close to the decision boundary  w

** The Algorithm
This section introduces the entropy based active learning framework extended by

** Content
An approximated mutual information exploration criterion is used

entropy

#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
\underset{\control \in \controlDomain}{\text{argmax}} H(\gatingFunc(\stateTraj, \controlTraj) \mid \controlTraj) &=
\underset{\control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd H_{\timeInd}(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \control_\timeInd)
\end{align}
#+END_EXPORT
where
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
p(\gatingFunc(\state_{1:\TimeInd}, \control_{1:\TimeInd}) \mid \control_{1:\TimeInd})&=
p(\state_0) \prod_{\timeInd=1}^{\TimeInd-1}
\int p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd} \mid \state_{\timeInd}, \control_{\timeInd})
p(\state_{\timeInd} \mid \state_{\timeInd-1}, \control_{\timeInd-1}) p(\control_{\timeInd}) \text{d}\state_\timeInd
\end{align}
#+END_EXPORT

* Conclusion
* Appendix
** Multivariate Normals
** Derivatives of a Gaussian Process
** Results
*** Velocity Controlled Quadcopter Experiments
Table ref:tab-params-quadcopter contains the optimisation settings and initial values for the optimisable parameters
that were used to train the model on the velocity controlled quadcopter data set in  Section ref:sec-brl-experiment.

*Experts* Both expert's GP priors were initialised with constant mean functions (with
a learnable parameter $c_{\modeInd}$) and separate independent squared exponential
kernels (with Automatic Relevance Determination) on each output dimension.
Table ref:tab-params-quadcopter shows a single set of kernel parameters $\sigma_f, l$ for each expert, as
the kernel associated with each output dimension was initialised with the same initial values.
Each experts likelihood was initialised with diagonal covariance matrices $\Sigma_{\epsilon_{\modeInd}}$.

*Gating Network* The gating network GP was initialised with a zero mean function, and a squared exponential kernel
with ARD.

#+begin_table
#+LATEX: \caption{Optimiser settings and initial values for optimisable parameters before training on the DJI Tello quadcopter data set.}
#+LATEX: \label{tab-params-quadcopter}
#+LATEX: \resizebox{\textwidth}{!}{
#+ATTR_LATEX: :center nil
|                 | Description                | Symbol                    | Value                                                                                   |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Num data                   | $\NumData$                | $1129$                                                                                  |
|                 | Batch size                 | $\NumData_b$              | $64$                                                                                    |
| Optimiser       | Num epochs                 | N/A                       | $6000$                                                                                  |
|                 | Num samples                | $S$                       | $1$                                                                                     |
|                 | Learning rate              | N/A                       | $0.01$                                                                                  |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Constant mean function     | $c_1$                     | $[0,0]$                                                                                 |
|                 | Kernel variance            | $\sigma_f$                | $0.1$                                                                                   |
|                 | Kernel lengthscales        | $l$                       | $[10, 10]$                                                                              |
|                 | Likelihood variance        | $\Sigma_{\epsilon_1}$     | $\diag([0.0011, 0.0011])$                                                               |
| Expert 1        | Num inducing points        | $\NumInducing$            | $100$                                                                                   |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Constant mean function     | $c_2$                     | $[0,0]$                                                                                 |
|                 | Kernel variance            | $\sigma_f$                | $20$                                                                                    |
|                 | Kernel lengthscales        | $l$                       | $[0.5, 0.5]$                                                                            |
|                 | Likelihood variance        | $\Sigma_{\epsilon_2}$     | $\diag([1.9,1.9])$                                                                      |
| Expert 2        | Num inducing points        | $\NumInducing$            | $100$                                                                                   |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel variance            | $\sigma_f$                | $0.6$                                                                                   |
|                 | Kernel lengthscales        | $l$                       | $[1.5, 1.5]$                                                                            |
| Gating function | Num inducing points        | $\NumInducing$            | $100$                                                                                   |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | $2 \times$ ones plus Gaussian noise                                                     |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
#+LATEX: }
#+end_table

*** Motorcycle Experiments

**** Two Experts :ignore:
\newline
*Model Training*
Table ref:tab-params-quadcopter contains the initial values for all of the trainable parameters in the model.

#+begin_table
#+LATEX: \caption{Initial parameter settings before training on motorcycle data set with two experts.}
#+LATEX: \label{tab-params-quadcopter}
#+LATEX: \resizebox{\textwidth}{!}{
#+ATTR_LATEX: :center nil
|                 | Description                | Symbol                    | Value                                                                                   |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Num data                   | $\NumData$                | $133$                                                                                   |
|                 | Batch size                 | $\NumData_b$              | $30$                                                                                    |
| Optimiser       | Epochs                     | N/A                       | $8000$                                                                                  |
|                 | Num samples                | $S$                       | $1$                                                                                     |
|                 | Learning rate              | N/A                       | $0.001$                                                                                 |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $0.1$                                                                                   |
|                 | Kernel Lengthscales        | $l$                       | $10$                                                                                    |
|                 | Likelihood Variance        | $\sigma_n$                | $0.0011$                                                                                |
| Expert 1        | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $20$                                                                                    |
|                 | Kernel Lengthscales        | $l$                       | $0.5$                                                                                   |
|                 | Likelihood Variance        | $\sigma_n$                | $0.9$                                                                                   |
| Expert 2        | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $3$                                                                                     |
|                 | Kernel Lengthscales        | $l$                       | $0.5$                                                                                   |
| Gating Function | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | $10 \times$ ones plus Gaussian noise                                                    |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
#+LATEX: }
#+end_table

**** Three Experts :ignore:
\newline
*Model Training*
Table ref:tab-params-quadcopter contains the initial values for all of the trainable parameters in the model.

#+begin_table
#+LATEX: \caption{Initial parameter settings before training on motorcycle data set with two experts.}
#+LATEX: \label{tab-params-quadcopter}
#+LATEX: \resizebox{\textwidth}{!}{
#+ATTR_LATEX: :center nil
|                 | Description                | Symbol                    | Value                                                                                   |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Num data                   | $\NumData$                | $133$                                                                                   |
|                 | Batch size                 | $\NumData_b$              | $30$                                                                                    |
| Optimiser       | Epochs                     | N/A                       | $8000$                                                                                  |
|                 | Num samples                | $S$                       | $1$                                                                                     |
|                 | Learning rate              | N/A                       | $0.001$                                                                                 |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $0.1$                                                                                   |
|                 | Kernel Lengthscales        | $l$                       | $10$                                                                                    |
|                 | Likelihood Variance        | $\sigma_n$                | $0.0011$                                                                                |
| Expert 1        | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $20$                                                                                    |
|                 | Kernel Lengthscales        | $l$                       | $0.5$                                                                                   |
|                 | Likelihood Variance        | $\sigma_n$                | $0.9$                                                                                   |
| Expert 2        | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $3$                                                                                     |
|                 | Kernel Lengthscales        | $l$                       | $0.5$                                                                                   |
| Gating Function | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | $10 \times$ ones plus Gaussian noise                                                    |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
#+LATEX: }
#+end_table

* Back Matter :ignore:
** Bibliography :ignore:

#+BEGIN_EXPORT latex
% \begingroup
% \sloppy
% \setstretch{1}
% \setlength\bibitemsep{3pt}
\printbibliography
% \endgroup
#+END_EXPORT

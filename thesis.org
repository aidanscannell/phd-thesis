* Config :ignore:
#+latex_class: mimosis
#+begin_src emacs-lisp :exports none  :results none
(unless (boundp 'org-latex-classes)
  (setq org-latex-classes nil))
(add-to-list 'org-latex-classes
             '("memoir"
               "\\documentclass{memoir}
    [NO-DEFAULT-PACKAGES]
    [PACKAGES]
    [EXTRA]
    \\newcommand{\\mboxparagraph}[1]{\\paragraph{#1}\\mbox{}\\\\}
    \\newcommand{\\mboxsubparagraph}[1]{\\subparagraph{#1}\\mbox{}\\\\}"
               ("\\chapter{%s}" . "\\chapter*{%s}")
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
;; ("\\mboxparagraph{%s}" . "\\mboxparagraph*{%s}")
;; ("\\mboxsubparagraph{%s}" . "\\mboxsubparagraph*{%s}")))
(add-to-list 'org-latex-classes
             '("mimosis"
               "\\documentclass{mimosis-class/mimosis}
  [NO-DEFAULT-PACKAGES]
  [PACKAGES]
  [EXTRA]"
               ("\\chapter{%s}" . "\\addchap{%s}")
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}\\newline" . "\\paragraph*{%s}\\newline")
               ("\\subparagraph{%s}\\newline" . "\\subparagraph*{%s}\\newline")))
#+end_src
# #+EXPORT_FILE_NAME: ./tmp/thesis.pdf
** Org Mode Export Options :noexport:
#+EXCLUDE_TAGS: journal noexport
#+OPTIONS: title:nil toc:nil date:nil author:nil H:6

** Macros :ignore:
# #+MACRO: acronym #+latex_header: \newacronym[description={$1}]{$2}{$2}{$3}
#+MACRO: glossaryentry #+latex_header: \newglossaryentry{$1}{name={$2},description={$3},sort={$4}}
#+MACRO: acronym #+latex_header: \newacronym{$1}{$2}{$3}
# #+MACRO: newline @@latex:\hspace{0pt}\\@@ @@html:<br>@@
# #+MACRO: fourstar @@latex:\bigskip{\centering\color{BrickRed}\FourStar\par}\bigskip@@
# #+MACRO: clearpage @@latex:\clearpage@@@@odt:<text:p text:style-name="PageBreak"/>@@

** LaTeX Export Headers and Options :noexport:
*** Packages :ignore:
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{amsmath,amssymb,amsfonts}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{todonotes}
*** Font Awesome icons
#+LATEX_HEADER: \usepackage{fontawesome}
*** Maths cancel
#+LATEX_HEADER: \usepackage[makeroom]{cancel}
*** Footnotes
#+LATEX_HEADER: \usepackage{footnote}
*** Tensor indexing (pre subscripts)
#+LATEX_HEADER: \usepackage{tensor}

*** Epigraph (chapter quotes)
#+LATEX_HEADER: \usepackage{epigraph}
*** Grey box for block quotes
#+LATEX_HEADER: \usepackage[most]{tcolorbox}
#+LATEX_HEADER: \definecolor{block-gray}{gray}{0.85}
#+LATEX_HEADER: \newtcolorbox{myquote}{colback=block-gray,boxrule=0pt,boxsep=0pt,breakable}
# #+LATEX_HEADER: \newtcolorbox{myquote}{colback=block-gray,grow to right by=-10mm,grow to left by=-10mm, boxrule=0pt,boxsep=0pt,breakable}
*** Acronym and Glossary :ignore:
#+latex_header: \usepackage[acronym]{glossaries}
#+latex_header: \makeglossaries

*** Equation Definitions

#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \newcommand{\defeq}{\vcentcolon=}

*** Create a Definition theorem
#+LATEX_HEADER: \newtheorem{definition}{Definition}[section]
#+LATEX_HEADER: \newtheorem{assumption}{Assumption}[section]
#+LATEX_HEADER: \newtheorem{theorem}{Theorem}[section]
#+LATEX_HEADER: \newtheorem{lemma}{Lemma}[section]
#+LATEX_HEADER: \newtheorem*{remark}{Remark}
*** Floating images configuration

By default,  if a figure consumes 60% of the page it will get its own float-page. To change that we have to adjust the value of the floatpagefraction derivative.
#+latex_header: \renewcommand{\floatpagefraction}{.8}%

See more information [[https://tex.stackexchange.com/questions/68516/avoid-that-figure-gets-its-own-page][here]].

Allow images to be cropped
#+LATEX_HEADER: \usepackage[export]{adjustbox}

*** Hyperref
Self-explanatory.
#+latex_header: \usepackage[colorlinks=true, citecolor=BrickRed, linkcolor=BrickRed, urlcolor=BrickRed]{hyperref}

*** Cleverref
#+latex_header: \usepackage[capitalise,noabbrev]{cleveref}
*** Bookmarks

The bookmark package implements a new bookmark (outline) organisation for package hyperref. This lets us change the "tree-navigation" associated with the generated pdf and constrain the menu only to H:2.
#+latex_header: \usepackage{bookmark}
#+latex_header: \bookmarksetup{depth=2}

*** BBding

Symbols such as diamond suit, which can be used for aesthetically separating paragraphs, could be added with the package =fdsymbol=. I'll use bbding which offers the more visually appealing =\FourStar=. I took this idea from seeing the thesis of the mimosis package author.
#+latex_header: \usepackage{bbding}

*** CS Quotes
The [[https://ctan.org/pkg/csquotes][csquotes]] package offers context sensitive quotation facilities, improving the typesetting of inline quotes.

Already imported by mimosis class.
# #+latex_header: \usepackage{csquotes}

To enclose quote environments with quotes from csquotes, see [[https://tex.stackexchange.com/questions/365231/enclose-a-custom-quote-environment-in-quotes-from-csquotes][the following TeX SE thread]].

#+latex_header: \def\signed #1{{\leavevmode\unskip\nobreak\hfil\penalty50\hskip1em
#+latex_header:   \hbox{}\nobreak\hfill #1%
#+latex_header:   \parfillskip=0pt \finalhyphendemerits=0 \endgraf}}

#+latex_header: \newsavebox\mybox
#+latex_header: \newenvironment{aquote}[1]
#+latex_header: {\savebox\mybox{#1}\begin{quote}\openautoquote\hspace*{-.7ex}}
#+latex_header:    {\unskip\closeautoquote\vspace*{1mm}\signed{\usebox\mybox}\end{quote}}

And then use quotes as:
#+begin_example
# The options derivative adds text after the environment. We use it to add the author.
#+ATTR_LATEX: :options {\cite{Frahm1994}}
#+begin_aquote
/Current (fMRI) applications often rely on "effects" or "statistically significant differences", rather than on a proper analysis of the relationship between neuronal activity, haemodynamic consequences, and MRI physics./
#+end_aquote
#+end_example

Note that org-ref links won't work here because the attr latex will be pasted as-is in the .tex file.

*** Date Time

The date time package allows us to specify a "formatted" date object, which will print different formats according to the current locale & language. I use this in my title page.
#+latex_header: \usepackage[level]{datetime}

*** Bibliography
General configuration.
# #+latex_header: \usepackage[autocite=plain, backend=biber, doi=true, url=true, hyperref=true,uniquename=false, maxbibnames=99, maxcitenames=2, sortcites=true, style=authoryear-comp]{biblatex}
# #+LATEX_HEADER: \usepackage[citestyle=authoryear-comp, maxcitenames=2, maxbibnames=99, doi=false, isbn=false, eprint=false, backend=bibtex, hyperref=true, url=false, natbib=true, style=authoryear-comp]{biblatex}
#+LATEX_HEADER: \usepackage[citestyle=authoryear-comp, maxcitenames=2, maxbibnames=99, doi=false, isbn=false, eprint=false, backend=bibtex, hyperref=true, url=false, natbib=true, style=authoryear-comp]{biblatex}
# #+LATEX_HEADER: \addbibresource{~/Dropbox/org/ref/mendeley/library.bib}
#+LATEX_HEADER: \addbibresource{~/Dropbox/org/ref/zotero-library.bib}

Improvements provided with the Mimosis class.
# #+latex_header: \input{bibliography-mimosis}

# And fix the andothers to show et al in English as well:
# #+latex_header: \DefineBibliographyStrings{english}{andothers={\textit{et\, al\adddot}}} 
# #+latex_header:\DefineBibliographyStrings{english}{and={\textit{and}}}


Remove ISSN, DOI and URL to shorten the bibliography.
#+latex_header: \AtEveryBibitem{%
#+latex_header:   \clearfield{urlyear}
#+latex_header:   \clearfield{urlmonth}
#+latex_header:   \clearfield{note}
#+latex_header:  \clearfield{issn} % Remove issn
#+latex_header:  \clearfield{doi} % Remove doi
#+latex_header: \ifentrytype{online}{}{% Remove url except for @online
#+latex_header:   \clearfield{url}
#+latex_header: }
#+latex_header: }

And increase the spacing between the entries, as per default they are too small.
#+latex_header: \setlength\bibitemsep{1.1\itemsep}

Also reduce the font-size
#+latex_header: \renewcommand*{\bibfont}{\footnotesize}

*** Improve chapter font colors and font size
The following commands make chapter numbers BrickRed, which look like the Donders color.
#+latex_header: \makeatletter
#+latex_header: \renewcommand*{\chapterformat}{  \mbox{\chapappifchapterprefix{\nobreakspace}{\color{BrickRed}\fontsize{40}{45}\selectfont\thechapter}\autodot\enskip}}
#+latex_header: \renewcommand\@seccntformat[1]{\color{BrickRed} {\csname the#1\endcsname}\hspace{0.3em}}
#+latex_header: \makeatother

*** Setspace for controlling line spacing

Already imported when using mimosis.
# #+latex_header: \usepackage{setspace}
#+latex_header: \setstretch{1.25} 

*** Parskip

Fine tuning of spacing between paragraphs. See [[https://tex.stackexchange.com/questions/161254/smaller-parskip-than-half-for-koma-script][thread here]].

#+latex_header: \setparsizes{0em}{0.1\baselineskip plus .1\baselineskip}{1em plus 1fil}

*** Table of Contents improvements

# TOC only the chapters, not their content.
# #+latex_header: \setcounter{tocdepth}{1}
#+latex_header: \setcounter{tocdepth}{2}

*** Possible Equation improvements

Make the equation numbers follow the chapter, not the whole thesis.
#+latex_header: \numberwithin{equation}{chapter}

*** TikZ and bayesnet for graphical models
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{bayesnet}

*** Notes in margins
# #+LATEX_HEADER: \usepackage{geometry}
#+LATEX_HEADER: \setlength{\marginparwidth}{3cm}
# #+LATEX_HEADER: \xdef\marginnotetextwidth{\textwidth}
#+LATEX_HEADER: \usepackage{marginnote}
# #+LATEX_HEADER: \renewcommand*{\marginfont}{\footnotesize}
# #+LATEX_HEADER: \newcommand{\parmarginnote}[1]{\hspace{\z@}\marginnote{#1}\ignorespaces}
#+LATEX_HEADER: \newcommand{\parmarginnote}[1]{\marginnote{#1}}
*** Captions
# #+LATEX_HEADER: \usepackage{caption}
# #+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \ifCLASSOPTIONcompsoc \usepackage[caption=false,font=footnotesize,labelfon
#+LATEX_HEADER: t=it,textfont=it]{subfig} \else
#+LATEX_HEADER: \usepackage[caption=false,font=footnotesize]{subfig}
#+LATEX_HEADER: \fi
#+LATEX_HEADER: \usepackage[format=plain,labelfont={bf},textfont=it]{caption} % make captions italic
*** Maths diag
#+LATEX_HEADER: \newcommand{\diag}{\mathop{\mathrm{diag}}}
*** Algorithms
#+LATEX_HEADER: \usepackage{algpseudocode}
#+LATEX_HEADER: \usepackage{algorithm}
** Text Variables :noexport:
#+latex_header: \newcommand{\ThesisTitle}{{Probabilistic Inference for Learning \& Control in Multimodal Dynamical Systems}}
# #+latex_header: \newcommand{\ThesisTitle}{{Data Efficient Learning for Control in Multimodal Dynamical Systems}}
#+latex_header: \newcommand{\ThesisSubTitle}{Synergising Bayesian Inference and Riemannian Geometry for Control}
#+latex_header: \newcommand{\FormattedThesisDefenseDate}{\mbox{\formatdate{1}{1}{2100}}}
#+latex_header: \newcommand{\FormattedAuthorDateOfBirth}{\mbox{\formatdate{1}{1}{2000}}}
#+latex_header: \newcommand{\FormattedThesisDefenseTime}{\mbox{10:00}}
#+latex_header: \newcommand{\AuthorShortName}{\mbox{Aidan Scannell}}
#+latex_header: \newcommand{\AuthorFullName}{\mbox{Aidan J. Scannell}}
#+latex_header: \newcommand{\ThesisISBN}{\mbox{}}

** Math Variables :noexport:
#+LATEX_HEADER: \DeclareMathOperator{\R}{\mathbb{R}}
#+LATEX_HEADER: \DeclareMathOperator{\E}{\mathbb{E}}
#+LATEX_HEADER: \DeclareMathOperator{\V}{\mathbb{V}}
#+LATEX_HEADER: \DeclareMathOperator{\K}{\mathbf{K}}

*** Num Data / Mode / State Dimension / Control Dimension (k, d, t/n)
#+LATEX_HEADER: \newcommand{\numData}{\ensuremath{t}}
# #+LATEX_HEADER: \newcommand{\numData}{\ensuremath{n}}
#+LATEX_HEADER: \newcommand{\numEpisodes}{\ensuremath{e}}
#+LATEX_HEADER: \newcommand{\numTimesteps}{\ensuremath{t}}
#+LATEX_HEADER: \newcommand{\numInd}{\ensuremath{m}}
#+LATEX_HEADER: \newcommand{\stateDim}{\ensuremath{d}}
#+LATEX_HEADER: \newcommand{\controlDim}{\ensuremath{f}}
#+LATEX_HEADER: \newcommand{\modeInd}{\ensuremath{k}}
#+LATEX_HEADER: \newcommand{\modeDesInd}{\ensuremath{\text{des}}}
#+LATEX_HEADER: \newcommand{\testInd}{\ensuremath{*}}
#+LATEX_HEADER: \newcommand{\NumData}{\ensuremath{\MakeUppercase{\numData}}}
#+LATEX_HEADER: \newcommand{\NumInd}{\ensuremath{\MakeUppercase{\numInd}}}
# #+LATEX_HEADER: \newcommand{\StateDim}{\ensuremath{\MakeUppercase{\stateDim}}}
# #+LATEX_HEADER: \newcommand{\ControlDim}{\ensuremath{\MakeUppercase{\controlDim}}}
#+LATEX_HEADER: \newcommand{\StateDim}{\ensuremath{{D_x}}}
#+LATEX_HEADER: \newcommand{\ControlDim}{\ensuremath{{D_u}}}
#+LATEX_HEADER: \newcommand{\ModeInd}{\ensuremath{\MakeUppercase{\modeInd}}}
#+LATEX_HEADER: \newcommand{\NumEpisodes}{\MakeUppercase{\numEpisodes}}
#+LATEX_HEADER: \newcommand{\NumTimesteps}{\MakeUppercase{\numTimesteps}}

# Macros for single/all data notation
#+LATEX_HEADER: \newcommand{\singleData}[1]{\ensuremath{#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\allData}[1]{\ensuremath{\MakeUppercase{#1}}}
# #+LATEX_HEADER: \newcommand{\singleData}[1]{\ensuremath{#1_{\numData}}}
# #+LATEX_HEADER: \newcommand{\allData}[1]{\ensuremath{#1}}
# #+LATEX_HEADER: \newcommand{\allData}[1]{\ensuremath{#1_{1:\NumData}}}

# Macros for data dimensions
# #+LATEX_HEADER: \newcommand{\singleDataDim}[1]{\ensuremath{#1_{\stateDim, \numData}}}
#+LATEX_HEADER: \newcommand{\singleDataDim}[1]{\ensuremath{_{\stateDim}#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\singleDim}[1]{\ensuremath{#1_{\stateDim}}}
# #+LATEX_HEADER: \newcommand{\singleDim}[1]{\ensuremath{_{\stateDim}#1}}
# #+LATEX_HEADER: \newcommand{\singleDimi}[2]{\ensuremath{\tensor*[_{#2}]{#1}{}}}
#+LATEX_HEADER: \newcommand{\singleDim}[1]{\ensuremath{\singleDimi{#1}{\stateDim}}}

# Macros for mode k notation
# #+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{#1^{(\modeInd)}}}
# #+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{#1^{\modeInd}}}
# #+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{\tensor*[^{\modeInd}]{#1}{}}}
#+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{#1_{\modeInd}}}
#+LATEX_HEADER: \newcommand{\modeDes}[1]{\ensuremath{#1^{\modeDesInd}}}

#+LATEX_HEADER: \newcommand{\singleDimiMode}[2]{\ensuremath{\tensor*[_#2^\modeInd]{#1}{}}}
#+LATEX_HEADER: \newcommand{\singleDimMode}[1]{\ensuremath{\singleDimiMode{#1}{\stateDim}}}
#+LATEX_HEADER: \newcommand{\singleDimModeData}[1]{\ensuremath{\tensor*[_\stateDim^\modeInd]{#1}{_\numData}}}

*** Data set
# Dataset/inputs/outputs
#+LATEX_HEADER: \newcommand{\state}{\ensuremath{\mathbf{x}}}
#+LATEX_HEADER: \newcommand{\control}{\ensuremath{\mathbf{u}}}
# #+LATEX_HEADER: \newcommand{\control}{\ensuremath{\mathbf{a}}}

#+LATEX_HEADER: \newcommand{\x}{\ensuremath{\mathbf{x}}}
# #+LATEX_HEADER: \newcommand{\y}{\ensuremath{\mathbf{y}}}
#+LATEX_HEADER: \newcommand{\y}{\ensuremath{y}}
# #+LATEX_HEADER: \newcommand{\x}{\ensuremath{\hat{\state}}}
# #+LATEX_HEADER: \newcommand{\y}{\ensuremath{\Delta\state}}
#+LATEX_HEADER: \newcommand{\dataset}{\ensuremath{\mathcal{D}}}

# Single/all input/output notation
# #+LATEX_HEADER: \newcommand{\singleInput}{\ensuremath{\singleData{\x}}}
#+LATEX_HEADER: \newcommand{\singleInput}{\ensuremath{\x_{\numData-1}}}
#+LATEX_HEADER: \newcommand{\singleOutput}{\ensuremath{\singleData{\y}}}
#+LATEX_HEADER: \newcommand{\allInput}{\ensuremath{\allData{\x}}}
#+LATEX_HEADER: \newcommand{\allOutput}{\ensuremath{\allData{\y}}}

# Single/all state/control notation
#+LATEX_HEADER: \newcommand{\singleState}{\ensuremath{\state_{\numData-1}}}
#+LATEX_HEADER: \newcommand{\singleControl}{\ensuremath{\control_{\numData-1}}}
#+LATEX_HEADER: \newcommand{\allState}{\ensuremath{\allData{\state}}}
#+LATEX_HEADER: \newcommand{\allControl}{\ensuremath{\allData{\control}}}

*** Noise Vars
#+LATEX_HEADER: \newcommand{\noiseVar}{\ensuremath{\sigma}}
#+LATEX_HEADER: \newcommand{\noiseVarK}{\ensuremath{\mode{\noiseVar}}}
#+LATEX_HEADER: \newcommand{\noiseVarOneK}{\ensuremath{\singleDimiMode{\noiseVar}{1}}}
#+LATEX_HEADER: \newcommand{\noiseVarDK}{\ensuremath{\singleDimiMode{\noiseVar}{\StateDim}}}
#+LATEX_HEADER: \newcommand{\noiseVardK}{\ensuremath{\singleDimMode{\noiseVar}}}
# #+LATEX_HEADER: \newcommand{\noiseVarOneK}{\ensuremath{\noiseVarK_{1}}}
# #+LATEX_HEADER: \newcommand{\noiseVarDK}{\ensuremath{\noiseVarK_{\StateDim}}}
# #+LATEX_HEADER: \newcommand{\noiseVardK}{\ensuremath{\noiseVarK_{\stateDim}}}
# #+LATEX_HEADER: \newcommand{\noiseVardK2}{\ensuremath{\left(\noiseVardK\right)^2}}

*** Mode Indicator Variable
#+LATEX_HEADER: \newcommand{\modeVar}{\ensuremath{\alpha}}
#+LATEX_HEADER: \newcommand{\modeVarn}{\ensuremath{\singleData{\modeVar}}}
#+LATEX_HEADER: \newcommand{\ModeVar}{\ensuremath{\bm{\modeVar}}}
# #+LATEX_HEADER: \newcommand{\ModeVar}{\ensuremath{\allData{\bm{\modeVar}}}}
#+LATEX_HEADER: \newcommand{\modeVarK}{\ensuremath{\modeVarn=\modeInd}}
# #+LATEX_HEADER: \newcommand{\ModeVarK}{\ensuremath{\mode{\bm{\modeVar}}}}
#+LATEX_HEADER: \newcommand{\ModeVarK}{\ensuremath{\ModeVar_{\modeInd}}}

*** Tensor Indexing
# Experts indexing
#+LATEX_HEADER: \newcommand{\nkd}[1]{\ensuremath{#1_{\numData,\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\nkD}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\NkD}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\nKD}[1]{\ensuremath{#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\Nkd}[1]{\ensuremath{#1_{:,\modeInd,\stateDim}}}

# Gating function indexing
#+LATEX_HEADER: \newcommand{\nk}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\Nk}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\nK}[1]{\ensuremath{#1_{\numData}}}

# Experts Inducing indexing
#+LATEX_HEADER: \newcommand{\mkd}[1]{\ensuremath{#1_{\numData,\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\mkD}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\MkD}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\mKD}[1]{\ensuremath{#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\Mkd}[1]{\ensuremath{#1_{:,\modeInd,\stateDim}}}

# Gating Inducing indexing
#+LATEX_HEADER: \newcommand{\mk}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\Mk}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\mK}[1]{\ensuremath{#1_{\numData}}}

# Desired Mode Gating indexing
#+LATEX_HEADER: \newcommand{\MDes}[1]{\ensuremath{#1_{:, k^*}}}

*** Gating Network New
# Function notation
#+LATEX_HEADER: \newcommand{\gatingFunc}{\ensuremath{h}}
#+LATEX_HEADER: \newcommand{\hk}{\ensuremath{\mode{\gatingFunc}}}

# Single data notation
#+LATEX_HEADER: \newcommand{\hkn}{\ensuremath{\nk{\gatingFunc}}}
#+LATEX_HEADER: \newcommand{\hn}{\ensuremath{\nK{\mathbf{\gatingFunc}}}}

# All inputs set/vector/tensor notation
#+LATEX_HEADER: \newcommand{\GatingFunc}{\ensuremath{\mathbf{\gatingFunc}}}
#+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\MakeUppercase\GatingFunc}}
#+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\Nk{\GatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\allData{\GatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\allData{\mode{\GatingFunc}}}}

*** Experts New
# Function notation
#+LATEX_HEADER: \newcommand{\latentFunc}{\ensuremath{f}}
#+LATEX_HEADER: \newcommand{\LatentFunc}{\ensuremath{\mathbf{\latentFunc}}}
#+LATEX_HEADER: \newcommand{\fkd}{\ensuremath{\latentFunc_{\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\fk}{\ensuremath{\mathbf{\latentFunc}_{\modeInd}}}
# #+LATEX_HEADER: \newcommand{\fk}{\ensuremath{\latentFunc_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\f}{\ensuremath{\mathbf{f}}}

# Vector/Matrix/Tensor notation
#+LATEX_HEADER: \newcommand{\F}{\ensuremath{\MakeUppercase{\mathbf{\latentFunc}}}}
# #+LATEX_HEADER: \newcommand{\Fnkd}{\ensuremath{\latentFunc_{\numData, \modeInd, \stateDim}}}
# #+LATEX_HEADER: \newcommand{\Fnk}{\ensuremath{\mathbf{\latentFunc}_{\numData, \modeInd}}}
# #+LATEX_HEADER: \newcommand{\Fk}{\ensuremath{\F_{:,\modeInd}}}
# #+LATEX_HEADER: \newcommand{\Fn}{\ensuremath{\F_{\numData}}}
#+LATEX_HEADER: \newcommand{\Fnkd}{\ensuremath{\nkd{\latentFunc}}}
#+LATEX_HEADER: \newcommand{\Fnk}{\ensuremath{\nkD{\mathbf{\latentFunc}}}}
#+LATEX_HEADER: \newcommand{\Fk}{\ensuremath{\NkD{\F}}}
#+LATEX_HEADER: \newcommand{\Fn}{\ensuremath{\nKD{\F}}}
#+LATEX_HEADER: \newcommand{\F}{\ensuremath{\F}}

# #+LATEX_HEADER: \newcommand{\Fdk}{\ensuremath{\mathbf{\latentFunc}_{:,\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\Nkd{\mathbf{\latentFunc}}}}

# Single input notation
#+LATEX_HEADER: \newcommand{\fn}{\ensuremath{\Fn}}
#+LATEX_HEADER: \newcommand{\fkn}{\ensuremath{\Fnk}}
#+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\Fnkd}}

# All inputs set/vector/tensor notation
# #+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\Fdk}}

*** Params
#+LATEX_HEADER: \newcommand{\gatingParams}{\ensuremath{\bm\phi}}
#+LATEX_HEADER: \newcommand{\expertParams}{\ensuremath{\bm\theta}}
#+LATEX_HEADER: \newcommand{\gatingParamsK}{\ensuremath{\mode{\bm\phi}}}
#+LATEX_HEADER: \newcommand{\expertParamsK}{\ensuremath{\mode{\bm\theta}}}
*** Sparse GPs
**** Experts
***** Variables
#+LATEX_HEADER: \newcommand{\uf}{\ensuremath{u}}
#+LATEX_HEADER: \newcommand{\uFkd}{\ensuremath{\Mkd{\mathbf{\uf}}}}
#+LATEX_HEADER: \newcommand{\uFk}{\ensuremath{\MkD{\MakeUppercase{\mathbf{\uf}}}}}
#+LATEX_HEADER: \newcommand{\uF}{\ensuremath{\MakeUppercase{\mathbf{\uf}}}}

***** Inputs
# #+LATEX_HEADER: \newcommand{\zf}{\ensuremath{\bm{\zeta}}}
# #+LATEX_HEADER: \newcommand{\zFkd}{\ensuremath{\Mkd{\zf}}}
# #+LATEX_HEADER: \newcommand{\zFk}{\ensuremath{\MkD{\zf}}}
# #+LATEX_HEADER: \newcommand{\zF}{\ensuremath{\MKD{\zf}}}
#+LATEX_HEADER: \newcommand{\zf}{\ensuremath{\mathbf{Z}}}
#+LATEX_HEADER: \newcommand{\zFkd}{\ensuremath{\Mkd{\zf}}}
#+LATEX_HEADER: \newcommand{\zFk}{\ensuremath{\MkD{\zf}}}
#+LATEX_HEADER: \newcommand{\zF}{\ensuremath{\MKD{\zf}}}

**** Gating
***** Variables
#+LATEX_HEADER: \newcommand{\uh}{\ensuremath{U}}
#+LATEX_HEADER: \newcommand{\uHk}{\ensuremath{\Mk{\hat{\mathbf{\uh}}}}}
#+LATEX_HEADER: \newcommand{\uH}{\ensuremath{\hat{\MakeUppercase{\mathbf{\uh}}}}}

#+LATEX_HEADER: \newcommand{\hu}{\ensuremath{\uh}}
#+LATEX_HEADER: \newcommand{\Hku}{\ensuremath{\uHk}}
#+LATEX_HEADER: \newcommand{\Hu}{\ensuremath{\uH}}

***** Inputs
# #+LATEX_HEADER: \newcommand{\zh}{\ensuremath{\bm{\xi}}}
# #+LATEX_HEADER: \newcommand{\zHk}{\ensuremath{\Mk{\zh}}}
# #+LATEX_HEADER: \newcommand{\zH}{\ensuremath{\MK{\zh}}}
#+LATEX_HEADER: \newcommand{\zh}{\ensuremath{\hat{\mathbf{Z}}}}
#+LATEX_HEADER: \newcommand{\zHk}{\ensuremath{\Mk{\zh}}}
#+LATEX_HEADER: \newcommand{\zH}{\ensuremath{\MK{\zh}}}

# #+LATEX_HEADER: \newcommand{\zHDes}{\ensuremath{\zH_{:, k^*}}}
#+LATEX_HEADER: \newcommand{\zHDes}{\ensuremath{\MDes{\zH}}}

**** Misc
#+LATEX_HEADER: \newcommand{\Z}{\ensuremath{\mathbf{Z}}}
**** Old
# Sparse GP macro
# #+LATEX_HEADER: \newcommand{\inducing}[1]{\ensuremath{\hat{#1}}}

# #+LATEX_HEADER: \newcommand{\fu}{\ensuremath{\inducing{\latentFunc}}}
# #+LATEX_HEADER: \newcommand{\Fu}{\ensuremath{\inducing{\mathbf{\latentFunc}}}}
# #+LATEX_HEADER: \newcommand{\Fku}{\ensuremath{\mode{\inducing{\mathbf{\latentFunc}}}}}
# #+LATEX_HEADER: \newcommand{\Fkdu}{\ensuremath{\singleDim{\Fku}}}
# #+LATEX_HEADER: \newcommand{\hu}{\ensuremath{\inducing{\gatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hu}{\ensuremath{\inducing{\mathbf{\gatingFunc}}}}
# #+LATEX_HEADER: \newcommand{\Hku}{\ensuremath{\mode{\inducing{\mathbf{\gatingFunc}}}}}

# #+LATEX_HEADER: \newcommand{\Zfk}{\ensuremath{\mode{\mathbf{Z}}_{\latentFunc}}}
# #+LATEX_HEADER: \newcommand{\Zfk}{\ensuremath{\mode{\bm{\zeta}}}}
# #+LATEX_HEADER: \newcommand{\Zf}{\ensuremath{\mathbf{Z}}}
# #+LATEX_HEADER: \newcommand{\Zf}{\ensuremath{\mathbf{Z}_{\latentFunc}}}

# #+LATEX_HEADER: \newcommand{\Zhk}{\ensuremath{\mode{\mathbf{Z}}_{\gatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Zh}{\ensuremath{\bm{\xi}}}
# #+LATEX_HEADER: \newcommand{\Zhk}{\ensuremath{\mode{\Zh}}}

# #+LATEX_HEADER: \newcommand{\ZhDes}{\ensuremath{\modeDes{\zH}}}

*** Continuous
#+LATEX_HEADER: \newcommand{\derivative}[1]{\ensuremath{\dot{#1}}}
#+LATEX_HEADER: \newcommand{\stateDerivative}{\ensuremath{\derivative{\state}}}
# #+LATEX_HEADER: \newcommand{\stateDerivative}{\ensuremath{\dot{\mathbf{x}}}}

*** Prob Dists New
#+LATEX_HEADER: \newcommand{\pFkd}{\ensuremath{p\left(\Fkd \right)}}
*** Prob Dists
#+LATEX_HEADER: \newcommand{\pFkd}{\ensuremath{p\left(\Fkd \mid \allInput \right)}}
#+LATEX_HEADER: \newcommand{\pFk}{\ensuremath{p\left(\Fk \mid \allInput, \expertParams\right)}}

#+LATEX_HEADER: \newcommand{\pF}{\ensuremath{p\left(\F \mid \allInput, \expertParams\right)}}
#+LATEX_HEADER: \newcommand{\pfk}{\ensuremath{p\left(\fk \mid \allInput, \expertParamsK \right)}}
#+LATEX_HEADER: \newcommand{\pfknd}{\ensuremath{p\left(\fknd \mid \allInput\right)}}

#+LATEX_HEADER: \newcommand{\pFkGivenUk}{\ensuremath{p\left(\Fk \mid \uFk \right)}}
# #+LATEX_HEADER: \newcommand{\pYkGivenUk}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pYkGivenFku}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \uFk \right)}}

#+LATEX_HEADER: \newcommand{\qF}{\ensuremath{q\left(\F \right)}}
#+LATEX_HEADER: \newcommand{\qFu}{\ensuremath{q\left(\uF \right)}}
#+LATEX_HEADER: \newcommand{\qFku}{\ensuremath{q\left(\uFk \right)}}
#+LATEX_HEADER: \newcommand{\pFku}{\ensuremath{p\left(\uFk \mid \zFk \right)}}
#+LATEX_HEADER: \newcommand{\pFkuGivenX}{\ensuremath{p\left(\uFk \mid \zFk \right)}}
#+LATEX_HEADER: \newcommand{\pFuGivenX}{\ensuremath{p\left(\uF \mid \zF \right)}}
#+LATEX_HEADER: \newcommand{\qFk}{\ensuremath{q\left(\Fk \right)}}
#+LATEX_HEADER: \newcommand{\qfk}{\ensuremath{q\left(\fk \right)}}
#+LATEX_HEADER: \newcommand{\qfkn}{\ensuremath{q\left(\fkn \right)}}
#+LATEX_HEADER: \newcommand{\qfn}{\ensuremath{q\left(\fn \right)}}
#+LATEX_HEADER: \newcommand{\pFkGivenFku}{\ensuremath{p\left(\Fk \mid \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pfkGivenFku}{\ensuremath{p\left(\fkn \mid \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenFku}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pYGivenUX}{\ensuremath{p\left(\allOutput \mid \uF, \allInput \right)}}
#+LATEX_HEADER: \newcommand{\pYGivenU}{\ensuremath{p\left(\allOutput \mid \uF \right)}}


#+LATEX_HEADER: \newcommand{\pY}{\ensuremath{p\left(\allOutput \right)}}
# #+LATEX_HEADER: \newcommand{\pykGivenfk}{\ensuremath{p\left(\singleOutputK \mid \fkn \right)}}
# #+LATEX_HEADER: \newcommand{\pYkGivenFk}{\ensuremath{p\left(\allOutputK \mid \Fk \right)}}
# #+LATEX_HEADER: \newcommand{\pYkGivenX}{\ensuremath{p(\allOutputK \mid \allInput)}}
#+LATEX_HEADER: \newcommand{\pykGivenx}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenxNegF}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \singleInput, \neg\Fk \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenfk}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \fkn \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenfkd}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \fknd \right)}}
#+LATEX_HEADER: \newcommand{\pYkGivenFk}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \Fk \right)}}
#+LATEX_HEADER: \newcommand{\pYkGivenX}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \allInput \right)}}
#+LATEX_HEADER: \newcommand{\pYGivenX}{\ensuremath{p\left(\allOutput \mid \allInput \right)}}

**** Gating network
#+LATEX_HEADER: \newcommand{\PrA}{\ensuremath{\Pr\left(\ModeVarK \right)}}
#+LATEX_HEADER: \newcommand{\Pra}{\ensuremath{\Pr\left(\modeVarK \right)}}
#+LATEX_HEADER: \newcommand{\PaGivenhx}{\ensuremath{P\left(\modeVarn \mid \hn, \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\PraGivenx}{\ensuremath{\Pr\left(\modeVarn \mid \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\PraGivenhx}{\ensuremath{\Pr\left(\modeVarK \mid \hn, \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\PraGivenxNegH}{\ensuremath{\Pr\left(\modeVarK \mid \singleInput, \neg\Hall \right)}}
#+LATEX_HEADER: \newcommand{\PrAGivenX}{\ensuremath{\Pr\left(\ModeVarK \mid \allInput \right)}}

#+LATEX_HEADER: \newcommand{\pHGivenX}{\ensuremath{p\left(\Hall \mid \allInput\right)}}
#+LATEX_HEADER: \newcommand{\pHkGivenX}{\ensuremath{p\left(\Hk \mid \allInput\right)}}

*** Kernels
# #+LATEX_HEADER: \newcommand{\Kkxx}{\mode{\mathbf{K}}_{\allInput\allInput}}
#+LATEX_HEADER: \newcommand{\Kkxx}{\mode{\mathbf{K}}_{d, \allInput\allInput}}

# TO derivative kernels
#+LATEX_HEADER: \newcommand{\ddK}{\ensuremath{\partial^2\K_{**}}}
#+LATEX_HEADER: \newcommand{\dK}{\ensuremath{\partial\K_{*}}}
#+LATEX_HEADER: \newcommand{\Kxx}{\ensuremath{\K_{}}}
#+LATEX_HEADER: \newcommand{\iKxx}{\ensuremath{\Kxx^{-1}}}

#+LATEX_HEADER: \newcommand{\dKz}{\ensuremath{\partial\K_{*\zH}}}
#+LATEX_HEADER: \newcommand{\Kzz}{\ensuremath{\K_{\zH\zH}}}
#+LATEX_HEADER: \newcommand{\iKzz}{\ensuremath{\Kzz^{-1}}}
*** Desired Mode
# Function notation
#+LATEX_HEADER: \newcommand{\HDes}{\ensuremath{\MDes{\GatingFunc}}}
#+LATEX_HEADER: \newcommand{\uHDes}{\ensuremath{\MDes{\uH}}}

# Inducing points
#+LATEX_HEADER: \newcommand{\pDes}{\ensuremath{p\left( \uHDes \mid \zHDes \right)}}
#+LATEX_HEADER: \newcommand{\qDes}{\ensuremath{q\left( \uHDes \right)}}
#+LATEX_HEADER: \newcommand{\mDes}{\ensuremath{\MDes{\mathbf{m}}}}
#+LATEX_HEADER: \newcommand{\SDes}{\ensuremath{\MDes{\mathbf{S}}}}

*** Jacobian
# Single data notation
#+LATEX_HEADER: \newcommand{\singleTest}[1]{\ensuremath{#1_{\testInd}}}
#+LATEX_HEADER: \newcommand{\testInput}{\ensuremath{\singleTest{\state}}}

# Jacobian notation
#+LATEX_HEADER: \newcommand{\Jac}{\ensuremath{\mathbf{J}}}
#+LATEX_HEADER: \newcommand{\testJac}{\ensuremath{\singleTest{\Jac}}}
#+LATEX_HEADER: \newcommand{\muJac}{\ensuremath{\mu_{\Jac}}}
#+LATEX_HEADER: \newcommand{\covJac}{\ensuremath{\Sigma_{\Jac}}}

*** Old
**** Gating Network Old
# # Function notation
# #+LATEX_HEADER: \newcommand{\gatingFunc}{\ensuremath{h}}
# #+LATEX_HEADER: \newcommand{\hk}{\ensuremath{\mode{\gatingFunc}}}

# # Single data notation
# #+LATEX_HEADER: \newcommand{\hkn}{\ensuremath{\singleData{\hk}}}
# #+LATEX_HEADER: \newcommand{\hn}{\ensuremath{\singleData{\mathbf{\gatingFunc}}}}

# # All inputs set/vector/tensor notation
# #+LATEX_HEADER: \newcommand{\GatingFunc}{\ensuremath{\mathbf{\gatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\GatingFunc}}
# #+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\mode{\GatingFunc}}}
# # #+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\allData{\GatingFunc}}}
# # #+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\allData{\mode{\GatingFunc}}}}
**** Desired Mode Old
# #+LATEX_HEADER: \newcommand{\HDes}{\ensuremath{\modeDes{\GatingFunc}}}
# #+LATEX_HEADER: \newcommand{\HuDes}{\ensuremath{\modeDes{\Hu}}}
# #+LATEX_HEADER: \newcommand{\mDes}{\ensuremath{\modeDes{\mathbf{m}}}}
# #+LATEX_HEADER: \newcommand{\SDes}{\ensuremath{\modeDes{\mathbf{S}}}}

**** Experts Old
# # Function notation
# #+LATEX_HEADER: \newcommand{\latentFunc}{\ensuremath{f}}
# #+LATEX_HEADER: \newcommand{\f}{\ensuremath{f}}
# #+LATEX_HEADER: \newcommand{\fk}{\ensuremath{\mode{\latentFunc}}}
# # #+LATEX_HEADER: \newcommand{\fkd}{\ensuremath{\singleDim{\fk}}}
# #+LATEX_HEADER: \newcommand{\fkd}{\ensuremath{\singleDimMode{\f}}}

# # Single input notation
# #+LATEX_HEADER: \newcommand{\fn}{\ensuremath{\singleData{\mathbf{\latentFunc}}}}
# #+LATEX_HEADER: \newcommand{\fkn}{\ensuremath{\singleData{\mode{\mathbf{\latentFunc}}}}}
# # #+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\singleDim{\singleData{\fk}}}}
# # #+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\singleDimMode{\singleData{\f}}}}
# #+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\singleDimModeData{\f}}}

# # All inputs set/vector/tensor notation
# # #+LATEX_HEADER: \newcommand{\F}{\ensuremath{\allData{\mathbf{\f}}}}
# #+LATEX_HEADER: \newcommand{\F}{\ensuremath{\mathbf{\f}}}
# #+LATEX_HEADER: \newcommand{\Fk}{\ensuremath{\mode{\F}}}
# # #+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\singleDim{\Fk}}}
# #+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\singleDimMode{\F}}}

#+LATEX_HEADER: \newcommand{\allOutputK}{\ensuremath{\mode{\allOutput}}}
#+LATEX_HEADER: \newcommand{\singleOutputK}{\ensuremath{\mode{\singleOutput}}}

** Acronyms :noexport:
Use with
- \acrfull{gp} prints Gaussian Process (GP)
- \acrshort{gp} prints GP
- \acrlong{gp} prints Gaussian Process

{{{glossaryentry(LaTeX,\LaTeX,A document preparation system,LaTeX)}}}
{{{glossaryentry(Real Numbers,$\real$,The set of Real numbers,Real Numbers)}}}

{{{acronym(mogpe,MoGPE,Mixtures of Gaussian Process Experts)}}}
{{{acronym(moe,MoE,Mixture of Experts)}}}
{{{acronym(mosvgpe,MoSVGPE,Mixtures of Sparse Variational Gaussian Process Experts)}}}
{{{acronym(gp,GP,Gaussian Process)}}}
{{{acronym(mdp,MDP,Markov Decision Process)}}}
{{{acronym(ard,ARD,Automatic Relevance Determination)}}}
{{{acronym(ode,ODE,Ordinary Differential Equation)}}}
{{{acronym(sde,SDE,Stochastic Differential Equation)}}}
{{{acronym(elbo,ELBO,Evidence Lower Bound)}}}
{{{acronym(vae,VAE,Variational Auto-Encoder)}}}
{{{acronym(mbrl,MBRL,Model-Based Reinforcement Learning)}}}
{{{acronym(hmm,HMM,Hidden Markov Model)}}}
{{{acronym(svi,SVI,Stochastic Variational Inference)}}}
{{{acronym(soc,SOC,Stochastic Optimal Control)}}}

{{{acronym(pets,PETS,Probabilistic Ensembles with Trajectory Sampling)}}}
{{{acronym(pipps,PIPPS,Probabilistic Inference for Particle-based Policy Search)}}}
{{{acronym(pilco,PILCO,Probabilistic Inference for Learning COntrol)}}}

{{{acronym(gpssm,GPSSM,Gaussian Process State Space Model)}}}

* Frontmatter :ignore:
#+BEGIN_EXPORT latex
\frontmatter
#+END_EXPORT
** Title Page :ignore:noexport:

#+BEGIN_EXPORT latex
\begin{titlepage}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % First page: Thesis Title and Author Name
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  % Uncomment when adding the background figure to the cover.
  \BgThispage

  \cleardoublepage
  \pagestyle{empty}

  \begin{center}
    \null\vfill
    {\huge{\bfseries \ThesisTitle}\par}
    \vspace{\stretch{0.5}}
    {\large \ThesisSubTitle \par}
    \vspace{\stretch{2}}
    \vspace{\baselineskip}
    {\large By \AuthorFullName\par}
    \vspace{\stretch{2}}
    %\vspace{\baselineskip}
    %\vspace{\baselineskip}
    \vspace{\baselineskip}
    \includegraphics[scale=0.6]{./logos/bristolcrest_colour}
    \hspace{5mm}
    \includegraphics[scale=0.35]{./logos/UWE_insignia.png}\\
    \vspace{10mm}
    {\large Department of Aerospace Engineering\\
     \textsc{University of Bristol}}
     \\
     \&
     \\
     {\large Department of Engineering Design and Mathematics\\
     \textsc{University of the West of England}}\\

    %{\large Faculty of Engineering\\
    %\textsc{University of Bristol}}\\
    %\vspace{6mm}
    \vspace{\baselineskip}
    \vspace{\baselineskip}
    \begin{minipage}{10cm}
      A dissertation submitted to the University of Bristol and the University of the West of England in accordance with the requirements of the degree of \textsc{Doctor of Philosophy} in the Faculty of Engineering.
    \end{minipage}\\
     \vspace{\baselineskip}
    % \vspace{\stretch{1}}
    \vspace{\baselineskip}
    \vspace{\stretch{1}}
    \noindent
    \begin{tabular}{@{}l@{\hspace{22pt}}ll}
      \textbf{Supervisors}:          & Prof.\ Arthur Richards\\
                                     & Dr.\ Carl Henrik Ek\\
    \end{tabular} \\
    %\vspace{\stretch{1}}
    %\vspace{\baselineskip}
    %\vspace{\baselineskip}
    \vspace{9mm}
    {\large\textsc{January 2022}}
    \vspace{12mm}
    \vfill
  \end{center}

  \cleardoublepage
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  % End Titlepage
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{titlepage}
#+END_EXPORT

** Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:
#+BEGIN_EXPORT latex
\begin{SingleSpace}
%\initial{R}einforcement learning and data-driven control have seen significant advances over the last decade,
%especially in simulated environments.
%Real world systems are often highly nonlinear, exhibit stochasticity and multimodalities,
%are expensive to run (slow, energy intensive, subject to wear and tear) and
%must be controlled subject to constraints (for safety, efficiency, etc).

%From robotics, to industrial processing, to finanace, learning-based approaches to control
%help alleviate the dependence on domain exerts for system identification and controller design.
This dissertation is concerned with \textit{learning} and \textit{control}
in unknown, (or partially unknown), multimodal dynamical systems.
It is motivated by controlling robotic systems in uncertain environments,
where both the underlying dynamics modes,
and how the dynamics switches between them, are \textit{not fully known a priori}.

%For example, controlling a quadcopter subject to inoperable dynamics modes that are
%induced via spatially varying turbulence
%i.e. fly a quadcopter to a target location, whilst remaining in the operable (non turbulent) dynamics mode.

%This dissertation is concerned with \textbf{learning} and \textbf{control}
%in unknown, (or partially unknown), multimodal dynamical systems.
%It is motivated by controlling a quadcopter with inoperable dynamics modes that are
%induced via spatially varying turbulence.
%The operable mode corresponds to regions of the state space subject to \textbf{low turbulence}, and the
%inoperable mode(s) corresponds to regions subject to \textbf{high turbulence}.
%The goal is to fly the quadcopter from an initial state in the desired (operable) dynamics mode,
%to a target state, whilst remaining in the desired dynamics mode.

This dissertation first considers learning representations of multimodal dynamical systems, assuming
access to a historial data set of state transitions.
\todo{add comment about MoGPE vs SVGP}
The model resembles the Mixture of Gaussian Process Experts model with a gating network based on Gaussian processes.
Motivated by synergising model learning and control,
this model infers latent \textit{geometric} structure in the gating network,
that is later exploited by a geometry inspired control algorithm.
Well-calibrated uncertainty estimates and scalability are obtained via
stochastic variational inference.
%variational lower bound based on sparse approximations, that can be optimised with
%stochastic gradient methods.
%A novel variational lower bound based on sparse approximations, that can be optimised with
%stochastic gradient methods, is derived.
%It provides scalability as well as well-calibrated uncertainty estimates.

%Secondly, this work considers trajectory optimisation algorithms,
%that exploit the learned dynamics model to achieve the aformentioned goal.
%In a \textbf{risk-averse setting}, it is also desirable to avoid entering regions of a learned dynamics model with
%high \textit{epistemic uncertainty}.
%This is because the state-control trajectory cannot be predicted confidently, and thus,
%constraints may be violated i.e. the system may enter inoperable dynamics modes.
%Still assuming access to a historical data set, the first approach presented in this dissertation
%exploits concepts of Riemannian geometry (extended to probabilistic geometries) to encode the trajectory optimisation
%goals into an objective function.
%A second, alternative approach, formulates the control problem as probabilistic inference
%in a graphical model by conditioning on a mode assignment variable.
%Both methods are evaluated via experiments on a simulated quadcopter, as well as a data set of a
%DJI Tello quadcopter flying in the Bristol Robotics Laboratory.

Secondly, this dissertation considers driving a dynamical system from an initial state (in a desired dynamics mode),
to a target state, whilst remaining in the desired dynamics mode.
For example, consider controlling a quadcopter in an environment subject to two dynamics modes: 1) a turbulent
dynamics mode in front of a fan, and 2) a non turbulent dynamics mode everywhere else.
The goal in this scenario is to fly the quadcopter to a target location,
whilst remaining in the operable (non turbulent) dynamics mode.

In a \textbf{risk-averse setting}, it is desirable to avoid entering regions of a learned dynamics model with
high \textit{epistemic uncertainty}, as well as remaining in the desired dynamics mode.
This is because the trajectory cannot be predicted confidently, and may leave the operable dynamics mode.
Given a partially learned dynamics model, this dissertation develops two trajectory optimisation algorithms
aimed at solving this risk-averse setting.
The first approach
exploits concepts of Riemannian geometry (extended to probabilistic geometries) to encode both of the goals
into a geometry inspired objective function.
The second approach formulates the control problem as probabilistic inference
in a graphical model, and encodes the goals by conditioning on a mode assignment variable.
Both methods are evaluated via experiments on a simulated quadcopter, as well as a data set collected onboard
a DJI Tello quadcopter.
%A second, alternative approach is also presented.
%Instead of exploiting the geometry of the learned model, it formulates the control problem as probabilistic inference
%in a graphical model by conditioning on a mode assignment variable.
%Based on these two goals, this dissertation develops two trajectory optimisation algorithms that exploit
%the learned dynamics to achieve them.

Finally, this dissertation considers the active learning setting, where it does
not assume access to a historical data set.
To achieve this goal, a constrained exploration algorithm is introduced.
The algorithm exploits the \textit{epistemic uncertainty} associated with the learned model, to guide
exploration into regions of the dynamics that have not previously been observed.
This experience can then be used to update the model and reduce the associated \textit{epistemic uncertainty}.
Exploration is subject to chance constraints that prevent the system from leaving the desired dynamics
mode, resulting in an overconstrained problem.
Loosening the chance constraints enables the algorithm to incrementally explore the environment,
becoming more confident in the dynamics,
until it can find a trajectory to the target state that does not violate the chance constraints.



\end{SingleSpace}
#+END_EXPORT

** Declaration :noexport:
:PROPERTIES:
:UNNUMBERED: t
:END:
#+BEGIN_EXPORT latex
\begin{SingleSpace}
\begin{quote}
\initial{I} declare that the work in this dissertation was carried out in accordance with the requirements of  the University's Regulations and Code of Practice for Research Degree Programmes and that it  has not been submitted for any other academic award. Except where indicated by specific  reference in the text, the work is the candidate's own work. Work done in collaboration with, or with the assistance of, others, is indicated as such. Any views expressed in the dissertation are those of the author.

\vspace{1.5cm}
\noindent
\hspace{-0.75cm}\textsc{SIGNED: .................................................... DATE: ..........................................}
\end{quote}
\end{SingleSpace}
#+END_EXPORT

** Acknowledgements :noexport:
:PROPERTIES:
:UNNUMBERED: t
:END:
#+BEGIN_EXPORT latex
\begin{SingleSpace}
\initial{H}ere goes the dedication.
\end{SingleSpace}
#+END_EXPORT
* TOC and Mainmatter :ignore:
#+BEGIN_EXPORT latex
\tableofcontents
% This ensures that the subsequent sections are being included as root
% items in the bookmark structure of your PDF reader.
\begingroup
    \let\clearpage\relax
    \glsaddall
    \printglossary[type=\acronymtype]
    \newpage
    \printglossary
\endgroup
\printindex

\mainmatter
#+END_EXPORT

* Testing Maths Variables :noexport:
** Tables :ignore:
#+CAPTION: Variables
| Name                    | Symbol     | Equation                                                   |
|-------------------------+------------+------------------------------------------------------------|
| State                   | $\state$   | $\R^{\StateDim}$                                           |
| Control                 | $\control$ | $\R^{\ControlDim}$                                         |
| Time                    | $t$        | $\R$                                                       |
| State-action input      | $\x$       | $(\state, \control) \in \R^{\StateDim \times \ControlDim}$ |
| State difference        | $\y$       | $\state_{t} - \state_{t-1} \in \R^{\StateDim}$             |
| Mode indicator variable | $\modeVar$ | $\{1,\ldots,\ModeInd\}$                                    |
|                         |            |                                                            |

#+CAPTION: Variables at single data points
| Name                    | Symbol           | Equation                                                          |
|-------------------------+------------------+-------------------------------------------------------------------|
| State                   | $\singleState$   | $\R^{\StateDim}$                                                  |
| Control                 | $\singleControl$ | $\R^{\ControlDim}$                                                |
| State-Action input      | $\singleInput$   | $(\singleState, \singleControl) \in \R^{\StateDim + \ControlDim}$ |
| State Difference        | $\singleOutput$  | $\R^{\StateDim}$                                                  |
| Mode indicator variable | $\modeVarn$      | $\{1,\ldots,\ModeInd\}$                                           |

#+CAPTION: Variables at all data points
| Name                    | Symbol        | Equation                                                                      |
|-------------------------+---------------+-------------------------------------------------------------------------------|
| State                   | $\allState$   | $\R^{\NumData \times \StateDim}$                                              |
| Control                 | $\allControl$ | $\R^{\NumData \times \ControlDim}$                                            |
| State-Action input      | $\allInput$   | $(\allState, \allControl) \in \R^{\NumData \times (\StateDim + \ControlDim)}$ |
| State Difference        | $\allOutput$  | $\R^{\NumData \times \StateDim}$                                              |
| Mode indicator variable | $\ModeVarK$   | $\{\singleData{\modeVar}=k\}_{\numData=1}^{\NumData}$                         |

#+CAPTION: Gating network notation
|                | Name                                   | Symbol        | Equation                                                                         |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| Function       | Gating function k                      | $\hk$         | $\hk : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R$                    |
|                | Gating function                        | $\gatingFunc$ | $\gatingFunc : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R^{\ModeInd}$ |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| $\singleInput$ | Gating function k at $\singleInput$    | $\hkn$        | $\hk(\singleInput) \in \R$                                                       |
|                | Gating function at $\singleInput$      | $\hn$         | $\gatingFunc(\singleInput) \in \R^{\ModeInd}$                                    |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| $\allInput$    | Gating function k                      | $\Hk$         | $\hk(\allInput) \in \R^{\NumData}$                                               |
|                | Gating functions                       | $\Hall$       | $\gatingFunc(\allInput) \in \R^{\NumData \times \ModeInd}$                       |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| $\zH$          | Inducing variables - gating function k | $\uHk$        | $\hk(\zHk) \in \R^{\NumInd}$                                                     |
|                | Inducing variables - gating functions | $\uH$         | $\h(\zH) \in \R^{\NumInd \times \ModeInd}$                                       |


#+CAPTION: Transition dynamics function notation
|                | Name                                    | Symbol  | Equation                                                                                 |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Dimension d of mode k                   | $\fkd$  | $\fkd : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R$                           |
| Function       | Mode k                                  | $\fk$   | $\fk : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R^{\StateDim}$                |
|                | All modes function                                          | $\f$    | $\f : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R^{\ModeInd \times \StateDim}$ |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Dimension d mode k                      | $\fknd$ | $\fkd(\singleInput) \in \R$                                                              |
| $\singleInput$ | Mode k                                  | $\fkn$  | $\fk(\singleInput) \in \R^{\StateDim}$                                                   |
|                | All modes                               | $\fn$   | $\f(\singleInput) \in \R^{\ModeInd \times \StateDim}$                                    |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Dimension d mode k                      | $\Fkd$  | $\fkd(\allInput) \in \R^{\NumData}$                                                      |
| $\allInput$    | Mode k                                  | $\Fk$   | $\fk(\allInput) \in \R^{\NumData \times \StateDim}$                                      |
|                | All modes                               | $\F$    | $\f(\allInput) \in \R^{\NumData \times \ModeInd \times \StateDim}$                       |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Inducing variables - dimension d mode k | $\uFkd$ | $\fkd(\zFkd) \in \R^{\NumInd}$                                                           |
| $\zF$          | Inducing variables - mode k             | $\uFk$  | $\fk(\zFk) \in \R^{\NumInd \times \StateDim}$                                            |
|                | Inducing variables - all modes          | $\uF$   | $\f(\zF) \in \R^{\NumInd \times \ModeInd \times \StateDim}$                              |
** Experts
GP prior over each output dimension $d$ for each dynamics mode $k$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior-single-dim}
p\left(\Fkd \mid \allInput \right) &= \mathcal{N}\left( \Fkd \mid \singleDimMode{\mu}(\allInput), \singleDimMode{k}(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
Assume each output dimension is independent,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior}
\pFk &= \prod_{\stateDim=1}^{\StateDim} \pFkd
\end{align}
#+END_EXPORT
Assume each dynamics mode $k$ is independent,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-prior}
\pF &= \prod_{k=1}^{K} \pFk
\end{align}
#+END_EXPORT
The process noise in each mode is modelled as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-likelihood}
\pYkGivenFk = \prod_{\numData=1}^{\NumData} \pykGivenfk &= \prod_{\numData=1}^{\NumData} \mathcal{N}\left( \singleOutput \mid \fkn, \text{diag}\left[ \left(\noiseVarOneK\right)^{2}, \ldots, \left( \noiseVarDK \right)^{2} \right]  \right)
\end{align}
#+END_EXPORT
where $\noiseVardK$ represents the noise variance associated with the $d$^{\text{th}} dimension of the $k$^{\text{th}} mode.

Each expert is then given by marginalising its associated latent function values,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert}
\pYkGivenX = \int  \pYkGivenFk \pFk \text{d} \Fk
\end{align}
#+END_EXPORT

The dynamics modes are combined via a distribution over the mode indicator variable $\modeVar$.
The resulting marginal likelihood is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-prior}
\pYGivenX = \sum_{\modeInd=1}^{\ModeInd} \Pr(\ModeVarK) \pYkGivenX
\end{align}
#+END_EXPORT

** Mixture of Experts
Mixture model marginal likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixture-marginal-likelihood}
\pYGivenX = \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \Pr(\modeVarK) p(\singleOutput \mid \modeVarn=\modeInd, \singleInput)
\end{align}
#+END_EXPORT
Mixture of experts marginal likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-moe-marginal-likelihood}
\pYGivenX &= \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \PraGivenx \pykGivenx
\end{align}
#+END_EXPORT

** Gating Network
This work is interested in transition dynamics where the governing mode varies over the input domain

This work specifies a probability mass function over the mode indicator variable that is governed by a set of input-dependent
latent functions. These model how the transition dynamics switch between modes over the input domain.
In the literature they are commonly referred to as gating functions.
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mode-indicator-dist}
\PaGivenhx = \prod_{\modeInd=1}^{\ModeInd} \PraGivenhx^{[\modeVarn = \modeInd]},
\end{align}
#+END_EXPORT
The probabilities $\Pr(\modeVarn=\modeInd \mid \hn )$ are obtained by normalising the outputs of all the gating functions, e.g.
$\text{softmax}(\hn)$.
Following a Bayesian formulation independent GP priors are placed on each of the gating functions,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-funcs-prior}
\pHGivenX = \prod_{\modeInd=1}^{\ModeInd} \pHkGivenX
= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \Hk \mid \mode{\mu}(\allInput), \mode{k}(\allInput, \allInput) \right).
\end{align}
#+END_EXPORT
Each GP models the epistemic uncertainty associated with its gating function.
The probabilities $\PraGivenx$ associated with the probability mass function over
the mode indicator variable are then obtained by marginalising the
latent gating functions,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-indicator-mult}
\PraGivenxNegH
&= \int \text{softmax}_k(\hn) p(\hn \mid \singleInput, \neg\Hall) \text{d} \mathbf{h}_t.
\end{align}
#+END_EXPORT
This equation integrates out the uncertainty associated with the gating functions.
High variance in the gating function GPs tends the distribution over the mode indicator variable
to a uniform distribution.

** Marginal Likelihood
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-expert}
\pykGivenxNegF = \pyk
\end{align*}
#+END_EXPORT

Our marginal likelihood can be written with the same factorisation as the \acrshort{moe}
marginal likelihood in Equation ref:eq-moe-marginal-likelihood,

#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood}
\pYGivenX &= \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \underset{\text{Mixing probability}}{\PraGivenxNegH} \underset{\text{Dynamics mode } k}{\pykGivenxNegF}
\end{align}
#+END_EXPORT
The
$\PraGivenxNegH$ contains $\ModeInd$ GP conditionals with complexity

$\pykGivenxNegF$ contains a GP conditional with complexity

** Inference

* lit review maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{\MakeUppercase{\timeInd}}}

\renewcommand{\allInput}{\ensuremath{\hat{\state}_{1:\TimeInd}}}
\renewcommand{\allOutput}{\ensuremath{{\Delta\state}_{1:\TimeInd}}}

\newcommand{\stateDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\controlDomain}{\ensuremath{\mathcal{U}}}

\newcommand{\dynamicsFunc}{\ensuremath{f}}

\newcommand{\costFunc}{\ensuremath{c}}
\newcommand{\terminalCostFunc}{\ensuremath{C_T}}
\newcommand{\integralCostFunc}{\ensuremath{C}}
\newcommand{\constraintsFunc}{\ensuremath{g}}

\newcommand{\stateTraj}{\ensuremath{\bar{\state}}}
\newcommand{\controlTraj}{\ensuremath{\bar{\control}}}

\newcommand{\policySpace}{\ensuremath{\Pi}}
\newcommand{\policy}{\ensuremath{\pi}}

\renewcommand{\u}{\ensuremath{\mathbf{u}}}
#+END_EXPORT

* Introduction
** maths :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\targetState}{\ensuremath{\state_f}}
#+END_EXPORT
** Intro :ignore:
# *Dynamical Systems*
# The modern world is pervaded with dynamical systems that we seek to control to achieve a desired behaviour.
# Examples include autonomous vehicles, aircraft, robotic manipulators, financial markets and energy management systems.
# In the last decade, learning-based control citep:hewingLearningBased2020,sutton2018reinforcement has become
# a popular paradigm for controlling dynamical systems.
# This can be accounted to significant improvements in sensing and computational capabilities as well as
# recent successes in machine learning.

# with the /hope/ of addressing some of the shortcomings associated with pure control theoretic approaches?
# Perhaps with the /hope/ of addressing some of the shortcomings associated with pure control theoretic approaches.
# This can be accounted to recent successes in machine learning and the hope of
# and significant improvements in sensing and computational capabilities.

# Modern artificial intelligence seeks solutions that allow machines to /understand/ and /learn/.
# In the field of machine learning, these challenges are often solved using probabilistic models.
# In this setting, /understanding/ is achieved by separating signal from noise and removing redundancies in
# complex and noisy data.
# Leveraging learned models for control requires the models to extrapolate beyond training observations.
# That is, they must enable the machine to /reason/ about previously unseen inputs.
# In the field of machine learning, /understanding/ is achieved by separating signal from noise and removing redundancies in
# complex and noisy data.
# Leveraging learned models for control requires the models to extrapolate beyond training observations.
# That is, they must be able to /reason/ about previously unseen inputs.



# 1. Learning the system dynamics: model based control strategies rely on suitable and sufficiently accurate model
#    representations of the system dynamics. A promising approach is to learn /unknown/, or /partially unknown/
#    dynamics from observations. This enables control in previously uncontrollable systems, and can improve control
#    by learning (and accounting) for any model errors.
# 2. Learning the controller design:

# (coming from both the reinforcement learning
# cite:sutton2018reinforcement and control theory \todo{cite control theory book?} communities),



The modern world is pervaded with dynamical systems that we seek to control to achieve a desired behaviour.
Examples include autonomous vehicles, aircraft, robotic manipulators, financial markets and energy management systems.
In the last decade, reinforcement learning, and learning-based
control in general, have become
popular paradigms for controlling dynamical systems citep:hewingLearningBased2020,sutton2018reinforcement.
This can be accounted to significant improvements in sensing and computational capabilities, as well as
recent successes in machine learning.
# From robotics, to industrial processing, to finance, learning-based control
# offers promise of solving problems that could not previously be solved with purely control
# theoretic approaches.

This growing interest in learning-based control
\parmarginnote{real-world systems}
has emphasised the importance for real-world considerations.
Real-world systems are often highly /nonlinear/, exhibit /stochasticity/ and /multimodalities/,
are expensive to run (energy intensive, subject to wear and tear) and
must be controlled subject to /constraints/ (for safety, efficiency, and so on).
In contrast to simulation, the control of physical systems also has real-world consequences:
components may get damaged, the system may damage its environment, or the system may catastrophically fail.
As such, any learning-based control strategy deployed in the real-world should handle both the uncertainty inherent
to the environment, and the uncertainty introduced by learning from observations.
# As such, any control strategy deployed in the real-world should ensure the safety of itself and
# its surrounding environment.
# Therefore, any control strategy deployed in the real-world should ensure the safety of itself and
# its surrounding environment.

Many dynamical systems exhibit multimodalities (in their transition dynamics), where some dynamics modes
\parmarginnote{multimodal systems}
are believed to be /inoperable/ or /undesirable/.
These multimodalities may be due to spatially varying model parameters, for example,
process noise terms modelling aircraft turbulence, or friction coefficients modelling
surface-tyre interactions for ground vehicles over different terrain.
In these systems, it is desirable to avoid entering specific dynamics mode that are believed to be /inoperable/.
Perhaps they are hard to control due to stability, or the mode switch itself is hard to control.
Or perhaps it is desirable to avoid specific modes for /efficiency/ or /performance/ reasons.
Given these motivations, this dissertation is interested in control techniques for
driving dynamical systems from an initial state,
to a target state, whilst avoiding specific dynamics modes.

# In particular, it is primarily interested in controlling a quadcopter in an environment
# subject to two dynamics modes: 1) a turbulent
# dynamics mode in front of a fan, and 2) a non turbulent dynamics mode everywhere else.
# The objective in this environment is to control the quadcopter, whilst navigating to a target location,
# and remaining in the operable (non turbulent) dynamics mode.

Model-based control comprises a powerful set of techniques for finding controls of /constrained/ dynamical
\parmarginnote{model-based control}
systems, given a transition dynamics model describing the evolution of the controlled system.
It is commonly used for controlling aircraft, robotic manipulators, and walking
robots citep:vonstrykDirect1992,bettsSurvey1998,gargUnified2010.
One caveat is that it requires a relatively accurate mathematical model of the system.
Traditionally, these mathematical models are built using first principles based on physics.
However, accurately modelling the underlying transition dynamics can be challenging and
lead to the introduction of model errors.
For example, incorrectly specifying model parameters, or
models themselves (modelling a nonlinear system to be linear, or a multimodal system to be unimodal).
Incorrectly specifying these model parameters (and their associated uncertainty)
can have a detrimental impact on controller performance,
and is an active area of research in the robust
and stochastic optimal control communities citep:freemanRobust1996,stengelStochastic1986.
# For example, model parameters might be,
# 1) hard to specify accurately, e.g. friction coefficients associated with surfaces,
# 2) assumed constant when in reality they vary spatially, e.g. process noise terms modelling the effect of
#    turbulence on aircraft,
# 3) assumed constant when in reality they vary with time, e.g. mass reducing due to fuel consumption.

The difficulties associated with constructing mathematical representations of dynamical systems
\parmarginnote{learning dynamics models}
can be overcome by learning from observations citep:ljungSystem1999.
Learning dynamics models has the added benefit that it
alleviates the dependence on domain experts for specifying accurate models, in turn making it easier to
deploy more general techniques.
However, learning dynamics models for control introduces other difficulties.
For example, it is important to know
where the model cannot predict confidently due to a lack of training observations.
This concept is known as /epistemic uncertainty/ and is reduced in the limit of infinite data.
Correctly quantifying uncertainty is crucial for intelligent decision-making.
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Epistemic Uncertainty}
is the uncertainty attributed to incomplete knowledge about a phenomenon that limits our ability to model it.
It manifests in parameters taking a range of values, there being a range of viable models,
the level of modelling detail, and statistical confidence.
It can be reduced by the accumulation of additional information.
%In machine learning, \textit{epistemic uncertainty} arises due to lack of training observations and model mis-specification.
%This dissertation, uses the notion of \textit{epistemic uncertainty} to refer to a
%models lack of knowledge due to limited training data.
\end{myquote}
#+END_EXPORT

In a *risk-averse setting*, it is desirable for control strategies
to avoid entering regions of a learned dynamics model with high /epistemic uncertainty/.
\parmarginnote{decision-making under uncertainty}
This is because it is impossible to guarantee /constraint/ satisfaction in a learned model, i.e.
if the trajectory will avoid the undesired dynamics mode.
Conversely, in an *explorative setting*,
if the /epistemic uncertainty/ has been quantified, then it can be used to guide exploration into
regions of the dynamics that have not previously been observed.
This experience can then be used to update the model and reduce its associated /epistemic uncertainty/.

These two settings are the main concepts explored in this dissertation.
If the dynamics are not fully /known a priori/, an agent will not be able to confidently plan a
risk-averse trajectory to the target state.
How can the agent explore its environment, in turn reducing the /epistemic uncertainty/
associated with its dynamics model?
It is assumed that complete knowledge of the transition dynamics and how they
switch between modes is /not known a priori/.
Therefore, it is interested in jointly inferring the mode /constraints/ alongside the underlying dynamics modes,
through repeated interactions with the system.
Once the agent has explored enough, how can the learned model be exploited to plan risk-averse trajectories
that attempt to remain in a desired dynamics mode, and in regions of the learned
dynamics with low /epistemic uncertainty/?

# More generally, this dissertation is interested in learning and control in multimodal dynamical systems.
# In Chapter ref:chap-dynamics, it starts by formulating learning as approximate Bayesian inference in
# a probabilistic representation of the transition dynamics.
# Assuming access to a historical data set of state transitions from the entire domain, Chapter ref:chap-traj-opt
# develops two risk-averse trajectory optimisation algorithms.
# Given the dynamics model from Chapter ref:chap-dynamics, trained on these observations,
# these algorithms are capable of finding trajectories that remain in the desired dynamics mode, whilst avoiding
# regions of the learned dynamics with high /epistemic uncertainty/.
# Finally, Chapter ref:chap-active-learning addresses the question of active learning, i.e. incrementally
# learning the dynamics model through repeated interactions with the environment.

# It assumes that complete knowledge of the transition dynamics and how they
# switch between modes is /not known a priori/.
# In essence, the system /constraints/ on the mode are /not known a priori/ and need to be inferred from observations.
# Therefore, it is interested in jointly inferring the /constraints/ alongside the underlying dynamics modes,
# through repeated interactions with the system.

# certain enough in its dynamics, it will be able to plan a risk-averse trajectory
# Once the agent is certain enough in its dynamics, it will be able to plan a risk-averse trajectory
# to the target state.

# Collectively, these two settings are known as the exploration-exploitation trade-off,
# which is well-known in the reinforcement learning and optimal control communities.


# As a result, it is interested in learning dynamics models incrementally, without violating some notion
# of constraint on the dynamics modes.



# This dissertation is interested in learning dynamics models for multimodal dynamical systems with /unknown/, or
# /partially unknown/, transition dynamics, where safety is governed by the underlying dynamics modes.
# It is assumed that complete knowledge of how the transition dynamics switch between modes is also /not known a priori/.
# In essence, the safety constraints are /not known a priori/ and should be learned from observations.
# Therefore, this dissertation is interested in jointly inferring the /constraints/ alongside the underlying dynamics modes,
# through repeated interactions with the system.


# Safe control in systems where the environment is /known a priori/
# has been well studied by the control and formal methods communities.
# However, safe control when the environment is /not known a priori/ has been less well studied.
# Recent work by cite:berkenkampSafe2019 attempts to address safe learning-based control in the
# reinforcement learning setting, where the environment is /not known a priori/.
# In cite:schreiterSafe2015 the authors address active learning in Gaussian process dynamics models, by
# deploying a binary GP classification model that indicates safe and unsafe regions.
# # Although this dissertation is not directly interested in safe control,
# # their work on constrained exploration is particularly relevant.


# It is important to note that the definition of safety varies,
# and should be defined on a system by system basis;
# cite:berkenkampSafe2019 define safety in terms of closed-loop
# stability but it is also common to define safety in terms of constraint satisfaction (on the states and controls).
# They define safety in terms of stability and exploit Lyapanov functions to construct a safe RL framework.
# Other approaches


# This dissertation is motivated by controlling dynamical systems that exhibit multimodalities, which are
# either unknown, or partially unknown.
# Incorrectly specifying a multimodal parameter as unimodal, or incorrectly specifying how a multimodal
# parameter switches between modes will have a detrimental impact on controller performance.
# In some cases, it may even lead to catastrophic failure.

** Illustrative Example label:illustrative_example
*** intro :ignore:
# The methods developed throughout this dissertation are evaluated on an illustrative 2D quadcotper navigation example.
The methods developed throughout this dissertation are motivated by a 2D quadcopter navigation example.
See cref:fig-problem-statement for a schematic of the environment and details of the problem.
The goal is to fly the quadcopter from an initial state $\state_0$, to a target state $\state_{f}$.
However, it considers a quadcopter operating in an environment subject to spatially varying wind --
induced by a fan -- where the system can be represented by two dynamics modes,
- Mode 1 :: an inoperable, /turbulent/ mode in front of the fan,
- Mode 2 :: a /non-turbulent/ mode everywhere else.
The turbulent dynamics mode is subject to higher drift (in the negative $x$ direction) and
to higher diffusion (aka process noise).
It is hard to know the exact turbulent dynamics due to complex and uncertain interactions between the
quadcopter and the wind field.
Further to this, it is believed that controlling the system in the turbulent dynamics mode may be infeasible.
This is because the unpredictability of the turbulence may cause catastrophic failure.
Therefore, when flying the quadcopter from an initial state $\state_0$, to a target state $\state_{f}$
it is desirable to find trajectories that avoid entering this turbulent dynamics mode.
However, the underlying dynamics modes and how the system switches between them, are \textit{not fully known a priori}.
To this end, this dissertation is interested in synergising model learning and model-based control (or planning),
to solve this quadcopter navigation problem.

#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
%\includegraphics[width=1.0\textwidth]{./images/point-mass-problem-statement-scenario-4.pdf}
\includegraphics[width=1.0\textwidth]{./images/quadcopter-domain-collocation-ppt.png}
\caption{\label{fig-problem-statement}\textbf{Quadcopter navigation problem}
Diagram showing a top down view of an environment, representing a quadcopter
subject to two dynamics modes: 1) a turbulent dynamics mode
induced by a fan (green) and 2) a non-turbulent dynamics mode everywhere else (blue).
The goal is to find trajectories from a start state $\state_0$, to the target state $\targetState$ (red star),
whilst avoiding the turbulent dynamics mode.}
\end{figure}
#+END_EXPORT


# Further to this, controlling the turbulent dynamics mode is believed to be difficult, as it may
# lead to catastrophic failure, due to the high process noise.

# The methods developed throughout this dissertation are evaluated on an illustrative 2D quadcotper example.
# The example considers a quadcopter operating in an environment subject to spatially varying turbulence (induced by a fan).
# It is hard to know the exact transition dynamics due to complex and uncertain
# interactions between the quadcopter and the fan.
# The system's transition dynamics can be represented by two dynamics modes,
# - Mode 1 :: a /turbulent/ mode in front of the fan,
# - Mode 2 :: a /non-turbulent/ mode everywhere else.
# However, the underlying dynamics modes, and how the dynamics switches between them, are \textit{not fully known a priori}.
# Fig. ref:fig-problem-statement shows a graphical representation of this environment.

# #+BEGIN_EXPORT latex
# \begin{figure}[!h]
# \centering
#   %\includegraphics[width=0.6\columnwidth]{images/quadcopter_bimodal_domain.pdf}
#   \includegraphics[width=0.9\columnwidth]{images/quadcopter-domain-ppt.png}
#   \caption{
#   Diagram showing a top down view of the environment for the 2D quadcopter example.
#   The quadcopter is subject to two dynamics modes: 1) a \textit{turbulent}
#   dynamics mode induced by a fan (green), and, 2) a \texit{non-turbulent} dynamics mode everywhere else (blue).}
# %The goal is learn a factorised representation of the underlying dynamics modes to find trajectories between $\mathbf{x}_0$ and $\mathbf{x}_f$ that either prioritise
# %remaining in the non-turbulent mode or prioritise avoiding regions of the learned dynamics
# %with high epistemic uncertainty due to lack of training observations.}
# \label{fig-problem-statement}
# \end{figure}
# #+END_EXPORT

*** Motivation label:to_motivation :noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{./images/traj-opt/mode-remaining/baseline-svgp-traj-opt/trajectories_over_desired_gating_mask.pdf}
\subcaption{SVGP dynamics model trained on state transitions sampled from \textbf{both} dynamics modes.}
\label{fig-traj-opt-over-gating-mask-svgp-all-baseline}
\end{minipage}
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{./images/traj-opt/mode-remaining/baseline-traj-opt/trajectories_over_desired_gating_mask.pdf}
\subcaption{SVGP dynamics model trained on state transitions sampled from \textbf{only the desired} (operable) dynamics mode.}
\label{fig-traj-opt-over-gating-mask-svgp-desired-baseline}
\end{minipage}
\caption{Trajectory optimisation results obtained using a SVGP dynamics model trained on different data sets.
The optimised control trajectories are rolled out in the SVGP dynamics model (magenta) and in the environment (cyan).
The state trajectories are overlayed on the gating mask, which indicates where each mode governs the dynamics.
White indicates the turbulent dynamics mode and red indicates the desired (operable) mode.
The hashed box indicates a subset of the environment that has not been observed.}
\label{fig-traj-opt-over-gating-mask-svgp-baseline}
\end{figure}
#+END_EXPORT
Given a historical data set of state transitions from the environment,
a common approach is to first learn a single-step dynamics model using a Gaussian process
citep:deisenrothPILCO2011,doerrOptimizing2017,vinogradskaStability2016,rohrProbabilistic2021,hewingLearningBased2020,kollerLearningBased2018.
This learned dynamics model can then be leveraged for model-based control.

cref:fig-traj-opt-over-gating-mask-svgp-all-baseline demonstrates the shortcomings of performing trajectory
optimisation in a GP dynamics model trained on state transitions sampled from *both* dynamics modes.
It shows that the GP is not able to learn a representation of the dynamics which is true to the underlying system.
This is because it cannot model the multimodal behaviour present in the environment.
The optimised controls successfully drive the GP dynamics from the start state $\state_0$,
to the target state $\targetState$.
However, as the GP dynamics model does not correspond to the true underlying dynamics,
the optimised controls do not successfully navigate to the target state $\targetState$ in the environment.
This example demonstrates the issues associated with learning a dynamics model that cannot model the discontinuities
associated with multimodal transition dynamics.
Further to this, it demonstrates the more general shortcomings of model-based control when leveraging learned
dynamics models that cannot represent the true underlying dynamics.
# This motivates cref:chap-dynamics that looks at learning representations of multimodal dynamical systems.
# This example demonstrates the issues associated with learning a dynamics model that cannot correctly identify
# the underlying dynamics modes.
# without deploying a technique that can infer both the underlying dynamics modes
# and how the system switches between them.
# The optimised controls are rolled out in the SVGP dynamics model (magenta)
# and in the environment (cyan) to highlight the issues with both approaches.

In contrast,
cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline shows results after training on state transitions from
only the desired (operable) dynamics mode.
It is able to accurately predict state transitions in the desired dynamics mode.
However, as this approach only considers the dynamics of the desired mode,
trajectories in the environment deviate from those planned in the learned dynamics model when they pass through the
turbulent mode.
This approach is problematic because the trajectory does not reach the target state $\targetState$,
nor does it avoid the turbulent dynamics mode, which may lead to catastrophic failure.
It is also worth noting here that the underlying dynamics modes and how the system switches between them,
are /not fully known a priori/.
As such, partitioning the data set and learning the dynamics model in
cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline is not possible in realistic scenarios.

#+BEGIN_EXPORT latex
\begin{myquote}
Following standard methodologies, these trajectories were found by minimising the expected cost
under the state distribution, resulting from cascading single-step predictions through the GP dynamics model.
A cost function consisting of a terminal state cost term ensures that trajectories end at the target state.
A quadratic integral control cost term was used to regularise the controls to encode the notion of
"minimal effort" trajectories.
\end{myquote}
#+END_EXPORT

This
cref:chap-dynamics strives to address these issues by learning representations of multimodal dynamical systems that
correctly identify the underlying dynamics modes and how the system switches between them.
cref:chap-traj-opt-geometry

Conveniently, the \acrshort{mosvgpe} method from cref:chap-dynamics can be used to learn a factorised representation of the underlying
dynamics modes.
This method correctly identifies the underlying dynamics modes and provides informative latent spaces that
can be used to encode mode remaining behaviour into control strategies.
In particular, the GP-based gating network infers informative latent structure.
The remainder of this chapter is concerned with encoding mode remaining behaviour into trajectory optimisation
algorithms after training a \acrshort{mosvgpe} dynamics model on the historical data set of state transitions.

The main goals of the trajectory optimisation in this Chapter can be summarised as follows,
- Goal 1 :: Remain in a desired dynamics mode $\desiredMode$, label:to-goal-mode
- Goal 2 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/ i.e. that cannot be predicted confidently. For example, due to limited training observations. label:to-goal-unc
  - in the desired dynamics mode,
  - in how the system switches between the underlying dynamics modes.



# #+BEGIN_EXPORT latex
# \begin{myquote}
# Following standard methodologies, trajectories were found by minimising the expected cost under the state distribution,
# resulting from cascading single-step predictions through the SVGP dynamics model.
# The cost function consisted of a quadratic integral control cost and a terminal state cost.
# \end{myquote}
# #+END_EXPORT
\todo{update fig-traj-opt-baseline with trajectories which don't use nominal mean function so that they deviate in the region with high epistemic uncertainty.}


# The first example in cref:fig-traj-opt-over-gating-mask-svgp-all-baseline demonstrates the issues of
# learning a dynamics model without deploying a technique that can infer both the underlying dynamics modes
# and how the system switches between them.
# cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline then demonstrates the
# difficulties of finding trajectories from the start state $\state_0$, to the target state $\targetState$,
# without avoiding the turbulent dynamics mode.



# #+BEGIN_EXPORT latex
# \begin{figure}[t!]
# \centering
# \includegraphics[width=1.0\textwidth]{./images/traj-opt/mode-remaining/baseline-traj-opt/trajectories_over_desired_gating_mask.pdf}
# \caption{\label{fig-traj-opt-baseline}
# Trajectory optimisation results obtained using a SVGP dynamics model trained only on state transitions from the
# desired (non-turbulent) dynamics mode.
# To demonstrate the methods shortcomings, the optimised controls are rolled out in the desired
# mode's GP dynamics (magenta) and in the environment (cyan).
# The state trajectories are overlayed on the gating mask, indicating where each mode governs the dynamics.
# White indicates the turbulent dynamics mode and red indicates the operable mode.
# The hashed box indicates a subset of the environment that has not been observed.}
# \end{figure}
# #+END_EXPORT

*** Motivation label:to_motivation
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{./images/traj-opt/mode-remaining/baseline-svgp-traj-opt/trajectories_over_desired_gating_mask.pdf}
\subcaption{SVGP dynamics model trained on state transitions sampled from \textbf{both} dynamics modes.}
\label{fig-traj-opt-over-gating-mask-svgp-all-baseline}
\end{minipage}
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{./images/traj-opt/mode-remaining/baseline-traj-opt/trajectories_over_desired_gating_mask.pdf}
\subcaption{SVGP dynamics model trained on state transitions sampled from \textbf{only the desired} (operable) dynamics mode.}
\label{fig-traj-opt-over-gating-mask-svgp-desired-baseline}
\end{minipage}
\caption{Trajectory optimisation results obtained using a SVGP dynamics model trained on different data sets.
The optimised control trajectories are rolled out in the SVGP dynamics model (magenta) and in the environment (cyan).
The state trajectories are overlayed on the gating mask, which indicates where each mode governs the dynamics.
White indicates the turbulent dynamics mode and red indicates the desired (operable) mode.
The hashed box indicates a subset of the environment that has not been observed.}
\label{fig-traj-opt-over-gating-mask-svgp-baseline}
\end{figure}
#+END_EXPORT
Given a historical data set of state transitions from the environment,
a common approach is to first learn a single-step dynamics model using a Gaussian process
citep:deisenrothPILCO2011,doerrOptimizing2017,vinogradskaStability2016,rohrProbabilistic2021,hewingLearningBased2020,kollerLearningBased2018.
This learned dynamics model can then be leveraged for model-based control.

cref:fig-traj-opt-over-gating-mask-svgp-all-baseline demonstrates the shortcomings of performing trajectory
optimisation in a GP dynamics model trained on state transitions sampled from *both* dynamics modes.
It shows that the GP is not able to learn a representation of the dynamics which is true to the underlying system.
This is because it cannot model the multimodal behaviour present in the environment.
The optimised controls successfully drive the GP dynamics from the start state $\state_0$,
to the target state $\targetState$.
However, as the GP dynamics model does not correspond to the true underlying dynamics,
the optimised controls do not successfully navigate to the target state $\targetState$ in the environment.
This example demonstrates the issues associated with learning a dynamics model that cannot model the discontinuities
associated with multimodal transition dynamics.
Further to this, it demonstrates the more general shortcomings of model-based control when leveraging learned
dynamics models that cannot represent the true underlying dynamics.
# This motivates cref:chap-dynamics that looks at learning representations of multimodal dynamical systems.
# This example demonstrates the issues associated with learning a dynamics model that cannot correctly identify
# the underlying dynamics modes.
# without deploying a technique that can infer both the underlying dynamics modes
# and how the system switches between them.
# The optimised controls are rolled out in the SVGP dynamics model (magenta)
# and in the environment (cyan) to highlight the issues with both approaches.

In contrast,
cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline shows results after training on state transitions from
only the desired (operable) dynamics mode.
It is able to accurately predict state transitions in the desired dynamics mode.
However, as this approach only considers the dynamics of the desired mode,
trajectories in the environment deviate from those planned in the learned dynamics model when they pass through the
turbulent mode.
This approach is problematic because the trajectory does not reach the target state $\targetState$,
nor does it avoid the turbulent dynamics mode, which may lead to catastrophic failure.
It is also worth noting here that the underlying dynamics modes and how the system switches between them,
are /not fully known a priori/.
As such, partitioning the data set and learning the dynamics model in
cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline is not possible in realistic scenarios.

#+BEGIN_EXPORT latex
\begin{myquote}
Following standard methodologies, these trajectories were found by minimising the expected cost
under the state distribution, resulting from cascading single-step predictions through the GP dynamics model.
A terminal state cost term ensured trajectories ended at the target state and
a quadratic integral control cost term regularised the controls to encode the notion of
"minimal effort" trajectories.
\end{myquote}
#+END_EXPORT




# #+BEGIN_EXPORT latex
# \begin{myquote}
# Following standard methodologies, trajectories were found by minimising the expected cost under the state distribution,
# resulting from cascading single-step predictions through the SVGP dynamics model.
# The cost function consisted of a quadratic integral control cost and a terminal state cost.
# \end{myquote}
# #+END_EXPORT
\todo{update fig-traj-opt-baseline with trajectories which don't use nominal mean function so that they deviate in the region with high epistemic uncertainty.}


# The first example in cref:fig-traj-opt-over-gating-mask-svgp-all-baseline demonstrates the issues of
# learning a dynamics model without deploying a technique that can infer both the underlying dynamics modes
# and how the system switches between them.
# cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline then demonstrates the
# difficulties of finding trajectories from the start state $\state_0$, to the target state $\targetState$,
# without avoiding the turbulent dynamics mode.



# #+BEGIN_EXPORT latex
# \begin{figure}[t!]
# \centering
# \includegraphics[width=1.0\textwidth]{./images/traj-opt/mode-remaining/baseline-traj-opt/trajectories_over_desired_gating_mask.pdf}
# \caption{\label{fig-traj-opt-baseline}
# Trajectory optimisation results obtained using a SVGP dynamics model trained only on state transitions from the
# desired (non-turbulent) dynamics mode.
# To demonstrate the methods shortcomings, the optimised controls are rolled out in the desired
# mode's GP dynamics (magenta) and in the environment (cyan).
# The state trajectories are overlayed on the gating mask, indicating where each mode governs the dynamics.
# White indicates the turbulent dynamics mode and red indicates the operable mode.
# The hashed box indicates a subset of the environment that has not been observed.}
# \end{figure}
# #+END_EXPORT

** Contributions
This dissertation explores mode constrained control in multimodal dynamical systems that explicitly reasons
about the uncertainties that arise during learning and control.
The primary contributions of this dissertation are as follows:
- cref:chap-dynamics: is concerned with learning representations of multimodal dynamical systems.
  Due to the complexity of the problem,
  this work first considers having prior access to the environment, where it can perform prior flight
  trials and observe state transitions over the domain, i.e. collect a data set of state transitions $\mathcal{D}$.
  Given such a data set, this chapter addresses learning environment models that
  can be exploited for model-based control (or planning). It strives to address the issues previously mentioned
  by learning representations of multimodal dynamical systems that correctly identify the underlying dynamics modes and
  how the system switches between them.
  The method is evaluated on a simple data set that contains discontinuities (the motorcycle data set)
  as well as a real-world quadcopter data set.
- cref:chap-traj-opt-geometry: investigates /risk-averse/ trajectory optimisation algorithms
  that leverage the models latent
  structure -- after performing Bayesian inference with the historical data set $\mathcal{D}$ -- to solve
  the quadcopter navigation problem.
  Due to lack of training observations, the learned dynamics model may be uncertain which mode governs the
  dynamics at a particular region (epistemic uncertainty).
  This chapter address the risk-averse setting and finds trajectories that avoid entering these regions of high
  epistemic uncertainty.
- cref:chap-active-learning: then considers the more realistic scenario of not having prior access
  to the environment. In this scenario, the quadcopter does not have access to a historical data set for model learning.
  Instead, it must actively explore its environment to collect data, whilst simultaneously attempting to avoid
  the inoperable, turbulent dynamics mode.

# Given such a data set, cref:chap-dynamics addresses learning environment models that can be exploited for
# model-based control (or planning).
# This chapter strives to address the issues previously mentioned
# by learning representations of multimodal dynamical systems that correctly identify the underlying dynamics modes and
# how the system switches between them.
# cref:chap-traj-opt-geometry,chap-traj-opt-inference
# then investigate trajectory optimisation
# algorithms that leverage the models latent structure -- after performing Bayesian inference
# with the historical data set $\mathcal{D}$ -- to solve the quadcopter navigation problem.
# #+BEGIN_EXPORT latex
# \begin{remark}[Epistemic Uncertainty]
# %Given that the underlying dynamics modes and how the dynamics switches between them, are
# %\textit{not fully known a priori}, these methods should \textit{air on the side of caution}.
# Due to lack of training observations, the learned model may be uncertain which mode governs the
# dynamics at a particular region. This is known as epistemic uncertainty.
# In a risk-averse setting, trajectories should also avoid entering these regions of high epistemic uncertainty.
# \end{remark}
# #+END_EXPORT
# cref:chap-active-learning then considers the more realistic scenario of not having prior access to the environment.
# In this scenario, the quadcopter does not have access to a historical data set for model learning.
# Instead, it must actively explore its environment to collect data, whilst simultaneously attempting to avoid
# the inoperable, turbulent dynamics mode.

# This dissertation explores mode constrained control in multimodal dynamical systems that explicitly reasons
# about uncertainties during learning and control.
# The risk-averse trajectory optimisation algorithms /know what they do not know/, and only evaluate regions that
# they are confident in.
# Conversely, the exploratory trajectory optimisation algorithm exploits this information to guide
# the system into regions that it has not previously observed.
# The primary contributions of this dissertation are as follows:
# - Chapter ref:chap-dynamics: details an approach to learning the underlying dynamics modes (and
#   how they're separated) in multimodal dynamical systems.
#   The approach formulates a probabilistic representation of the transition dynamics resembling a Mixture of
#   Gaussian Process Experts model. It then performs approximate Bayesian inference via a novel variational lower
#   bound that principally handles uncertainty and provides scalability via stochastic gradient methods.
#   The method is tested on a real-world quadcopter data set and two data sets obtained from simulated environments ...
# - Chapter ref:chap-traj-opt: introduces two trajectory optimisation techniques that find trajectories that attempt
#   to remain in a desired dynamics mode, and in regions of the learned dynamics that have been observed, so can be
#   predicted confidently. The first approach (published in cite:scannellTrajectory2021)
#   exploits the latent /geometry/ and well-calibrated /epistemic uncertainty/ estimates
#   inferred by the probabilistic model from Chapter ref:chap-dynamics.
#   The second approach formulates trajectory optimisation as probabilistic inference in a graphical model,
#   and achieves the desired behaviour by conditioning on a mode indicator variable.
# - Chapter ref:chap-active-learning:

** Associated Publications
# The probabilistic model and variational inference scheme presented in Chapter ref:chap-dynamics is used to learn
# the transition dynamics of a DJI Tello quadcopter in cite:scannellTrajectory2021.
The first trajectory optimisation algorithm presented in Chapter ref:chap-traj-opt-geometry, as well as the approach
to learning multimodal dynamical systems in Chapter ref:chap-dynamics are published in:

#+BEGIN_EXPORT latex
{\color{BrickRed}\fullcite{scannellTrajectory2021}}
#+END_EXPORT

** Introduction traj opt paper :noexport:
Many physical systems operate under switching dynamics modes due to
changing environmental or internal conditions.
Examples include: robotic grasping where objects with different
properties have to be manipulated, robotic locomotion in environments with varying surface types
and the control of aircraft in environments subject to different levels of turbulence.
When controlling these systems, it may be preferred to find trajectories that remain
in a single dynamics mode.
This paper is interested in controlling a DJI Tello quadcopter in an environment
with spatially varying turbulence induced by a fan at the side of the room, shown
in Fig. ref:fig-problem-statement.
It is hard to know the exact transition dynamics due to complex and uncertain
interactions between the quadcopter and the fan.
The system's transition dynamics resemble a mixture of two modes: a turbulent mode in front of
the fan and a non-turbulent mode everywhere else.
When planning a trajectory from start state $\mathbf{x}_0$ to desired state $\mathbf{x}_f$
it is preferred to avoid entering the turbulent mode, as it
results in poor performance and sometimes even system failure.

#+BEGIN_EXPORT latex
\begin{figure}[!t]
\centering
  %\includegraphics[width=0.9\columnwidth]{images/quadcopter_bimodal_domain.pdf}
  \includegraphics[width=0.9\columnwidth]{images/quadcopter-domain-ppt.png}
  \caption{
This work seeks to velocity control a DJI Tello quadcopter in an indoor environment
subject to two modes of operation characterised by process noise (turbulence).
A high turbulence mode is induced by placing a desktop fan at the right side of the room.
Data from four trajectories following a single 2D $\mathbf{x}=(x,y)$ target trajectory captures the variability
(process noise) in the dynamics.
Our goal is to find trajectories between $\mathbf{x}_0$ and $\mathbf{x}_f$ that either prioritise
remaining in the non-turbulent mode or prioritise avoiding regions of the learned dynamics
with high epistemic uncertainty due to lack of training observations.}
\label{fig-problem-statement}
\end{figure}
#+END_EXPORT

Trajectory optimisation comprises a powerful set of techniques for finding open-loop controls of dynamical
systems such that an objective function is minimised whilst satisfying a set of
constraints.
It is commonly used for controlling aircraft, robotic manipulators, and walking
robots cite:VonStryk1992,Betts1998,Garg2010.
One caveat to trajectory optimisation is that it requires a relatively accurate mathematical model of
the system.
Traditionally, these mathematical models are built using first principles based on physics.
However, accurately modelling the underlying transition dynamics can be challenging and
lead to the introduction of model errors.
For example, both observation and process noise
are inherent in many real-world systems and can be hard to model
due to both spatial and temporal variations.
Incorrectly accounting for this uncertainty can have a detrimental impact on controller performance
and is an active area of research in the robust
and stochastic optimal control communities cite:FreemanRandyA.2009,Stengel1988.

The difficulties associated with constructing mathematical models can be overcome by learning from
observations cite:Ljung1997.
However, learning dynamics models for control introduces other difficulties.
For example, it is important to know
where the model cannot predict confidently due to a lack of training observations.
This concept is known as epistemic uncertainty and is reduced in the limit of infinite data.
Probabilistic models have been used to account for epistemic uncertainty and also
provide a principled approach to modelling stochasticity i.e. aleatoric uncertainty
cite:Schneider1996,Deisenroth2011.
For example, cite:Cutler,Deisenroth2011,Pan2014 use Gaussian processes (GPs) to learn
transition dynamics.
GPs lend themselves to data-efficient learning through the selection of informative priors, and
when used in a Bayesian setting offer well calibrated uncertainty estimates.
Methods for learning probabilistic multimodal transition dynamics have also been proposed:
cite:Mckinnon used a mixture of GP experts method,
cite:Moerland studied the used of deep generative models and
cite:Kaiser2020a proposed a Bayesian model that learns independent
dynamics modes whilst maintaining a
probabilistic belief over which mode is responsible for predicting at a given input location.

There has also been work developing control algorithms exploiting learned multimodal transition dynamics
cite:Herzallah2020.
However, our work differs as it seeks to find trajectories that
remain in a single dynamics mode
whilst avoiding regions of the transition dynamics that cannot be predicted confidently.
To the best of our knowledge, there is no previous work addressing such trajectory optimisation
in transition dynamics models.

Probabilistic modelling and Bayesian inference are a promising
avenue for learning dynamics models to be used for controlling real-world systems.
\parmarginnote{probabilistic modelling}
The Bayesian framework provides a principled approach to modelling both the
/epistemic uncertainty/ associated with the model,
and the /aleatoric uncertainty/ inherent to the system (e.g. process noise).
For example, cite:deisenrothPILCO2011,cutlerEfficient2015,panProbabilistic2014
use Gaussian processes (GPs) to learn
transition dynamics from observations.
GPs lend themselves to data-efficient learning through the selection of informative priors, and
when used in a Bayesian setting offer well calibrated uncertainty estimates.
Methods for learning probabilistic multimodal transition dynamics have also been proposed:
cite:mckinnonLearning2017 use a Mixture of GP Experts method,
cite:moerlandLearning2017 study the use of deep generative models and
cite:kaiserBayesian2020 propose a Bayesian model that learns independent
dynamics modes whilst maintaining a
probabilistic belief over which mode is responsible for predicting at a given input location.

* Motivation
** Motivation label:to_motivation
As this work is interested in learning and control in multimodal dynamical systems,
a primary interest is correctly /identifying/ the underlying dynamics modes.
That is, ensuring that the learned dynamics modes accurately model the true underlying dynamics modes.
\marginpar{identifiability}
Incorrectly identifying the underlying dynamics modes will have a detrimental impact on performance
and may even lead to system failure.
# Learning
# and inferring latent spaces that are convenient for control.
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{./images/traj-opt/mode-remaining/baseline-svgp-traj-opt/trajectories_over_desired_gating_mask.pdf}
\subcaption{SVGP dynamics model trained on state transitions sampled from \textbf{both} dynamics modes.}
\label{fig-traj-opt-over-gating-mask-svgp-all-baseline}
\end{minipage}
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{./images/traj-opt/mode-remaining/baseline-traj-opt/trajectories_over_desired_gating_mask.pdf}
\subcaption{SVGP dynamics model trained on state transitions sampled from \textbf{only the desired} (operable) dynamics mode.}
\label{fig-traj-opt-over-gating-mask-svgp-desired-baseline}
\end{minipage}
\caption{Trajectory optimisation results obtained using a SVGP dynamics model trained on different data sets.
The optimised control trajectories are rolled out in the SVGP dynamics model (magenta) and in the environment (cyan).
The state trajectories are overlayed on the gating mask, which indicates where each mode governs the dynamics.
White indicates the turbulent dynamics mode and red indicates the desired (operable) mode.
The hashed box indicates a subset of the environment that has not been observed.}
\label{fig-traj-opt-over-gating-mask-svgp-baseline}
\end{figure}
#+END_EXPORT
Given a historical data set of state transitions from the environment,
a common approach is to first learn a single-step dynamics model using a Gaussian process
citep:deisenrothPILCO2011,doerrOptimizing2017,vinogradskaStability2016,rohrProbabilistic2021,hewingLearningBased2020,kollerLearningBased2018.
This learned dynamics model can then be leveraged for model-based control.
cref:fig-traj-opt-over-gating-mask-svgp-all-baseline
demonstrates the shortcomings of learning a GP dynamics model for the quadcopter navigation problem in the
illustrative example from cref:illustrative_example.
It shows the results of performing trajectory
optimisation in a GP dynamics model trained on state transitions sampled from *both* dynamics modes.
The GP has not been able to learn a representation of the dynamics which is true to the underlying system,
due to the discontinuities associated with the multimodal transition dynamics (changing lengthscales/noise variances etc).
The optimised controls successfully drive the GP dynamics from the start state $\state_0$,
to the target state $\targetState$.
However, as the GP does not represent the true underlying dynamics accurately,
the optimised controls do not successfully navigate to the target state $\targetState$ in the environment.
This example motivates the need to correctly identify the underlying modes when learning dynamics models for
model-based control.
# Further to this, it demonstrates the more general shortcomings of model-based control when leveraging learned
# dynamics models that cannot represent the true underlying dynamics.
# This motivates cref:chap-dynamics that looks at learning representations of multimodal dynamical systems.
# This example demonstrates the issues associated with learning a dynamics model that cannot correctly identify
# the underlying dynamics modes.
# without deploying a technique that can infer both the underlying dynamics modes
# and how the system switches between them.
# The optimised controls are rolled out in the SVGP dynamics model (magenta)
# and in the environment (cyan) to highlight the issues with both approaches.

In contrast,
cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline shows results after training on state transitions from
only the desired (operable) dynamics mode.
It is able to accurately predict state transitions in the desired dynamics mode.
However, as this approach only considers the dynamics of the desired mode,
trajectories in the environment deviate from those planned in the learned dynamics model when they pass through the
turbulent mode.
This approach is problematic because the trajectory passes through the turbulent dynamics mode,
which may lead to catastrophic failure.
Further to this, the trajectory does not reach the target state $\targetState$.
Without inferring information regarding how the system switches between its underlying dynamics modes, it is not
obvious how mode remaining/avoiding behaviour can be encoded into control algorithms.
#+BEGIN_EXPORT latex
\begin{remark}
As the underlying dynamics modes and how the system switches between them,
are \textit{not fully known a priori}, partitioning the data set and learning the dynamics model in
\cref{fig-traj-opt-over-gating-mask-svgp-desired-baseline} is not possible in realistic scenarios.
\end{remark}
#+END_EXPORT
# This example motivates the need for dynamics models that can automatically infer the underlying dynamics modes, whilst
# also learning how the system switches between its underlying dynamics modes.

# This approach is problematic because the trajectory does not reach the target state $\targetState$,
# nor does it avoid the turbulent dynamics mode, which may lead to catastrophic failure.


# It is also worth noting here that the underlying dynamics modes and how the system switches between them,
# are /not fully known a priori/.
# As such, partitioning the data set and learning the dynamics model in
# cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline is not possible in realistic scenarios.

#+BEGIN_EXPORT latex
\begin{myquote}
Following standard methodologies, these trajectories were found by minimising the expected cost
under the state distribution, resulting from cascading single-step predictions through the GP dynamics model.
A terminal state cost term ensured trajectories ended at the target state and
a quadratic integral control cost term regularised the controls to encode the notion of
"minimal effort" control.
\end{myquote}
#+END_EXPORT




# #+BEGIN_EXPORT latex
# \begin{myquote}
# Following standard methodologies, trajectories were found by minimising the expected cost under the state distribution,
# resulting from cascading single-step predictions through the SVGP dynamics model.
# The cost function consisted of a quadratic integral control cost and a terminal state cost.
# \end{myquote}
# #+END_EXPORT
\todo{update fig-traj-opt-baseline with trajectories which don't use nominal mean function so that they deviate in the region with high epistemic uncertainty.}


# The first example in cref:fig-traj-opt-over-gating-mask-svgp-all-baseline demonstrates the issues of
# learning a dynamics model without deploying a technique that can infer both the underlying dynamics modes
# and how the system switches between them.
# cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline then demonstrates the
# difficulties of finding trajectories from the start state $\state_0$, to the target state $\targetState$,
# without avoiding the turbulent dynamics mode.



# #+BEGIN_EXPORT latex
# \begin{figure}[t!]
# \centering
# \includegraphics[width=1.0\textwidth]{./images/traj-opt/mode-remaining/baseline-traj-opt/trajectories_over_desired_gating_mask.pdf}
# \caption{\label{fig-traj-opt-baseline}
# Trajectory optimisation results obtained using a SVGP dynamics model trained only on state transitions from the
# desired (non-turbulent) dynamics mode.
# To demonstrate the methods shortcomings, the optimised controls are rolled out in the desired
# mode's GP dynamics (magenta) and in the environment (cyan).
# The state trajectories are overlayed on the gating mask, indicating where each mode governs the dynamics.
# White indicates the turbulent dynamics mode and red indicates the operable mode.
# The hashed box indicates a subset of the environment that has not been observed.}
# \end{figure}
# #+END_EXPORT

** latent spaces :ignore:
\newline

The introduction of /latent variables/ into probabilistic models is a key component providing them with interesting
and powerful capabilities for synergising model learning and control.
For example, cite:hafnerLearning2019,rybkinModelBased2021
\todo{add more latent space dynamics refs}
learn /latent spaces/ which provide convenient spaces for planning.
\marginpar{latent spaces for control}
cref:fig-traj-opt-over-gating-mask-svgp-desired-baseline highlights the need for learning
informative latent variables representing how the system switches between the underlying dynamics modes.
Without such information, it is not possible to encode the notion of mode remaining/avoiding behaviour.
As such, this work is interested in learning /latent spaces/
that are rich with information regarding how a system switches between its underlying dynamics modes.

With this in mind, the main goals of this chapter are to construct a model which,
1. can accurately /identify/ the true underlying dynamics modes,
2. has convenient /latent spaces/ for planning/control,
   - rich with information regarding the mode switching behaviour.
   # - this could be achieved by providing handles for encoding informative domain knowledge, in turn, constraining the set of admissible functions,

* Literature Review
* Probabilistic Inference for Learning Multimodal Dynamical Systems label:chap-dynamics
#+begin_export latex
\epigraph{All models are wrong, but some are useful.}{\textit{George Box}}
#+end_export
** Maths Symbols :ignore:
*** Domains :ignore:
#+BEGIN_EXPORT latex
\newcommand{\stateDomain}{\ensuremath{\mathcal{X}}}
%\renewcommand{\stateDomain}{\ensuremath{\mathcal{S}}}
\renewcommand{\controlDomain}{\ensuremath{\mathcal{U}}}
\renewcommand{\modeDomain}{\ensuremath{\mathcal{A}}}
%\renewcommand{\inputDomain}{\ensuremath{\mathcal{X}}}
\renewcommand{\inputDomain}{\ensuremath{\mathcal{Z}}}

%\renewcommand{\state}{\ensuremath{\mathbf{s}}}
\renewcommand{\state}{\ensuremath{\mathbf{x}}}

\renewcommand{\nominalDynamics}{\ensuremath{\mathbf{n}}}
\renewcommand{\unknownDynamics}{\ensuremath{\mathbf{f}}}
\renewcommand{\nominalDynamicsK}{\ensuremath{\mode{\mathbf{n}}}}
\renewcommand{\unknownDynamicsK}{\ensuremath{\mode{\mathbf{f}}}}

\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{T}}
\newcommand{\inputDim}{\ensuremath{d}}
\newcommand{\InputDim}{\ensuremath{D}}
#+END_EXPORT

*** Bounds :ignore:
#+BEGIN_EXPORT latex
\newcommand{\tightBound}{\ensuremath{\mathcal{L}_{\text{tight}}}}
\newcommand{\furtherBound}{\ensuremath{\mathcal{L}_{\text{further}}}}
\newcommand{\furtherBoundTwo}{\ensuremath{\mathcal{L}_{\text{further}^2}}}
#+END_EXPORT
*** maths :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\mode}[1]{\ensuremath{#1_{\modeInd}}}
\renewcommand{\modei}[2]{\ensuremath{#1_{#2}}}
%\renewcommand{\singleOutput}{\ensuremath{\Delta x_{\numData}}}
%\renewcommand{\allOutput}{\ensuremath{\Delta\mathbf{\state}}}
\newcommand{\singleModeVar}{\ensuremath{\singleData{\modeVar}}}
\newcommand{\allModeVar}{\ensuremath{\bm{\modeVar}}}
\newcommand{\singleModeVarK}{\ensuremath{\singleModeVar = \modeInd}}
\newcommand{\allModeVarK}{\ensuremath{\bm{\modeVar}_{\modeInd}}}
%\newcommand{\allModeVarK}{\ensuremath{\{\singleModeVarK\}_{\numData=1}^\NumData}}
\newcommand{\modeVarnk}{\ensuremath{\modeVar_{\numData,\modeInd}}}

% new
\renewcommand{\numData}{\ensuremath{n}}
\renewcommand{\NumData}{\ensuremath{N}}
\renewcommand{\singleOutput}{\ensuremath{y_{\numData}}}
\renewcommand{\singleInput}{\ensuremath{\mathbf{x}_{\numData}}}
\renewcommand{\allInput}{\ensuremath{\mathbf{X}}}
\renewcommand{\allOutput}{\ensuremath{\mathbf{y}}}
\renewcommand{\allOutput}{\ensuremath{\mathbf{y}}}
%\renewcommand{\allInputK}{\ensuremath{\{\singleInput : \singleModeVarK \}}}
%\renewcommand{\allOutputK}{\ensuremath{\{\singleOutput : \singleModeVarK\}}}
%\renewcommand{\allInputK}{\ensuremath{\allInput^{\modeInd}}}
%\renewcommand{\allOutputK}{\ensuremath{\allOutput^{\modeInd}}}
\renewcommand{\singleInputK}{\ensuremath{\mathbf{x}_{\numData, \modeInd}}}
\renewcommand{\allInputK}{\ensuremath{\mode{\allInput}}}
\renewcommand{\allOutputK}{\ensuremath{\mode{\allOutput}}}

%\renewcommand{\x}{\ensuremath{\mathbf{z}}}
%\renewcommand{\y}{\ensuremath{y}}
%\renewcommand{\singleInput}{\ensuremath{\mathbf{z}_{\numData}}}
%\renewcommand{\allInput}{\ensuremath{\mathbf{Z}}}
%\renewcommand{\singleInputK}{\ensuremath{\mathbf{z}_{\numData, \modeInd}}}
%\renewcommand{\allInputK}{\ensuremath{\mode{\allInput}}}

%\newcommand{\expertPrior}{\ensuremath{p\left(\mode{f}(\allInput) \right)}}
\newcommand{\expertPrior}{\ensuremath{p\left(\mode{f}(\allInputK) \right)}}
\newcommand{\expertsPrior}{\ensuremath{p\left(\LatentFunc(\allInput) \right)}}
\newcommand{\expertMeanFunc}{\ensuremath{\mode{\mu}}}
\newcommand{\expertCovFunc}{\ensuremath{\mode{k}}}
\newcommand{\expertLikelihood}{\ensuremath{p\left(\allOutput \mid \mode{f}(\allInput)\right)}}
\newcommand{\singleExpertLikelihood}{\ensuremath{p(\singleOutput \mid \mode{f}(\singleInput))}}
%\newcommand{\allExpertLikelihood}{\ensuremath{p(\allOutput \mid \mode{f}(\allInput))}}
%\newcommand{\allExpertLikelihood}{\ensuremath{p(\allOutputK \mid \mode{f}(\allInputK))}}
\newcommand{\allExpertLikelihood}{\ensuremath{p(\allOutputK \mid \mode{f}(\allInputK))}}
\newcommand{\expertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVarK, \allInput \right)}}
\newcommand{\singleExpertPosterior}{\ensuremath{p\left(\singleOutput \mid \singleModeVarK, \allInput \right)}}
% \newcommand{\expertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVarK \right)}}

\newcommand{\gatingPrior}{\ensuremath{p\left(\GatingFunc(\allInput ) \right)}}
\newcommand{\gatingMeanFunc}{\ensuremath{\mode{\hat{\mu}}}}
\newcommand{\gatingCovFunc}{\ensuremath{\mode{\hat{k}}}}
\newcommand{\singleGatingLikelihood}{\ensuremath{\Pr\left(\singleModeVarK \mid \GatingFunc(\singleInput) \right)}}
%\newcommand{\allGatingLikelihood}{\ensuremath{\Pr\left(\allModeVarK \mid \GatingFunc(\allInput) \right)}}
\newcommand{\allGatingLikelihood}{\ensuremath{p\left(\allModeVar \mid \GatingFunc(\allInput) \right)}}
\newcommand{\gatingLikelihood}{\ensuremath{P\left(\singleModeVar \mid \GatingFunc(\singleInput) \right)}}
%\newcommand{\gatingPosterior}{\ensuremath{\Pr\left( \singleModeVar \mid \singleInput \right)}}
\newcommand{\gatingPosterior}{\ensuremath{\Pr\left( \allModeVarK \mid \allInput \right)}}
\newcommand{\singleGatingPosterior}{\ensuremath{\Pr\left( \singleModeVarK \mid \singleInput, \gatingParams \right)}}
\newcommand{\evidence}{\ensuremath{p\left(\allOutput \mid \allInput \right)}}

\newcommand{\moeExpertPosterior}{\ensuremath{p\left(\singleOutput \mid \singleModeVarK, \singleInput, \expertParamsK \right)}}
\newcommand{\moeGatingPosterior}{\ensuremath{\Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right)}}
\newcommand{\moeEvidence}{\ensuremath{p\left(\allOutput \mid \allInput, \expertParams, \gatingParams \right)}}
\newcommand{\singleMoeEvidence}{\ensuremath{p\left(\singleOutput \mid \singleInput, \expertParams, \gatingParams \right)}}

\newcommand{\npmoeExpertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVar, \allInput, \expertParams \right)}}
\newcommand{\npmoeGatingPosterior}{\ensuremath{p\left(\allModeVar \mid \allInput, \gatingParams \right)}}

\newcommand{\moeLikelihood}{\ensuremath{p\left(\allOutput \mid \LatentFunc(\allInput), \GatingFunc (\allInput) \right)}}
\newcommand{\singleMoeLikelihood}{\ensuremath{p\left(\singleOutput \mid \mode{\latentFunc}(\allInput), \GatingFunc (\allInput) \right)}}

#+END_EXPORT
*** kernels :ignore:
#+BEGIN_EXPORT latex
%\renewcommand{\expertKernelnn}{\ensuremath{k_{\singleInput\singleInput}}}
%\renewcommand{\expertKernelnM}{\ensuremath{\mathbf{k}_{\singleInput \expertInducingInput}}}
%\renewcommand{\expertKernelMM}{\ensuremath{\mathbf{K}_{\expertInducingInput\expertInducingInput}}}
%\renewcommand{\expertKernelMn}{\ensuremath{\mathbf{k}_{\expertInducingInput \singleInput}}}
\renewcommand{\expertKernelnn}{\ensuremath{k_{\modeInd \numData \numData}}}
\renewcommand{\expertKernelNN}{\ensuremath{\mathbf{K}_{\modeInd \NumData \NumData}}}
\renewcommand{\expertKernelnM}{\ensuremath{\mathbf{k}_{\modeInd \numData \NumInducing}}}}
\renewcommand{\expertKernelNM}{\ensuremath{\mathbf{K}_{\modeInd \NumData \NumInducing}}}}
\renewcommand{\expertKernelMM}{\ensuremath{\mathbf{K}_{\modeInd \NumInducing \NumInducing}}}
\renewcommand{\expertKernelMn}{\ensuremath{\mathbf{k}_{\modeInd \NumInducing \numData}}}
\renewcommand{\expertKernelMN}{\ensuremath{\mathbf{K}_{\modeInd \NumInducing \NumData}}}
\renewcommand{\expertKernelsM}{\ensuremath{\mathbf{k}_{\modeInd * \NumInducing}}}}
\renewcommand{\expertKernelss}{\ensuremath{k_{\modeInd **}}}
\renewcommand{\expertKernelSM}{\ensuremath{\mathbf{K}_{\modeInd * \NumInducing}}}}
\renewcommand{\expertKernelSS}{\ensuremath{\mathbf{K}_{\modeInd **}}}

%\renewcommand{\gatingKernelnn}{\ensuremath{\hat{k}_{\singleInput\singleInput}}}
%\renewcommand{\gatingKernelnM}{\ensuremath{\hat{\mathbf{k}}_{\singleInput \gatingInducingInput}}}
%\renewcommand{\gatingKernelMM}{\ensuremath{\hat{\mathbf{K}}_{\gatingInducingInput\gatingInducingInput}}}
%\renewcommand{\gatingKernelMn}{\ensuremath{\hat{\mathbf{k}}_{\gatingInducingInput \singleInput}}}
\renewcommand{\gatingKernelnn}{\ensuremath{\hat{k}_{\modeInd \numData \numData}}}
\renewcommand{\gatingKernelNN}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumData \NumData}}}
\renewcommand{\gatingKernelnM}{\ensuremath{\hat{\mathbf{k}}_{\modeInd \numData \NumInducing}}}
\renewcommand{\gatingKernelNM}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumData \NumInducing}}}
\renewcommand{\gatingKernelMM}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumInducing \NumInducing}}}
\renewcommand{\gatingKernelMn}{\ensuremath{\hat{\mathbf{k}}_{\modeInd \NumInducing \numData}}}
\renewcommand{\gatingKernelss}{\ensuremath{\hat{k}_{\modeInd **}}}
\renewcommand{\gatingKernelsM}{\ensuremath{\hat{\mathbf{k}}_{\modeInd * \NumInducing}}}
\renewcommand{\gatingKernelMs}{\ensuremath{\hat{\mathbf{k}}_{\modeInd \NumInducing *}}}
\renewcommand{\gatingKernelSM}{\ensuremath{\hat{\mathbf{K}}_{\modeInd * \NumInducing}}}
\renewcommand{\gatingKernelMS}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumInducing *}}}
\renewcommand{\gatingKernelSS}{\ensuremath{\hat{\mathbf{K}}_{\modeInd **}}}

\renewcommand{\expertA}{\ensuremath{\mode{\mathbf{A}}}}
\renewcommand{\gatingA}{\ensuremath{\mode{\hat{\mathbf{A}}}}}
#+END_EXPORT

*** inference :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\input}{\ensuremath{\hat{\state}}}
\renewcommand{\output}{\ensuremath{\Delta \state}}

\newcommand{\kernel}{\ensuremath{k}}
\newcommand{\expertKernel}{\ensuremath{\mode{\kernel}}}
\newcommand{\gatingKernel}{\ensuremath{\mode{\hat{\kernel}}}}

\newcommand{\numInducing}{\ensuremath{m}}
\newcommand{\NumInducing}{\ensuremath{\MakeUppercase{\numInducing}}}
%\newcommand{\inducingInput}{\ensuremath{\mathbf{Z}}}
\newcommand{\inducingInput}{\ensuremath{\bm{\zeta}}}
\newcommand{\inducingOutput}{\ensuremath{\mathbf{u}}}

%\newcommand{\expertInducingInput}{\ensuremath{\mode{\inducingInput}}}
%\newcommand{\expertsInducingInput}{\ensuremath{\inducingInput}}}
\newcommand{\expertInducingInput}{\ensuremath{\mode{\bm{\zeta}}}}
\newcommand{\expertsInducingInput}{\ensuremath{\bm{\zeta}}}
%\newcommand{\expertInducingOutput}{\ensuremath{\mode{\inducingOutput}}}
%\newcommand{\expertsInducingOutput}{\ensuremath{\MakeUppercase{\inducingOutput}}}
%\newcommand{\expertInducingOutput}{\ensuremath{\mode{\hat{\mathbf{\latentFunc}}}}}
%\newcommand{\expertsInducingOutput}{\ensuremath{\hat{\MakeUppercase{\mathbf{\latentFunc}}}}}
%\newcommand{\expertInducingOutput}{\ensuremath{\mode{\tilde{\mathbf{\latentFunc}}}}}
%\newcommand{\expertsInducingOutput}{\ensuremath{\tilde{\MakeUppercase{\mathbf{\latentFunc}}}}}
\newcommand{\expertInducingOutput}{\ensuremath{\mode{\latentFunc}(\expertInducingInput)}}
\newcommand{\expertsInducingOutput}{\ensuremath{\mathbf{\latentFunc}(\expertsInducingInput)}}

%\newcommand{\gatingInducingInput}{\ensuremath{\hat{\inducingInput}}}
\newcommand{\gatingInducingInput}{\ensuremath{\bm{\xi}}}
%\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\hat{\inducingOutput}}}}
%\newcommand{\gatingsInducingOutput}{\ensuremath{\hat{\MakeUppercase{\inducingOutput}}}}
%\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\hat{\mathbf{\gatingFunc}}}}}
%\newcommand{\gatingsInducingOutput}{\ensuremath{\hat{\MakeUppercase{\mathbf{\gatingFunc}}}}}
%\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\tilde{\mathbf{\gatingFunc}}}}}
%\newcommand{\gatingsInducingOutput}{\ensuremath{\tilde{\MakeUppercase{\mathbf{\gatingFunc}}}}}
\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\gatingFunc}(\gatingInducingInput)}}
\newcommand{\gatingsInducingOutput}{\ensuremath{\mathbf{\gatingFunc}(\gatingInducingInput)}}

%\newcommand{\expertInducingPrior}{\ensuremath{p(\mode{\latentFunc}(\expertInducingInput))}}
%\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput \mid \expertInducingInput)}}
%\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput \mid \expertsInducingInput)}}
\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput)}}
\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput)}}
%\newcommand{\expertInducingVariational}{\ensuremath{q(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\expertInducingVariational}{\ensuremath{q(\expertInducingOutput)}}
\newcommand{\expertsInducingVariational}{\ensuremath{q(\expertsInducingOutput)}}
\newcommand{\expertVariational}{\ensuremath{q(\mode{\latentFunc}(\singleInput))}}
\newcommand{\expertsVariational}{\ensuremath{q(\LatentFunc_\numData)}}
%\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \expertInducingOutput)}}
\newcommand{\allExpertGivenInducing}{\ensuremath{p(\allOutput \mid \expertInducingOutput)}}
%\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \expertInducingOutput)}}
\newcommand{\allLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\allInput) \mid \expertInducingOutput)}}


%\newcommand{\gatingInducingPrior}{\ensuremath{p(\GatingFunc(\gatingInducingInput))}}
%\newcommand{\gatingInducingPrior}{\ensuremath{p(\gatingInducingOutput \mid \gatingInducingInput)}}
%\newcommand{\gatingsInducingPrior}{\ensuremath{p(\gatingsInducingOutput \mid \gatingInducingInput)}}
\newcommand{\gatingInducingPrior}{\ensuremath{p(\gatingInducingOutput)}}
\newcommand{\gatingsInducingPrior}{\ensuremath{p(\gatingsInducingOutput)}}
%\newcommand{\gatingInducingVariational}{\ensuremath{q(\GatingFunc(\gatingInducingInput))}}
\newcommand{\gatingInducingVariational}{\ensuremath{q(\gatingInducingOutput)}}
\newcommand{\gatingsInducingVariational}{\ensuremath{q(\gatingsInducingOutput)}}
\newcommand{\gatingsVariational}{\ensuremath{q(\GatingFunc_\numData)}}
\newcommand{\singleGatingGivenInducing}{\ensuremath{\Pr(\singleModeVarK \mid \gatingsInducingOutput)}}
\newcommand{\allGatingGivenInducing}{\ensuremath{\Pr(\allModeVarK \mid \gatingInducingOutput)}}
\newcommand{\allGatingsGivenInducing}{\ensuremath{\Pr(\allModeVarK \mid \gatingsInducingOutput)}}
\newcommand{\singleLatentGatingsGivenInducing}{\ensuremath{p(\GatingFunc(\singleInput) \mid \gatingsInducingOutput)}}
\newcommand{\allLatentGatingsGivenInducing}{\ensuremath{p(\GatingFunc(\allInput) \mid \gatingsInducingOutput)}}
\newcommand{\singleLatentGatingGivenInducing}{\ensuremath{p(\mode{\gatingFunc}(\singleInput) \mid \gatingInducingOutput)}}

\newcommand{\expertKL}{\ensuremath{\text{KL}\left( \expertInducingVariational \mid\mid \expertInducingPrior \right)}}
\newcommand{\expertsKL}{\ensuremath{\sum_{\modeInd=1}^\ModeInd\text{KL}\left( \expertInducingVariational \mid\mid \expertInducingPrior \right)}}
\newcommand{\gatingKL}{\ensuremath{\text{KL}\left( \gatingInducingVariational \mid\mid \gatingInducingPrior \right)}}
\newcommand{\gatingsKL}{\ensuremath{\sum_{\modeInd=1}^\ModeInd \text{KL}\left( \gatingInducingVariational \mid\mid \gatingInducingPrior \right)}}
#+END_EXPORT
** Intro :ignore:
This chapter is interested in /learning/ representations of multimodal dynamical systems,
that can be leveraged to control robotic systems in uncertain environments, where both the underlying
dynamics modes, and how the system switches between them, are /not fully known a priori/.
This chapter assumes access to a data set of state transitions $\dataset$, that
have previously been sampled from the system at a constant frequency, i.e. with a fixed time-step.

# However, it does assume that the dynamics modes vary over the
# input (state, control) domain, such that the mode switching can be modelled by input-dependent functions.
# This set of functions governing the mode switching is commonly referred to as a gating network.

# This work is motivated by learning synergising model learning for controlling multimodal dynamical systems.

# This work is motivated by learning /latent/ structure that can be exploited for model-based control.

# In particular

# As such, correctly identifying the underlying dynamics modes is extremely important

# Identifiability

# Given such a data set, this chapter first constructs a discrete-time representation of
# multimodal dynamical systems.
# It then formulates a probabilistic representation of this model and details an approach to performing Bayesian
# inference in the model.

# This chapter is motivated by synergising model learning and control.
Following the motivation in cref:to_motivation, this chapter
seeks to correctly identify the underlying dynamics modes, whilst inferring
latent structure that can be exploited for control.
The main goals of this chapter can be summarised as follows,
1. accurately /identify/ the true underlying dynamics modes,
2. learn /latent spaces/ for planning/control,
   - rich with information regarding the mode switching behaviour.

The probabilistic model constructed in this chapter
resembles a \acrfull{mogpe} with a \acrshort{gp}-based gating network and  is named \acrfull{mosvgpe}.
Following other \acrshort{mogpe} methods, the \acrshort{mosvgpe} is evaluated on the motorcyle data set citep:Silverman1985.
It is then tested on the real-world quadcopter data set from the illustrative example detailed in cref:illustrative_example.
The work in this chapter is implemented in GPflow/TensorFlow and is available on GitHub[fn::Code accompanying this Chapter can be found @ [[https://github.com/aidanscannell/mogpe][\icon{\faGithub}/aidanscannell/mogpe]].]
citep:matthewsGPflow2017,GPflow2017,tensorflow2015-whitepaper.




# and a data set obtained from a
# velocity controlled point mass simulation.

# The model and inference are tested on the real-world quadcopter data set and a data set obtained from a
# velocity controlled point mass simulation.
# \todo{Add that this method is tested on mcycle?}
# The work in this chapter is implemented in GPflow/TensorFlow and is available on GitHub
# cite:matthewsGPflow2017,GPflow2017,tensorflow2015-whitepaper[fn:1:Code accompanying this Chapter can be found @ [[https://github.com/aidanscannell/mogpe][\icon{\faGithub}/aidanscannell/mogpe]].].

# This chapter first introduces the set of continuous-time multimodal dynamical systems



# Based on this assumption, this chapter constructs a discrete-time representation of the system's transition dynamics
# and then formulates it as a probabilistic model based on GPs.

# This thesis is interested in multimodal systems where the dynamics modes vary over the
# input (state, control) domain, such that the mode switching can by modelled
# by input-dependent functions.
# This set of functions governing the mode switching is commonly referred to as a gating network.

# Motivated by data-efficient learning this work formulates multimodal on probabilistic models


# This work is motivated by data-efficient learning and specifically focuses on gating networks
# where prior knowledge of the system can be encoded into the model via informative priors.
# As a result, the probabilistic model constructed in this chapter resembles a \acrfull{mogpe}
# with a \acrshort{gp}-based gating network.
# This work derives a novel variational lower bound based on sparse GPs that
# enables the model to be trained with stochastic gradient methods.


# that this work considers and then details an approach to performing Bayesian inference in such models.

# Motivated by control,
# Given this data set $\dataset$, this chapter constructs a discrete-time, probabilistic
# representation of the multimodal transition dynamics
# and then formulates it as a probabilistic model based on GPs.

# the system's transition dynamics
# and then formulates it as a probabilistic model based on GPs.

# that pairs of input $\x$ and output $\y$ observations
# have previously been sampled from the system at a constant frequency,
# (i.e. with a fixed time-step) to give a data set $\dataset$.

# introduces the class of continuous-time multimodal dynamical systems

# This chapter introduces the class of continuous-time multimodal dynamical systems
# that this work considers and then details an approach to performing Bayesian inference in such models.

# Throughout this chapter it is assumed that pairs of input $\x$ and output $\y$ observations
# have previously been sampled from the system at a constant frequency,
# (i.e. with a fixed time-step) to give a data set $\dataset$.
# Based on this assumption, this chapter constructs a discrete-time representation of the system's transition dynamics
# and then formulates it as a probabilistic model based on GPs.

# This thesis is interested in multimodal systems where the dynamics modes vary over the
# input (state, control) domain, such that the mode switching can by modelled
# by input-dependent functions.
# This set of functions governing the mode switching is commonly referred to as a gating network.

# This work is motivated by data-efficient learning and specifically focuses on gating networks
# where prior knowledge of the system can be encoded into the model via informative priors.
# As a result, the probabilistic model constructed in this chapter resembles a \acrfull{mogpe}
# with a \acrshort{gp}-based gating network.
# This work derives a novel variational lower bound based on sparse GPs that
# enables the model to be trained with stochastic gradient methods.
# The work in this chapter is implemented in GPflow/TensorFlow and is available on GitHub
# cite:matthewsGPflow2017,GPflow2017,tensorflow2015-whitepaper[fn:1:Code accompanying this Chapter can be found @ [[https://github.com/aidanscannell/mogpe][\icon{\faGithub}/aidanscannell/mogpe]].].

** Problem Statement
\todo{change t-1 to t and t to t+1 to be consistent with control section}
This work considers /unknown/, or /partially unknown/,  multimodal,
stochastic, nonlinear dynamical systems,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-multimodal-dynamics-disc}
\Delta \state_{\timeInd+1}
&= \unknownDynamics(\state_{t}, \control_{t}; \Delta t = t_*) + \bm\epsilon & \nonumber \\
&= \unknownDynamicsK (\state_{\timeInd}, \control_{\timeInd} ; \Delta t = t_*) + \mode{\bm\epsilon}
&\text{if} \quad \modeVar_{\timeInd} = \modeInd
\end{align}
#+END_EXPORT
with continuous states $\state_t \in \stateDomain \subseteq \R^\StateDim$ and controls
$\control_t \in \controlDomain \subseteq \R^\ControlDim$ and time index $t$.
The state difference between time $\timeInd$ and $\timeInd+1$ is denoted
$\Delta \state_{\timeInd+1} = \state_{\timeInd+1} - \state_{\timeInd}$.
At any given time step $t$, one of the $\ModeInd$ dynamics modes
$\{\unknownDynamicsK : \stateDomain \times \controlDomain \rightarrow \stateDomain \}_{\modeInd=1}^\ModeInd$,
and associated noise models
$\mode{\bm\epsilon} &\sim \mathcal{N}\left(\mathbf{0}, \bm\Sigma_{\mode{\bm\epsilon}} \right)$, where
$\bm\Sigma_{\mode{\epsilon}} = \text{diag}\left[ \sigma_{\modeInd,1}^{2}, \ldots, \sigma_{\modeInd,\StateDim}^2 \right]$,
are selected by a discrete mode indicator variable,
$\alpha_t \in \modeDomain = \mathbb{Z} \cap [1,\ModeInd]$.
This work considers systems that are stochastic (subject to process noise), but
are not subject to observation noise.
Thus, the $\mode{\bm\epsilon}$ term solely represents the process noise.

# The nominal dynamics
# $\{\nominalDynamicsK : \stateDomain \times \controlDomain \rightarrow \stateDomain \}_{\modeInd=1}^\ModeInd$
# represents our /a priori/ knowledge of the dynamics (i.e. the assumed dynamics)
# and the additive dynamics
# $\{\unknownDynamicsK : \stateDomain \times \controlDomain \rightarrow \stateDomain \}_{\modeInd=1}^\ModeInd$
# represents the initially unknown dynamics, which are to be learned from observations.


This chapter assumes access to historical data comprising state transitions from $\NumEpisodes$ trajectories
of length $T$, sampled  with a  fixed  time step $\Delta t=t_{*}$.
The data set has ${N=ET}$ elements
and we abuse notation by appending the independent trajectories along
time to get the data set $\mathcal{D}=\{(\state_{\timeInd},\control_{\timeInd}),\Delta \state_{t+1}\}^\NumData_{\timeInd=1}$.

To help with understanding and ease of notation, our modelling only considers a single output dimension,
i.e. $\stateDomain \subseteq \R$ as $\fk : \stateDomain \times \controlDomain \rightarrow \stateDomain$.
The extension to multiple output dimensions follows from standard GP methodologies and is detailed where necessary.
To further ease notation, the state-control input domain is denoted
$\inputDomain = \stateDomain \times \controlDomain \subseteq \R^{\InputDim}$
and a single state-control input is dentoed
$\singleInput = (\state_{\timeInd},\control_{\timeInd})$.
Given this formulation, the goal of this chapter is to learn the mapping $\unknownDynamics$,
which switches between $\ModeInd$ different functions $\mode{\latentFunc}$.
This is a regression problem,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-multimodal-dynamics-disc-regression}
\underbrace{\Delta \state_{t+1}}_{\text{\singleOutput}}
= \underbrace{\unknownDynamicsK(\state_{t}, \control_{t} ; \Delta t = t_*)}_{\text{\mode{\latentFunc} (\singleInput)}}
+ \mode{\epsilon}
\quad \text{if} \quad \modeVarK,
\end{align}
#+END_EXPORT
where both the latent dynamics functions $\unknownDynamics$ and how the system switches between them
$\modeVar$, must be inferred from observations.
A single observation is denoted as $(\singleInput, \singleOutput) = ((\state_{t},\control_{t}), \Delta \state_{t+1})$.
The set of all inputs is denoted as $\allInput \in \R^{\NumData \times \InputDim$
and the set of all outputs as $\allOutput \in \R^{\NumData \times 1}$.
With this notation, the regression problem can be written as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-multimodal-dynamics-disc-regression-simple}
\singleOutput
= \mode{\latentFunc}(\singleInput) + \mode{\epsilon}
\quad \text{if} \quad \modeVarK.
\end{align}
#+END_EXPORT


# from inputs $\singleInput$ to
# outputs $\singleOutput$,


# Given this notation, our task is to learn the mapping $\f$, from inputs $\singleInput$ to
# outputs $\singleOutput$,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-multimodal-dynamics-disc}
# \singleOutput = \mode{\latentFunc} (\singleInput) + \mode{\epsilon}
# \quad \text{if} \quad \modeVarK,
# \end{align}
# #+END_EXPORT
# where the mapping switches between $\ModeInd$ different functions $\mode{\latentFunc}$.

# The $\stateDim^{\text{th}}$ dimension of the $\modeInd^{\text{th}}$ mode's latent transition dynamics
# function $\fk$, evaluated at $\singleInput$,
# is denoted $\fknd = \fkd (\singleInput)$,
# for all dimensions as $\fkn$ and at all data points as $\Fk$.

# A time series of observations from time $a$ to time $b$ (inclusive)
# is denoted by $\mathbf{x}_{a:b}$ (analogously for other variables).
# The concatenation of the state and control domains is
# denoted $\inputDomain \coloneqq \stateDomain \times \controlDomain$.
# A single state-control input is denoted as
# $\singleInput = (\state_{t-1}, \control_{t-1})$, all inputs are denoted as
# $\allInput$, and the set of all outputs as $\allOutput$.
# The $\stateDim^{\text{th}}$ dimension of the $\modeInd^{\text{th}}$ mode's latent transition dynamics
# function $\fk$, evaluated at $\singleInput$,
# is denoted $\fknd = \fkd (\singleInput)$,
# for all dimensions as $\fkn$ and at all data points as $\Fk$.

# This chapter assumes access to a historical data set, $\dataset$, comprising state transitions
# from $\NumEpisodes$ trajectories (episodes), sampled with a fixed time step, $\Delta t=t_*$, resulting in
# $\NumTimesteps$ steps per episode.
# This gives a data set with $N = \NumEpisodes \NumTimesteps$ elements.


# giving the data set,
# $~{\mathcal{D} = \left\{ \left\{ (\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t} \right\}_{\numTimesteps=1}^{\NumTimesteps} \right\}_{\numEpisodes=1}^{\NumEpisodes}}$.

# This chapter assumes access to a historical data set, $\dataset$, comprising state transitions
# from $\NumEpisodes$ trajectories (episodes), sampled with a fixed time step, $\Delta t=t_*$, resulting in
# $\NumTimesteps$ steps per episode.
# This gives a data set with $N = \NumEpisodes \NumTimesteps$ elements.
# We combine the independent episodes to get the data set
# $\mathcal{D} = \left\{ \left\{ (\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t} \right\}_{\numTimesteps=1}^{\NumTimesteps} \right\}_{\numEpisodes=1}^{\NumEpisodes}$.
# We abuse notation (considering time indexing) and denote the data set
# $\mathcal{D} = \left\{ (\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t} \right\}_{\numTimesteps=1}^{\NumTimesteps}$.

# We combine the independent episodes to get the data set
# $\mathcal{D} = \left\{(\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t}\right\}_{t=1}^{T}$
# where we have abused the time indexing notation.

# This work learns a discrete-time representation of cref:eq-multimodal-dynamics-cont,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-multimodal-dynamics-disc}
# \singleOutput = \fk (\singleState, &\singleControl ; \Delta t = t_*) + \mode{\epsilon_{t-1}}
# \quad \text{if} \quad \modeVarK,
# \end{align}
# #+END_EXPORT
# where $\state_t \in \R^D$ and $\control_t \in \R^F$ are the states and controls
# at time $t$ respectively, and $\alpha_t \in \modeDomain = \{1, \dotsc, \ModeInd\}$ is the mode indicator variable (that
# indicates one of $\ModeInd$ dynamics modes) at time $t$.

# A time series of observations from time $a$ to time $b$ (inclusive)
# is denoted by $\mathbf{x}_{a:b}$ (analogously for other variables).
# The concatenation of the state and control domains is
# denoted $\inputDomain \coloneqq \stateDomain \times \controlDomain$.
# A single state-control input is denoted as
# $\singleInput = (\state_{t-1}, \control_{t-1})$, all inputs are denoted as
# $\allInput$, and the set of all outputs as $\allOutput$.
# The $\stateDim^{\text{th}}$ dimension of the $\modeInd^{\text{th}}$ mode's latent transition dynamics
# function $\fk$, evaluated at $\singleInput$,
# is denoted $\fknd = \fkd (\singleInput)$,
# for all dimensions as $\fkn$ and at all data points as $\Fk$.

** Problem Statement :noexport:
\todo{change t-1 to t and t to t+1 to be consistent with control section}
This work considers /unknown/, or /partially unknown/,  multimodal,
stochastic, nonlinear dynamical systems,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-multimodal-dynamics-disc}
\Delta \state_{\timeInd+1}
&= \unknownDynamics(\state_{t}, \control_{t}; \Delta t = t_*) + \bm\epsilon & \nonumber \\
&= \unknownDynamicsK (\state_{\timeInd}, \control_{\timeInd} ; \Delta t = t_*) + \mode{\bm\epsilon}
&\text{if} \quad \modeVar_{\timeInd} = \modeInd
\end{align}
#+END_EXPORT
with continuous states $\state \in \stateDomain \subseteq \R^\StateDim$ and controls
$\control \in \controlDomain \subseteq \R^\ControlDim$.
The state and control at time $t$ are denoted as $\state_t \in \stateDomain$ and $\control_t \in \controlDomain$
respectively and
$\Delta \state_{\timeInd+1} = \state_{\timeInd+1} - \state_{\timeInd}$ denotes the change in state between time $\timeInd-1$
and $\timeInd$.
At any given time step $t$, one of the $\ModeInd$ dynamics modes
$\{\unknownDynamicsK : \stateDomain \times \controlDomain \rightarrow \stateDomain \}_{\modeInd=1}^\ModeInd$,
and associated noise models
$\mode{\bm\epsilon} &\sim \mathcal{N}\left(\mathbf{0}, \bm\Sigma_{\mode{\bm\epsilon}} \right)$, where
$\bm\Sigma_{\mode{\epsilon}} = \text{diag}\left[ \sigma_{\modeInd,1}^{2}, \ldots, \sigma_{\modeInd,\StateDim}^2 \right]$,
are selected by a discrete mode indicator variable,
$\alpha_t \in \modeDomain = \mathbb{Z} \cap [1,\ModeInd]$.
This work considers systems that are stochastic (subject to process noise), but
are not subject to observation noise.
Thus, the $\mode{\bm\epsilon}$ term solely represents the process noise.

# The nominal dynamics
# $\{\nominalDynamicsK : \stateDomain \times \controlDomain \rightarrow \stateDomain \}_{\modeInd=1}^\ModeInd$
# represents our /a priori/ knowledge of the dynamics (i.e. the assumed dynamics)
# and the additive dynamics
# $\{\unknownDynamicsK : \stateDomain \times \controlDomain \rightarrow \stateDomain \}_{\modeInd=1}^\ModeInd$
# represents the initially unknown dynamics, which are to be learned from observations.


This chapter assumes access to historical data comprising state transitions from $\NumEpisodes$ trajectories
of length $T$, sampled  with a  fixed  time step $\Delta t=t_{*}$.
The data set has ${N=ET}$ elements
and we abuse notation by appending the independent trajectories along
time to get the data set $\mathcal{D}=\{(\state_{\timeInd},\control_{\timeInd}),\Delta \state_{t+1}\}^\NumData_{\timeInd=1}$.

To help with understanding and ease of notation, our modelling only considers a single output dimension,
i.e. $\stateDomain \subseteq \R$ as $\fk : \stateDomain \times \controlDomain \rightarrow \stateDomain$.
The extension to multiple output dimensions follows from standard GP methodologies and is detailed where necessary.
To further ease notation, the state-control input domain is denoted
$\inputDomain = \stateDomain \times \controlDomain \subseteq \R^{\InputDim}$
and a single state-control input is dentoed
$\singleInput = (\state_{\timeInd},\control_{\timeInd})$.
Given this formulation, the goal of this chapter is to learn the mapping $\unknownDynamics$,
which switches between $\ModeInd$ different functions $\mode{\latentFunc}$.
This is a regression problem,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-multimodal-dynamics-disc-regression}
\underbrace{\Delta \state_{t+1}}_{\text{\singleOutput}}
= \underbrace{\unknownDynamicsK(\state_{t}, \control_{t} ; \Delta t = t_*)}_{\text{\mode{\latentFunc} (\singleInput)}}
+ \mode{\epsilon}
\quad \text{if} \quad \modeVarK,
\end{align}
#+END_EXPORT
where both the latent dynamics functions $\unknownDynamics$ and how the system switches between them
$\modeVar$, must be inferred from observations.
A single observation is denoted as $(\singleInput, \singleOutput) = ((\state_{t},\control_{t}), \Delta \state_{t+1})$.
The set of all inputs is denoted as $\allInput \in \R^{\NumData \times \InputDim$
and the set of all outputs as $\allOutput \in \R^{\NumData \times 1}$.
With this notation, the regression problem can be written as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-multimodal-dynamics-disc-regression-simple}
\singleOutput
= \mode{\latentFunc}(\singleInput) + \mode{\epsilon}
\quad \text{if} \quad \modeVarK.
\end{align}
#+END_EXPORT


# from inputs $\singleInput$ to
# outputs $\singleOutput$,


# Given this notation, our task is to learn the mapping $\f$, from inputs $\singleInput$ to
# outputs $\singleOutput$,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-multimodal-dynamics-disc}
# \singleOutput = \mode{\latentFunc} (\singleInput) + \mode{\epsilon}
# \quad \text{if} \quad \modeVarK,
# \end{align}
# #+END_EXPORT
# where the mapping switches between $\ModeInd$ different functions $\mode{\latentFunc}$.

# The $\stateDim^{\text{th}}$ dimension of the $\modeInd^{\text{th}}$ mode's latent transition dynamics
# function $\fk$, evaluated at $\singleInput$,
# is denoted $\fknd = \fkd (\singleInput)$,
# for all dimensions as $\fkn$ and at all data points as $\Fk$.

# A time series of observations from time $a$ to time $b$ (inclusive)
# is denoted by $\mathbf{x}_{a:b}$ (analogously for other variables).
# The concatenation of the state and control domains is
# denoted $\inputDomain \coloneqq \stateDomain \times \controlDomain$.
# A single state-control input is denoted as
# $\singleInput = (\state_{t-1}, \control_{t-1})$, all inputs are denoted as
# $\allInput$, and the set of all outputs as $\allOutput$.
# The $\stateDim^{\text{th}}$ dimension of the $\modeInd^{\text{th}}$ mode's latent transition dynamics
# function $\fk$, evaluated at $\singleInput$,
# is denoted $\fknd = \fkd (\singleInput)$,
# for all dimensions as $\fkn$ and at all data points as $\Fk$.

# This chapter assumes access to a historical data set, $\dataset$, comprising state transitions
# from $\NumEpisodes$ trajectories (episodes), sampled with a fixed time step, $\Delta t=t_*$, resulting in
# $\NumTimesteps$ steps per episode.
# This gives a data set with $N = \NumEpisodes \NumTimesteps$ elements.


# giving the data set,
# $~{\mathcal{D} = \left\{ \left\{ (\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t} \right\}_{\numTimesteps=1}^{\NumTimesteps} \right\}_{\numEpisodes=1}^{\NumEpisodes}}$.

# This chapter assumes access to a historical data set, $\dataset$, comprising state transitions
# from $\NumEpisodes$ trajectories (episodes), sampled with a fixed time step, $\Delta t=t_*$, resulting in
# $\NumTimesteps$ steps per episode.
# This gives a data set with $N = \NumEpisodes \NumTimesteps$ elements.
# We combine the independent episodes to get the data set
# $\mathcal{D} = \left\{ \left\{ (\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t} \right\}_{\numTimesteps=1}^{\NumTimesteps} \right\}_{\numEpisodes=1}^{\NumEpisodes}$.
# We abuse notation (considering time indexing) and denote the data set
# $\mathcal{D} = \left\{ (\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t} \right\}_{\numTimesteps=1}^{\NumTimesteps}$.

# We combine the independent episodes to get the data set
# $\mathcal{D} = \left\{(\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t}\right\}_{t=1}^{T}$
# where we have abused the time indexing notation.

# This work learns a discrete-time representation of cref:eq-multimodal-dynamics-cont,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-multimodal-dynamics-disc}
# \singleOutput = \fk (\singleState, &\singleControl ; \Delta t = t_*) + \mode{\epsilon_{t-1}}
# \quad \text{if} \quad \modeVarK,
# \end{align}
# #+END_EXPORT
# where $\state_t \in \R^D$ and $\control_t \in \R^F$ are the states and controls
# at time $t$ respectively, and $\alpha_t \in \modeDomain = \{1, \dotsc, \ModeInd\}$ is the mode indicator variable (that
# indicates one of $\ModeInd$ dynamics modes) at time $t$.

# A time series of observations from time $a$ to time $b$ (inclusive)
# is denoted by $\mathbf{x}_{a:b}$ (analogously for other variables).
# The concatenation of the state and control domains is
# denoted $\inputDomain \coloneqq \stateDomain \times \controlDomain$.
# A single state-control input is denoted as
# $\singleInput = (\state_{t-1}, \control_{t-1})$, all inputs are denoted as
# $\allInput$, and the set of all outputs as $\allOutput$.
# The $\stateDim^{\text{th}}$ dimension of the $\modeInd^{\text{th}}$ mode's latent transition dynamics
# function $\fk$, evaluated at $\singleInput$,
# is denoted $\fknd = \fkd (\singleInput)$,
# for all dimensions as $\fkn$ and at all data points as $\Fk$.

** Preliminaries
Gaussian processes are the state-of-the art approach for Bayesian nonparametric regression
and they provide a powerful mechanism for encoding expert domain knowledge.
As such, \acrshort{mogpe} methods are a promising direction for modelling multimodal systems.
This section recaps the \acrshort{mogpe} concepts that this chapter builds upon.
*** mixture models :ignore:

\newline

*Mixture Models*
Mixture models are a natural choice for modelling multimodal systems.
Given an input $\singleInput$ and an output $\singleOutput$, mixture models
model a mixture of distributions over the output,
\marginpar{mixture models}
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-mixture-model}
\singleMoeEvidence = \sum_{\modeInd=1}^\ModeInd
\underbrace{\Pr(\singleModeVarK \mid \gatingParams)}_{\text{mixing probability}}
\underbrace{p(\singleOutput \mid \modeVarK, \singleInput, \expertParamsK)}_{\text{component } k}.
\end{equation}
#+END_EXPORT
where $\gatingParams$ and $\expertParams$ represent model parameters
and $\singleModeVar$ is a discrete indicator variable assigning observations to components.
The predictive distribution $\singleMoeEvidence$ consists of
$K$ mixture components ${p(\singleOutput \mid \modeVarK, \singleInput, \expertParamsK)}$ that are weighted according to
their mixing probabilities $\Pr(\modeVarK \mid \gatingParams)$.

*** mixture of experts :ignore:

\newline

*Mixture of Experts* The \acrfull{moe} model citep:jacobsAdaptive1991 is
an extension where the mixing probabilities
depend on the input variable $\moeGatingPosterior$, and are
collectively referred to as the *gating network*.
The individual component densities $\moeExpertPosterior$ are then referred to as *experts*,
as at different regions in the input space, different components are responsible for predicting.
Given $\ModeInd$ experts $\moeExpertPosterior$, each with parameters $\expertParamsK$,
the MoE marginal likelihood is given by,
\marginpar{mixture of experts}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixture-marginal-likelihood}
\moeEvidence = \prod_{\numData=1}^\NumData \sum_{\modeInd=1}^{\ModeInd}
\underbrace{\moeGatingPosterior}_{\text{gating network}}
\underbrace{\moeExpertPosterior}_{\text{expert } k},
\end{align}
#+END_EXPORT
where $\singleModeVar$ is the expert indicator variable assigning observations to experts.
The probability mass function over the expert indicator variable is referred to as the gating network and
indicates which expert governs the model at a given input location.
See cite:yukselTwenty2012 for a survey of \acrshort{moe} methods.
*** Nonparametric Mixtures of Experts :ignore:

\newline
*Nonparametric Mixtures of Experts*
Modelling the experts as GPs gives rise to a class of powerful models known as \acrfull{mogpe}.
They can model multimodal distributions as they model a mixture of distributions
over the outputs, usually a Gaussian mixture in the regression setting citep:trespMixtures2000a,rasmussenInfinite2001.
They are able to model non-stationary functions as
each expert learns separate hyperparameters (lengthscales, noise variances etc).
Many \acrshort{mogpe} methods have been proposed and in general they differ via
the formulation of their gating network and their approximate inference algorithms.
# and the gating network can turn each expert "on" and "off" in different regions of the input space.

*** npmoe graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
    \begin{minipage}[r]{0.49\textwidth}
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInputK$};

      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\allInputK)$};

      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[latent, below=of x, xshift=0.0cm] (a) {$\modeVarnk$};

      \node[const, right=of a, xshift=-0.4cm] (phik) {$\gatingParams$};

      \node[obs, below=of f, yshift=0.4cm] (y) {$\allOutputK$};

      \node[const, left=of y] (sigmak) {$\noiseVarK$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      %\draw[post] (f)--(yk);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)--(a);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(a);
      %\draw[post] (sigmak)|-(yk);
      \draw[post] (sigmak)|-(y);
      \draw[post] (f)--(y);

      \plate {} {(x) (yk) (a)} {$\NumData_{\modeInd}$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      %\plate {} {(f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(x) (y) (f) (sigmak) (thetak) (a)} {$\ModeInd$};
    \end{tikzpicture}
    %}
    \subcaption{}
\label{fig-graphical-model-npmoe}
\end{minipage}
    \begin{minipage}[r]{0.49\textwidth}
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInputK$};

      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\allInputK)$};
      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[latent, below=of x, xshift=0.0cm] (a) {$\modeVarnk$};
      \node[latent, right=of a, yshift=0.0cm] (h) {$\mode{\gatingFunc}(\allInput)$};
      \node[const, right=of h, xshift=-0.4cm] (phik) {$\gatingParamsK$};

      \node[obs, below=of f, yshift=0.4cm] (y) {$\allOutputK$};
      \node[const, left=of y] (sigmak) {$\noiseVarK$};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      \draw[post] (x)-|(h);
      \draw[post] (h)--(a);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      %\draw[post] (sigmak)|-(yk);
      \draw[post] (sigmak)|-(y);
      \draw[post] (f)--(y);

      \plate {} {(x) (yk) (a)} {$\NumData_{\modeInd}$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      %\plate {} {(f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(h) (phik)} {$\ModeInd$};
      \plate {} {(x) (y) (f) (sigmak) (thetak)  (a)} {$\ModeInd$};
    \end{tikzpicture}
    %}
    \subcaption{}
\label{fig-graphical-model-gp-gating-network}
\end{minipage}
  %\caption{Graphical model where the output $\singleOutput$}
  \caption{
  Graphical models where the outputs $\allOutput = \{\allOutputK\}_{\modeInd=1}^{\ModeInd}$
are generated by mapping the inputs $\allInput = \{\allInputK\}_{\modeInd=1}^{\ModeInd}$ through the latent process.
  An input assigned to expert $\modeInd$ is denoted $\singleInputK$
  and the sets of all $\NumData_{\modeInd}$ inputs and outputs assigned to expert $\modeInd$ are denoted
  $\allInputK$ and $\allOutputK$ respectively.
The experts are shown on the left of each model and the gating network on the right.
The generative process involves evaluating the gating network
and sampling an expert mode indicator variable $\singleModeVar$.
The indicated mode's latent function $\mode{f}$ and noise model $\mathcal{N}(0, \noiseVarK)$ are
then evaluated to generate the output $\singleOutput$.
  (\subref{fig-graphical-model-npmoe}) shows
   the Mixture of Gaussian Process Experts model first presented in
  \cite{rasmussenInfinite2001} but without the Dirichlet process prior on the gating network.
  This represents the basic conditional model, not the full generative model over both the inputs and outputs as
  presented in \cite{NIPS2005_f499d34b}.
  (\subref{fig-graphical-model-gp-gating-network}) shows our model with a GP-based gating network
which involves evaluating $\ModeInd$ latent gating functions $\GatingFunc$
and normalising their output to obtain the mixing probabilities $\singleGatingLikelihood$.
The mode indicator variable $\singleModeVar$ is then sampled from the Categorical distribution
governed by these probabilities.}
\label{fig-graphical-model-comparison}
\end{figure}
#+END_EXPORT

*** after graphical model :ignore:

\newline

As highlighted by cite:rasmussenInfinite2001,
the traditional MoE marginal likelihood does not apply when the
experts are nonparametric.
This is because the model assumes that the observations are i.i.d. given the model parameters,
which is contrary to \acrshort{gp} models, which model the dependencies in the joint distribution, given the
hyperparameters.
\marginpar{mixtures of nonparametric experts}
cite:rasmussenInfinite2001 point out that there is a joint distribution corresponding to every possible
combination of assignments (of observations to experts).
The marginal likelihood is then a sum over exponentially many ($\ModeInd^{\NumData}$) sets of assignments,
#+BEGIN_EXPORT latex
\small
\begin{align}  \label{eq-np-moe-marginal-likelihood-assign}
\moeEvidence &= \sum_{\allModeVar} \npmoeGatingPosterior \npmoeExpertPosterior  \nonumber \\
&= \sum_{\allModeVar} \npmoeGatingPosterior
\left[ \prod_{\modeInd=1}^\ModeInd p\left(\allOutputK \mid \allInputK, \expertParamsK \right) \right],
\end{align}
\normalsize
#+END_EXPORT
where $\allModeVar = \{\modeVar_1, \ldots, \modeVar_\NumData \}$ represents a set of assignments for all observations.
cref:fig-graphical-model-npmoe shows its graphical model representation.
Note that $\allInputK = \{\singleInput : \singleModeVarK \}_{\numData=1}^{\NumData_{\modeInd}}$ and
$\allOutputK = \{\singleOutput : \singleModeVarK \}_{\numData=1}^{\NumData_{\modeInd}}$
represent the $\NumData_{\modeInd}$ inputs and outputs assigned to the $\modeInd^{\text{th}}$
expert respectively.
This distribution factors into the product over experts, where each expert models the joint Gaussian distribution
over the observations assigned to it.
Assuming a mixture of Gaussian process regression models,
the marginal likelihood in cref:eq-np-moe-marginal-likelihood-assign
can be expanded to show each of the experts' latent variables,
#+BEGIN_EXPORT latex
\marginpar{marginal likelihood}
\begin{align} \label{eq-np-moe-marginal-likelihood}
%\moeEvidence
\evidence
%&= \sum_{\allModeVar} \npmoeGatingPosterior \npmoeExpertPosterior  \\
%&= \sum_{\allModeVar} \npmoeGatingPosterior
%\left[ \prod_{\modeInd=1}^\ModeInd p\left(\allOutputK \mid \allInputK, \expertParamsK \right) \right]   \\
&= \sum_{\allModeVar} \npmoeGatingPosterior
\left[ \prod_{\modeInd=1}^\ModeInd
\E_{\expertPrior \right} \left[
\prod_{\numData=1}^{\NumData_{\modeInd}}
p\left(\singleOutput \mid \mode{\latentFunc}(\singleInput) \right)
\right] \right],
\end{align}
#+END_EXPORT
where each expert follows the standard Gaussian likelihood model,
\marginpar{expert's Gaussian likelihood}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-likelihood}
\singleOutput &= \mode{f}(\singleInput) + \epsilon_{k}, \quad \epsilon_{k} \sim \mathcal{N}(0, \mode{\noiseVar}^2), \\
\singleExpertLikelihood &= \mathcal{N}\left( \singleOutput \mid \mode{f}(\singleInput), \mode{\noiseVar}^2 \right),
%\singleOutput \mid \modeVar, \mode{\latentFunc}(\singleInput) &\sim \mathcal{N}\left( \mode{f}(\singleInput), \mode{\noiseVar}^2 \right),
\end{align}
#+END_EXPORT
with $\mode{f}$ and $\mode{\noiseVar}^2$ representing the latent function and the noise variance associated
with the $\modeInd$^{\text{th}} expert.
Note that for notational conciseness the dependency on $\modeVarK$ is dropped from
$\singleExpertLikelihood$ as it is implied by the mode indexing $\mode{\latentFunc}$.
The dependence on $\gatingParams$ and $\expertParams$ is also dropped from here on in.
Independent GP priors are placed on each of the expert's latent functions,
\marginpar{expert's GP prior}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior}
%\expertPrior &= \mathcal{N}\left( \mode{f}(\allInput) \mid \expertMeanFunc(\allInput), \expertCovFunc(\allInput, \allInput) \right) \\
%\mode{\latentFunc}(\allInput) &\sim \mathcal{N}\left( \expertMeanFunc(\allInput), \expertCovFunc(\allInput, \allInput) \right) \\
\mode{\latentFunc}(\allInputK) &\sim \mathcal{N}\left( \expertMeanFunc(\allInputK), \expertCovFunc(\allInputK, \allInputK) \right)
\end{align}
#+END_EXPORT
where $\expertMeanFunc(\cdot)$ and $\expertCovFunc(\cdot, \cdot)$ represent the mean and
covariance functions associated with the $\modeInd^{\text{th}}$ expert respectively.
This leads to each expert resembling a standard \acrshort{gp} regression model with a Gaussian likelihood.
For notational conciseness the dependence on the inputs $\allInputK$ and hyperparameters $\expertParamsK$
is dropped for each \acrshort{gp} prior,
i.e. $\expertPrior = p\left( \mode{f}(\allInputK) \mid \allInputK, \expertParamsK \right)$.
# Note that the expectation over each experts' latent function in cref:eq-np-moe-marginal-likelihood
# is inside the marginalisation of the expert indicator variable $\modeVar$.

** Related Work :noexport:
Modelling the experts as GPs gives rise to a class of powerful models known as
Mixture of Gaussian Process Experts (MoGPE).
These models address both of the limitations that we discussed.
They can model multimodal predictive distributions as they model a mixture of distributions
over the outputs, usually a Gaussian mixture in the regression setting (cite:Tresp,Rasmussen2002).
They are able to model non-stationary functions as
each expert can learn different lengthscales, noise variances etc and the gating
network can turn each one "on" and "off" in different regions of the input space.
Many approaches to MoGPE have been proposed and in general the key differences between them are
the formulation of the gating network
and their inference algorithms (and associated approximations) cite:Yuksel2012.
The original MoGPE work by cite:Tresp proposed a gating network as a softmax of $K$ GPs
(bottom left plot in Figure [[ref:gating_network_comparison]]).
An EM inference scheme is proposed, which assuming there are $N$ observations requires
$\mathcal{O}(3KN^3)$ computations per iteration.


Although this gating function divides up the input space cite:Rasmussen2002 argue that
data not assigned to a GP expert will lead to bias near the boundaries.
Instead cite:Rasmussen2002 formulate the gating network in terms of conditional
distributions on the expert indicator variable, on which they place an input-dependent
Dirichlet process prior.
cite:Meeds2005 proposed an alternative infinite MoGPE similar to cite:Rasmussen2002 except
that they specify a full generative model over the input and output space $p(\mathbf{y}, \mathbf{x})$
as opposed to just a
conditional model $p(\mathbf{y} | \mathbf{x})$.
Each experts inputs are now Gaussian distributed
(middle top plot of Figure [[ref:gating_network_comparison]]) giving rise to the gating
network in the middle bottom plot of Figure [[ref:gating_network_comparison]].
This gating network is not capable of turning an expert "on" in multiple regions
of the input space. It will instead generate a new cluster and assign it a new expert.
These approaches reduce the computational burden by associating each expert with a
subset of the observations but rely on Markov Chain Monte Carlo (MCMC) inference schemes.

cite:Yuan proposed a MoGPE with a variational inference scheme which is much
faster than using MCMC.
They assume that the expert indicators $\alpha$ are generated by a Dirichlet distribution and
that given the expert indicator the inputs follow a Gaussian mixture model (GMM), i.e. they
have introduced a cluster indicator variable $z$.
This contrasts earlier approaches that let the expert indicator act as the input cluster
indicator.
This is illustrated in the right hand plots of Figure [[ref:gating_network_comparison]] which
show two Gaussian mixtures over the inputs (one for each expert) and
the resulting expert mixing probabilities.
Introducing the separate cluster indicator variable has given rise to a
gating network that can turn a single expert "on" in multiple regions of the input space.
However, it does not provide a handle for encoding prior information like the GP based gating network.

We note that mixture models are inherently unidentifiable as
different combinations of mixture components
and mixing coefficients can generate the same predictive distributions.
The gating network can be considered a handle for encoding prior knowledge that can be used to constrain
the set of admissible functions.
This can improve identifiability and lead to learned representations that better reflect
our understanding of the system. The simplest case being reordering the experts.
Figure ref:gating_network_comparison provides a visual comparison of
different gating network formulations,
providing intuition for how they can differ in restricting the set of admissible functions.


Modelling the gating network with GPs enables us
to encode informative prior knowledge through the choice of mean and covariance functions.
For example, adopting a squared exponential covariance function would encode prior belief that
the mixing probabilities should vary smoothly across the input space.
If modelling a dynamical system with oscillatory behaviour (e.g. an engine)
a periodic kernel could be adopted.
Prior knowledge of the mixing probability values can be encoded through
the choice of mean function.
We may also be interested in exploiting techniques from differential geometry
e.g. finding probabilistic geodesics (cite:Tosi2014) on the gating network.
Again, our choice of covariance function can be used to encode how differentiable
the gating network should be.
Learning better representations will also improve the models
ability to extrapolate as it will be more true to the underlying system.

We can also consider the implications of the mentioned gating networks from an inference perspective.
It is known that developing inference algorithms for propagating uncertain (Gaussian) inputs
through GPs is itself a challenging problem (see cite:Ustyuzhaninov2019).
Therefore, developing a scalable inference algorithm to propagate inputs that are distributed according to
a Gaussian mixture will likely lead to valuable information loss.
Theoretically the approaches by cite:Rasmussen2002,Meeds2005 are able to achieve very
accurate results but their inference relies on MCMC sampling methods,
which can be slow to converge.

We consider our approach as trading in the computational benefits that can be obtained through
the formulation of the gating network
for the ability to improve identifiability with informative GP priors.
The main contributions of this paper are twofold. Firstly, we re-formulate a gating network
based on GPs to improve indentifiability.
Secondly, we derive an evidence lower bound
which improves on the complexity issues associated with inference when adopting such a gating network.
Motivated by learning representations of dynamical systems with two regimes we instantiate
the model with two experts as it is  a special case where the gating network can be calculated
in closed form.
This is because we seek to learn an operable mode with one expert and explain away the inoperable mode
with the other. This results
in the gating network indicating which regions of the
input space are operable, providing a convenient space for planning.

# The remainder of the paper is organised as follows. We first introduce our model
# in Section ref:sec-modelling
# and then derive our variational lower
# bound in Section ref:sec-variational-approximation.
# In Section ref:sec-model-validation we test our method on an artificial data set where we show
# the benefits of adopting informative covariance functions in the gating network.
# We then test our model on the motorcycle data set and compare it to a sparse variational GP.

** Identifiable Mixtures of Gaussian Process Experts
*** intro :ignore:noexport:
# Motivated by synergising model learning and control, the probabilistic model constructed in this chapter
# Given this motivation, the probabilistic model constructed in this chapter
# is a \acrfull{mogpe} with a \acrshort{gp}-based gating network.
# Motivated by identifiability this work adopts a GP-based gating network resembling a GP classification model.
# Such a gating network is able to turn a single expert on in multiple subsets of the domain.

# The \acrshort{gp}-based gating network infers latent /geometric/ structure that is exploited by the control
# algorithms in cref:chap-traj-opt-geometry.

*** intro :ignore:noexport:
Motivated by synergising model learning and control, the probabilistic model constructed in this chapter
resembles a \acrfull{mogpe} with a \acrshort{gp}-based gating network.
The \acrshort{gp}-based gating network infers latent /geometric/ structure that is exploited by the control
algorithm in Section ref:sec-traj-opt-geometric.
# This work derives a novel variational lower bound based on sparse GP approximations, that provides
# well-calibrated uncertainty estimates and scalability via stochastic gradient methods.

# Given a factorised likelihood for each expert (e.g. Gaussian),
# an alternative approach to cref:eq-np-moe-marginal-likelihood-assign is to let each expert
# predict on the basis of all observations and leave the gating network to soft assign the observations.
# An alternative approach to cref:eq-np-moe-marginal-likelihood-assign is to let each expert
# predict on the basis of all observations (i.e. $\ModeVarK = \{\modeVarK\}_{\numData=1}^{\NumData}$)
# and leave the gating network to soft assign the observations.
# The marginal likelihood of this model is then given by,

The marginal likelihood in cref:eq-np-moe-marginal-likelihood-assign
can be expanded to show each of the experts' latent variables,
#+BEGIN_EXPORT latex
\marginpar{marginal likelihood}
\begin{align} \label{eq-np-moe-marginal-likelihood}
\moeEvidence &= \sum_{\allModeVar} \npmoeGatingPosterior \npmoeExpertPosterior  \\
&= \sum_{\allModeVar} \npmoeGatingPosterior
\left[ \prod_{\modeInd=1}^\ModeInd p\left(\allOutputK \mid \allInputK, \expertParamsK \right) \right]   \\
&= \sum_{\allModeVar} \npmoeGatingPosterior
\left[ \prod_{\modeInd=1}^\ModeInd
\prod_{\numData=1}^{\NumData}
p\left(\singleOutput \mid \mode{\latentFunc}(\singleInput) \right)
p\left(\mode{\latentFunc}(\allInputK) \mid \allInputK \right)
\right],
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \marginpar{marginal likelihood}
# \begin{align} \label{eq-np-moe-marginal-likelihood}
# \moeEvidence &=
# \sum_{\modeInd=1}^\ModeInd
# \underbrace{ \Pr\left(\allModeVarK \mid \allInput, \gatingParams \right)}_{\text{gating network}}
# \underbrace{ p(\allOutput \mid \allModeVarK, \allInput, \expertParamsK) }_{\text{expert } \modeInd} \\
# &=
# \sum_{\modeInd=1}^\ModeInd
# \underbrace{ \left[ \prod_{\numData=1}^\NumData
# \Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right) \right]}_{\text{gating network}}
# \underbrace{\E_{\expertPrior} \left[ \prod_{\numData=1}^\NumData \singleExpertLikelihood \right]}_{\text{expert } \modeInd},
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \marginpar{marginal likelihood}
# \begin{align} \label{eq-np-moe-marginal-likelihood}
# \moeEvidence &=
# \prod_{\numData=1}^\NumData \sum_{\modeInd=1}^\ModeInd
# \underbrace{\Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right)}_{\text{gating network}}
# \underbrace{\E_{\expertPrior} \left[ \singleExpertLikelihood \right]}_{\text{expert } \modeInd},
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \marginpar{marginal likelihood}
# \begin{align} \label{eq-np-moe-marginal-likelihood}
# \moeEvidence &=
# \underbrace{\E_{\expertPrior}}_{\text{expert } k \text{ prior}} \left[
# \prod_{\numData=1}^\NumData \sum_{\modeInd=1}^\ModeInd
# \underbrace{\Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right)}_{\text{gating network}}
# \underbrace{ \singleExpertLikelihood}_{\text{expert } k \text{ likelihood}}
# \right],
# \end{align}
# #+END_EXPORT
where each expert follows the standard Gaussian likelihood model,
\marginpar{expert's Gaussian likelihood}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-likelihood}
\singleOutput &= \mode{f}(\singleInput) + \epsilon_{k}, \quad \epsilon_{k} \sim \mathcal{N}(0, \mode{\noiseVar}^2), \\
\singleExpertLikelihood &= \mathcal{N}\left( \singleOutput \mid \mode{f}(\singleInput), \mode{\noiseVar}^2 \right),
%\singleOutput \mid \modeVar, \mode{\latentFunc}(\singleInput) &\sim \mathcal{N}\left( \mode{f}(\singleInput), \mode{\noiseVar}^2 \right),
\end{align}
#+END_EXPORT
with $\mode{f}$ and $\mode{\noiseVar}^2$ representing the latent function and the noise variance associated
with the $\modeInd$^{\text{th}} expert.
Note that the dependency on $\modeVarK$ is dropped from $\singleExpertLikelihood$ for notational conciseness.
Independent GP priors are placed on each of the expert's latent functions,
\marginpar{expert's GP prior}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior}
%\expertPrior &= \mathcal{N}\left( \mode{f}(\allInput) \mid \expertMeanFunc(\allInput), \expertCovFunc(\allInput, \allInput) \right) \\
%\mode{\latentFunc}(\allInput) &\sim \mathcal{N}\left( \expertMeanFunc(\allInput), \expertCovFunc(\allInput, \allInput) \right) \\
\mode{\latentFunc}(\allInputK) &\sim \mathcal{N}\left( \expertMeanFunc(\allInputK), \expertCovFunc(\allInputK, \allInputK) \right)
\end{align}
#+END_EXPORT
where $\expertMeanFunc(\cdot)$ and $\expertCovFunc(\cdot, \cdot)$ represent the mean and
covariance functions associated with the $\modeInd^{\text{th}}$ expert respectively.
This leads to each expert resembling a standard GP regression model with a Gaussian likelihood.
For notational conciseness the dependence on the inputs $\allInputK$ and hyperparameters $\expertParamsK$
is dropped for each GP prior,
i.e. $\expertPrior = p\left( \mode{f}(\allInputK) \mid \allInputK, \expertParamsK \right)$.
\todo{add reference to section number}
Note that the expectation over each experts' latent function in cref:eq-np-moe-marginal-likelihood
is inside the marginalisation of the expert indicator variable $\modeVar$.
# Importantly, this model retains the uncertainty in the assignment of observations to experts
# and will lead to each expert not overfitting to the observations assigned to it.
# This approach can be see as trading in the computational benefits of assigning observations to experts
# (Equation ref:eq-np-moe-marginal-likelihood-assign) in favour
# of directly capturing the correlations between all observations.

For ease of notation and understanding, only a single output dimension has been considered,
although in most scenarios the output dimension will be greater than $1$.
This work can be extended to multiple output dimensions following standard multioutput GP
methodologies cite:vanderwilkFramework2020.

*** Identifiable Mixtures of Sparse Variational Gaussian Process Experts :ignore:
# Motivated by synergising model learning and control, the probabilistic model constructed in this chapter
# Given this motivation, the probabilistic model constructed in this chapter
# is a \acrfull{mogpe} with a \acrshort{gp}-based gating network.

Motivated by improving identifiability and learning latent spaces for control,
this work adopts a GP-based gating network resembling a GP classification model,
similar to that used in the original \acrshort{mogpe} model citep:trespMixtures2000a.
The marginal likelihood is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood-assign}
\evidence &=
\sum_{\allModeVar}
\underbrace{\E_{\gatingPrior} \left[
%\frac{1}{Z}
\prod_{\numData=1}^{\NumData}
\gatingLikelihood
% p(\modeVarn \mid \GatingFunc(\singleInput))
 \right]}_{\text{GP gating network}}
  \underbrace{\prod_{\modeInd=1}^\ModeInd p\left(\{\singleOutput : \singleModeVarK \} \mid \{\singleInput : \singleModeVarK \} \right)}_{\text{experts}}
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-augmented-marginal-likelihood}
# \evidence &\approx
# \E_{\expertsInducingPrior \gatingPrior} \left[
# \prod_{\numData=1}^{\NumData}
# \sum_{\modeInd=1}^{\ModeInd}
#  \singleGatingLikelihood \singleExpertGivenInducing \right],
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-marginal-likelihood}
# \evidence &=
# \sum_{\modeInd=1}^\ModeInd
# \underbrace{\E_{\gatingPrior} \left[ \prod_{\numData=1}^\NumData \singleGatingLikelihood \right]}_{\text{gating network}}
# \underbrace{\E_{\expertPrior} \left[ \prod_{\numData=1}^\NumData \singleExpertLikelihood \right]}_{\text{expert } k}.
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-marginal-likelihood}
# \evidence &=
# \E_{\gatingPrior \expertPrior} \left[
#  \sum_{\modeInd=1}^\ModeInd
#  \prod_{\numData=1}^\NumData
# \underbrace{\singleGatingLikelihood}_{\text{gating network}}
# \underbrace{\singleExpertLikelihood}_{\text{expert } k}
# \right]
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-marginal-likelihood}
# \evidence &=
# \prod_{\numData=1}^\NumData \sum_{\modeInd=1}^\ModeInd
# \underbrace{\E_{\gatingPrior} \left[ \singleGatingLikelihood \right]}_{\text{gating network}}
# \underbrace{\E_{\expertPrior} \left[ \singleExpertLikelihood \right]}_{\text{expert } k}.
# \end{align}
# #+END_EXPORT
where the gating network resembles a Gaussian process
classification model, with a factorised classification likelihood $\gatingLikelihood$ dependent on
input dependent functions
$\GatingFunc = \{\mode{\gatingFunc} : \inputDomain \rightarrow \R \}_{\modeInd=1}^\ModeInd$,
known as gating functions.
The probability mass function over the expert indicator variable is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-prob-mass}
\gatingLikelihood &= \prod_{\modeInd=1}^\ModeInd \left(\singleGatingLikelihood\right)^{[\singleModeVarK]},
%\singleModeVar \mid \GatingFunc(\singleInput) &\sim \text{Categorical}\left(\ModeInd, \text{softmax}(\GatingFunc(\singleInput)) \right)
%= \prod_{\modeInd=1}^\ModeInd \left(\singleGatingLikelihood\right)^{[\singleModeVarK]}
\end{align}
#+END_EXPORT
where $[\singleModeVarK]$ denotes the Iverson bracket.
The probabilities of this Categorical distribution $\singleGatingLikelihood$
are governed by a classification likelihood (Bernoulli/Softmax).
Fig. [[ref:fig-graphical-model-gp-gating-network]] shows the graphical model where the $\ModeInd$ latent gating functions $\GatingFunc$
are evaluated and normalised to obtain the mixing probabilities
$\singleGatingLikelihood$.
The mode indicator variable $\singleModeVar$ is then sampled from the Categorical distribution
governed by these probabilities.
The indicated mode's latent function $\mode{f}$ and process noise $\noiseVarK$ are
then evaluated to generate the output $\singleOutput$.


*Softmax ($\ModeInd>2$)* In the general case, that is, when there are more than two experts,
$\ModeInd > 2$, the gating network's likelihood is defined as the Softmax function,
\marginpar{softmax}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-softmax}
\singleGatingLikelihood = \text{softmax}_{\modeInd}\left(\GatingFunc(\singleInput)\right) = \frac{\text{exp}\left(\mode{\gatingFunc}(\singleInput)\right)}{\sum_{j=1}^{\ModeInd} \text{exp}\left(\gatingFunc_j(\singleInput) \right)}.
\end{align}
#+END_EXPORT
Each gating function $\mode{\gatingFunc}$ describes how its corresponding mode's mixing
probability varies over the input space.
Modelling the gating network with input-dependent functions enables
informative prior knowledge to be encoded through the placement of GP priors on each gating function.
Further to this, if the modes are believed to only vary over a subset of the state-control input space,
then the gating functions can depend only on this subset.
Independent GP priors are placed on each gating function, giving the gating network prior,
\marginpar{GP priors}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-prior}
%\gatingPrior &= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \mode{\gatingFunc}(\allInput) \mid \gatingMeanFunc(\allInput), \gatingCovFunc(\allInput, \allInput) \right) \\
\GatingFunc(\allInput) &\sim \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \gatingMeanFunc(\allInput), \gatingCovFunc(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
where $\gatingMeanFunc(\cdot)$ and $\gatingCovFunc(\cdot,\cdot)$ are the mean and covariance functions
associated with the $\modeInd^\text{th}$ gating function.
Similarly to the experts, dependence on the inputs and hyperparameters is dropped from the gating network's GP prior,
i.e. $\gatingPrior = p(\GatingFunc(\allInput) \mid \allInput, \gatingParams)$.
In contrast to the experts, partitioning of the data set is not desirable for the gating network GPs,
as each gating function should depend on all of the training observations.

# Each mode's mixing probability $\singleGatingPosterior$ is then obtained by marginalising
# *all* of the gating functions,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-gating-posterior}
# \singleGatingPosterior
# &= \int \gatingPrior \singleGatingLikelihood \text{d} \GatingFunc(\allInput).
# %&= \E_{\gatingPrior}\left[ \singleGatingLikelihood \right].
# \end{align}
# #+END_EXPORT
# In the general case where $\singleGatingLikelihood$ uses the softmax function
# (cref:eq-softmax) this integral is intractable, so it is approximated with Monte carlo quadrature.
Each mode's mixing probability $\singleGatingPosterior$ is then obtained by marginalising
*all* of the gating functions.
In the general case where $\singleGatingLikelihood$ uses the softmax function
(cref:eq-softmax) this integral is intractable, so it is approximated with Monte carlo quadrature.

*Bernoulli ($\ModeInd=2$)* Instantiating the model with two experts, $\singleModeVar \in \{1, 2\}$, is a special case
where only a single gating function is needed.
\marginpar{two experts}
This is because the output of a function $\gatingFunc(\singleInput)$ can be mapped through a sigmoid
function $\text{sig} : \R \rightarrow [0, 1]$ and interpreted as a probability,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sigmoid}
\Pr(\singleModeVar=1 \mid \modei{\gatingFunc}{1}(\singleInput)) = \text{sig}(\modei{\gatingFunc}{1}(\singleInput)).
\end{align}
#+END_EXPORT
If this sigmoid function satisfies the point symmetry condition then
the following holds,
$\Pr(\singleModeVar=2 \mid \modei{\gatingFunc}{1}(\singleInput)) = 1 - \Pr(\singleModeVar=1 \mid \modei{\gatingFunc}{1}(\singleInput))$.
This only requires a single gating function and no normalisation term needs to be calculated.
If the sigmoid function in cref:eq-sigmoid is selected
to be the Gaussian cumulative distribution function
$\Phi(\gatingFunc(\cdot)) = \int^{\gatingFunc(\cdot)}_{-\infty} \mathcal{N}(\tau | 0, 1) \text{d} \tau$,
then the mixing probability can be calculated in closed-form,
# then the integral in cref:eq-gating-posterior can be calculated in closed-form,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixing-probabilities-analytic}
\Pr(\modeVarn=1 \mid \singleInput, \gatingParams) &=
 \int \Phi\left(\gatingFunc(\singleInput)\right) \mathcal{N}\left(\gatingFunc(\singleInput) \mid \mu_h, \sigma^2_h \right) \text{d} \gatingFunc(\singleInput) \nonumber \\
&= \Phi \left(\frac{\mu_{h}}{\sqrt{1 + \sigma^2_{h} }}\right),
\end{align}
#+END_EXPORT
where $\mu_h$ and $\sigma^2_{h}$ represent the mean and variance of the gating GP at $\singleInput$ respectively.


# This model makes single-step probabilistic predictions,
# where the predictive distribution over the output $\singleOutput$ is
# given by a mixture of Gaussians.
# This provides the model flexibility to model multimodal transition dynamics $f$ as mixtures of $\modeInd$ experts.
# In the two expert case the marginal likelihood is a mixture of two Gaussians and can be calculated in
# closed-form.
# \todo{can it be calculated in closed form?}
# It has complexity $\mathcal{O}(\ModeInd\NumData^4)$ to evaluate
# and the general case (with more than two experts) has complexity $\mathcal{O}(\ModeInd^2\NumData^4)$.

*** graphical model :ignore:noexport:

#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};

      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\singleInput)$};

      \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\expertInducingOutput$};
      \node[const, left=of uk, xshift=0.4cm] (zk) {$\expertInducingInput$};


      \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\expertParamsK$};
%      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[const, below=of f] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      \node[latent, right=of a, yshift=1.4cm, xshift=-0.4cm] (h) {$\mode{\gatingFunc}(\allInput)$};
      \node[const, right=of h, xshift=-0.4cm] (phik) {$\gatingParamsK$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \draw[post] (uk)--(f);
      \draw[post] (zk)--(uk);
      \draw[post] (thetak)--(f);

      \plate {} {(x) (y) (a) (f)} {$\NumData$};
      \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    %}
  \caption{Graphical model where the output $\singleOutput$
is generated by mapping the input $\singleInput$ through the latent process. The experts are shown on the
left and the gating network is shown on the right.
The geneartive process involves evaluating the $\ModeInd$ latent gating functions $\GatingFunc$
and normalising their output to obtain the mixing probabilities $\singleGatingLikelihood$.
The mode indicator variable $\singleModeVar$ is then sampled from the Categorical distribution
governed by these probabilities.
The indicated mode's latent function $\mode{f}$ and noise model $\mathcal{N}(\mathbf{0}, \noiseVarK)$ are
then evaluated to generate the output $\singleOutput$.}
\label{fig-graphical-model}
\end{figure}
#+END_EXPORT

*** graphical model non sparse experts :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\allInput)$};
     % \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\hk$};


      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[const, below=of f] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      \node[latent, right=of a, yshift=1.4cm, xshift=-0.4cm] (h) {$\mode{\gatingFunc}(\allInput)$};
      \node[const, right=of h, xshift=-0.4cm] (phik) {$\gatingParamsK$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    %}
  \caption{Graphical model of the transition dynamics where the state difference $\singleOutput$
is generated by pushing the state and control $\singleInput$ through the latent process.}
\label{fig-graphical-model}
\end{figure}
#+END_EXPORT


# #+BEGIN_EXPORT latex
# \begin{figure}[t]
#   \centering
#   \resizebox{0.8\columnwidth}{!}{
#     \begin{tikzpicture}[
#       pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
#       post/.style={->,shorten >=0.4pt,>=stealth',semithick}
#       ]
#       \node[const] (x) {$\hat{\mathbf{x}}_{t-1}$};
#       \node[latent, left=of x, yshift=-1.4cm] (f) {$\mathbf{f}^{(k)}_t$};
#       %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
#       \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {${h}^{(k)}_t$};

#       \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\hat{\mathbf{f}}^{(k)}$};
#       \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\hat{\mathbf{h}}^{(k)}$};
#       \node[const, left=of uk, xshift=0.4cm] (zk) {$\bm\xi^{(k)}_f$};
#       \node[const, right=of uh, xshift=-0.4cm] (zh) {$\bm\xi^{(k)}_h$};

#       \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\bm\theta^{(k)}$};
#       \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\bm\phi^{(k)}$};

#       \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\sigma^{(k)}$};

#       \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\Delta\mathbf{x}_t$};
#       %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
#       \node[latent, right=of y, xshift=-0.4cm] (a) {${\alpha}_t$};

#       %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

#       \factor[above=of a] {h-a} {left:Cat} {h} {a};

#       \draw[post] (a)--(y);
#       \draw[post] (x)-|(f);
#       %\draw[post] (f)--(yk);
#       \draw[post] (f)--(y);
#       %\draw[post] (yk)--(y);
#       %\draw[post] (h)--(a);
#       \draw[post] (x)-|(h);
#       \draw[post] (uk)--(f);
#       \draw[post] (uh)--(h);
#       \draw[post] (zk)--(uk);
#       \draw[post] (zh)--(uh);
#       \draw[post] (thetak)--(f);
#       \draw[post] (phik)--(h);
#       \draw[post] (sigmak)|-(y);

#       \plate {} {(x) (y) (a) (f) (h)} {$T$};
#       %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
#       \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$K$};
#       \plate {} {(zh) (uh) (h) (phik)} {$K$};
#     \end{tikzpicture}
#     }
#   \caption{Graphical model of the transition dynamics
# where the state difference $\Delta\mathbf{x}_{t}$
# is generated by pushing the state and control $\hat{\mathbf{x}}_{t-1}$
# through the latent process.}
# %\label{fig:graphical_model}
# \end{figure}
# #+END_EXPORT

*** Generative Model :noexport:
# The marginal likelihood of this model can be written to clearly show the factorised likelihood (mixture of Gaussians)
# and the expectation over the latent variables,
With this formulation, the marginal likelihood of this model can be written to clearly show the
expectations over the latent variables,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood}
\evidence &=
\sum_{\modeInd=1}^\ModeInd
\underbrace{\E_{\gatingPrior} \left[ \prod_{\numData=1}^\NumData \singleGatingLikelihood \right]}_{\text{gating network}}
\underbrace{\E_{\expertPrior} \left[ \prod_{\numData=1}^\NumData \singleExpertLikelihood \right]}_{\text{expert } k}.
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood-factorised}
\evidence &=
\sum_{\modeInd=1}^\ModeInd
\E_{\gatingPrior \expertsPrior} \left[
\prod_{\numData=1}^\NumData
\singleGatingLikelihood \singleExpertLikelihood \right],
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-marginal-likelihood-factorised}
# \evidence &=
# \E_{\gatingPrior \expertsPrior} \left[
# \sum_{\modeInd=1}^\ModeInd
# \prod_{\numData=1}^\NumData
# \singleGatingLikelihood \singleExpertLikelihood \right],
# \end{align}
# #+END_EXPORT
# where $\expertsPrior = \prod_{\modeInd=1}^\ModeInd \expertPrior$ is the experts' prior and
where $\expertPrior$ is the $\modeInd^{\text{th}}$ expert's prior and
$\gatingPrior$ is the gating network's prior.
Observe that  the  GP  priors  have  removed  the  factorisation  over  data
which  is  present  in  the  ME  marginal  likelihood  cref:eq-mixture-marginal-likelihood.
Fig. [[ref:fig-graphical-model]]
shows the graphical model where the $\ModeInd$ latent gating functions $\GatingFunc$
are evaluated and normalised to obtain the mixing probabilities
$\singleGatingLikelihood$.
The mode indicator variable $\singleModeVar$ is then sampled from the Categorical distribution
governed by these probabilities.
The indicated mode's latent function $\mode{f}$ and process noise $\noiseVarK$ are
then evaluated to generate the state difference $\singleOutput$.

This model makes single-step probabilistic predictions,
where the predictive distribution over the output $\singleOutput$ is
given by a mixture of Gaussians.
This provides the model flexibility to model multimodal transition dynamics $f$ as mixtures of $\modeInd$ experts.
In the two expert case the marginal likelihood is a mixture of two Gaussians and can be calculated in
closed-form.
\todo{can it be calculated in closed form?}
It has complexity $\mathcal{O}(\ModeInd\NumData^4)$ to evaluate
and the general case (with more than two experts) has complexity $\mathcal{O}(\ModeInd^2\NumData^4)$.

For ease of notation and understanding, only a single output dimension has been considered,
although in most scenarios the state dimension will be greater than $1$.
This work considers independent output dimensions which follow trivially from multioutput GP
methodologies.
# The extension to multiple output dimensions is fairly straight forward.
\todo{Can I just say it's left out because trivial?}
*** Gating Network :noexport:
The gating network governs how the dynamics switch between modes.
This work is interested in spatially varying modes so
formulates an input dependent Categorical distribution over $\alpha_t$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-prob-mass}
P\left(\alpha_t \mid \mathbf{h}_t\right) = \prod_{k=1}^K \left(\Pr(\alpha_t=k \mid \mathbf{h}_t)\right)^{[\alpha_t = k]},
\end{align}
#+END_EXPORT
where $[\alpha_t=k]$ denotes the Iverson bracket.
It should be noted that other gating networks have been proposed that provide computational benefits
for inference. However, these gating networks often lack a handle for encoding domain knowledge
that can be used to restrict the set of possible

\todo[inline]{insert gating network references}
The probabilities of this Categorical distribution $\Pr(\alpha_t=k \mid \mathbf{h}_t)$
are obtained by evaluating $K$ latent
gating functions $\{h^{(k)}\}_{k=1}^K$ and normalising their output.
Each gating function evaluated at $\hat{\mathbf{x}}_{t-1}$ is denoted
as $h^{(k)}_t = h^{(k)}(\hat{\mathbf{x}}_{t-1})$
and at all observations ${h}^{(k)}_{1:T}$.
The set of all gating functions evaluated at $\hat{\mathbf{x}}_{t-1}$ is denoted
as $\mathbf{h}_t$ and at all observations as $\mathbf{h}_{1:T}$.
Each gating function $h^{(k)}$ describes how its corresponding mode's mixing
probability varies over the input space.


This work is interested in finding trajectories that can avoid areas of the transition dynamics model
that cannot be predicted confidently.
Placing independent sparse GP priors on each gating function
provides a principled approach to
modelling the epistemic uncertainty associated with each gating function.
The gating function's posterior covariance is a quantitative value that can be exploited by the
trajectory optimisation.
# GPs also provide data-efficient and interpretable learning through the selection of informative mean
# and covariance functions.
# For example, if the transition dynamics modes are subject to oscillatory mixing then a periodic
# covariance function could be selected.

Each gating function's inducing inputs are denoted
$\bm\xi_h^{(k)}$ and outputs as $\hat{\mathbf{h}}^{(k)}$.
For all gating functions, they are collected as $\bm\xi_h$ and $\hat{\mathbf{h}}$ respectively.
The probability that the $t^{\text{th}}$ observation is generated by mode $k$
given the inducing inputs is obtained by marginalising $\mathbf{h}_t$,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha=\modeInd \mid \mathbf{h}(\cdot) )
&= \E_{p(\mathbf{h}(\cdot))} \left[ \Pr(\alpha_t=k \mid \mathbf{h}(\cdot)) \right],
\end{align*}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha=\modeInd \mid \mathbf{h}(\cdot) )
&= \E_{p(\mathbf{h}(\cdot))} \left[ \text{softmax}_k(\mathbf{h}(\cdot)) \right],
\end{align*}
#+END_EXPORT
#+BEGIN_EXPORT latex
% \small
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha_t=k \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}} )
&= \int \text{softmax}_k(\mathbf{h}_t) p(\mathbf{h}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}) \text{d} \mathbf{h}_t,
\end{align*}
#+END_EXPORT
where $p(\mathbf{h}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}) = \prod_{k=1}^K p\left({h}^{(k)}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}^{(k)}\right)$
is the $K$ independent sparse GP priors on the gating functions.

**** Two Experts
Instantiating the model with two experts is a special case where only a single gating function is needed.
The output of a function can be mapped through a sigmoid
function $\text{sig} : \R \rightarrow [0, 1]$ and interpreted as
a probability $\Pr(\alpha=1 \mid \gatingFunc(\cdot))$.
If this sigmoid function satisfies the point symmetry condition then
we know that $\Pr(\alpha=2 \mid \gatingFunc(\cdot)) = 1 - \Pr(\alpha =1 \mid \gatingFunc(\cdot))$.

The distribution over $\gatingFunc(\cdot)$

We note that $p({h}_n^{(k)} \mid \mathbf{h}_{\neg n}, \mathbf{X})$
is a GP conditional and denote its mean $\mu_h$ and variance $\sigma^2_h$.
Choosing the sigmoid as the Gaussian cdf
$\Phi(h_n) = \int^{h_n}_{-\infty} \mathcal{N}(\tau | 0, 1) \text{d} \tau$
leads to,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixing-probabilities-analytic}
\Pr(\alpha_n=1 | \mathbf{X}) &=
 \int \Phi({h}_n) \mathcal{N}(h_n \mid \mu_h, \sigma^2_h) \text{d} {h}_n
= \Phi \left(\frac{\mu_{h}}{\sqrt{1 + \sigma^2_{h} }}\right).
\end{align}
#+END_EXPORT
Given this gating network our marginal likelihood is now an analytic mixture of two Gaussians.

can analytically marginalise.

** Approximate Inference [[label:sec-inference]]
#+begin_export latex
\epigraph{Nature laughs at the difficulties of integration.}{\textit{Pierre-Simon Laplace}}
#+end_export
*** intro :ignore:
Performing Bayesian inference involves finding the posterior over the latent variables,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\GatingFunc(\allInput), \LatentFunc(\allInput) \mid \allInput, \allOutput, \gatingParams, \expertParams)
&= \frac{\sum_{\allModeVar} \allGatingLikelihood \gatingPrior
\prod_{\modeInd=1}^{\ModeInd} \allExpertLikelihood \expertPrior}{\evidence}
\end{align}
#+END_EXPORT
where the denominator is the marginal likelihood from cref:eq-marginal-likelihood-assign.
Exact inference in our model is intractable, so we resort to a variational approximation.
The rich structure of our model makes it hard to construct an ELBO that can
be evaluated in closed-form, whilst accurately modelling the complex dependencies.
Further to this, the marginal likelihood is extremely expensive to evaluate,
as there are $\ModeInd^{\NumData}$ sets of assignments $\allModeVar$ that need to be marginalised.
For each set of assignments, there are $\ModeInd$ GP experts that need to be evaluated, each with
complexity $\mathcal{O}(\NumData^{3})$.
\todo{add marginal likelihood's correct complexity}
For these reasons, this work derives a variational approximation based on inducing variables, that provides scalability
by utilising stochastic gradient-based optimisation.
# The resulting complexity is  $\mathcal{O}(\ModeInd^{\NumData} \NumData_{\modeInd}^3)$,
# where $\NumData_{\modeInd}$ represents the largest number of data points assigned to an expert for particular
# set of assignments.
# and is also intractable due to the marginalisation of $\mathbf{h}$ in the gating network.

\acrfull{svi} citep:hoffmanStochastic2013 relies upon having a set of local variables
factorised across observations and a set of global variables.
The marginalisation over the set of expert indicator variables $\allModeVar$
in cref:eq-np-moe-marginal-likelihood is prohibitive to \acrshort{svi}.
Following the approach by cite:titsiasVariational2009, the probability space is augmented
with a set of $\NumInducing$ inducing variables for each GP.
However, instead of collapsing these inducing variables they are explicitly
represented as variational distributions and used to lower bound
(and then further bound) the marginal likelihood, similar to
cite:hensmanGaussian2013,hensmanScalable2015.
cref:fig-graphical-model-sparse shows the graphical model of the augmented joint probability space.

# The marginal likelihood in cref:eq-marginal-likelihood-factorised is extremely expensive
# to evaluate $\mathcal{O}(\ModeInd^2 \NumData^{4})$
# \todo{add marginal likelihood's correct complexity}
# and is also intractable due to the marginalisation of $\mathbf{h}$ in the gating network
# (except for the two expert case).
# For these reasons, this work derives a variational approximation based on inducing variables that provides scalability
# by utilising stochastic gradient-based optimisation.


# *Inducing Variables* As this approach essentially parameterises a nonparametric model,
# it is interesting to pause here and consider the implications of defining inducing inputs in different ways.
# For example, what are the implications of having shared or separate inducing inputs
# for the gating network GPs, for the expert GPs and for combinations of the experts and gating functions?

# 1. *Separate inducing inputs* for each *expert* GP, i.e. $\expertInducingOutput=\mode{\latentFunc}(\expertInducingInput)$,
#    - Introducing separate inducing inputs for each expert's GP can loosely be seen as "partitioning"
#       the observations between experts. This requires less inducing points for each expert and achieves data partitioning behaviour like other MoGPE methods.
# - Introducing separate inducing inputs for each expert's GP can loosely be seen as "partitioning"
#   the observations between experts,
# - Less inducing inputs needed for each expert,
# - Achieves data partitioning behaviour like other MoGPE methods.
# 3. *Shared inducing inputs* for the *gating network* GPs, i.e. $\gatingInducingOutput=\mode{\gatingFunc}(\gatingInducingInput)$,
#    - Partitioning of the data set is not desirable for the gating function GPs as each gating function
#       should depend on all of the training observations. For this reason the inducing inputs should be shared between each gating function GP.
# - Partitioning of the data set is not desirable for the gating function GPs as each gating function
#   should depend on all of the training observations,
# - For this reason the inducing inputs should be shared between each gating function GP.

*** sparse graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\singleInput)$};
      %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
      \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\mode{\gatingFunc}(\singleInput)$};

      \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\expertInducingOutput$};
      \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\gatingInducingOutput$};
      \node[const, left=of uk, xshift=0.4cm] (zk) {$\expertInducingInput$};
      \node[const, right=of uh, xshift=-0.4cm] (zh) {$\gatingInducingInput$};

      \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\expertParamsK$};
      \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\gatingParamsK$};

      \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (uk)--(f);
      \draw[post] (uh)--(h);
      \draw[post] (zk)--(uk);
      \draw[post] (zh)--(uh);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a) (f) (h)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(uh) (h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    }
  \caption{Graphical model of the augmented probability space where the joint distribution over the data is captured by the inducing variables $\expertInducingOutput=\mode{\latentFunc}(\expertInducingInput)$ and $\gatingInducingOutput=\mode{\gatingFunc}(\gatingInducingInput)$.
  The observations assigned to expert $\modeInd$ are modelled by the
  inducing points $\{\expertInducingInput, \expertInducingOutput\}_{\modeInd=1}^{\ModeInd}$.
  This model avoids the hard assignment of observations to experts by letting the gating network
  softly assign them in the ELBO.}
\label{fig-graphical-model-sparse}
\end{figure}
#+END_EXPORT

*** Augmented Probability Space :ignore:

*Augmented experts*
This work sidesteps the hard assignment of observations to experts by augmenting each expert with a set
of separate independent inducing points,
$(\expertInducingInput, \expertInducingOutput)$.
Each expert's inducing points are assumed to be from its GP prior,
# Each set of inducing points are assumed to be from the GP prior associated with the expert,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-inducing-prior}
\expertsInducingPrior
&= \prod_{\modeInd=1}^\ModeInd \expertInducingPrior
= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \expertInducingOutput \mid
\expertMeanFunc(\expertInducingInput),
\expertCovFunc(\expertInducingInput, \expertInducingInput) \right),
\end{align}
#+END_EXPORT
where the set of all inducing inputs associated with the experts has been denoted $\expertsInducingInput$
and the  set of all inducing variables as $\expertsInducingOutput$.
Note that the dependence on the inducing inputs has dropped for notational conciseness.
Introducing separate inducing points from each expert's GP can loosely be seen as "partitioning"
the observations between experts.
However, as the assignment of observations to experts is /not known a priori/, the inducing inputs
$\expertInducingInput$ and variables $\expertInducingOutput$, must be inferred from observations.
# , i.e. the inducing points can be seen as approximating the data partition if
# $\expertInducingOutput \approx \mode{\latentFunc}(\allInputK)$ and $\expertInducingInput \approx \allInputK$.

*Augmented gating network* Following a similar approach for the gating network, each gating function is augmented with a
set of $\NumInducing$ inducing points from its corresponding GP prior,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-inducing-prior}
\gatingsInducingPrior
&= \prod_{\modeInd=1}^\ModeInd \gatingInducingPrior
= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \gatingInducingOutput \mid
\gatingMeanFunc(\gatingInducingInput),
\gatingCovFunc(\gatingInducingInput, \gatingInducingInput) \right),
\end{align}
#+END_EXPORT
where  $\gatingInducingOutput = \mode{\gatingFunc}(\gatingInducingInput)$
are the inducing points associated with the $\modeInd^{\text{th}}$ gating function.
Again, the dependence on the inducing inputs has been dropped for notational conciseness.
The set of all inducing variables associated with the gating network has been
denoted $\gatingsInducingOutput$.
Partitioning of the data set is not desirable for the gating function GPs as each gating function
should depend on all of the training observations.
For this reason the gating functions share the same inducing inputs $\gatingInducingInput$.

*Marginal likelihood* These inducing points are used to approximate the marginal likelihood with a factorisation over observations
that is favourable for constructing a GP-based gating network.
Our approximate marginal likelihood is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-augmented-marginal-likelihood}
\evidence &\approx
\E_{\gatingsInducingPrior \expertsInducingPrior} \left[
\prod_{\numData=1}^{\NumData}
\sum_{\modeInd=1}^{\ModeInd}
 \singleGatingGivenInducing \singleExpertGivenInducing \right],
\end{align}
#+END_EXPORT
where the conditional distributions $\singleExpertGivenInducing$ and $\singleGatingGivenInducing$
follow from standard sparse GP methodologies,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\singleExpertGivenInducing &= \E_{\singleLatentExpertGivenInducing} \left[ \singleExpertLikelihood \right] \\
\singleGatingGivenInducing &= \E_{\singleLatentGatingsGivenInducing} \left[ \singleGatingLikelihood \right] \\
\singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
\expertKernelnM \expertKernelMM^{-1} \expertInducingOutput,
\expertKernelnn - \expertKernelnM \expertKernelMM^{-1} \expertKernelMn \right), \\
\singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mode{\gatingFunc}(\singleInput) \mid
\gatingKernelnM \gatingKernelMM^{-1} \gatingInducingOutput,
\gatingKernelnn - \gatingKernelnM \gatingKernelMM^{-1} \gatingKernelMn \right),
\end{align}
#+END_EXPORT
where
$\expertKernelMM = k_{\modeInd} (\expertInducingInput,\expertInducingInput)$
represents the $\modeInd^{\text{th}}$ expert's kernel evaluated between its inducing inputs,
$\expertKernelnn = k_k (\singleInput, \singleInput)$
represents it evaluated between the $\numData^{\text{th}}$ training input and
$\expertKernelnM = k_k (\singleInput, \expertInducingInput)$
between the $\numData^{\text{th}}$ training input and its inducing inputs.
Similarly for the gating network.
Figure ref:fig-graphical-model-sparse shows the graphical model of this augmented joint probability model.

Our work follows from sparse GP methodologies, that assume,
given the inducing variables, the latent function values factorise over observations.
Our approximation assumes that given the inducing points,
the marginalisation over every possible assignment of data points to experts, can be factorised over data.
In a similar spirit to the FITC approximation citep:naish-guzmanGeneralized2008,quinonero-candelaUnifying2005,
this can be viewed as a likelihood approximation,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-likelihood-approximation}
p(\allOutput \mid \expertsInducingOutput)
&\approx \prod_{\numData=1}^{\NumData} p(\singleOutput \mid \expertsInducingOutput)
= \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \moeGatingPosterior
\prod_{\modeInd=1}^{\ModeInd} \singleExpertGivenInducing.
\end{align}
#+END_EXPORT
Importantly, the factorisation over observations has been moved
outside of the marginalisation over the expert indicator variable, i.e.
the expert indicator variable can be marginalised for each data point separately.
This approximation assumes that the inducing variables,
$\{\expertInducingOutput\}_{\modeInd=1}^\ModeInd$, are
a sufficient statistic for their associated latent function values,
$\{\mode{\latentFunc}(\allInputK) \}_{\modeInd=1}^\ModeInd$
and the set of assignments $\allModeVar$.
This approximation becomes exact in the limit $\ModeInd\NumInducing=\NumData$,
if each expert's inducing points represent the true data partition
$(\expertInducingInput, \expertInducingOutput) = (\allInputK, \mode{\latentFunc}(\allInputK))$.
It is also worth noting that cref:eq-augmented-marginal-likelihood captures a rich approximation of each
expert's covariance but as $\ModeInd\NumInducing \ll \NumData$ the computational complexity is
much lower.
This approximation efficiently couples the gating network and the experts by marginalising the expert
indicator variable for each data point separately.

Our approximate marginal likelihood captures
the joint distribution over the data and assignments through the inducing variables
$\expertsInducingOutput$.
As such, information regarding the assignment of observations to experts must pass through the bottleneck of the
inducing variables.
This approximation induces a local factorisation over observations and a set of global variables
-- the necessary conditions for \acrshort{svi}.
cref:fig-graphical-model-sparse shows the graphical model of this augmented joint probability model.
This approach can loosely be viewed as parameterising the full nonparametric model
in cref:eq-np-moe-marginal-likelihood-assign to obtain a desirable
factorisation for 1) constructing a GP-based gating network and 2) deriving an ELBO that can
be optimised with stochastic gradient methods,
whilst still capturing the complex dependencies between the gating network and experts.

*** Augmented probability space :ignore:noexport:

\acrfull{svi} citep:hoffmanStochastic2013 relies upon having a set of local variables
factorised across observations and a set of global variables.
Annoyingly, the order of the marginalisation over the expert indicator variable
and the product over observations $\NumData$
in cref:eq-marginal-likelihood is prohibitive to \acrshort{svi}.


# which is detailed in Section ref:sec-sparse-gps Eqs. ref:eq-titsias-bound - ref:eq-hensman-bound,
*Augmented Probability Space* Following the approach by cite:titsiasVariational2009 and cite:hensmanGaussian2013,
the probability space is first augmented with a set of $\NumInducing$ inducing variables from each GP prior,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-inducing-prior}
\expertsInducingPrior
&= \prod_{\modeInd=1}^\ModeInd \expertInducingPrior
= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \expertInducingOutput) \mid
\expertMeanFunc(\expertInducingInput),
\expertCovFunc(\expertInducingInput, \expertInducingInput) \right) \\
\gatingsInducingPrior
&= \prod_{\modeInd=1}^\ModeInd \gatingInducingPrior
= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \gatingInducingOutput) \mid
\gatingMeanFunc(\gatingInducingInput),
\gatingCovFunc(\gatingInducingInput, \gatingInducingInput) \right).
\end{align}
#+END_EXPORT
where  $\gatingInducingOutput = \mode{\gatingFunc}(\gatingInducingInput)$
are the inducing points associated with the $\modeInd^{\text{th}}$ gating function.
Partitioning of the data set is not desirable for the gating function GPs as each gating function
should depend on all of the training observations.
For this reason the gating functions share the same inducing inputs $gatingInducingInput$.
The set of all inducing variables associated with the gating network has been
denoted $\gatingsInducingOutput$.

Our approximate marginal likelihood is then given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-augmented-marginal-likelihood}
\evidence &\approx
\E_{\gatingsInducingPrior \expertsInducingPrior} \left[
\prod_{\numData=1}^{\NumData}
\sum_{\modeInd=1}^{\ModeInd}
 \singleGatingGivenInducing \singleExpertGivenInducing \right],
\end{align}
#+END_EXPORT
where the joint distribution over the data is captured by the inducing variables
$(\gatingsInducingOutput, \expertsInducingOutput)$.
As such, information regarding the assignment of observations to experts must pass through the bottleneck of the
inducing variables.
This approximation induces a local factorisation over observations and a set of global variables -- the inducing
variables -- the necessary conditions for \acrshort{svi}.
Figure ref:fig-graphical-model-sparse shows the graphical model of this augmented joint probability model.


The augmented marginal likelihood is then given by,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-augmented-marginal-likelihood}
# \evidence &=
# \E_{\gatingsInducingPrior \expertsInducingPrior} \left[
# \sum_{\modeInd=1}^{\ModeInd} \allGatingGivenInducing \allExpertGivenInducing
# \right],
# \end{align}
# #+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-augmented-marginal-likelihood-not-factorised}
\evidence &=
\E_{\gatingsInducingPrior} \left[
\sum_{\modeInd=1}^{\ModeInd}
 \allGatingsGivenInducing
\E_{\expertInducingPrior} \left[ \allExpertGivenInducing \right]
\right],
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-augmented-marginal-likelihood}
# \evidence &\approx
# \sum_{\modeInd=1}^{\ModeInd}
# \E_{\gatingsInducingPrior} \left[ \prod_{\numData=1}^\NumData \singleGatingGivenInducing \right]
# \E_{\expertsInducingPrior} \left[ \prod_{\numData=1}^\NumData \singleExpertGivenInducing \right]
# \right],
# \end{align}
# #+END_EXPORT
where the conditional distributions $\allExpertGivenInducing$ and $\allGatingGivenInducing$
follow from standard sparse GP methodologies,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\allExpertGivenInducing &= \E_{\allLatentExpertGivenInducing} \left[ \allExpertLikelihood \right] \\
\allGatingGivenInducing &= \E_{\allLatentGatingsGivenInducing} \left[ \allGatingLikelihood \right] \\
%\allLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\allInput) \mid
%\mode{\mathbf{A}} \expertInducingOutput,
%\expertKernel(\allInput, \allInput) -
%\mode{\mathbf{A}}
%\expertKernel(\expertInducingInput, \expertInducingInput)
%\mode{\mathbf{A}}^T \right), \\
%\allLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mathbf{\gatingFunc}(\allInput) \mid
%\mode{\hat{\mathbf{A}}} \gatingInducingOutput,
%\gatingKernel(\allInput, \allInput) -
%\mode{\hat{\mathbf{A}}}
%\gatingKernel(\gatingInducingInput, \gatingInducingInput)
%\mode{\hat{\mathbf{A}}}^T \right), \\
\allLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\allInput) \mid
\expertKernelNM \expertKernelMM^{-1} \expertInducingOutput,
\expertKernelNN - \expertKernelNM \expertKernelMM^{-1} \expertKernelMN \right), \\
\allLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd}
\mathcal{N}\left( \mode{\gatingFunc}(\allInput) \mid
\gatingKernelNM \gatingKernelMM^{-1} \gatingInducingOutput,
\gatingKernelNN - \gatingKernelNM \gatingKernelMM^{-1} \gatingKernelMN \right),
\end{align}
#+END_EXPORT
where
$\expertKernelMM = k_{\modeInd} (\expertInducingInput,\expertInducingInput)$
represents the $\modeInd^{\text{th}}$ experts kernel evaluated between its inducing inputs,
$\expertKernelNN = k_k (\allInput, \allInput)$
represents it evaluated between the training inputs and
$\expertKernelNM = k_k (\allInput, \expertInducingInput)$
between the training inputs and its inducing inputs.
Similarly for the gating network.

A central assumption of our work follows from sparse GP methodologies, that assume,
given the inducing variables, the latent function values factorise over observations.
In a similar spirit to the FITC approximation cite:naish-guzmanGeneralized2008,quinonero-candelaUnifying2005,
we propose the following likelihood approximation,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-likelihood-approximation}
p(\allOutput \mid \gatingsInducingOutput, \expertsInducingOutput)
\approx \prod_{\numData=1}^{\NumData} p(\singleOutput \mid \gatingsInducingOutput, \expertsInducingOutput)
= \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \singleGatingGivenInducing \singleExpertGivenInducing.
\end{align}
#+END_EXPORT
# This approximation factorises over the observed outputs given the inducing points.
Importantly, this approximation moves the factorisation over observations
outside of the marginalisation over the expert indicator variable.
It captures a rich approximation of each expert's covariance whilst marginalising the expert
indicator variable.
This approximation assumes that the inducing variables,
$\{\gatingInducingOutput, \expertInducingOutput\}_{\modeInd=1}^\ModeInd$, are
a sufficient statistic for their associated latent function values,
$\{\mode{\latentFunc}(\allInput), \mode{\gatingFunc}(\allInput) \}_{\modeInd=1}^\ModeInd$.
When all of the expert's inducing inputs $\expertInducingInput$
and the gating network's inducing inputs
$\gatingInducingInput$ are equal to the training inputs
$\expertInducingInput = \gatingInducingInput=\allInput$, this approximation is exact.
This approximation becomes exact in the limit $M=N$.
Our approximate marginal likelihood is then given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-augmented-marginal-likelihood}
\evidence &\approx
\E_{\gatingsInducingPrior \expertsInducingPrior} \left[
\prod_{\numData=1}^{\NumData}
\sum_{\modeInd=1}^{\ModeInd}
 \singleGatingGivenInducing \singleExpertGivenInducing \right],
\end{align}
#+END_EXPORT
where the joint distribution over the data is captured by the inducing variables
$(\gatingsInducingOutput, \expertsInducingOutput)$.
As such, information regarding the assignment of observations to experts must pass through the bottleneck of the
inducing variables.
This approximation induces a local factorisation over observations and a set of global variables -- the inducing
variables -- the necessary conditions for \acrshort{svi}.
Figure ref:fig-graphical-model-sparse shows the graphical model of this augmented joint probability model.


# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \allExpertGivenInducing &= \prod_{\numData=1}^{\NumData} \singleExpertGivenInducing
# = \prod_{\numData=1}^{\NumData} \E_{\singleLatentExpertGivenInducing} \left[ \singleExpertLikelihood \right] \\
# \allGatingsGivenInducing &= \prod_{\numData=1}^{\NumData} \singleGatingGivenInducing
# = \prod_{\numData=1}^{\NumData} \E_{\singleLatentGatingsGivenInducing} \left[ \singleGatingLikelihood \right] \\
# %\singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
# %\mode{\mathbf{A}} \expertInducingOutput,
# %\expertKernel(\singleInput, \singleInput) -
# %\mode{\mathbf{A}} \expertKernel(\expertInducingInput, \expertInducingInput) \mode{\mathbf{A}}^T \right), \\
# %\singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mathbf{\gatingFunc}(\singleInput) \mid
# %\mode{\hat{\mathbf{A}}} \gatingInducingOutput,
# %\gatingKernel(\singleInput, \singleInput) -
# %\mode{\hat{\mathbf{A}}} \gatingKernel(\gatingInducingInput, \gatingInducingInput) \mode{\hat{\mathbf{A}}}^T \right), \\
# %\singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
# %\expertA \expertInducingOutput,
# %\expertKernelnn - \expertA \expertKernelMM \expertA^T \right), \\
# %\singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mathbf{\gatingFunc}(\singleInput) \mid
# %\gatingA \gatingInducingOutput,
# %\gatingKernelnn - \gatingA \gatingKernelMM \gatingA^T \right), \\
# \singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
# \expertKernelnM \expertKernelMM^{-1} \expertInducingOutput,
# \expertKernelnn - \expertKernelnM \expertKernelMM^{-1} \expertKernelMn \right), \\
# \singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mode{\gatingFunc}(\singleInput) \mid
# \gatingKernelnM \gatingKernelMM^{-1} \gatingInducingOutput,
# \gatingKernelnn - \gatingKernelnM \gatingKernelMM^{-1} \gatingKernelMn \right),
# \end{align}
# #+END_EXPORT
# where,
# $\mathbf{K}_{\modeInd \numInducing \numInducing} = k_{\modeInd} (
# ${k}_{\modeInd \numData \numData} = k_k (\singleInput, \singleInput)
# $\mathbf{k}_{\modeInd \numData \numInducing} = k_k (\singleInput, \eInput)
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \mode{\mathbf{A}} &= \expertKernelnM \expertKernelMM^{-1} \\
# \mode{\mathbf{A}} &= \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1} \\
# \mode{\hat{\mathbf{A}}} &= \gatingKernel(\singleInput, \gatingInducingInput)\left(\gatingKernel(\gatingInducingInput, \gatingInducingInput)\right)^{-1}.
# \end{align}
# #+END_EXPORT


# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \singleExpertGivenInducing &= \E_{\singleLatentExpertGivenInducing} \left[ \singleExpertLikelihood \right] \\
# \singleGatingGivenInducing &= \E_{\singleLatentGatingsGivenInducing} \left[ \singleGatingLikelihood \right] \\
# \singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
# \mode{\mathbf{A}} \expertInducingOutput,
# \expertKernel(\singleInput, \singleInput) -
# \mode{\mathbf{A}}
# \expertKernel(\expertInducingInput, \expertInducingInput)
# \mode{\mathbf{A}}^T
# \right), \\
# \singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mathbf{\gatingFunc}(\singleInput) \mid
# \mode{\hat{\mathbf{A}}} \gatingInducingOutput,
# \gatingKernel(\singleInput, \singleInput) -
# \mode{\hat{\mathbf{A}}}
# \gatingKernel(\gatingInducingInput, \gatingInducingInput)
# \mode{\hat{\mathbf{A}}}^T
# \right),
# \end{align}
# %\mode{\K}
# %\mode{\tilde{\K}}
# #+END_EXPORT
# where,
# # #+BEGIN_EXPORT latex
# # \begin{align} \label{eq-}
# # \mode{\mathbf{A}} &= \expertKernel(\allInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1} \\
# # \mode{\hat{\mathbf{A}}} &= \gatingKernel(\allInput, \gatingInducingInput)\left(\gatingKernel(\gatingInducingInput, \gatingInducingInput)\right)^{-1}.
# # \end{align}
# # #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \mode{\mathbf{A}} &= \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1} \\
# \mode{\hat{\mathbf{A}}} &= \gatingKernel(\singleInput, \gatingInducingInput)\left(\gatingKernel(\gatingInducingInput, \gatingInducingInput)\right)^{-1}.
# \end{align}
# #+END_EXPORT
# $\mode{\mathbf{A}} = \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}$ and
# $\mode{\hat{\mathbf{A}}} = \gatingKernel(\singleInput, \gatingInducingInput)\left(\expertKernel(\gatingInducingInput, \gatingInducingInput)\right)^{-1}$.

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \singleExpertGivenInducing &= \E_{\singleLatentExpertGivenInducing} \left[ \singleExpertLikelihood \right] \\
# \singleGatingGivenInducing &= \E_{\singleLatentGatingsGivenInducing} \left[ \singleGatingLikelihood \right] \\
# \singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
# \expertKernel(\singleInput, \expertInducingInput) \expertKernel^{-1}(\expertInducingInput, \expertInducingInput) \expertInducingOutput,
# \mode{\K}
# \right), \\
# \singleLatentGatingsGivenInducing &= \mathcal{N}\left( \mathbf{\gatingFunc}(\singleInput) \mid
# \gatingKernel(\singleInput, \gatingInducingInput) \gatingKernel^{-1}(\gatingInducingInput, \gatingInducingInput) \gatingInducingOutput,
# \mode{\tilde{\K}}
# \right),
# \end{align}
# #+END_EXPORT
# where,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \mode{\K} = \expertKernel(\singleInput, \singleInput) - \expertKernel(\singleInput, \gatingInducingInput) (\expertKernel(\gatingInducingInput,\gatingInducingInput))^{-1} \expertKernel(\gatingInducingInput, \singleInput) \\
# \mode{\tilde{\K}} = \gatingKernel(\singleInput, \singleInput) - \gatingKernel(\singleInput, \gatingInducingInput) (\gatingKernel(\gatingInducingInput,\gatingInducingInput))^{-1} \gatingKernel(\gatingInducingInput, \singleInput).
# \end{align}
# #+END_EXPORT


# $\expertInducingInput=\allInput$, and for the gating network's inducing inputs,
# $\gatingInducingInput=\allInput$, this approximation becomes exact in the limit $M=N$.

# For each expert's inducing inputs,
# $\expertInducingInput=\allInput$, and for the gating network's inducing inputs,
# $\gatingInducingInput=\allInput$, this approximation becomes exact in the limit $M=N$.

# This minimises the KL divergence and ensures that $\expertsInducingInput$ are distributed amongst the training
# inputs $\allInput$ such that ....

*** Evidence Lower Bounds
Instead of collapsing the inducing variables as seen in cite:titsiasVariational2009,
they can be explicitly represented as variational distributions,
$(\expertsInducingVariational, \gatingsInducingVariational)$
and used to obtain a variational lower bound, aka \acrfull{elbo}.
This section derives three \acrshort{elbo}s.
The first bound is the tightest but requires approximating $\NumInducing$ dimensional integrals
for each expert and gating function.
Two further lower bounds which replace some (or all) of the $\NumInducing$ dimensional integrals
with one dimensional integrals are derived.
These further bounds offer improved computational properties at the cost of loosening the bound.
All three of these bounds are evaluated in cref:sec-mcycle-results.

# \newline
*Tight lower bound*
Following a similar approach to cite:hensmanGaussian2013,hensmanScalable2015,
a lower bound on cref:eq-augmented-marginal-likelihood can be obtained,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-lower-bound-tight}
\text{log} \evidence &\geq  \sum_{\numData=1}^\NumData \E_{\gatingsInducingVariational \expertsInducingVariational}
\left[ \text{log} \left( \sum_{\modeInd=1}^\ModeInd
\singleGatingGivenInducing \singleExpertGivenInducing \right) \right] \nonumber \\
&- \gatingsKL \nonumber \\
&- \expertsKL := \tightBound,
\end{align}
#+END_EXPORT
# From cref:eq-lower-bound-tight it is clear that the optimal distribution for gating network's variational
# distribution consists of independent Guassians.
# The experts' variational posterior is not tractable so we assume a Gaussian approximate posterior.
where we parameterise the variational posteriors to be independent Gaussians,
# Our variational posteriors are then given by,
# From cref:eq-lower-bound-tight it is clear that the optimal distribution for each of the variational distributions
# is Guassian, so we parameterise them as such,
# The posterior is not tractable so we assume a Gaussian approximate posterior.
#+BEGIN_EXPORT latex
\begin{align} \label{eq-variational-inducing-dist}
\expertsInducingVariational &= \prod_{\modeInd=1}^{\ModeInd} \expertInducingVariational
= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left(\expertInducingOutput \mid \mode{\mathbf{m}}, \mode{\mathbf{S}} \right) \\
\gatingsInducingVariational &= \prod_{\modeInd=1}^{\ModeInd} \gatingInducingVariational
= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left(\gatingInducingOutput \mid \mode{\hat{\mathbf{m}}}, \mode{\hat{\mathbf{S}}} \right).
\end{align}
#+END_EXPORT
The bound in cref:eq-lower-bound-tight meets the necessary conditions to perform stochastic gradient methods on
$\expertsInducingVariational$ and $\gatingsInducingVariational$, as the expected log likelihood (first term)
is written as a sum over input-output pairs.
However, this expectation cannot be calculated in closed-form and must be approximated.
The joint distributions over the inducing variables for each expert GP $\expertInducingVariational$
and each gating function GP $\gatingInducingVariational$,
are $\NumInducing$ dimensional multivariate normal distributions.
Therefore, each expectation requires an $\NumInducing$ dimensional integral to be approximated.
# integral being approximated is $\NumInducing$ dimensional.
\todo{add more on why we don't want M dimensional integral}

*Further lower bound* Following cite:hensmanScalable2015, these issues can be overcome
by further bounding the bound in cref:eq-lower-bound-tight ($\tightBound$).
This removes the $\NumInducing$ dimensional integrals associated with each of the gating functions.
Jensen's inequality can be applied to the conditional probability $\singleLatentGatingGivenInducing$,
obtaining the further bound,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-lower-bound-further}
\tightBound \geq \sum_{\numData=1}^\NumData &\E_{\gatingsVariational \expertsInducingVariational}
\left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertGivenInducing \right] \nonumber \\
&- \gatingsKL \nonumber \\
&- \expertsKL := \furtherBound,
\end{align}
#+END_EXPORT
where $\gatingsVariational$ represents the variational posterior given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-variational-posteriors}
%\expertsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{ \expertInducingVariational} \left[ \singleLatentExpertGivenInducing \right] \\
\gatingsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{\gatingInducingVariational} \left[ \singleLatentGatingGivenInducing \right].
\end{align}
#+END_EXPORT
Moving the marginalisation over the latent gating functions $\GatingFunc(\singleInput)$
outside of the marginalisation over the expert indicator
variable is possible because the mixing probabilities are dependent on *all* of the gating functions and
not just their associated gating function.
In contrast, moving the marginalisation over each expert's latent function $\mode{\latentFunc}(\singleInput)$
outside of the marginalisation over the expert
indicator variable, corresponds to changing the underlying model, in particular, the likelihood
approximation in cref:eq-likelihood-approximation.
# This is not the case for the experts' as they only depend on their corresponding latent function.

*Further^2 lower bound*
Nevertheless, we proceed and further bound the experts for comparison.
Jensen's inequality is applied to the conditional probability $\singleLatentExpertGivenInducing$,
obtaining the further^2 bound,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-lower-bound-further-2}
\furtherBound \geq \sum_{\numData=1}^\NumData &\E_{\gatingsVariational \expertsVariational}
\left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \nonumber \\
&- \gatingsKL \nonumber \\
&- \expertsKL := \furtherBoundTwo,
\end{align}
#+END_EXPORT
where $\expertsVariational$ represents the variational posterior given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-variational-posteriors}
\expertsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{ \expertInducingVariational} \left[ \singleLatentExpertGivenInducing \right].
\end{align}
#+END_EXPORT
Intuitively, this bound can be seen as modifying the likelihood approximation in cref:eq-likelihood-approximation.
Instead of mixing the GPs associated with each expert, this approximation simply mixes their associated
noise models.

# *Further Lower Bound* Following cite:hensmanScalable2015, the bound in
# cref:eq-lower-bound-tight ($\tightBound$) can be further bounded to remove the $\NumInducing$ dimensional integrals.
# Applying Jensen's inequality to the conditional probability $\singleGatingGivenInducing \singleExpertGivenInducing$,
# obtains a further bound,
# # #+BEGIN_EXPORT latex
# # \begin{align} \label{eq-further-bound}
# # \singleGatingGivenInducing \singleExpertGivenInducing \geq
# # \E_{\singleLatentExpertGivenInducing \singleLatentGatingsGivenInducing}
# # \left[ \text{log} \left( \singleGatingLikelihood \singleExpertLikelihood \right ) \right]
# # \end{align}
# # #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-lower-bound-further}
# \tightBound \geq \sum_{\numData=1}^\NumData &\E_{\gatingsVariational \expertsVariational}
# \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \nonumber \\
# &- \gatingsKL \nonumber \\
# &- \expertsKL := \furtherBound,
# \end{align}
# #+END_EXPORT
# # #+BEGIN_EXPORT latex
# # \begin{align} \label{eq-lower-bound-further}
# # \mathcal{L}_{1}
# # \geq \sum_{\numData=1}^\NumData &\E_{\expertsInducingVariational \gatingsInducingVariational}
# # \left[ \E_{\singleLatentGatingsGivenInducing \singleLatentExpertGivenInducing} \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \right] \nonumber \\
# # &- \gatingsKL - \expertsKL \\
# # = \sum_{\numData=1}^\NumData &\E_{\gatingsVariational \expertsVariational}
# # \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \nonumber \\
# # &- \gatingsKL - \expertsKL := \mathcal{L}_{2},
# # \end{align}
# # #+END_EXPORT
# where $\gatingsVariational$ and $\expertsVariational$ represents the variational posteriors given by,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-variational-posteriors}
# \expertsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{ \expertInducingVariational} \left[ \singleLatentExpertGivenInducing \right] \\
# \gatingsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{\gatingInducingVariational} \left[ \singleLatentGatingGivenInducing \right].
# \end{align}
# #+END_EXPORT
As each GP's inducing variables are Normally distributed, the functional form of the
variational posteriors are given by,
#+BEGIN_EXPORT latex
\begin{align}
\label{eq--variational-posteriors-functional-experts}
\expertsVariational &= \prod_{\modeInd=1}^\ModeInd \mathcal{N} \left( \mode{\latentFunc}(\singleInput) \mid
\mode{\mathbf{A}} \mode{\mathbf{m}},
\expertKernelnn
+ \mode{\mathbf{A}}
(\mode{\mathbf{S}} - \expertKernelMM)
\mode{\mathbf{A}}^T
\right) \\
\gatingsVariational &= \prod_{\modeInd=1}^\ModeInd \mathcal{N} \left( \mode{\gatingFunc}(\singleInput) \mid
\mode{\hat{\mathbf{A}}} \mode{\hat{\mathbf{m}}},
\gatingKernelnn
+ \mode{\hat{\mathbf{A}}}
(\mode{\hat{\mathbf{S}}} - \gatingKernelMM)
\mode{\hat{\mathbf{A}}}^T
\right), \label{eq--variational-posteriors-functional-gating}
\end{align}
#+END_EXPORT
# $\mode{\mathbf{A}} = \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}$ and
where
$\mode{\mathbf{A}} = \expertKernelnM \expertKernelMM^{-1}$ and
$\mode{\hat{\mathbf{A}}} = \gatingKernelnM \gatingKernelMM^{-1}$.
Importantly, these variational posteriors marginalise the inducing variables in closed-form,
with Gaussian convolutions.
$\furtherBound$ removes $\ModeInd$ of the undesirable approximate $M$ dimensional integrals from
cref:eq-lower-bound-tight and $\furtherBoundTwo$ removes $\ModeInd^2$.
The variational expectation in cref:eq-lower-bound-further still requires approximation,
however, the integrals are now only one dimensional.
These integrals are approximated with Gibbs sampling and
in practice only single samples are used because the added stochasticity helps the optimisation.

The tight lower bound $\tightBound$ is the most accurate lower bound but it is also the most
computationally expensive.
The further $\furtherBound$ and the further^2 $\furtherBoundTwo$ lower bounds
have lower computational complexity at the cost of being looser bounds.
The performance of these bounds is evaluated in cref:sec-mcycle-results.

*** Optimisation
#+BEGIN_EXPORT latex
\renewcommand{\expertSampleInd}{\ensuremath{s}}
\renewcommand{\ExpertSampleInd}{\ensuremath{S}}
\renewcommand{\gatingSampleInd}{\ensuremath{\hat{s}}}
\renewcommand{\GatingSampleInd}{\ensuremath{\hat{S}}}
\renewcommand{\batchSampleInd}{\ensuremath{i}}

\newcommand{\singleExpertLikelihoodSample}{\ensuremath{p\left(\y_{\batchSampleInd} \mid \expertInducingOutput^{(\expertSampleInd)}\right)}}
%\newcommand{\singleGatingLikelihoodSample}{\ensuremath{\Pr\left(\alpha_\batchSampleInd = \modeInd \mid \gatingsInducingOutput^{(\gatingSampleInd)} \right)}}
\renewcommand{\singleGatingLikelihoodSample}{\ensuremath{\Pr\left(\alpha_\batchSampleInd = \modeInd \mid \GatingFunc(\x_{\batchSampleInd})^{(\gatingSampleInd)} \right)}}

%\newcommand{\singleExpertLikelihoodSample}{\ensuremath{p\left(\y_{\batchSampleInd} \mid \mode{f}(\x_{\batchSampleInd})^{(\expertSampleInd)}\right)}}
\newcommand{\singleGatingLikelihoodSample}{\ensuremath{\Pr\left(\alpha_\batchSampleInd = \modeInd \mid \GatingFunc_{\batchSampleInd}^{(\gatingSampleInd)} \right)}}
%\newcommand{\gatingsVariationalSample}{\ensuremath{q(\GatingFunc(\x_{\batchSampleInd-1}))}}
%\newcommand{\expertVariationalSample}{\ensuremath{q(\mode{\latentFunc}(\x_{\batchSampleInd-1}))}}

\newcommand{\gatingsVariationalSample}{\ensuremath{q(\GatingFunc(\x_{\batchSampleInd}))}}
#+END_EXPORT
\todo{add unbiased estimate of ELBO}
The bounds in  cref:eq-lower-bound-further,eq-lower-bound-tight,eq-lower-bound-further-2
meet the necessary conditions to perform stochastic
gradient methods on $\expertsInducingVariational$ and $\gatingsInducingVariational$.
Firstly, they contain a sum of $\NumData$ terms corresponding to input-output pairs, enabling optimisation
with mini-batches.
Secondly, the expectations over the log-likelihood are calculated using Monte Carlo samples.

*Stochastic optimisation*
At each iteration $j$, a random subset of $\NumData_b$ data points are sampled from the data set $\mathcal{D}$,
to get a minibatch $\mathcal{D}_j = \{\x_i, \y_i\}_{i=1}^{\NumData_b}$.
The further lower bound $\furtherLowerBound$ is then approximated by,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-approx-lower-bound-further}
# \hat{\mathcal{L}}_{2} = \frac{\NumData}{\NumData_b} \sum_{\singleInput, \singleOutput \in \mathcal{D}_i}
# &\E_{\gatingsVariational \expertsVariational}
# \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \\
# &- \gatingsKL - \expertsKL.
# \end{align}
# #+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-approx-lower-bound-further}
\hat{\mathcal{L}}_{\text{further}} = \frac{\NumData}{\NumData_b}
&\sum_{\x_{\batchSampleInd}, \y_{\batchSampleInd} \in \mathcal{D}_j}
\left(
\frac{1}{\ExpertSampleInd} \sum_{\expertSampleInd=1}^{\ExpertSampleInd}
\frac{1}{\GatingSampleInd} \sum_{\gatingSampleInd=1}^{\GatingSampleInd} \nonumber
\text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihoodSample \singleExpertLikelihoodSample \right)  \\
- &\gatingsKL \nonumber \\
- &\expertsKL,
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \renewcommand{\singleExpertLikelihoodSample}{\ensuremath{p\left(\y_{\batchSampleInd} \mid \mode{f}(\x_{\batchSampleInd-1})^{(\expertSampleInd)}\right)}}
# \renewcommand{\singleGatingLikelihoodSample}{\ensuremath{\Pr\left(\alpha_\batchSampleInd = \modeInd \mid \GatingFunc(\x_{\batchSampleInd})^{(\gatingSampleInd)} \right)}}
# \newcommand{\expertVariationalSample}{\ensuremath{q(\mode{\latentFunc}(\x_{\batchSampleInd-1}))}}
# \small
# %\sum_{\singleInput, \singleOutput \in \mathcal{D}_i}
# \begin{align} \label{eq-approx-lower-bound-further}
# \hat{\mathcal{L}}_{2} &= \frac{\NumData}{\NumData_b}
# \sum_{d_n \in \mathcal{D}_i}
# \left(
# \frac{1}{\ExpertSampleInd} \sum_{\expertSampleInd=1}^{\ExpertSampleInd}
# \frac{1}{\GatingSampleInd} \sum_{\gatingSampleInd=1}^{\GatingSampleInd} \nonumber
# \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihoodSample \singleExpertLikelihoodSample \right)  \\
# &- \gatingsKL - \expertsKL, \\
# \text{where} \quad &\mode{\latentFunc}(\x_{\batchSampleInd-1})^{(\expertSampleInd)} \sim \expertVariationalSample, \\
# &\mathbf{\gatingFunc}(\x_{\batchSampleInd-1})^{(\gatingSampleInd)} \sim \gatingsVariationalSample
# \end{align}
# \normalsize
# #+END_EXPORT
where $\expertInducingOutput^{(\expertSampleInd)} &\sim \expertInducingVariational$
and
$\mathbf{\gatingFunc}(\x_{\batchSampleInd})^{(\gatingSampleInd)} &\sim \gatingsVariationalSample$
denote samples from the variational posteriors.
The variational distributions over the inducing variables are represented using the mean vector $\mode{\mathbf{m}}$
and the lower triangular $\mode{\mathbf{L}}$ of the covariance matrix
$\mode{\mathbf{S}} = \mode{\mathbf{L}} \mode{\mathbf{L}}^T$.
A downside to this formulation is that $(\ModeInd^2)(M-1)M/2 + \ModeInd^2 M$ extra parameters need to be optimised.
In the two expert case this reduces to $(\ModeInd+1)(M-1)M/2 + (\ModeInd+1) M$ extra parameters.
Optimising the inducing inputs ($\gatingInducingInput$ and $\expertsInducingInput$) introduces a further
$\ModeInd^2\NumInducing\InputDim$ optimisation parameters.
The inducing inputs $\gatingInducingInput,\{\expertInducingInput\}_{\modeInd=1}^{\ModeInd}$,
kernel hyperparameters and noise variances, are treated as
variational hyperparameters and optimised alongside the variational parameters, using stochastic gradient descent
e.g. Adam citep:kingmaAdam2017.

# Note that the augmented model captures the dependencies in the joint distribution of the data through the
# inducing variables, but as $M \ll \NumData$ these have a much lower computational burden.

# ${\E_{\gatingsVariational \expertsInducingVariational} \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertGivenInducing \right]}$,

*Computational complexity* Assuming that each expert has the same number of inducing points $\NumInducing$,
the cost of computing the KL divergences and their derivatives is $\mathcal{O}\left( \ModeInd \NumInducing^{3} \right)$.
The cost of computing the expected likelihood term is dependent on the batch size $\NumData_b$.
For each data point in the minibatch, each of the $\ModeInd$ gating function variational posteriors has complexity
$\mathcal{O}\left( \NumInducing^{2} \right)$ to evaluate.
For each data point, only a single sample is drawn from each of these distributions.
Sampling each expert's inducing variable distribution $\expertInducingVariational$
has complexity $\mathcal{O}(\NumInducing^2)$ because
the covariance is represented as the lower triangular (via the cholesky decomposition).
In addition to this sampling, calculating each experts conditional $\singleExpertGivenInducing$ given these samples
has complexity $\mathcal{O}(\NumInducing^2)$.
\todo{How to combine all of these complexities?}


# For each data point in the minibatch, each of the $\ModeInd$ gating function GPs and $\ModeInd$ expert GPs has complexity
# $\mathcal{O}\left( \NumInducing^{2} \right)$ to evaluate.


# The gating network's variational posterior has complexity
# $\mathcal{O}\left( \ModeInd \NumInducing^{2} \right)$ to evaluate.
# Drawing a single sample from this

# Each expert's variational posterior $\expertInducingVariational$ is represented using a cholesky decomposition
# so sampling from all of them has complexity $\mathcal{O}\left( \ModeInd \NumInducing^{2} \right)$.

# # The cost of computing the expected likelihood term
# # is dependent on the batch size $\NumData_b$ and has complexity
# # $\mathcal{O}\left( \NumData_{b} \ModeInd^2 \NumInducing^{2} \right)$ to evaluate.


# When the number of inducing points $\NumInducing$ is smaller than the batch size, most of the cost will arise from
# computing the expected likelihood term.
# The bound has complexity
# $\max\left(\mathcal{O}\left( \ModeInd \NumInducing^{3} \right),\mathcal{O}\left( \NumData_{b} \ModeInd \NumInducing^{2} \right)\right)$.
# \todo{check complexities. What is complexity of approximating M dimensional integral with gibbs sampling?}

*** Predictions
#+BEGIN_EXPORT latex
\renewcommand{\testInput}{\ensuremath{\mathbf{X}^*}}
\renewcommand{\testOutput}{\ensuremath{\mathbf{y}^*}}
\renewcommand{\NumTest}{\ensuremath{\NumData^*}}
\renewcommand{\singleTestInput}{\ensuremath{\mathbf{x}_n^*}}
\renewcommand{\singleTestOutput}{\ensuremath{y_n^*}}
\newcommand{\testModeVarK}{\ensuremath{\bm\modeVar^* = \modeInd}}
\newcommand{\singleTestModeVar}{\ensuremath{\modeVar_n^*}}
\newcommand{\singleTestModeVarK}{\ensuremath{\singleTestModeVar = \modeInd}}

%\newcommand{\predictivePosterior}{\ensuremath{p(\testOutput \mid \testInput, \allOutput, \allInput)}}
%\newcommand{\predictiveProb}{\ensuremath{\Pr(\singleTestModeVarK \mid \singleTestInput, \allOutput, \allInput)}}
%\newcommand{\predictiveExpert}{\ensuremath{p(\singleTestOutput \mid \singleTestModeVarK, \singleTestInput, \allOutput, \allInput)}}
\newcommand{\predictivePosterior}{\ensuremath{p(\testOutput \mid \allOutput)}}
\newcommand{\predictiveProb}{\ensuremath{\Pr(\singleTestModeVarK \mid \allOutput)}}
\newcommand{\predictiveProbBernoulli}{\ensuremath{\Pr(\singleTestModeVar=1 \mid \allOutput)}}
\newcommand{\predictiveExpert}{\ensuremath{p(\singleTestOutput \mid \singleTestModeVarK, \allOutput)}}

\newcommand{\approxPredictiveProb}{\ensuremath{q(\singleTestModeVarK)}}
\newcommand{\approxPredictiveExpert}{\ensuremath{q(\singleTestOutput \mid \singleTestModeVarK)}}

\newcommand{\predictiveExpertLikelihood}{\ensuremath{p(\singleTestOutput \mid \mode{f}(\singleTestInput))}}
\newcommand{\predictiveGatingLikelihood}{\ensuremath{\Pr(\singleTestModeVarK \mid \GatingFunc(\singleTestInput))}}
\newcommand{\predictiveGatingLikelihoodBernoulli}{\ensuremath{\Pr(\modeVar_n^*=1 \mid \modei{\gatingFunc}{1}(\singleTestInput))}}

%\renewcommand{\expertPosterior}{\ensuremath{p(\mode{\latentFunc}(\singleTestInput) \mid \singleTestInput, \allOutput, \allInput)}}
%\renewcommand{\gatingPosterior}{\ensuremath{p(\GatingFunc(\singleTestInput) \mid \singleTestInput, \allOutput, \allInput)}}
\renewcommand{\expertPosterior}{\ensuremath{p(\mode{\latentFunc}(\singleTestInput) \mid \allOutput)}}
\renewcommand{\gatingPosterior}{\ensuremath{p(\GatingFunc(\singleTestInput) \mid \allOutput)}}

\renewcommand{\expertVariationalPosterior}{\ensuremath{q(\mode{\latentFunc}(\singleTestInput)}}
\renewcommand{\gatingVariationalPosterior}{\ensuremath{q(\GatingFunc(\singleTestInput)}}
\renewcommand{\gatingVariationalPosteriorBernoulli}{\ensuremath{q(\modei{\gatingFunc}{1}(\singleTestInput))}}
#+END_EXPORT
For a given set of test inputs $\testInput \in \R^{\NumTest \times \InputDim$,
this model makes probabilistic predictions following
a mixture of $\ModeInd$ Gaussians.
Making predictions with this model involves calculating a density over the output for each expert and combining
them using the probabilities obtained from the gating network, i.e. marginalising the expert indicator variable,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-predictive-posterior}
\predictivePosterior &= \prod_{\numData=1}^{\NumTest} \sum_{\modeInd=1}^\ModeInd \underbrace{\predictiveProb}_{\text{gating network posterior}} \underbrace{\predictiveExpert}_{\text{expert } \ModeInd \text{ posterior}} \\
&\approx \prod_{\numData=1}^{\NumTest} \sum_{\modeInd=1}^\ModeInd \approxPredictiveProb \approxPredictiveExpert
\end{align}
#+END_EXPORT

*Experts*
The experts make predictions at new test locations by integrating over their latent function posteriors,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-prediction}
\predictiveExpert
&= \int \underbrace{\predictiveExpertLikelihood}_{\text{likelihood}}
\underbrace{\expertPosterior}_{\text{posterior}} \text{d} \mode{\latentFunc}(\singleTestInput) \nonumber \\
&\approx \int \underbrace{\predictiveExpertLikelihood}_{\text{likelihood}}
\underbrace{\expertVariationalPosterior}_{\text{approx posterior}} \text{d} \mode{\latentFunc}(\singleTestInput)
\coloneqq \approxPredictiveExpert.
\end{align}
#+END_EXPORT
However, the experts' true posteriors $\expertPosterior$ are not known and have been approximated.
Each expert's approximate posterior is given by
$q(\mode{\latentFunc}(\allInputK), \expertInducingOutput) = p(\mode{\latentFunc}(\allInputK) \mid \expertInducingOutput) q(\expertInducingOutput)$.
To make a prediction at a set of test locations $\testInput$, we substitute our approximate posterior
into the standard probabilistic rule,
# *Experts* Each expert's predictive distribution over the output $\predictiveExpert$,
# is obtained by marginalising its predictive posterior,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-predictive-expert}
%\predictiveExpert &=
\underbrace{\expertPosterior}_{\text{posterior}} &=
\int p(\mode{\latentFunc}(\singleTestInput) \mid \mode{\latentFunc}(\allInputK), \expertInducingOutput)
p(\mode{\latentFunc}(\allInputK), \expertInducingOutput \mid \allOutput)
\text{d} \mode{\latentFunc}(\allInputK) \text{d} \expertInducingOutput \nonumber \\
&\approx \int p(\mode{\latentFunc}(\singleTestInput) \mid \mode{\latentFunc}(\allInputK), \expertInducingOutput)
p(\mode{\latentFunc}(\allInputK) \mid \expertInducingOutput) q(\expertInducingOutput)
\text{d} \mode{\latentFunc}(\allInputK) \text{d} \expertInducingOutput \nonumber \\
&= \int p(\mode{\latentFunc}(\singleTestInput) \mid \expertInducingOutput)
\expertInducingVariational
\text{d} \expertInducingOutput \nonumber \\
&= \mathcal{N} \left( \mode{\latentFunc}(\singleTestInput) \mid
\mode{\mathbf{A}} \mode{\mathbf{m}},
\expertKernelss
+ \mode{\mathbf{A}}
(\mode{\mathbf{S}} - \expertKernelMM)
\mode{\mathbf{A}}^T \right) \coloneqq \underbrace{\expertVariationalPosterior}_{\text{approx posterior}},
%\E_{\predictiveExpertPrior} \left[ \predictiveExpertLikelihood \right],
\end{align}
#+END_EXPORT
where $\mode{\mathbf{A}} = \expertKernelsM \expertKernelMM^{-1}$.
This integral has complexity $\mathcal{O}(\NumInducing^2)$.


*Gating network* The mixing probabilities associated with the gating network are obtained
by integrating the gating network's posterior through the gating likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-prediction}
\predictiveProb
&= \int \underbrace{\predictiveGatingLikelihood}_{\text{likelihood}}
\underbrace{\gatingPosterior}_{\text{posterior}} \text{d} \GatingFunc(\singleTestInput) \nonumber \\
&\approx \int \underbrace{\predictiveGatingLikelihood}_{\text{likelihood}}
\underbrace{\gatingVariationalPosterior}_{\text{approx posterior}} \text{d} \GatingFunc(\singleTestInput)
\coloneqq \approxPredictiveProb.
\end{align}
#+END_EXPORT
Again, the gating network's true posterior $\gatingPosterior$ has been approximated,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-predictive-gating}
\underbrace{\gatingPosterior}_{\text{posterior}}
&\approx \int p(\GatingFunc(\singleTestInput) \mid \gatingInducingOutput)
\gatingInducingVariational \text{d} \gatingInducingOutput \nonumber \\
&= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N} \left( \GatingFunc(\singleTestInput) \mid
\mode{\hat{\mathbf{A}}} \mode{\hat{\mathbf{m}}},
\gatingKernelss
+ \mode{\hat{\mathbf{A}}}
(\mode{\hat{\mathbf{S}}} - \gatingKernelMM)
\mode{\hat{\mathbf{A}}}^T \right) \coloneqq \underbrace{\gatingVariationalPosterior}_{\text{approx posterior}},
\end{align}
#+END_EXPORT
where $\mode{\hat{\mathbf{A}}} = \gatingKernelsM \gatingKernelMM^{-1}$.
In the general case, when $\ModeInd > 2$, the gating likelihood is the softmax,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-likelihood-softmax}
\underbrace{\predictiveGatingLikelihood}_{\text{likelihood}} = \text{softmax}(\GatingFunc(\singleTestInput)),
\end{align}
#+END_EXPORT
so cref:eq-gating-prediction is approximated with Monte Carlo quadrature.
In the two expert case there is only a single gating function, $\gatingFunc_1$, as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\Pr(\singleTestModeVar=2 \mid \modei{\gatingFunc}{1}(\singleTestInput)) = 1 - \Pr(\singleTestModeVar=1 \mid \modei{\gatingFunc}{1}(\singleTestInput)).
\end{align}
#+END_EXPORT
In this case, the gating likelihood is the Gaussian cdf,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-likelihood-bernoulli}
\underbrace{\predictiveGatingLikelihoodBernoulli}_{\text{likelihood}} = \Phi(\gatingFunc_1(\singleTestInput)),
\end{align}
#+END_EXPORT
so cref:eq-gating-prediction can be calculated in closed-form with,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-predictive-gating-bernoulli}
\predictiveProbBernoulli &=
\int \gatingVariationalPosteriorBernoulli \Phi\left(\modei{\gatingFunc}{1}(\singleTestInput)\right) \text{d} \modei{\gatingFunc}{1}(\singleTestInput) \nonumber \\
&= \Phi \left(\frac{\mu_{h*}}{\sqrt{1 + \sigma^2_{h_*} }}\right).
\end{align}
%\mathcal{N}\left(\gatingFunc(\x_*) \mid \mu_h, \sigma^2_h \right) \text{d} \gatingFunc(\x_*) \\
#+END_EXPORT
where $\mu_{h_*}$ and $\sigma^2_{h_*}$ are the mean and variance of the variational posterior
$\gatingVariationalPosteriorBernoulli$ at $\singleTestInput$.



# # $\predictiveGatingLikelihood$,
# # *Gating Network* The mixing probabilities associated with the gating network are obtained
# # by taking the expectation of the gating likelihood $\predictiveGatingLikelihood$,
# # under the predictive posterior of the gating network,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-predictive-gating}
# \predictiveProb &= \int \predictiveGatingPrior \predictiveGatingLikelihood \text{d} \GatingFunc(\x_*),
# %\predictiveProb &= \E_{\predictiveGatingPrior} \left[ \predictiveGatingLikelihood \right],
# \end{align}
# #+END_EXPORT
# where the predictive posterior $\predictiveGatingPrior$ is given by
# cref:eq--variational-posteriors-functional-gating.

# In the general case, when $\ModeInd > 2$, the gating likelihood is the softmax,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-gating-likelihood-softmax}
# \predictiveGatingLikelihood = \text{softmax}(\GatingFunc(\x_*)),
# \end{align}
# #+END_EXPORT
# so cref:eq-predictive-gating is approximated with Monte Carlo quadrature.
# In the two expert case there is only a single gating function, $\gatingFunc_1$, as,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# ${\Pr(\singleModeVar=2 \mid \modei{\gatingFunc}{1}(\singleInput)) = 1 - \Pr(\singleModeVar=1 \mid \modei{\gatingFunc}{1}(\singleInput))}$.
# \end{align}
# #+END_EXPORT
# In this case, the gating likelihood is the Gaussian cdf,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-gating-likelihood-bernoulli}
# \predictiveGatingLikelihoodBernoulli = \Phi(\gatingFunc_1(\x_*)),
# \end{align}
# #+END_EXPORT
# so cref:eq-predictive-gating can be calculated in closed-form with,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-predictive-gating-bernoulli}
# \Pr(\modeVar_*=1 \mid \x_*) &=
# \E_{\predictiveGatingPriorBernoulli} \left[ \Phi\left(\gatingFunc(\x_*)\right) \right] \\
# &= \Phi \left(\frac{\mu_{h*}}{\sqrt{1 + \sigma^2_{h_*} }}\right).
# \end{align}
# %\mathcal{N}\left(\gatingFunc(\x_*) \mid \mu_h, \sigma^2_h \right) \text{d} \gatingFunc(\x_*) \\
# #+END_EXPORT
# where $\mu_{h_*}$ and $\sigma^2_{h_*}$ are the mean and variance of the predictive posterior
# $\predictiveGatingPriorBernoulli$ at $\x_*$.

*** sparse graphical model old :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\fkn$};
      %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
      \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\hkn$};

      \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\uFk$};
      \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\Hku$};
      \node[const, left=of uk, xshift=0.4cm] (zk) {$\zFk$};
      \node[const, right=of uh, xshift=-0.4cm] (zh) {$\zHk$};

      \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\expertParamsK$};
      \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\gatingParamsK$};

      \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (uk)--(f);
      \draw[post] (uh)--(h);
      \draw[post] (zk)--(uk);
      \draw[post] (zh)--(uh);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a) (f) (h)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(zh) (uh) (h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    }
  \caption{Graphical model of the transition dynamics where the state difference $\singleOutput$
is generated by pushing the state and control $\singleInput$ through the latent process.}
\label{fig-graphical-model-sparse}
\end{figure}
#+END_EXPORT



#+BEGIN_EXPORT latex
\todo[inline]{
\begin{itemize}
\item We want $\pFGivenFu$ outside the log (and not just $\qFu$) so we can analytically integrate $\qFu$ (like in Eq. \ref{eq-experts-var-dist})??
\item In the ICRA paper I had only $\qFu$ outside the log so we had to approximate the M-dimensional integral over $\qFu$ with samples.
\end{itemize}
}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo[inline]{
This is the bound I had coded up previously,
\begin{align} \label{eq-expert-bound-old}
\mathcal{L}_{\text{old}} =
&\sum_{\numData}^{\NumData} E_{\prod_{\modeInd=1}^{\ModeInd} \qFku} \text{log} \sum_{k=1}^{K} \PrA \left[ \pykGivenFku \right] \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right]
\end{align}
This bound is valid but it is nicer to move the expectations over $\pFk$ outside the log, like in Eq. \ref{eq-experts-bound-4-fact}.
}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo[inline]{
This is another bound I implemented,
\begin{align} \label{eq-expert-bound-old}
\mathcal{L}_{\text{old}} =
&\sum_{\numData}^{\NumData} \text{log} \sum_{k=1}^{K} \PrA \E_{\prod_{\modeInd=1}^{\ModeInd} \qFku} \left[ \pykGivenFku \right] \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right]
\end{align}
\begin{itemize}
\item THIS IS WRONG as the expectation over $\qFku$ should be outside the log.
\item which is why I used to approximate it with single samples....
\end{itemize}
}
#+END_EXPORT

\todo[inline]{Need to extend bound for the gating network}

** Evaluation of Model and Approximate Inference
*** Intro :ignore:
# As a mixture of experts method we aim to improve on standard GP regression
# with the ability to model non-stationary functions and multimodal distributions.
# This work also proposes to address their inherent identifiability issues, by adopting a gating network
# that we can place informative priors on.
# As such, the method is tested on two data sets,
# 1) *Artificial data set* to demonstrate how the method improves identifiability,
# 2) *Motorcycle data set* cite:Silverman1985 to provide a comparison to other MoGPE methods and to,
#    - thoroughly compare the different ELBO's in Section ref:sec-inference,
#    - evaluate the impact of the number of inducing points $M$ and the batch size $\NumData_b$.

# This work also proposes to address their inherent identifiability issues, by adopting a gating network
# that can encode domain knowledge through informative priors.
# The method is then tested on an artificial data set to demonstrate its power at improving identifiability.

As a mixture of experts method, our model aims to improve on standard \acrshort{gp} regression
with the ability to model non-stationary functions and multimodal distributions over the output variable.
With this in mind, the model and approximate inference scheme are evaluated on two data sets.
Following other \acrshort{mogpe} work, they are first tested on the motorcycle data set citep:Silverman1985.
Although this data set does not represent state transitions from a dynamical system,
it does contain non-stationary points and heterogeneous noise,
making it interesting to study from the \acrshort{mogpe} perspective.
Secondly, they are tested on the illustrative example from cref:illustrative_example.
That is, a data set collected onboard a DJI Tello quadcopter flying in an environment subject to two dynamics modes.
# Secondly, they are tested on a data set collected onboard a DJI Tello quadcopter flying in the Bristol Robotics
# Laboratory.
*** Experiments
#+BEGIN_EXPORT latex
\newcommand{\numTest}{\ensuremath{n}}
\newcommand{\NumTest}{\ensuremath{N}}
%\newcommand{\testSingleInput}{\ensuremath{\x_{\numTest}}}
%\newcommand{\testSingleOutput}{\ensuremath{\y_{\numTest}}}
%\newcommand{\allTestInput}{\ensuremath{\allInput_*}}
%\newcommand{\allTestOutput}{\ensuremath{\allOutput_*}}
\newcommand{\predictedSingleInput}{\ensuremath{\testSingleInput^*}}
\newcommand{\predictedSingleOutput}{\ensuremath{\testSingleOutput^*}}
\renewcommand{\predictedSingleInput}{\ensuremath{\testSingleInput^*}}
\renewcommand{\predictedSingleInput}{\ensuremath{\hat{\x}^*_{\numTest}}}
\renewcommand{\predictedSingleOutput}{\ensuremath{\hat{\y}^*_{\numTest}}}
#+END_EXPORT

Each experiment was carried out on a system with an Intel Core i9 CPU at 2.4GHz with 16GB DDR4 RAM.
All data sets were split into test and training sets with $70\%$ for training and $30\%$ for testing.
In order to evaluate and compare the full predictive posteriors the Negative Log Predictive Probability (NLPP)
is computed on the test set.
The models are also compared using the Root Mean Squared Error (RMSE) and the Mean Absolute Error (MAE).
Given a test data set
$(\testInput, \testOutput) = \{(\singleTestInput, \singleTestOutput)\}_{\numTest=1}^{\NumTest}$,
they are calculated as follows,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-scores}
\text{RMSE} &= \sqrt{\frac{1}{\NumTest} \sum_{\numTest=1}^{\NumTest} (\predictedSingleOutput - \singleTestOutput)^2} \\
\text{MAE} &= \frac{1}{\NumTest} \sum_{\numTest=1}^{\NumTest} | \predictedSingleOutput - \singleTestOutput | \\
\text{NLPP} &= \frac{1}{\NumTest} \sum_{\numTest=1}^{\NumTest} - \log (\singleTestOutput \mid \singleTestInput, \mathcal{D}, \expertParams, \gatingParams)
\end{align}
#+END_EXPORT
where $\predictedSingleOutput$ is the models prediction at $\singleTestInput$.
Note that all figures in this section show models that were trained on the full data set, i.e. no test/train split.

*** Evaluation on Motorcycle Data Set label:sec-mcycle-results
**** intro :ignore:
The Motorcycle data set
(discussed in cite:Silverman1985) contains 133 data points
($\allInput \in \inputDomain \subseteq \R^{133 \times 1}$ and
$\allOutput \in \outputDomain \subseteq \R^{133 \times 1}$)
and input dependent noise.
The data set represents motorcycle impact data -- time (ms) vs acceleration (g).
The data set is represented by the black crosses in cref:fig-y-mcycle-two-experts.


# The model and inference are evaluated by instantiating the model with both two $\ModeInd=2$
# and three $\ModeInd=3$ experts.
To test the performance of our method (\acrshort{mosvgpe}), the model is
instantiated with $\ModeInd=2$ and $\ModeInd=3$ experts.
All experiments on the Motorcycle data set use $\NumInducing=32$ inducing points for
all GPs and are trained for $25,000$ iterations
with Adam citep:kingmaAdam2017, with a learning rate of $0.01$ and a batch size of $\NumData_b=16$.
The results are compared against a Gaussian process (GP) and a sparse variational Gaussian
process (SVGP),
which use Squared Exponential (SE) kernels with automatic relevance determination (ARD) and
a Gaussian likelihood.
# The method is compared to a Sparse Variational Gaussian Process (SVGP) trained with the same
# number of inducing points and training parameters.

cref:tab-mcycle-metrics summarises the results for the three ELBOs
($\tightBound$, $\furtherBound$, $\furtherBoundTwo$)
and compares them to a standard GP regression model
and a SVGP method instantiated with $\NumInducing=16$ and $\NumInducing=32$ inducing points.
Both methods use Gaussian likelihoods and optimise their hyperparameters, noise variances
(and inducing inputs in the SVGP case) using their well known
objectives -- the marginal likelihood and evidence lower bound.

***** Results table :ignore:

#+Name: tab-mcycle-metrics
#+Caption: Results on the Motorcycle data set cite:Silverman1985 with different instantiations of our model (\acrshort{mosvgpe}).
#+Caption: Comparison of the root mean squared error (RMSE) mean absolute error (MAE)
#+Caption: and negative log predictive probability (NLPP) on the test data set.
#+Caption: Results for a Gaussian process (GP) and a sparse variational Gaussian process (SVGP) with
#+Caption: $\NumInducing=16$ and $\NumInducing=32$ inducing points are shown for comparison.
#+Caption: All models were instantiated with Squared Exponential kernels and were
#+Caption: trainind for $25,000$ iterations.
#+Caption: The GP's hyperparamters were optimised using SciPy's citep:2020SciPy-NMeth L-BFGS-B optimiser.
#+Caption: The SVGP and \acrshort{mosvgpe} models were trained with Adam citep:kingmaAdam2017 using a learning rate
#+Caption: of $0.01$ and a minibatch size of $\NumData_b=16$.
#+Caption: The \acrshort{mosvgpe} expertiments used $\NumInducing=32$ inducing points for each expert GP
#+Caption: and each gating function GP.
|-----------------------------------+-------------------+-------------------+-------------------|
|                                   | RMSE              | NLPP              | MAE               |
|-----------------------------------+-------------------+-------------------+-------------------|
| GP                                | $\mathbf{0.4357}$ | $0.9886$          | $0.3242$          |
|-----------------------------------+-------------------+-------------------+-------------------|
| SVGP $(M=16)$                     | $0.4427$          | $0.9762$          | $0.3257$          |
| SVGP $(M=32)$                     | $0.4437$          | $0.9832$          | $0.3271$          |
|-----------------------------------+-------------------+-------------------+-------------------|
| \acrshort{mosvgpe} $(k=2, \tightBound)$      | $0.4442$          | $0.4863$          | $0.3260$          |
| \acrshort{mosvgpe} $(k=2, \furtherBound)$    | $0.4590$          | $0.5073$          | $0.3355}$         |
| \acrshort{mosvgpe} $(k=2, \furtherBoundTwo)$ | $0.4472$          | $0.5271$          | $\mathbf{0.3218}$ |
| \acrshort{mosvgpe} $(k=3, \tightBound)$      | $0.4569$          | $\mathbf{0.2634}$ | $0.3301$          |
| \acrshort{mosvgpe} $(k=3 ,\furtherBound)$    | $0.4866$          | $0.2695$          | $0.3449$          |
| \acrshort{mosvgpe} $(k=3 ,\furtherBoundTwo)$ | $0.4575$          | $0.5467$          | $0.3270$          |

***** After results table :ignore:
The NLPP indicates the probability of the data given the
parameters which are not marginalised, e.g. hyperparameters and inducing inputs.
Following Bayesian model selection, it is known that lower values indicate higher performing models, i.e.
predictive posteriors that more accurately match the distribution of the data.
The predictive posterior is most accurate when \acrshort{mosvgpe} is instantiated with three experts $\ModeInd=3$
and trained using the tight lower bound $\tightBound$.
In both the two and three expert experiments,
the tight lower bound $\tightBound$ achieved better NLPP than both of the further/further^2
lower bounds,  $\furtherBound$ and $\furtherBoundTwo$.
This is expected as it is a tighter bound.
As both of the further/further^2 lower bounds offer improved computational properties,
it is interesting to compare their performance.
The NLPP scores for the further lower bound $\furtherBound$ are almost equal to the tight lower bound $\tightBound$.
In contrast, the NLPP score in the three expert experiment for the further^2 lower bound $\furtherBoundTwo$ is
significantly worse.
This indicates that valuable information is lost in this bound.
This was expected as this bound corresponds to a further likelihood approximation,
which mixes the experts' noise models as opposed to their full SVGP models.


# It is worth noting here that the tight lower bound $\tightBound$ takes
# longer to compute than the further lower bound $\furtherBound$.
# \todo{add something on further bound being better computationally????}

With regards to the accuracy of the predictive means,
the standard GP regression model achieved the best RMSE, followed by the SVGP models and then the
\acrshort{mosvgpe} models.
It is worth noting that all of the RMSE and MAE scores are very similar.
Although adding more experts to the \acrshort{mosvgpe} model appears to learn more accurate predictive posteriors, the
predictive means appear to deteriorate ever so slightly (indicated by higher RMSE/MAE values).
This is most likely due to bias at the boundaries between the experts,
resulting from the mixing behaviour arising from our GP-based gating network.
If the gating functions do not have extremely low lengthscales then they will not be able to immediately switch
from one expert to another.
Although this appears to negatively impact performance here, it should be noted that in theory
the GP-based gating network can offer superior generalisation and identifiability.

**** two expert y fig :ignore:
#+BEGIN_EXPORT latex
%\begin{figure}[t!]
\begin{figure}[hbt!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_means.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
\subcaption{$\furtherBound$, poseterior mean and data set}
\label{fig-y-means-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L3/y_means.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
\subcaption{$\furtherBoundTwo$, poseterior mean and data set}
\label{fig-y-means-mcycle-two-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_samples.pdf}
\subcaption{$\furtherBound$, posterior samples}
\label{fig-y-samples-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L3/y_samples.pdf}
\subcaption{$\furtherBoundTwo$, posterior samples}
\label{fig-y-samples-mcycle-two-experts-further}
\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L1/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-two-experts}
%\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-three-experts}
%\end{minipage}
\caption{Visualisation of the model instantiated with $\ModeInd=2$ experts, after training on the motorcycle data set \citep{Silverman1985} with $\furtherBound$ (left column) and with $\furtherBoundTwo$ (right column).  (\subref{fig-y-means-mcycle-two-experts-tight}-\subref{fig-y-means-mcycle-two-experts-further}) show the data set (black crosses) and the posterior means associated with the \acrshort{mosvgpe} (black solid line) and the SVGP (red dashed line) for comparison. (\subref{fig-y-samples-mcycle-two-experts-tight}-\subref{fig-y-samples-mcycle-two-experts-further}) show samples from the \acrshort{mosvgpe} posterior where colour indicates the underlying expert (cyan $K=1$, magenta $\ModeInd=2$) and the red lines show the $\pm 2$ standard deviation error ($95\%$ confidence interval) of the SVGP posterior. All experiments were trained with $25,000$ iterations of Adam \citep{kingmaAdam2017} using a learning rate of $0.01$ and a minibatch size of $\NumData_b=16$. All GPs use Squared Exponential kernels and $M=32$ inducing points.}
\label{fig-y-mcycle-two-experts}
\end{figure}
#+END_EXPORT

# #+caption: \textbf{Motorcycle data} The left plot shows the observations (red crosses) together with the means of our model (black solid line) and a SVGP with noise (blue dashed line). It also shows probabilities assocaited with the gating network for $k=1$ (green line) and $k=2$ (blue line) on the right axis. The right plot shows 100 samples (blue) from the predictive posterior at 200 evenly spaced input locations as well as the $\pm 2$ std error (95% confidence interval) of the SVGP (solid red line). Both models use squared exponential covariance functions for all GPs.

**** Two Experts
The two further lower bounds ($\furtherBound$ and $\furtherBoundTwo$),
derived in cref:sec-inference, are compared by training each instantiation of
the model using the same model and training parameters.
They are compared by instantiating the model with two experts $\ModeInd=2$
and comparing their performance.
The results are shown in cref:fig-y-mcycle-two-experts,fig-latent-mcycle-two-experts,
where cref:fig-y-mcycle-two-experts visualises the predictive posteriors and
cref:fig-latent-mcycle-two-experts visualises the posteriors over the latent variables.
The left column shows results for $\furtherBound$ and the right column shows results for $\furtherBoundTwo$.
This layout is used in
cref:fig-y-mcycle-two-experts,fig-latent-mcycle-two-experts,fig-y-mcycle-three-experts,fig-latent-mcycle-three-experts.

# For each lower bound, cref:fig-y-mcycle-two-experts visualises the predictive mean (top row) and
# predictive density (bottom row) and compares them to a Sparse Variational Gaussian Process (SVGP).
# cref:fig-latent-mcycle-two-experts then visualises the posteriors over the latent variables associated with
# each model.

# for the tight bound $\tightBound$ (ref:fig-y-means-two-experts-tight)
# and the further bound $\furtherBound$ (ref:fig-y-means-two-experts-further) respectively.
cref:fig-y-means-mcycle-two-experts-tight,fig-y-means-mcycle-two-experts-further
compare the posterior means (black solid line) to the SVGP's posterior mean (red dashed line) and
cref:fig-y-samples-mcycle-two-experts-tight,fig-y-samples-mcycle-two-experts-further
compare the posterior densities to the SVGP.
The red lines show plus or minus two standard deviations of the SVGP's posterior variance.
As the \acrshort{mosvgpe} posterior is a Gaussian mixture, it is visualised by drawing samples
from its posterior, i.e. sample a mode indicator variable $\modeVar_*$ and then draw a sample
from the corresponding expert.

*Predictive posteriors* Both \acrshort{mosvgpe} results are capable of modelling the non-stationarity
at $x \approx -0.7$ better than the sparse variational Gaussian process (SVGP).
\todo{quantify this with local RMSE?}
At this non-stationary point there are clearly two modes in the \acrshort{mosvgpe} predictive distributions,
indicated by the overlap in samples from each expert
in cref:fig-y-samples-mcycle-two-experts-tight,fig-y-samples-mcycle-two-experts-further.
It is clear that the SVGP has explained the observations by increasing its single
noise variance term. In contrast, both of the \acrshort{mosvgpe} results have been able to learn
two noise variances and these reflect the noise in the observations much better.
This is indicated by expert one learning a low noise variance and expert two a high noise
variance (similar to the SVGP's noise variance).
\todo{Add values for noise variances}

*Latent variables* More insight into this behaviour can be obtained by considering the latent variables.
Figure ref:fig-latent-mcycle-two-experts shows the posteriors over the latent variables where
cref:fig-expert-gps-mcycle-two-experts-tight,fig-expert-gps-mcycle-two-experts-further
show the GP posteriors over each expert's latent function $q(\mode{\latentFunc}(\x_*))$.
cref:fig-gating-gps-mcycle-two-experts-tight,fig-gating-gps-mcycle-two-experts-further
show the GP posteriors over the latent gating functions $q(\mode{\gatingFunc}(\x_*))$
and cref:fig-mixing-probs-mcycle-two-experts-tight,fig-mixing-probs-mcycle-two-experts-further
show the mixing probabilities associated with the probability mass function over the expert
indicator variable $\modeVar$.
# These figures highlight differences between the two lower bounds,
# in particular, how they represent the uncertainty in the gating network.
# As our variational inference scheme couples the learning of the experts and the gating network
# it is interesting to see their impact on the posteriors over the latent variables.

The lengthscale of the gating network kernel governs how fast the model can shift responsibility from
expert one to expert two.
For both lower bounds
the distribution over the expert indicator variable tends to a uniform distribution (maximum entropy)
at $x \geq 1.5$.
Optimising with both bounds resulted in expert one learning a
long lengthscale to fit the horizontal line from $-2$ to $-1$ and
expert two learning a shorter lengthscale function to fit the wiggly section from $-0.5$ to $1.2$.
The noise variance inferred by expert one is larger for $\furtherBoundTwo$ than for $\furtherBound$.
The uncertainty in the experts' latent functions is also higher for $\furtherBoundTwo$.
This is because $\furtherBoundTwo$ is attempting to fit both experts to the entire data set and only
mixes their noise models.
In contrast, $\furtherBound$ fits each expert only in the regions where the gating network has
assigned it responsibility.

# This is in contrast to the tight lower bound $\tightBound$, whose distribution over the expert indicator variable
# assigns responsibility to expert two and represents the uncertainty in the
# GP posterior over the gating functions, instead of the posterior over the mode indicator variable.

# \todo{to make this point I need to need to train bounds with some missing data and see what happens?}
# The gating network serves as a probabilistic decision boundary that is dependent on both the mean and
# the variance of the GPs over the gating functions $\gatingFunc$.
# As such, the model loses a degree of freedom, w.r.t. interpretability, making it difficult to
# interpret the meaning of $\Pr(\alpha_*=k \mid \mathbf{x}_*, \mathcal{D}, \bm\phi)= 0.5$.
# It could mean that it is confident that the observations were generated by a 50:50 mixture of the two experts.
# Alternatively, the variance of $p(h_*  \mid  \mathbf{x}_*, \mathcal{D}, \bm\phi)$
# may be high indicating that the gating function can not be confident making a prediction at $\mathbf{x}_*$.
# It could also indicate that the model is confident that neither expert explains the data well
# and so the optimisation has set the mixing probability to $0.5$ to prevent either experts
# "fit" degrading.
# For example, if the model needs a third expert.


# This is due to the gating network sharing the responsibility at $x \geq 1.5$ and expert one increasing its
# noise variance to help explain away the data.

**** two expert latent fig :ignore:
#+BEGIN_EXPORT latex
%\begin{figure}[t!]
\begin{figure}[hbt!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/experts_f.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
\subcaption{$\furtherBound$, experts' GP posteriors}
\label{fig-expert-gps-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L3/experts_f.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
\subcaption{$\furtherBoundTwo$, experts' GP posteriors}
\label{fig-expert-gps-mcycle-two-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/gating_gps.pdf}
%\subcaption{Gating function posteriors $\predictiveGatingPrior$ with $\ModeInd=2$.}
\subcaption{$\furtherBound$, gating network's GP posteriors}
\label{fig-gating-gps-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L3/gating_gps.pdf}
%\subcaption{Gating function posteriors $\predictiveGatingPrior$ with $\ModeInd=3$.}
\subcaption{$\furtherBoundTwo$, gating network's GP posteriors}
\label{fig-gating-gps-mcycle-two-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/gating_mixing_probs.pdf}
%\subcaption{Gating network's predictive mixing probabilities $\predictiveProb$ with $\ModeInd=2$.}
\subcaption{$\furtherBound$, mixing probabilities}
\label{fig-mixing-probs-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L3/gating_mixing_probs.pdf}
%\subcaption{Gating network's predictive mixing probabilities $\predictiveProb$ with $\ModeInd=3$.}
\subcaption{$\furtherBoundTwo$, mixing probabilities}
\label{fig-mixing-probs-mcycle-two-experts-further}
\end{minipage}
\caption{Visualisation of the model's latent variables (instantiated with $\ModeInd=2$ experts), after training on the
Motorcycle data set \citep{Silverman1985}, with $\furtherBound$ (left column)
and with $\furtherBoundTwo$ (right column).
(\subref{fig-expert-gps-mcycle-two-experts-tight}-\subref{fig-expert-gps-mcycle-two-experts-further}) show the GP
posteriors associated with the experts' latent functions $q(\mode{\latentFunc}(\x_*))$,
where the solid lines show the mean and the shaded regions show the $95\%$ confidence intervals, i.e. $\pm2\sigma$.
The gating network's GP posteriors $q(h_{k}(\x_*))$ are shown in
(\subref{fig-gating-gps-mcycle-two-experts-tight}-\subref{fig-gating-gps-mcycle-two-experts-further}) and
their associated mixing probabilities $\approxPredictiveProb$ in
(\subref{fig-mixing-probs-mcycle-two-experts-tight}-\subref{fig-mixing-probs-mcycle-two-experts-further}).}
\label{fig-latent-mcycle-two-experts}
\end{figure}
\clearpage
#+END_EXPORT

**** three expert y fig :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/y_means.pdf}
\subcaption{$\furtherBound$, poseterior mean and data set}
\label{fig-y-means-three-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L3/y_means.pdf}
\subcaption{$\furtherBoundTwo$, poseterior mean and data set}
\label{fig-y-means-three-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/y_samples.pdf}
\subcaption{$\furtherBound$, posterior samples}
\label{fig-y-samples-mcycle-three-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L3/y_samples.pdf}
\subcaption{$\furtherBoundTwo$, posterior samples}
\label{fig-y-samples-mcycle-three-experts-further}
\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L1/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-two-experts}
%\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-three-experts}
%\end{minipage}
\caption{Visualisation of the model instantiated with $\ModeInd=3$ experts, after training on the motorcycle data set \citep{Silverman1985} with $\furtherBound$ (left column) and with $\furtherBoundTwo$ (right column).  (\subref{fig-y-means-three-experts-tight}-\subref{fig-y-means-three-experts-further}) show the data set (black crosses) and the posterior means associated with the \acrshort{mosvgpe} (black solid line) and the SVGP (red dashed line) for comparison. (\subref{fig-y-samples-mcycle-three-experts-tight}-\subref{fig-y-samples-mcycle-three-experts-further}) show samples from the \acrshort{mosvgpe} posterior where colour indicates the underlying expert (cyan $K=1$, magenta $\ModeInd=2$, yellow $\ModeInd=3$) and the red lines show the $\pm 2$ standard deviation error ($95\%$ confidence interval) of the SVGP posterior. All experiments were trained with $25,000$ iterations of Adam \citep{kingmaAdam2017} using a learning rate of $0.01$ and a minibatch size of $\NumData_b=16$. All GPs use Squared Exponential kernels and $M=32$ inducing points.}
\label{fig-y-mcycle-three-experts}
\end{figure}
#+END_EXPORT


# #+BEGIN_EXPORT latex
# \begin{figure}[t!]
# \centering
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/two-experts/y_samples.pdf}
# \subcaption{}
# \label{fig-y-samples-mcycle-two-experts}
# \end{minipage}
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/three-experts/y_samples.pdf}
# \subcaption{}
# \label{fig-y-samples-mcycle-three-experts}
# \end{minipage}
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/two-experts/y_samples_density.pdf}
# \subcaption{}
# \label{fig-y-samples-mcycle-two-experts}
# \end{minipage}
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/three-experts/y_samples_density.pdf}
# \subcaption{}
# \label{fig-y-samples-mcycle-three-experts}
# \end{minipage}
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/two-experts/y_means.pdf}
# %\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
# \subcaption{}
# \label{fig-gating-gps-mcycle-two-experts}
# \end{minipage}
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/three-experts/y_means.pdf}
# %\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
# \subcaption{}
# \label{fig-gating-gps-mcycle-three-experts}
# \end{minipage}
# \caption{Visualisation of the model after training on the motorcycle data set with two experts (left column) and  three experts (right column). (a-b) show the GP posteriros associated with the gating funcions, $q(h_{k}(\state_*))$, where the solind lines show the means $\E[h_{k}(\state_*)]$ and the shaded regions show the 95\% confidence interval, i.e. $\pm2\simga$. (c-d) show the predictive mixing probabilities $\predictiveProb$.}
# \label{fig-gating-network-mcycle-subset}
# \end{figure}
# #+END_EXPORT

**** Three Experts
# Given that the two lower bounds lead to different ways for explaining the data
# at $x \geq 1.5$, it is interesting to consider adding a third expert.
The model was then instantiated with three experts $\ModeInd=3$ and trained
following the same procedure as the two experts' experiments,
i.e. with the same initial model and training parameters.
The results are show in cref:fig-y-mcycle-three-experts,
where the top row visualises the predictive mean and
the bottom row the predictive density, for $\furtherBound$ (left column) and $\furtherBoundTwo$ (right column).
cref:fig-latent-mcycle-three-experts then visualises the posteriors over the latent variables associated with
each model/bound combination.

From cref:tab-mcycle-metrics, it is clear that the predictive posterior associated with $\furtherBound$
is the most accurate as it obtained the highest NLPP score.
As expected, the two lower bounds explain the data completely differently.
Instantiating the model with three experts $\ModeInd=3$ and training with $\furtherBound$,
leads to the extra expert fitting to the data at $x \geq 1.5$ and the gating network assigning responsibility to it
in this region.
In contrast, instantiating the model with three experts $\ModeInd=3$ and training with
$\furtherBoundTwo$, results in the gating network never using the extra expert.
Similar to the two expert case, the distribution over the expert indicator variable at $x \geq 1.5$
tends to a uniform distribution (maximum entropy).

In cref:fig-expert-gps-mcycle-three-experts-tight the third expert's posterior returns to the prior at
$x \geq 1.5$.
This demonstrates that not only is the gating network turning the experts "on" and "off" in different regions
but the model is also exhibiting data assignment behaviour.
That is, each expert appears to only be fitting to the observations in the regions where the gating network
has assigned it responsibility.
In our case, this behaviour is achieved via the inducing variables capturing the joint distribution over the
experts and the set of assignments, i.e. implicitly assigning data points to experts.

# This demonstrates the flexibility of \acrshort{mosvgpe} to assign data points "softly" via the inducing variables.

**** three expert latent fig :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[hbt!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/experts_f.pdf}
\subcaption{$\furtherBound$, experts' GP posteriors}
\label{fig-expert-gps-mcycle-three-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L3/experts_f.pdf}
\subcaption{$\furtherBoundTwo$, experts' GP posteriors}
\label{fig-expert-gps-mcycle-three-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/gating_gps.pdf}
\subcaption{$\furtherBound$, gating network's GP posteriors}
\label{fig-gating-gps-mcycle-three-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L3/gating_gps.pdf}
\subcaption{$\furtherBoundTwo$, gating network's GP posteriors}
\label{fig-gating-gps-mcycle-three-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/gating_mixing_probs.pdf}
\subcaption{$\furtherBound$, mixing probabilities}
\label{fig-mixing-probs-mcycle-three-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L3/gating_mixing_probs.pdf}
%\subcaption{Gating network's predictive mixing probabilities $\predictiveProb$ with $\ModeInd=3$.}
\subcaption{$\furtherBoundTwo$, mixing probabilities}
\label{fig-mixing-probs-mcycle-three-experts-further}
\end{minipage}
\caption{Visualisation of the model's latent variables (instantiated with $\ModeInd=3$ experts), after training on the
Motorcycle data set \citep{Silverman1985}, with $\furtherBound$ (left column)
and with $\furtherBoundTwo$ (right column).
(\subref{fig-expert-gps-mcycle-three-experts-tight}-\subref{fig-expert-gps-mcycle-three-experts-further}) show the GP
posteriors associated with the experts' latent functions $q(\mode{\latentFunc}(\state_*))$,
where the solid lines show the mean and the shaded regions show the $95\%$ confidence intervals, i.e. $\pm2\sigma$.
The gating network's GP posteriors $q(h_{k}(\state_*))$ are shown in
(\subref{fig-gating-gps-mcycle-three-experts-tight}-\subref{fig-gating-gps-mcycle-three-experts-further}) and
their associated mixing probabilities $\approxPredictiveProb$ in
(\subref{fig-mixing-probs-mcycle-three-experts-tight}-\subref{fig-mixing-probs-mcycle-three-experts-further}).}
\label{fig-latent-mcycle-three-experts}
\end{figure}
\clearpage
#+END_EXPORT

**** Batch Size vs Number of Inducing Points
# This section provides a brief evaluation on the effect of the number of inducing points and batch size.

*Batch size*
One of the main benefits of the variational inference scheme presented in this chapter, is that the bound can be
calculated with minibatches of a data set $\dataset$ and used as the objective for stochastic gradient descent.
Decreasing the batch size increases the stochasticity in the bound and leads to convergence in less evaluations of
the ELBO.
This is evident in cref:fig-mcycle-training-curve, which compares the negative ELBO for different
numbers of inducing points and batch sizes.
Further to this, computing the ELBO is less computationally demanding for smaller batch sizes.
However, if the batch size is made too small, then the optimisation can become unstable and prevent the optimiser
from finding a good solution.
The (blue) learning curve for batch size $N_b=32$ in cref:fig-mcycle-training-curve-133 shows an example of this behaviour.
In this case, the learning rate had to be made smaller, leading to slower convergence.
This is shown by the orange learning curve, which has not been able to reach the same negative ELBO as the other batch
sizes.
This is most likely due to the lower learning rate.
This interplay between the batch size and learning rate is well known in machine learning.
# Setting the batch size and learning rate is known to be awkward due to their interplay.

*Number of inducing points*
Our variational inference scheme models the joint distribution over the data and assignments via the inducing variables
($\expertsInducingOutput$ and $\gatingsInducingOutput$).
In practice, the number of inducing points should be less than the number
of data points $(\NumInducing \ll \NumData)$, to obtain improved computational performance.
The learning curves in cref:fig-mcycle-training-curve visualise the learning process for different numbers
of inducing points $\NumInducing$.
Best performance is obtained when the model is instantiated with $\NumInducing=133$ inducing inputs,
i.e. a one-to-one correspondence between inducing inputs and
data inputs, $\expertInducingInput=\gatingInducingInput=\allInput$.
As the number of inducing points decreases the model is still able to recover the same negative ELBO.

\todo{add results for num inducing points leading to worse performance e.g. M=8}

*Evidence lower bounds*
The tight lower bound $\tightBound$ and further lower bound $\furtherBound$ recovered similar results in
all experiments.
This indicates that $\furtherBound$ does not loosen the bound to a point where it loses valuable information.
In contrast, $\furtherBoundTwo$ is not able to recover the same results.
This was expected as $\furtherBoundTwo$ corresponds to a further likelihood approximation, where
the experts' noise models are mixed instead of their full SVGPs.
$\furtherBound$ offers a rich ELBO for optimising \acrshort{mosvgpe} that achieves similar results to $\tightBound$,
whilst having lower computational complexity per evaluation.
For this reason, the remainder of this dissertation uses $\furtherBound$ for all experiments.

**** two vs three expert y fig :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_means.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-y-means-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/y_means.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-y-means-mcycle-two-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_samples.pdf}
\subcaption{}
\label{fig-y-samples-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/y_samples.pdf}
\subcaption{}
\label{fig-y-samples-mcycle-two-experts-further}
\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L1/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-two-experts}
%\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-three-experts}
%\end{minipage}
\caption{Visualisation of the model instantiated with $\ModeInd=2$ experts, after training on the motorcycle data set \citep{Silverman1985} with $\tightBound$ (left column) and with $\furtherBound$ (right column).  \subref{fig-y-means-mcycle-two-experts-tight}-\subref{fig-y-means-mcycle-two-experts-further} show the data set (black crosses) and the posterior means associated with the \acrshort{mosvgpe} (black solid line) and a SVGP (red dashed line) for comparison. \subref{fig-y-samples-mcycle-two-experts-tight}-\subref{fig-y-samples-mcycle-two-experts-further} show samples from the \acrshort{mosvgpe} posterior where colour indicates the underlying expert (cyan $K=1$, magenta $\ModeInd=2$) and the red lines show the $\pm 2$ standard deviation error ($95\%$ confidence interval) of the SVGP posterior. All experiments were trained with $25,000$ iterations of Adam \citep{kingmaAdam2017} using a learning rate of $0.01$ and a minibatch size of $\NumData_b=16$. All GPs use Squared Exponential kernels and $M=32$ inducing points.}
\label{fig-y-mcycle-two-vs-three-experts}
\end{figure}
#+END_EXPORT

# #+caption: \textbf{Motorcycle data} The left plot shows the observations (red crosses) together with the means of our model (black solid line) and a SVGP with noise (blue dashed line). It also shows probabilities assocaited with the gating network for $k=1$ (green line) and $k=2$ (blue line) on the right axis. The right plot shows 100 samples (blue) from the predictive posterior at 200 evenly spaced input locations as well as the $\pm 2$ std error (95% confidence interval) of the SVGP (solid red line). Both models use squared exponential covariance functions for all GPs.

**** two vs three expert latent fig :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/experts_f.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-gating-gps-mcycle-two-experts}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/experts_f.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-gating-gps-mcycle-three-experts}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/gating_gps.pdf}
%\subcaption{Gating function posteriors $\predictiveGatingPrior$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-gating-gps-mcycle-two-experts}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/gating_gps.pdf}
%\subcaption{Gating function posteriors $\predictiveGatingPrior$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-gating-gps-mcycle-three-experts}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/gating_mixing_probs.pdf}
%\subcaption{Gating network's predictive mixing probabilities $\predictiveProb$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-gating-mixing-probs-mcycle-two-experts}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/gating_mixing_probs.pdf}
%\subcaption{Gating network's predictive mixing probabilities $\predictiveProb$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-gating-mixing-probs-mycle-three-experts}
\end{minipage}
\caption{Visualisation of the model after training on the motorcycle data set with two experts (left column) and  three experts (right column). (a-b) show the GP posteriros associated with the gating funcions, $q(h_{k}(\state_*))$, where the solind lines show the means $\E[h_{k}(\state_*)]$ and the shaded regions show the 95\% confidence interval, i.e. $\pm2\simga$. (c-d) show the predictive mixing probabilities $\predictiveProb$.}
\label{fig-gating-network-mcycle-subset}
\end{figure}
#+END_EXPORT

**** training loss fig :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/training_curves/training_loss_num_ind_133.png}
\subcaption{}
\label{fig-mcycle-training-curve-133}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/training_curves/training_loss_num_ind_64.png}
\subcaption{}
\label{fig-mcycle-training-curve-64}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/training_curves/training_loss_num_ind_32.png}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-mcycle-training-curve-32}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/training_curves/training_loss_num_ind_16.png}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-mcycle-training-curve-16}
\end{minipage}
\caption{Training curves showing the negative ELBO (- $\furtherBound$) vs step when training
\acrshort{mosvgpe} (instantiated with two experts $\ModeInd=2$), on the
Motorcycle data set \citep{Silverman1985} with different numbers of inducing points $M$ and different
batch sizes $\NumData_b$. All experiments used the Adam optimiser \citep{kingmaAdam2017} with a learning rate
of $0.01$ and Squared Exponential kernels for all GPs.}
\label{fig-mcycle-training-curve}
\end{figure}
#+END_EXPORT

**** End :ignore:
\newpage

*** Evaluation on Velocity Controlled Quadcopter label:sec-brl-experiment
**** intro :ignore:
# The goal of this dissertation is to control a DJI Tello quadcopter in an indoor environment
# subject to two modes of operation characterised by turbulence.
In order to verify that \acrshort{mosvgpe} works on real-world systems,
it was tested on a real-world quadcopter data set following the illustrative example detailed
in cref:illustrative_example.
The data set was collected at the Bristol Robotics Laboratory using
a velocity controlled DJI Tello quadcopter and a Vicon tracking system.
A high turbulence dynamics mode was induced by placing a desktop fan at the right side of a room.
cref:fig-quadcopter-environment shows a diagram of the environment.
The data set represents samples from a dynamical system with constant controls, i.e.
$\Delta\state_{\timeInd} = \latentFunc(\state_{\timeInd-1};\control_{\timeInd-1}=\control_*)$.
# The resulting data set has two-dimensional inputs and two-dimensional outputs, making it easy to visualise
# the different components of the model.
#+BEGIN_EXPORT latex
\begin{figure}[h]
\centering
\begin{minipage}[r]{0.55\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/brl-quadcopter-domain-figure.png}
\subcaption{Diagram showing a top down view of the environment (a room in the Bristol Robotics Laboratory).}
\label{fig-quadcopter-environment}
\todo{add better diagram of environment}
\end{minipage}
\begin{minipage}[r]{0.44\columnwidth}
%\includegraphics[width=\textwidth]{./images/quiver_step_20_direction_down.png}
\includegraphics[width=\textwidth]{./images/model/quadcopter/subset-10/dataset_quiver.pdf}
\subcaption{Quiver plot showing the data set of state transitions from 9 trajectories flown 7 times.}
\label{fig-quiver}
\end{minipage}
\caption{Illustration of (\subref{fig-quadcopter-environment}) the environment (a room in the Bristol Robotics Laboratory) and (\subref{fig-quiver}) the data set of state transitions. A turbulent dynamics mode is induced by a desk top fan at the right hand side of the room and a subset of the enivornment has not been observed.}
\end{figure}
#+END_EXPORT



*Environment* The environment is modelled with two dimensions
(the $x$ and $y$ coordinates), which is a realistic assumption,
as altitude control can be achieved with a separate controller.
The state space is then the 2D coordinates $\state = [x, y]$ and the control is simply the velocity
$\control = [\dot{x}, \dot{y}]$.

*Data collection* The Vicon system provided access to the true position of the quadcopter at all times, which
enabled pre-planned trajectories to be flown, using a simple PID controller on
feedback from the Vicon system.
To simplify data collection,
nine trajectories from $y=2$ to $y=-3$, with different initial $x$ locations,
were used as target trajectories to be tracked by the PID controller.
Each trajectory was repeated 7 times to capture the variability (process noise) in the dynamics.
# To simplify data collection and visualisation, the controls were kept constant during data
# collection, reducing the dynamics to
# ${\Delta \state_\numData= f(\state_{\numData-1}; \control=\control_{\text{fixed}}) + \epsilon}$.

# To simplify data collection and visualisation, the controls were kept constant during data
# collection, reducing the dynamics to
# ${\Delta \state_\numData= f(\state_{\numData-1}; \control=\control_{\text{fixed}}) + \epsilon}$.
# Data from multiple trajectories following the same target trajectory were collected to
# capture the variability (process noise) in the dynamics.

# Nine trajectories (with different starting states but the same target velocity) were used as
# target trajectories
# and repeated
# multiple times
# from multiple trajectories following the same target trajectory were collected to
# capture the variability (process noise) in the dynamics.

*Data processing* The Vicon stream recorded data at 100Hz, which was then down sampled to
give a time step of $\Delta t = 0.1s$.
This reduced the size of the data set and left reasonable lengthscales.
The data set consists of $\NumData=2081$ state transitions.
\todo{update data set size}
cref:fig-quiver visualises the state transition data set as a quiver plot.
# cref:fig-quiver is a quiver plot showing the resulting data set.

**** Results
The model was instantiated with two experts, with the goal of each expert learning a separate dynamics mode and the
gating network learning a representation of how the underlying dynamics modes vary over the state space.
The model was trained using the model and training parameters in cref:tab-params-quadcopter.

# #+NAME: fig-gating-mixing-probs-quadcopter-subset
# #+ATTR_LATEX: :width 1\textwidth :placement [hbt!] :center t
# #+caption: Gating network's predictive mixing probabilties $\predictiveProb$ after training on the quadcopter data set.
# [[file:./images/model/quadcopter/subset/gating_mixing_probs.pdf]]
# #+NAME: fig-gating-gps-quadcopter-subset
# #+ATTR_LATEX: :width 1\textwidth :placement [hbt!] :center t
# #+caption: Gating network's predictive posterior $\predictiveGatingPrior$ after training on the quadcopter data set. Each row corresponds to a single GP posterior, $q(h_{k}(\state_*))$, corresponding to expert $k$. The mean $\E[h_{k}(\state_*)]$ is on the left and the variance $\V[h_{k}(\x_*)]$ is on the right.
# [[file:./images/model/quadcopter/subset/gating_gps.pdf]]

#+NAME: fig-y-mm-quadcopter-subset
#+ATTR_LATEX: :width 1\textwidth :placement [hbt!] :center t
#+caption: Moment matched predictive posterior $p(\Delta \state_* \mid \x_*)$ after training on the quadcopter data set. Each row corresponds to an output dimension $d$ where the left plot shows the moment matched mean $\E[\Delta \state_d]$ and the right plot shows the moment matched variance $\V[\Delta \state_d]$.
[[file:./images/model/quadcopter/subset-10/y_moment_matched.pdf]]

#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{1.0\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/quadcopter/subset-10/gating_mixing_probs.pdf}
\subcaption{Posterior over mode indicator variable.}
\label{fig-gating-mixing-probs-quadcopter-subset}
\end{minipage}
\begin{minipage}[r]{1.0\textwidth}
\includegraphics[width=\textwidth]{./images/model/quadcopter/subset-10/gating_gps.pdf}
\subcaption{GP posteriors over gating functions.}
\label{fig-gating-gps-quadcopter-subset}
\end{minipage}
\caption{Visualisation of the gating network after training on the quadcopter data set. The plots in (\subref{fig-gating-mixing-probs-quadcopter-subset}) show the predictive mixing probabilities $\predictiveProb$ for Expert 1 (left) and Expert 2 (right). The plots in (\subref{fig-gating-gps-quadcopter-subset}) show the predictive GP posteriors $q(h_{k}(\singleTestInput))$ associated with Expert 1 (top) and Expert 2 (bottom). The left hand plots show the means and the right hand plots show the variances.}
\label{fig-gating-network-quadcopter-subset}
\end{figure}
#+END_EXPORT

#+NAME: fig-experts-f-quadcopter-subset
#+ATTR_LATEX: :width 1\textwidth :placement [hbt!] :center t
#+caption: Visualisation of the experts' predictive posteriors $\predictiveExpertsPrior$ after training on the quadcopter data set. Each row corresponds to a single GP posterior, $q(f_{kd}(\singleTestInput))$, corresponding to dimension $d$ of expert $\modeInd$. The mean $\E[f_{kd}(\singleTestInput)]$ is on the left and the variance $\V[f_{kd}(\x_*)]$ is on the right. The noise variances learned by Expert 1 and Expert 2 were $\Sigma_1 = \diag\left([0.0063, 0.0259]]\right)$ and $\Sigma_2 = \diag\left([0.0874, 0.0432]\right)$ respectively.
[[file:./images/model/quadcopter/subset-10/experts_f.pdf]]


# #+NAME: fig-experts-y-quadcopter-subset
# #+ATTR_LATEX: :width 1\textwidth :placement [hbt!] :center t
# #+caption: Experts' predictive posterior over the output $q(\Delta\state_{kd} \mid \state_*)$ after training on the quadcopter data set. Each row corresponds to a single GP posterior, $q(\Delta\state_{kd} \mid \state_*)$, corresponding to dimension $d$ of expert $k$. The mean $\E[\Delta\state_{kd}\mid \state_*]$ is on the left and the variance $\V[\Delta\state_{kd} \mid \x_*]$ is on the right.
# [[file:./images/model/quadcopter/subset/experts_y.pdf]]

At a new input location $\singleTestInput$ the density over the output,
$p(\singleTestOutput \mid \singleTestInput)$,
follows a mixture of $\ModeInd$ Gaussians.
Visualising a mixture of two Gaussians with a two-dimensional input space and a two-dimensional output space
requires the components and mixing probabilities to be visualised separately.
To aid with visualisation, Figure ref:fig-y-mm-quadcopter-subset shows the predictive density
approximated as a unimodal Gaussian density (via moment matching), where each row corresponds to an
output dimension.
The predictive mean is fairly constant over the domain, except for the region in front of the fan, where it is
higher.
This result makes sense as the data set was assumed to be collected with constant controls.
The region with high predictive mean in front of the fan, is modelling the drift arising from the fan blowing the quadcopter in the negative $x$ direction.
The right hand plots of Figure ref:fig-y-mm-quadcopter-subset
show the predictive variance. It is high where there are no training observations,
indicating that the method has successfully represented the model's /epistemic uncertainty/.
It is also high in the region in front of the fan, showing that the model has successfully inferred
the high process noise, associated with the turbulence induced by the fan.
Let us now visualise the individual experts and the gating network separately.
# As the data set was assumed to have constant controls, this result aligns with our knowledge of the environment.
# That is, the dynamics should be constant
# dynamics should be constant

*Gating network*
Figure ref:fig-gating-network-quadcopter-subset shows the gating network after training on the
data set.
Figure ref:fig-gating-mixing-probs-quadcopter-subset indicates that the model has assigned responsibility
to Expert 2 in front of the fan, as its mixing probability
$\Pr(\singleTestModeVar=1 \mid \singleTestInput)$ is high in this region.
This implies that Expert 2 represents the turbulent dynamics mode in front of the fan.
Figure ref:fig-gating-gps-quadcopter-subset shows the GP posteriors associated with the gating functions.
The mean of the gating function associated with Expert 1 $\E[h_{1}(\singleTestInput)]$ is
high in the low-turbulence regions and low in the high-turbulence region in front of the fan.
The posterior variance associated with the gating function GPs is high in the region with no training
observations. This is a desirable behaviour because it is modelling the /epistemic uncertainty/.
These results demonstrate that the gating network infers important information regarding how the
system switches between dynamics modes over the input space.
# The mean also tends to zero where the model has had no training observations.

*Identifiability*
These results show that the GP-based gating network is capable of turning a single expert
on in multiple regions of the input space.
This is a desirable behaviour as it has enabled only two underlying dynamics modes to be identified.
In contrast, other MoGPE methods may have assigned an extra expert to one of the regions modelled by Expert 1.
In particular, the regions at $y>0$ and $y<-1$ may have been assigned to separate experts.

*Latent spaces for control*
The gating network consists of two spaces which are rich with information regarding how the
system switches between dynamics modes.
Firstly, the pmf over the expert indicator variable.
Secondly, the GP posteriors over the gating functions.
It is worth noting that all MoGPE methods obtain a pmf over the expert indicator variable and
this space suffers from interpretability issues.
Consider the meaning of the mixing probabilities tending to a uniform
distribution ${\Pr(\singleTestModeVarK \mid \singleTestInput)=0.5}$.
This corresponds to maximum entropy for a categorical distribution and could mean two different things.
It could mean that,
1) the model has training data in this region, so can confidently predict, but is unsure which expert is responsible,
   - perhaps the observations do not belong to any expert and an extra expert is required,
2) the model does not have training data in this region, so cannot confidently predict which expert is responsible.
# 2) the model has training data in this region, so can confidently predict and is confident that the observations are generated by an equal mixture of the experts,
# It could mean that the model is confident that the observations are generated by an equal mixture of the experts,
# or it could mean that the gating network is not confident making predictions in this region.
This interpretability issue is overcome by our GP-based gating network, as these two cases are modelled differently.
Either the gating function(s) are all equal and their posterior variance(s) are low, implying that the gating network
can predict confidently but is unsure which expert is responsible.
This could mean that an extra expert is responsible for predicting at this location.
Alternatively, the gating functions' posterior variance(s) could be high, implying
that the model is not confident in predicting which expert is responsible at the given input location.
Importantly, the GP posteriors associated with our gating network, not only infer information regarding the
mode switching, but also model the gating network's /epistemic uncertainty/.
These GP posteriors provide convenient latent spaces for control and
are exploited by the trajectory optimisation algorithms presented later in this dissertation.

*Experts*
Figure ref:fig-experts-f-quadcopter-subset shows the predictive posteriors $q(\latentFunc_{kd}(\singleTestInput))$
associated with each dimension $d$ of each expert $\modeInd$.
The method has successfully learned a factorised representation of the underlying dynamics, where
Expert 1 has learned a dynamics mode with low process noise
$\Sigma_1 = \diag\left([0.0063, 0.0259]]\right)$
and Expert 2 a mode with high process noise
$\Sigma_2 = \diag\left([0.0874, 0.0432]\right)$.
Expert 2 has also clearly learned the drift induced by the fan, indicated by the dark red region at $y=0$
in the two bottom left plots of Figure ref:fig-experts-f-quadcopter-subset.
It has also learned the control response of the PID controller correcting for the deviation
from the reference trajectory, indicated by the white region below $y=0$.
The control response is an artifact of the data collection process.
It is clear that Expert 2 has learned both the drift and process noise terms
associated with the turbulent dynamics mode.

Both experts were initialised with independent inducing inputs, $\expertInducingInput$, providing the model
flexibility to "soft" partition the data set.
That is, each expert has the freedom to set its inducing inputs, $\expertInducingInput$,
to support only a subset of the data set.
The posterior (co)variance associated with each expert represents their /epistemic uncertainty/.
The top right plot in Figure ref:fig-experts-f-quadcopter-subset shows the posterior variance associated with
the $x$ dimension of Expert 1.
The posterior variance is high in front of the fan because the gating network has assigned responsibility to the
other expert in this region.
It is also high in the region where the model has had no training observations, as would be expected.
However, the posterior variance associated with the $y$ dimension of Expert 1, is not high in this region.
This is due to the lengthscale of the second output dimension allowing Expert 1 to confidently extrapolate.
\todo{is this because of the lengthscale or is it due to gating network}

The bottom right two plots in Figure ref:fig-experts-f-quadcopter-subset show the posterior variance
associated with the $x$ and $y$ dimensions of Expert 2.
The posterior variance is high everywhere except for the region in front of the fan.
Again, this is due to the gating network assigning responsibility to the other expert outside of the region in
front of the fan.
It is clear from these results that the likelihood approximation in cref:eq-likelihood-approximation, combined with our gating network
and variational inference scheme, are capable of modelling the assignment of observations to experts via
the inducing points.




# *Gating Network*
# Figure ref:fig-gating-network-quadcopter-subset shows the gating network after training on the
# data set.
# Figure ref:fig-gating-mixing-probs-quadcopter-subset indicates that the model has assigned responsibility
# to expert 2 in front of the fan as its mixing probability
# $\Pr(\singleTestModeVar=1 \mid \singleTestInput)$ is high in front of the fan, i.e. the high-turbulence region.
# The mixing probabilities tend to a uniform distribution
# ${\Pr(\singleTestModeVarK \mid \singleTestInput)=0.5}$
# in the region with no training observations.
# This corresponds to maximum entropy for categorical distributions and is a desirable behaviour.
# However, it is worth noting that the mixing probabilities can tend to a uniform distribution for multiples reasons.
# It could mean that the model is confident that the observations are generated by an equal mixture of the experts
# i.e. the gating function(s) are all equal and their posterior variance(s) are low.
# Alternatively, it could mean that the model is not confident in predicting which expert is responsible at the given
# input location i.e. the variance(s) of the GP posterior(s) associated with the gating function(s) could be high.


# # It is worth noting that the mixing probabilities lose a degree of freedom which makes it difficult to
# # interpret the meaning of $\Pr(\modeVar_*=\modeInd \mid \state_*= 0.5$.
# # It could mean that it is confident that the observations were generated by a 50:50 mixture of the two experts
# # i.e. the gating functions are all equal to zero and the posterior variance is low.
# # Alternatively, the variance of the gating function GP posterior could be high, resulting in the probability tending to
# # maximum entropy.

# Figure ref:fig-gating-gps-quadcopter-subset shows the GP posteriors associated with the gating functions.
# The mean of the gating function associated with expert 1 $\E[h_{1}(\singleTestInput)]$ is
# high in the low-turbulence regions and low in front of the fan, i.e. the high-turbulence mode.
# The mean also tends to zero where the model has had no training observations.
# The posterior variance is also high in this region, indicating that the gating
# network GPs have successfully modelled the /epistemic uncertainty/.
# Exploiting a GP-based gating network has clearly provided a principled approach to distinguishing
# between the different reasons the mixing probabilities may tend to a uniform distribution.





# The posterior variance is high where the model has not observed
# the system, indicating that the gating network GPs successfully infer the epistemic uncertainty when using the
# inference scheme in Section ref:sec-inference.
# Formulating the gating network based on GPs has clearly provided a principled approach to distinguishing
# between the different reasons the mixing probabilities may tend to a uniform distribution.

# The predictive posterior is mixture of $\ModeInd$ Gaussian resulting from combining the experts according to
# the gating network.

*** Evaluation on Simulated Quadcopter Data Set :noexport:
#+BEGIN_EXPORT latex
\newcommand{\roll}{\ensuremath{\phi}}
\newcommand{\pitch}{\ensuremath{\theta}}
\newcommand{\yaw}{\ensuremath{\psi}}

\newcommand{\world}{\ensuremath{w}}
\newcommand{\body}{\ensuremath{b}}
\newcommand{\intermediate}{\ensuremath{c}}
\renewcommand{\frame}{\ensuremath{F}}
\newcommand{\worldFrame}{\ensuremath{\frame_\world}}
\newcommand{\bodyFrame}{\ensuremath{\frame_\body}}
\newcommand{\intermediateFrame}{\ensuremath{\frame_\intermediate}}

\newcommand{\inertiaMatrix}{\ensuremath{J}}
\newcommand{\inertiaZ}{\ensuremath{I_{zz}}}
\newcommand{\zw}{\ensuremath{\mathbf{z}_\world}}

\newcommand{\positions}{\ensuremath{\mathbf{h}}}
\newcommand{\velocity}{\ensuremath{v}}
\newcommand{\velocityx}{\ensuremath{\dot{x}}}
\newcommand{\velocityy}{\ensuremath{\dot{y}}}
\newcommand{\Velocity}{\ensuremath{\mathbf{\velocity}}}

\newcommand{\rotationMatrix}{\ensuremath{\mathbf{R}}}
\newcommand{\angularVelocity}{\ensuremath{\bm\omega}}
\newcommand{\angularVelocityMatrix}{\ensuremath{\bm\Omega}}

\renewcommand{\thrust}{\ensuremath{f_{x}}}
\renewcommand{\Thrust}{\ensuremath{\mathbf{f_{x}}}}
\renewcommand{\torque}{\ensuremath{\tau_z}}

\newcommand{\quadcopterDynamics}{\ensuremath{f_{\text{quad}}}}
#+END_EXPORT
The quadcopter frames and Euler angles (roll $\roll$, pitch $\pitch$ and yaw $\yaw$) are shown
in Figure ref:fig-quadcopter-frames.
The subscripts $\world$, $\body$ and $\intermediate$ are used to denote the world
$\worldFrame$, body $\bodyFrame$ and intermediate $\intermediateFrame$ (after yaw angle rotation) frames respectively.
The the 3D nonlinear quadcopter dynamics are described with the quadcopter model from cite:wangSafe2018,zhouVector2014.

The 2D nonlinear quadcopter dynamics are based on the
state vector is given by $\state = [x, y, \velocityx, \velocityy, \yaw]$
where $\positions = [x, y]$ is the Euclidean coordinates relative to the world frame $\worldFrame$,
$\Velocity = [\velocityx, \velocityy]$ is the velocity in the world frame $\worldFrame$ and
$\yaw$ is the yaw angle, i.e. the angle around the $z$ axis.
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadcopter-dynamics}
\dot{\state} &= \quadcopterDynamics(\state, \control)
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadcopter-dynamics}
\rotationMatrix_{\body,\world} =
\begin{bmatrix}
\cos{\yaw} & -\sin(\yaw) \\
\sin{\yaw} & \cos(\yaw)
\end{bmatrix}
\end{align}
#+END_EXPORT
where,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadcopter-dynamics}
\dot{\state}_{1:2} &= \dot{\positions} = \state_{3:4} \\
\dot{\state}_{3:4} &= \dot{\Velocity} = \frac{1}{m} \rotationMatrix \Thrust \\
\dot{\state}_{5} &= \dot{\yaw} = \frac{1}{\inertiaZ} \torque
\end{align}
#+END_EXPORT
where $\Thrust = [\thrust, 0]^T$ is the total thrust force in the quadcopters from the rotors and $\torque$ is the torque on the quadcopter
around the $z$ axis of the world frame $\worldFrame$.
The thrust and torque are realistic controls for a 2D quadcopter system and gives the
control vector \control = [\thrust, \torque].

#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadcopter-dynamics}
\dot{\Velocity} &= g \zw + \frac{1}{m} \thrust \rotationMatrix \zw \\
\dot{\rotationMatrix} &= \rotationMatrix \angularVelocityMatrix \\
\dot{\angularVelocity} &= \inertiaMatrix^{-1} \torque - \inertiaMatrix^{-1} \angularVelocityMatrix \inertiaMatrix \angularVelocity \\
\dot{\positions} &= \velocity
\end{align}
#+END_EXPORT
where $g=9.81ms^{-2}$ is the acceleration due to gravity, $m$ is the mass, $\zw=[0,0,1]^T$,
$\positions=[x, y, z]^T$ is the position of the quadcopter in the world frame $\worldFrame$,
$\Velocity=[\velocity_x, \velocity_y, \velocity_z]^T$ is the
and velocity of the quadcopter in the world frame $\worldFrame$ respectively.
The angular velocity of the quadcopter in the body frame $\bodyFrame$ is denoted $\angularVelocity=[p, q ,r]^T$
and in tensor form $\angularVelocityMatrix=[[0,-p,q];[r,0,-p];[-q,p,0]]$

$\state=[x, y, z, \velocity_x, \velocity_y, \velocity_z, \roll, \pitch, \yaw, \dot{\roll}, \dot{\pitch}, \dot{\yaw}]$

$\state=[x, y, z, \velocity_x, \velocity_y, \velocity_z, \roll, \pitch, \yaw]^T$
$\state=[\ddot{x}, \ddot{y}, \ddot{z}, \dot{\roll}, \dot{\pitch}, \dot{\yaw}]^T$

\newpage
*** Point Mass 2D Dynamics :noexport:
#+BEGIN_EXPORT latex
\newcommand{\roll}{\ensuremath{\phi}}
\newcommand{\pitch}{\ensuremath{\theta}}
\newcommand{\yaw}{\ensuremath{\psi}}

\newcommand{\world}{\ensuremath{w}}
\newcommand{\body}{\ensuremath{b}}
\newcommand{\intermediate}{\ensuremath{c}}
\renewcommand{\frame}{\ensuremath{F}}
\newcommand{\worldFrame}{\ensuremath{\frame_\world}}
\newcommand{\bodyFrame}{\ensuremath{\frame_\body}}
\newcommand{\intermediateFrame}{\ensuremath{\frame_\intermediate}}

\newcommand{\inertiaMatrix}{\ensuremath{J}}
\newcommand{\inertiaZ}{\ensuremath{I_{zz}}}
\newcommand{\zw}{\ensuremath{\mathbf{z}_\world}}

\newcommand{\positions}{\ensuremath{\mathbf{h}}}
\newcommand{\velocity}{\ensuremath{v}}
\newcommand{\velocityx}{\ensuremath{\dot{x}}}
\newcommand{\velocityy}{\ensuremath{\dot{y}}}
\newcommand{\Velocity}{\ensuremath{\mathbf{\velocity}}}

\newcommand{\rotationMatrix}{\ensuremath{\mathbf{R}}}
\newcommand{\angularVelocity}{\ensuremath{\bm\omega}}
\newcommand{\angularVelocityMatrix}{\ensuremath{\bm\Omega}}

\renewcommand{\thrust}{\ensuremath{f_{x}}}
\renewcommand{\Thrust}{\ensuremath{\mathbf{f_{x}}}}
\renewcommand{\torque}{\ensuremath{\tau_z}}

\newcommand{\quadcopterDynamics}{\ensuremath{f_{\text{quad}}}}
#+END_EXPORT
cite:williamsAdvancing
cite:watsonStochastic2020
cite:watsonAdvancing2021

cite:levineVariational2013


cite:bhardwajDifferentiable2020

cite:mukadamContinuoustime2018

The dynamics of a point mass in 2D can represented with the
state vector $\state = [x, y, \velocityx, \velocityy, \yaw]$,
where $\positions = [x, y]$ denotes the 2D Euclidean coordinates relative to the world frame $\worldFrame$,
$\Velocity = [\velocityx, \velocityy]$ is the velocity in the world frame $\worldFrame$ and
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{tikzpicture}
% body frame
\draw[thick,->] (0,0) -- (1.5,1.5) node[anchor=south] {$x$};
\draw[thick,->] (0,0) -- (1.5,-1.5) node[anchor=north] {$y$};

% world frame
\draw[thick,->] (-3,-3) -- (3,-3) node[anchor=north west] {$x$};
\draw[thick,->] (-3,-3) -- (-3,3) node[anchor=south east] {$y$};
\foreach \x in {-3, -2, -1, 0,1,2,3}
   \draw (\x cm,-3) -- (\x cm,-3) node[anchor=north] {$\x$};
\foreach \y in {-3, -2, -1, 0,1,2,3}
    \draw (-3,\y cm) -- (-3,\y cm) node[anchor=east] {$\y$};
\end{tikzpicture}
\end{figure}
#+END_EXPORT


The point mass can apply a force along its $x$ axis (known as the thrust vector), which is denoted
$\Thrust = [\thrust, 0]^T$.
It can also rotate itself by applying a torque $\torque$ around its $z$ axis.
The resulting control vector for the 2D point mass is given by $\control = [\thrust, \torque]$.
The nonlinear dynamics of the system are given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadcopter-dynamics}
\dot{\state} &=
\begin{bmatrix}
    \dot{\state}_{1} \\
    \dot{\state}_{2} \\
    \dot{\state}_{3} \\
    \dot{\state}_{4} \\
    \dot{\state}_{5}
\end{bmatrix}
=
\begin{bmatrix}
    \state_{3} \\
    \state_{4} \\
    \frac{1}{m} \thrust \cos{(\yaw)} \\
    \frac{1}{m} \thrust \sin{(\yaw)} \\
    \frac{1}{\inertiaZ} \torque
\end{bmatrix}
\end{align}
#+END_EXPORT
where $m$ is the mass and $\inertiaZ$ is the moment of inertia around the vertical $z$ axis.
Defining the rotation matrix from the body frame $\bodyFrame$ to the world frame $\worldFrame$ as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadcopter-dynamics}
\rotationMatrix_{\body,\world} =
\begin{bmatrix}
\cos{\yaw} & -\sin(\yaw) \\
\sin{\yaw} & \cos(\yaw)
\end{bmatrix}
\end{align}
#+END_EXPORT
the dynamics can be calculated as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadcopter-dynamics}
\dot{\state}_{1:2} &= \dot{\positions} = \state_{3:4} \\
\dot{\state}_{3:4} &= \dot{\Velocity} = \frac{1}{m} \rotationMatrix \Thrust \\
\dot{\state}_{5} &= \dot{\yaw} = \frac{1}{\inertiaZ} \torque
\end{align}
#+END_EXPORT

The unknown dynamics can be modelled by placing GP priors on the accelerations ($\dot{\dot{x}}, \dot{\dot{y}}$) and
the angular (yaw) velocity $\dot{\yaw}$,
#+BEGIN_EXPORT latex
\renewcommand{\input}{\ensuremath{\hat{\state}}}
\begin{align} \label{eq-quadcopter-dynamics}
\dot{\state} &=
\begin{bmatrix}
    \dot{\state}_{1} \\
    \dot{\state}_{2} \\
    \dot{\state}_{3} \\
    \dot{\state}_{4} \\
    \dot{\state}_{5}
\end{bmatrix}
=
\begin{bmatrix}
    \state_{3} \\
    \state_{4} \\
    \frac{1}{m} \thrust \cos{(\yaw)} + \mathcal{N} \left( \mu_3(\input),  k_3(\input, \input) \right) \\
    \frac{1}{m} \thrust \sin{(\yaw)} + \mathcal{N} \left( \mu_4(\input),  k_4(\input, \input) \right) \\
    \frac{1}{\inertiaZ} \torque + \mathcal{N} \left( \mu_5(\input),  k_5(\input, \input) \right)
\end{bmatrix}
\end{align}
#+END_EXPORT


\newpage
** Remarks

*Implicit data assignment*
It is worth noting that in contrast to other MoGPE methods, this model does not directly assign observations to experts.
However, after augmenting each expert with separate inducing points,
the model has the flexibility to loosely /partition/ the data set.
Just as sparse GP methods can be viewed as methods that parameterise a full nonparametric GP,
our approach can be viewed as parameterising the nonparametric Mixture of Gaussian Process Experts.
Conveniently, our parameterisation, in particular the likelihood approximation in cref:eq-likelihood-approximation,
deals with the issue of marginalising exponentially many sets of assignments of observations to experts.
Evident from the results in this chapter, this likelihood approximation appears to retain important information
regarding the assigning of observations to experts, whilst efficiently marginalising the expert indicator variable.
It is also worth noting that the number of inducing points $\NumInducing$ associated with each expert,
could be set by considering the number of data points believed to belong to a particular expert.
Currently, each expert's inducing inputs are initialised by randomly sampling a subset of the data inputs.
Future work could explore different techniques for initialising each expert's inducing inputs.

*Full Bayesian treatment of inducing inputs*
Common practice in sparse GP methods is to jointly optimise the hyperparameters and the inducing inputs.
Optimising only some of the parameters, instead of marginalising all of them, is known as Type-II maximum likelihood.
In Bayesian model selection, it is well known that Type-II maximum likelihood can lead to overfitting
if the number of parameters being optimised is large.
In the case of inducing inputs, there can often be beyond hundreds or thousands that need to be optimised.
Further to this, cite:rossiSparse2021 show that optimising the inducing inputs
relies on being able to optimise both the prior and the posterior, therefore contradicting Bayesian inference.
Our variational inference scheme follows common practice and optimises the inducing inputs
jointly with the hyperparameters.
In some instances, we observe that optimising the inducing inputs leads to them taking values far away from the
training data.
Often this can be avoided by simply sampling the inducing inputs from
the training inputs and fixing them, i.e. not optimising them.
This often leads to better NLPP scores as well.
This observation highlights that a full Bayesian treatment of the inducing inputs is an interesting direction for
future work.
However, specifying priors and performing /efficient/ posterior inference over the inducing inputs
is a challenging problem.


*Latent spaces for control*
In conventional \acrshort{mogpe} methods, the epistemic uncertainty associated with the
gating network is not decoupled from the probability mass function over the expert indicator variable.
This is because an uncertain gating network is represented by a uniform distribution over the expert indicator variable.
In contrast, \acrshort{mosvgpe} models the uncertainty associated with each gating function via
their GPs' posterior covariance.
Further to this, formulating the gating network GPs
with differentiable mean and covariance functions, enables techniques from Riemannian geometry
to be deployed on the gating functions citep:carmoRiemannian1992.
The power of the \acrshort{gp}-based gating network will become apparent
when its latent /geometry/ is leveraged for control in cref:chap-traj-opt-geometry.
# In particular, cref:chap-traj-opt-geometry is interested in finding length minimising trajectories on the
# gating functions' GP posteriors, aka geodesic trajectories citep:tosiMetrics2014.

** Conclusion
This chapter has presented a method for learning representations of multimodal dynamical systems using
a \acrshort{mogpe} method.
Motivated by correctly identify the underlying dynamics modes and inferring latent structure that can
be exploited for control,
this work formulated a gating network based on input-dependent gating functions.
This aids the inherent identifiability issues associated with mixture models
as it can be used to constrain the set of admissible functions through the placement of informative
GP priors on the gating functions.
Further to this, the GP posteriors over the gating functions provide convenient latent spaces for control.
This is because they are rich with information regarding the separation of the underlying dynamics modes
and also model the /epistemic uncertainty/ associated with the gating network.
# Motivated by learning latent spaces for control
# and ensuring that the true underlying dynamics modes are identified,
# this work formulated a gating network based on input-dependent gating functions.
# As we shall see in cref:chap-traj-opt-geometry, the GP posteriors over the gating functions provide convenient
# latent spaces for control.




The variational inference scheme presented in this chapter addresses the issue of marginalising
every possible set of assignments of observations to experts
-- of which there are $\ModeInd^{\NumData}$ possibilities
-- in the MoGPE marginal likelihood.
It overcomes the issue of assigning observations to experts by augmenting each expert GP
with a set of inducing points.
These inducing points are assumed to be a sufficient statistic for the joint distribution
over every possible set of assignments to experts.
This induces a factorisation over data which
is used to derive three ELBOs that provide a coupling between the
optimisation of the experts and the gating network, by efficiently marginalising the expert indicator variable for single
data points.
The ELBOs are compared on the Motorcycle data set citep:Silverman1985.
The $\furtherBound$ bound provides the best performance as it balances the accuracy offered by the tight bound $\tightBound$,
with the computational improvements offered by further bounding the GPs.
The results demonstrate that the variational inference scheme principally handles uncertainty whilst
providing scalability via stochastic variational inference.
The method is further evaluated on a real-world quadcopter example demonstrating that
it can successfully learn a factorised representation of a real-world, multimodal, robotic system.

* Mode Remaining Trajectory Optimisation label:chap-traj-opt-control
# * A Geometric Take on Model-Based Control for Multimodal Dynamical Systems label:chap-traj-opt-geometry
# * Synergising Bayesian Inference and Riemannian Geometry for Mode Remaining Control label:chap-traj-opt-geometry
# * Mode Remaining Control via Latent Geometry label:chap-traj-opt-geometry
# * A Geometric Take on Mode Remaining Model-Based Control label:chap-traj-opt-geometry
# * A Geometric Take on Model-Based Control in Multimodal Dynamical Systems label:chap-traj-opt-geometry
** Maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\nominalStateTraj}{\ensuremath{\stateTraj_*}}
\newcommand{\nominalControlTraj}{\ensuremath{\controlTraj_*}}
\newcommand{\fixedControl}{\ensuremath{\control_{*}}}
\newcommand{\velocity}{\ensuremath{v}}

\newcommand{\trajectory}{\ensuremath{\bar{\state}}}
\newcommand{\stateControlTraj}{\ensuremath{\bm\tau}}
\newcommand{\jacTraj}{\ensuremath{\bar{\mathbf{J}}}}
\renewcommand{\modeVarTraj}{\ensuremath{\modeVar_{0:\TimeInd}=\desiredMode}}

\renewcommand{\stateDiffTraj}{\ensuremath{\Delta\bar{\state}}}
\renewcommand{\stateCol}{\ensuremath{\mathbf{z}}}

%\renewcommand{\modeInd}{\ensuremath{\modeVar}}

\newcommand{\desiredMode}{\ensuremath{\modeInd^{*}}}
\renewcommand{\modeDes}[1]{\ensuremath{#1_{\desiredMode}}}
\newcommand{\desiredGatingFunction}{\ensuremath{\modeDes{\gatingFunc}}}
%\newcommand{\desiredDynamicsFunc}{\ensuremath{\mode{\latentFunc}}}
\newcommand{\desiredDynamicsFunc}{\ensuremath{\modeDes{\latentFunc}}}
%\newcommand{\desiredDynamicsFunc}{\ensuremath{\latentFunc_{\modeVar_{\timeInd}}}}
\newcommand{\desiredStateDomain}{\ensuremath{\modeDes{\stateDomain}}}
%\newcommand{\desiredStateDomain}{\ensuremath{\mode{\stateDomain}}}

%\newcommand{\controlledDynamicsFunc}{\ensuremath{\modeDes{\latentFunc}}}
\newcommand{\controlledDynamicsFunc}{\ensuremath{\latentFunc_{\controlTraj}}}

\newcommand{\valueFunc}{\ensuremath{V}}

\renewcommand{\controlledPolicyDist}{\ensuremath{q_\policy}}

\renewcommand{\satisfactionProb}{\ensuremath{p_{\modeVar}}}
#+END_EXPORT
# *** Geometry Maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\manifold}{\ensuremath{\mathcal{M}}}
\newcommand{\manifoldFunction}{\ensuremath{h}}
\newcommand{\manifoldDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\manifoldCodomain}{\ensuremath{\mathcal{Z}}}
\newcommand{\ManifoldDim}{\ensuremath{D}}
\newcommand{\manifoldDim}{\ensuremath{d}}
\newcommand{\manifoldDomainDim}{\ensuremath{d_{\manifoldDomain}}}
\newcommand{\manifoldCodomainDim}{\ensuremath{d_{\manifoldCodomain}}}
\newcommand{\manifoldInput}{\ensuremath{\mathbf{x}}}

% \newcommand{\jacobian}{\ensuremath{\mathbf{J}_{\mathbf{x}_t}}}
\newcommand{\jacobian}{\ensuremath{\mathbf{J}(\state(t))}}
\newcommand{\metricTensor}{\ensuremath{\mathbf{G}}}
\newcommand{\metricTensorTraj}{\ensuremath{\bar{\mathbf{G}}}}

\newcommand{\geodesicFunction}{\ensuremath{f_G}}

%\newcommand{\gatingDomain}{\ensuremath{\hat{\mathcal{X}}}}
%\newcommand{\gatingCodomain}{\ensuremath{\mathcal{A}}}
\newcommand{\gatingDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\gatingCodomain}{\ensuremath{\mathcal{Z}}}

\newcommand{\desiredManifold}{\ensuremath{\mathcal{M}_{k^*}}}
%\newcommand{\desiredMetricTensor}{\ensuremath{\mathbf{G}_{k^*}}}
\newcommand{\desiredMetricTensor}{\ensuremath{\mathbf{G}}}
%\newcommand{\desiredJacobian}{\ensuremath{\mathbf{J}_{k^*}(\state(t))}}
\newcommand{\desiredJacobian}{\ensuremath{\mathbf{J}}}
%\newcommand{\GatingDim}{\ensuremath{D_{x+u}}}
\newcommand{\GatingDim}{\ensuremath{D}}
\newcommand{\gatingDim}{\ensuremath{d}}

% Manfiold kernels
\renewcommand{\manifoldKernelMM}{\ensuremath{\mathbf{K}_{\NumInducing \NumInducing}}}
\newcommand{\jacManifoldKernelsM}{\ensuremath{\partial \mathbf{K}_{* \NumInducing}}}
\newcommand{\jacManifoldKernelMs}{\ensuremath{\partial \mathbf{K}_{\NumInducing *}}}
\newcommand{\hessManifoldKernel}{\ensuremath{\partial^2 \mathbf{K}_{**}}}
\renewcommand{\manifoldKernelNN}{\ensuremath{\mathbf{K}_{\NumData \NumData}}}
\newcommand{\jacManifoldKernelsN}{\ensuremath{\partial \mathbf{K}_{* \NumData}}}
\newcommand{\jacManifoldKernelNs}{\ensuremath{\partial \mathbf{K}_{\NumData *}}}
\newcommand{\hessManifoldKerneldd}{\ensuremath{\partial^2 k(\cdot, \cdot')}}
\newcommand{\jacManifoldKerneldN}{\ensuremath{\partial \mathbf{K}_{\cdot \NumData}}}
\newcommand{\jacManifoldKernelNd}{\ensuremath{\partial \mathbf{K}_{\NumData \cdot}}}

\newcommand{\manifoldInducingInput}{\ensuremath{\bm\xi}}
%\newcommand{\manifoldInducingOutput}{\ensuremath{\mathbf{u}}}
\newcommand{\manifoldInducingOutput}{\ensuremath{\manifoldFunction(\manifoldInducingInput)}}
\newcommand{\manifoldInducingVariational}{\ensuremath{q(\mathbf{u})}}
\newcommand{\manifoldInducingOutputMean}{\ensuremath{\mathbf{m}}}
\newcommand{\manifoldInducingOutputCov}{\ensuremath{\mathbf{S}}}
\newcommand{\manifoldMeanFunc}{\ensuremath{\mu}}


%\newcommand{\manifoldFunc}{\ensuremath{\mathbf{h}}}
%\newcommand{\desiredMeanFunc}{\ensuremath{\mu}}
\renewcommand{\muJac}{\ensuremath{\bm\mu_{\mathbf{J}}}}
\renewcommand{\covJac}{\ensuremath{\bm\Sigma_{\mathbf{J}}}}
\renewcommand{\testInput}{\ensuremath{\mathbf{x}_*}}

\newcommand{\stateDiff}{\ensuremath{\Delta \state}}

\renewcommand{\stateCostMatrix}{\ensuremath{\mathbf{Q}}}
\renewcommand{\controlCostMatrix}{\ensuremath{\mathbf{R}}}
\renewcommand{\terminalStateCostMatrix}{\ensuremath{\mathbf{H}}}
\renewcommand{\approxExpectedCost}{\ensuremath{J(\stateTraj, \controlTraj)}}

\renewcommand{\terminalState}{\ensuremath{\state_{\TimeInd}}}

\newcommand{\stateMean}{\ensuremath{\bm\mu_{\state_\timeInd}}}
\newcommand{\stateCov}{\ensuremath{\bm\Sigma_{\state_\timeInd}}}
\newcommand{\terminalStateMean}{\ensuremath{\bm\mu_{\state_\TimeInd}}}
\newcommand{\terminalStateCov}{\ensuremath{\bm\Sigma_{\state_\TimeInd}}}
\newcommand{\controlMean}{\ensuremath{\bm\mu_{\control_\timeInd}}}
\newcommand{\controlCov}{\ensuremath{\bm\Sigma_{\control_\timeInd}}}
\newcommand{\stateDiff}{\ensuremath{\Delta \state}}
\newcommand{\stateDiffMean}{\ensuremath{\bm\mu_{\stateDiff_\timeInd}}}
\newcommand{\stateDiffCov}{\ensuremath{\bm\Sigma_{\stateDiff_\timeInd}}}

\renewcommand{\transitionDistK}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \modeVar_{\timeInd}=\modeInd)}}
#+END_EXPORT
** Intro :ignore:
This chapter is concerned with controlling /unknown/ or /partially unknown/, multimodal dynamical systems,
given a single-step predictive dynamics model learned using the \acrshort{mosvgpe} method from cref:chap-dynamics.
In particular, it is concerned with /mode remaining/ trajectory optimisation, which
is formally defined in cref:def-mode-remaining.
Informally, /mode remaining/ trajectory optimisation attempts to find trajectories
from an initial state $\state_0$ -- in a desired dynamics mode -- to a target state $\state_f$,
whilst remaining in the desired dynamics mode.

The \acrshort{mosvgpe} method from cref:chap-dynamics was intentionally formulated with latent variables
to represent the mode switching behaviour and its associated uncertainty.
This chapter unleashes the power of these latent variables by making decisions under their uncertainty.
# The methods presented in this chapter are model-based control techniques that leverage
# a single-step predictive dynamics model learned using the \acrshort{mosvgpe} method from cref:chap-dynamics.

The remainder of this chapter is organised as follows.
cref:sec-problem-statement formally states the problem.
cref:chap-traj-opt-geometry details two methods that leverage the geometry of the \acrshort{mosvgpe} gating network.
The first method in cref:sec-traj-opt-collocation
resembles an indirect control method as it solves the necessary conditions which /indirectly/ represent the
original optimal control problem.
In contrast, the second method in cref:sec-traj-opt-energy takes the more standard approach and directly solves the
optimal control problem.
cref:chap-traj-opt-inference then introduces an alternative approach to mode remaining trajectory optimisation, which
does not leverage the geometry of the gating network.
Instead, it extends the control-as-inference framework and encodes mode remaining behaviour via conditioning on
the mode indicator variable.

cref:chap-traj-opt-results evaluates and compares all three methods using the illustrative
example from cref:illustrative_example.
The work in this chapter is implemented in JAX and TensorFlow/GPflow and is available on GitHub[fn::Code accompanying
cref:sec-traj-opt-collocation can be found @ [[https://github.com/aidanscannell/ModeOpt][\icon{\faGithub}/aidanscannell/ModeOpt and]]
[[https://github.com/aidanscannell/trajectory-optimisation-in-learned-multimodal-dynamical-systems][\icon{\faGithub}/aidanscannell/trajectory-optimisation-in-learned-multimodal-dynamical-systems]]
and code accompanying cref:sec-traj-opt-energy can be found @ [[https://github.com/aidanscannell/ModeOpt][\icon{\faGithub}/aidanscannell/ModeOpt]].]
citep:jax2018github,tensorflow2015-whitepaper,GPflow2017.

# The remainder of this chapter is organised as follows.
# cref:sec-problem-statement formally states the problem
# and cref:sec-geometry-recap introduces the relevant background on Riemannian geometry and its extension to
# probabilistic manifolds.
# cref:sec-traj-opt-collocation,sec-traj-opt-energy
# detail the two trajectory optimisation algorithms.
# The methods are then evaluated using the illustrative example from cref:illustrative_example.
# The work in this chapter is implemented in JAX and TensorFlow/GPflow and is available on GitHub[fn::Code accompanying
# cref:sec-traj-opt-collocation can be found @ [[https://github.com/aidanscannell/ModeOpt][\icon{\faGithub}/aidanscannell/ModeOpt and]]
# [[https://github.com/aidanscannell/trajectory-optimisation-in-learned-multimodal-dynamical-systems][\icon{\faGithub}/aidanscannell/trajectory-optimisation-in-learned-multimodal-dynamical-systems]]
# and code accompanying cref:sec-traj-opt-energy can be found @ [[https://github.com/aidanscannell/ModeOpt][\icon{\faGithub}/aidanscannell/ModeOpt]].]
# citep:jax2018github,tensorflow2015-whitepaper,GPflow2017.

# This section first recaps the relevant concepts from Riemannian geometry.
# It then details two control methods that exploit the geometry of the \acrshort{mosvgpe} model to obtain mode remaining behaviour.
# The first approach is an indirect optimal control method based on differential flatness and collocation.
# The second method embeds the mode remaining behaviour into a more conventional Gaussian process
# control framework.
# Finally, this section evaluates the two approaches using the illustrative example from cref:illustrative_example.


\todo{cite ICRA paper}

** Problem Statement label:sec-problem-statement
# #+BEGIN_EXPORT latex
# \begin{align}
# \mode{\latentFunc} : \stateDomain \times \controlDomain \rightarrow \stateDomain \quad \text{if} \quad \modeVar=\modeInd,
# \end{align}
# #+END_EXPORT
# $\stateDomain = \bigcup_{\modeInd \in \modeVarDomain} \mode{\stateDomain} = \{ \state \subseteq \stateDomain \mid \modeInd \in \modeVarDomain \}$.
# $\stateDomain = \bigcup_{\modeInd=1}^{\ModeInd} \mode{\stateDomain}$.
# This chapter considers multimodal dynamical systems where the dynamics modes are defined by disjoint
# state domains.
The goal of this chapter is to control nonlinear, stochastic, multimodal dynamical systems,
#+BEGIN_EXPORT latex
\begin{subequations}
\begin{align}
\state_{\timeInd+1} &= \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \bm\epsilon \\
&= \mode{\dynamicsFunc}(\state_\timeInd, \control_\timeInd) + \mode{\bm\epsilon}
\quad \text{if} \quad \modeVar(\state_{\timeInd})=\modeInd \nonumber \\
\mode{\bm\epsilon} &\sim \mathcal{N}(\mathbf{0}, \Sigma_{\mode{\bm\epsilon}}),
\end{align}
\end{subequations}
#+END_EXPORT
from an initial state $\state_{0}$, to a target state $\targetState$,
whilst remaining in a desired dynamics mode $\desiredMode$.
It considers systems where the underlying dynamics modes are defined by disjoint state domains.
That is, each dynamics mode is defined by its state domain
$\mode{\stateDomain} = \{ \state \in \stateDomain \mid \modeVar(\state) = \modeInd \}$, with
$\stateDomain_{i} \cap \stateDomain_{j} = \emptyset$ for $i \neq j$.
Each mode's dynamics are then given by,
#+BEGIN_EXPORT latex
\begin{align}
\mode{\latentFunc}: \mode{\stateDomain} \times \controlDomain \rightarrow \stateDomain.
\end{align}
#+END_EXPORT
Notice that each mode's transition dynamics are free to leave their state space $\mode{\stateDomain}$
and enter another mode.
Ideally, this work seeks to enforce the controlled system to remain in a given mode at every time step.
A mode remaining controlled system is defined as follows.
#+BEGIN_EXPORT latex
\begin{definition}[Mode Remaining] \label{def-mode-remaining}
Let $\modeInd$ denote a dynamics mode, defined by its state domain $\mode{\stateDomain} \subseteq \stateDomain$.
Given an inital state $\state_0 \in \mode{\stateDomain}$, and a sequence of controls $\controlTraj=\control_{0:\TimeInd-1}$,
the controlled system is said to be mode-remaining iff:
\begin{align} \label{eq-mode-remaining-def}
\dynamicsFunc(\state_{\timeInd}, \control_{\timeInd}) &\in \mode{\stateDomain} \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd]
\end{align}
\end{definition}
#+END_EXPORT
More formally, the problem this work seeks to solve is given by,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-mode-soc-problem}
\begin{align}
\min_{\controlTraj} \sum_{\timeInd=0}^{\TimeInd} &\costFunc(\state_\timeInd, \control_\timeInd) \\
\text{s.t.} \quad \state_{\timeInd+1} &= \mode{\dynamicsFunc}(\state_\timeInd, \control_\timeInd) + \mode{\bm\epsilon}
\quad \text{if} \quad \modeVar(\state_{\timeInd}) = \modeInd  \quad &\forall \timeInd \in \{0,\TimeInd-1\} \\
%\text{s.t.} \quad &\state_{\timeInd+1} = \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \epsilon,
%\quad \epsilon \sim \mathcal{N}(\mathbf{0}, \Sigma_{\epsilon}) \\
%&\text{\cref{eq-mode-remaining-def}} \\
\dynamicsFunc(\state_{\timeInd}, \control_{\timeInd}) &\in \desiredStateDomain \quad &\forall \timeInd \in [0,\TimeInd-1] \\
%\state_{\timeInd} &\in \desiredStateDomain \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] \\
\state_0 &= \state_0 \\
\state_\TimeInd &= \targetState,
\end{align}
\end{subequations}
#+END_EXPORT
That is, given a desired dynamics mode $\desiredMode$, this work seeks to find a control
trajectory $\controlTraj = \control_{0:\TimeInd-1}$,
to navigate from an initial state $\state_{0} \in \desiredStateDomain$, to a
target state $\targetState \in \desiredStateDomain$, over a horizon $\TimeInd$,
whilst minimising a cost function, $\costFunc: \stateDomain \times \controlDomain \rightarrow \R$
and keeping the system in the desired dynamics mode $\desiredMode$.
To simplify notation, the state and control trajectories are denoted
as $\stateTraj = \state_{1:T}$ and $\controlTraj = \control_{0:T-1}$ respectively.
The novelty of this problem arises from the mode remaining behaviour.
To the best of our knowledge, there is no prior work addressing how to find
trajectories that remain in a desired dynamics mode,
when knowledge of how the system switches between the dynamics modes is /not known a priori/.

This chapter assumes prior access to the environment, such that a data set of state transitions has previously
been collected and used to learn a single-step dynamics model using the \acrshort{mosvgpe} method from cref:chap-dynamics.
#+BEGIN_EXPORT latex
\begin{assumption} \label{}
A data set $\dataset$ of state transitions has previously been collected from the system and used to learn
a single-step dynamics model using the \acrshort{mosvgpe} method from \cref{chap-dynamics}.
%A single-step dynamics model has been learned using
%the \acrshort{mosvgpe} method from \cref{chap-dynamics} and a data set $\dataset$ of state transitions
%from the system.
\end{assumption}
#+END_EXPORT
Given that neither the underlying dynamics modes, nor how the system switches between them, are /known a priori/,
it is not possible to solve cref:eq-mode-soc-problem with the mode remaining guarantee in cref:def-mode-remaining.
However, the well-calibrated uncertainty estimates associated with the learned dynamics model's make it possible to
find mode remaining trajectories with high probability.
Therefore, this work relaxes the requirement to finding mode remaining trajectories with high probability.
#+BEGIN_EXPORT latex
\begin{definition}[$\delta$-mode remaining] \label{def-delta-mode-remaining}
Let $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ denote a multimodal dynamical system
and $\desiredMode$ a desired dynamics mode defined by its state domain $\desiredStateDomain \subseteq \stateDomain$.
Given an initial state $\state_0 \in \desiredStateDomain$ and $\delta \in (0,1]$,
a system is said to be $\delta$-mode remaining under the controls $\controlTraj$ iff:
\begin{align}
\Pr( \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] : \dynamicsFunc(\state_{\timeInd},\control_{\timeInd}) &\in \desiredStateDomain,
\control_{\timeInd} \in \controlDomain) \geq 1 - \delta
\end{align}
\end{definition}
#+END_EXPORT
$\delta-\text{mode remaining}$ trajectories are guaranteed to remain in the desired dynamics mode with probability
up to $1-\delta$. Therefore, smaller $\delta$ values correspond to higher confidence of remaining in the desired
dynamics mode.

Given this learned dynamics model, it is assumed that a desired dynamics mode $\desiredMode$ is either known, or
can easily be identified.
This is a realistic assumption as the parameters associated with each dynamics GP can be used to identify
different behaviours.
For example, the noise variance associated with each mode's dynamics GP models its process noise.
If it is desirable to avoid modes with high process noise, then they can easily be identified by their noise
variances.
#+BEGIN_EXPORT latex
\begin{assumption} \label{}
A desired dynamics mode $\desiredMode$ is known.
\end{assumption}
#+END_EXPORT
Given a learned dynamics model and a desired dynamics mode $\desiredMode$,
the goals of the trajectory optimisation in this chapter can be summarised as follows,
- Goal 1 :: Navigate to a target state $\targetState$,
- Goal 2 :: Remain in the operable, desired dynamics mode $\desiredMode$,
- Goal 3 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/,
  - in the desired dynamics mode  $\latentFunc_{\desiredMode}$, i.e. where the underlying dynamics are not known,
  - in the gating network $\modeVar$, i.e. where it is not known which mode governs the dynamics.
Goal 3 arises due to learning the dynamics model from observations.
The learned model may not be able to confidently predict which mode governs the dynamics at a given region.
This is due to lack of training observations and is known as /epistemic uncertainty/.
It is desirable to avoid entering these regions as it may result in the system leaving the desired dynamics mode.

** Problem Statement label:sec-problem-statement :noexport:
This chapter considers nonlinear, stochastic, multimodal dynamical systems,
#+BEGIN_EXPORT latex
\begin{subequations}
\begin{align}
\state_{\timeInd+1} &= \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \bm\epsilon \\
&= \mode{\dynamicsFunc}(\state_\timeInd, \control_\timeInd) + \mode{\bm\epsilon}
\quad \text{if} \quad \modeVar_{\timeInd}=\modeInd \nonumber \\
\mode{\bm\epsilon} &\sim \mathcal{N}(\mathbf{0}, \Sigma_{\mode{\bm\epsilon}}),
\end{align}
\end{subequations}
#+END_EXPORT
where the dynamics modes are defined by disjoint state domains.
That is, each dynamics mode is defined by its state domain
$\mode{\stateDomain} = \{ \state \in \stateDomain \mid \modeVar = \modeInd \}$, with
$\stateDomain_{i} \cap \stateDomain_{j} = \emptyset$ for $i \neq j$.
Each mode's dynamics are then given by,
#+BEGIN_EXPORT latex
\begin{align}
\mode{\latentFunc}: \mode{\stateDomain} \times \controlDomain \rightarrow \stateDomain.
\end{align}
#+END_EXPORT
Notice that each mode's transition dynamics are free to leave their state space $\mode{\stateDomain}$
and enter another mode.
Ideally, this work seeks to enforce the controlled system to remain in a given mode at every time step.
#+BEGIN_EXPORT latex
\begin{definition}[Mode Remaining] \label{def-mode-remaining}
Let $\modeInd$ denote a dynamics mode, defined by its state domain $\mode{\stateDomain} \subseteq \stateDomain$.
Given an inital state $\state_0 \in \mode{\stateDomain}$, and a sequence of controls $\controlTraj=\control_{0:\TimeInd-1}$,
the controlled system is said to be mode-remaining iff:
\begin{align} \label{eq-mode-remaining-def}
\dynamicsFunc(\state_{\timeInd}, \control_{\timeInd}) &\in \mode{\stateDomain} \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd]
\end{align}
\end{definition}
#+END_EXPORT
Given a desired dynamics mode $\desiredMode$, this work seeks to find a control
trajectory $\controlTraj = \control_{0:\TimeInd-1}$,
to navigate from an initial state $\state_{0} \in \desiredStateDomain$, to a
target state $\targetState \in \desiredStateDomain$,
over a horizon $\TimeInd$,
whilst minimising a cost function, $\costFunc: \stateDomain \times \controlDomain \rightarrow \R$
and keeping the system in the desired dynamics mode $\desiredMode$.
Formally, this problem is given by,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-mode-soc-problem}
\begin{align}
\min_{\controlTraj} \sum_{\timeInd=0}^{\TimeInd} &\costFunc(\state_\timeInd, \control_\timeInd) \\
\text{s.t.} \quad \state_{\timeInd+1} &= \mode{\dynamicsFunc}(\state_\timeInd, \control_\timeInd) + \mode{\bm\epsilon}
\quad \text{if} \quad \modeVar_{\timeInd}=\modeInd  \quad &\forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] \\
%\text{s.t.} \quad &\state_{\timeInd+1} = \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \epsilon,
%\quad \epsilon \sim \mathcal{N}(\mathbf{0}, \Sigma_{\epsilon}) \\
%&\text{\cref{eq-mode-remaining-def}} \\
\dynamicsFunc(\state_{\timeInd}, \control_{\timeInd}) &\in \desiredStateDomain \quad &\forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] \\
%\state_{\timeInd} &\in \desiredStateDomain \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] \\
\state_0 &= \state_0, \quad \state_\TimeInd = \targetState,
\end{align}
\end{subequations}
#+END_EXPORT
# where the expectation is taken w.r.t. the distribution over state
# trajectories,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-trajectory-dist}
# %\controlledPolicyDist(\stateTraj \mid \state_0, \controlTraj) =
# p(\stateTraj \mid \state_0, \controlTraj) =
# \prod_{\timeInd=0}^{\TimeInd-1}
# p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd).
# \end{align}
# #+END_EXPORT
To simplify notation, the state and control trajectories are denoted
as $\stateTraj = \state_{1:T}$ and $\controlTraj = \control_{0:T-1}$ respectively.
The novelty of this problem arises from the mode remaining behaviour.
To the best of our knowledge, there is no prior work addressing how to find
trajectories that remain in a desired dynamics mode,
when knowledge of how the system switches between the dynamics modes is /not known a priori/.

# where the expectation is taken w.r.t. the distribution over state-control
# trajectories under controller $\policy$,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-trajectory-dist}
# \controlledPolicyDist(\stateTraj \mid \state_0, \controlTraj) =
# \prod_{\timeInd=0}^{\TimeInd-1}
# p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)
# \policy(\control_\timeInd).
# \end{align}
# #+END_EXPORT


This chapter assumes prior access to the environment, enabling a data set of state transitions to be collected.
This data set is used to learn a factorised representation of the underlying dynamics modes
with the \acrshort{mosvgpe} method from cref:chap-dynamics.
# This method correctly identifies the underlying dynamics modes and provides informative latent spaces that
# can be used to encode mode remaining behaviour into control strategies.
# In particular, the GP-based gating network infers informative latent structure.
# cref:fig-traj-opt-gating-network-gp shows the GP posterior associated with the desired mode's gating function
# after training
# \acrshort{mosvgpe} on the historical data set of state transitions from the quadcopter experiment in cref:illustrative_example.
#+BEGIN_EXPORT latex
\begin{assumption} \label{}
A single-step dynamics model has been learned using
the \acrshort{mosvgpe} method from \cref{chap-dynamics} and a data set $\dataset$ of state transitions
from the system.
\end{assumption}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# A data set $\dataset$ of state transitions have previously been sampled from the
# system (at constant frequency) and used to train a single-step dynamics model using
# the \acrshort{mosvgpe} method from \cref{chap-dynamics}.
# \end{assumption}
# #+END_EXPORT
# As the true underlying dynamics modes and how the system switches between them are /not fully known a priori/,
# it is impossible to develop algorithms that can find trajectories that are guaranteed to be

# As the control algorithms presented in this chapter leverage this learned dynamics model,
# it is impossible to find trajectories that are guaranteed to be Mode Remaining (cref:def-mode-remaining).
Leveraging a learned dynamics model in this way makes
it impossible to find trajectories that are guaranteed to be Mode Remaining (cref:def-mode-remaining).
This work relaxes the requirement to finding mode remaining trajectories with high probability.
#+BEGIN_EXPORT latex
\begin{definition}[$\delta$-mode remaining] \label{def-delta-mode-remaining}
Let $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ denote a multimodal dynamical system
and $\desiredMode$ a desired dynamics mode defined by its state domain $\desiredStateDomain \subseteq \stateDomain$.
Given an initial state $\state_0 \in \desiredStateDomain$ and $\delta \in (0,1]$,
a system is said to be $\delta$-mode remaining under the controls $\controlTraj$ iff:
\begin{align}
\Pr( \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] : \dynamicsFunc(\state_{\timeInd},\control_{\timeInd}) &\in \desiredStateDomain,
\control_{\timeInd} \in \controlDomain) \geq 1 - \delta
\end{align}
\end{definition}
#+END_EXPORT

Given this learned dynamics model, it is assumed that a desired dynamics mode $\desiredMode$ can easily be identified.
This is a realistic assumption as the parameters associated with each dynamics GP can be used to identify
different behaviours.
For example, the noise variance associated with each mode's dynamics GP models its process noise.
If it is desirable to avoid modes with high process noise, then they can easily be identified by their noise
variances.
#+BEGIN_EXPORT latex
\begin{assumption} \label{}
A desired dynamics mode $\desiredMode$ is known.
\end{assumption}
#+END_EXPORT
Given a learned dynamics model and a desired dynamics mode $\desiredMode$,
the goals of the trajectory optimisation in this chapter can be summarised as follows,
# - Goal 1 :: Remain in a desired dynamics mode $\desiredMode$,
# - Goal 2 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/.
# - Goal 1 :: Remain in the operable, desired dynamics mode $\desiredMode$,
# - Goal 2 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/,
#   - in the desired dynamic mode,
#   - in the gating network.
- Goal 1 :: Navigate to a target state $\targetState$,
- Goal 2 :: Remain in the operable, desired dynamics mode $\desiredMode$,
- Goal 3 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/, in the desired dynamics mode and in the gating network.
Goal 3 arises due to learning the dynamics model from observations.
The learned model may not be able to confidently predict which mode governs the dynamics at a given region.
This is due to lack of training observations and is known as /epistemic uncertainty/.
It is desirable to avoid entering these regions as it may result in the system leaving the desired dynamics mode.

# #+BEGIN_EXPORT latex
# \begin{remark}[Epistemic Uncertainty]
# %Given that the underlying dynamics modes and how the dynamics switches between them, are
# %\textit{not fully known a priori}, these methods should \textit{air on the side of caution}.
# Due to lack of training observations, the learned model may be uncertain which mode governs the
# dynamics at a particular region. This is known as epistemic uncertainty.
# In a risk-averse setting, trajectories should also avoid entering these regions of high epistemic uncertainty.
# \end{remark}
# #+END_EXPORT

# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# The system has previously been observed to obtain a data set $\dataset$ of state transitions sampled at constant frequency.
# \end{assumption}
# #+END_EXPORT
# Given this learned dynamics model, it is assumed that a desired dynamics mode $\desiredMode$ can easily be identified.
# This is a realistic assumption as the parameters associated with each dynamics GP represent different
# characteristics of the system.
# For example, the noise variance associated with each mode's dynamics GP models the process noise that the mode
# is subject to.
# A high noise variance can therefore be used to identify dynamics modes that are undesirable.


# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# A desired dynamics mode $\desiredMode$  is known.
# Moreover, its transition dynamics are given by
# $\desiredDynamicsFunc: \desiredStateDomain \times \controlDomain \rightarrow \stateDomain$.
# \end{assumption}
# #+END_EXPORT
# the \acrshort{mosvgpe} method from cref:chap-dynamics to learn a factorised
# representation of the underlying dynamics modes.

# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# Further to this, it is assumed that the mode switching behaviour is governed by the state space, i.e.
# each dynamics modes is governed by its state domain
# $\stateDomain_{\modeInd} = \{ \state \in \stateDomain \mid \modeVar = \modeInd \}$,
# giving each mode's dynamics,
# $\mode{\latentFunc}: \mode{\stateDomain} \times \controlDomain \rightarrow \stateDomain$.

# The system switches between its dynamics modes over the systems state space, i.e.
# each dynamics modes is governed by its state domain
# $\stateDomain_{\modeInd} = \{ \state \in \stateDomain \mid \modeVar = \modeInd \}$,
# giving each mode's dynamics,
# $\mode{\latentFunc}: \mode{\stateDomain} \times \controlDomain \rightarrow \stateDomain$.
# \end{assumption}
# #+END_EXPORT

# and used to train a single-step dynamics model using
# the \acrshort{mosvgpe} method from \cref{chap-dynamics}.

# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# A data set $\dataset$ of state transitions have previously been sampled from the
# system (at constant frequency) and used to train a single-step dynamics model using
# the \acrshort{mosvgpe} method from \cref{chap-dynamics}.
# \end{assumption}
# #+END_EXPORT
# Further to this, it assumes that a desired dynamics mode $\desiredMode$ is /known a priori/.
# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# Let $\desiredMode$ denote the desired dynamics mode governed
# by its state domain
# $\desiredStateDomain = \{ \state \in \stateDomain \mid \modeVar = \desiredMode \}$.
# The desired mode's transition dynamics are given by
# $\desiredDynamicsFunc: \desiredStateDomain \times \controlDomain \rightarrow \stateDomain$,
# with continuous states $\state \in \stateDomain \subseteq \R^\StateDim$ and controls
# $\control \in \controlDomain \subseteq \R^\ControlDim$.
# \end{assumption}
# #+END_EXPORT

# The desired mode's transition dynamics are given by
# $\desiredDynamicsFunc: \desiredStateDomain \times \controlDomain \rightarrow \stateDomain$,
# with continuous states $\state \in \stateDomain \subseteq \R^\StateDim$ and controls
# $\control \in \controlDomain \subseteq \R^\ControlDim$,
# where $\desiredStateDomain = \{ \state \in \stateDomain \mid \modeVar = \desiredMode \}$
# denotes the subset of the state space associated with the desired dynamics mode.
# Note that the desired mode's transition dynamics $\desiredDynamicsFunc$
# are free to leave the desired mode's state space $\desiredStateDomain$.






\todo{cite active learning paper and multimodal mogpe paper?}
# The most similar works are cite:schreiterSafe2015 where active learning is gu



# Of particular interest in this chapter is the GP posterior over the desired mode's gating function.
# cref:fig-traj-opt-gating-network-gp visualises this GP posterior after training on the historical data set of state
# transitions from the illustrative example in cref:illustrative_example.
# One approach to encoding mode remaining behaviour is to consider the geometry of the latent gating functions.

** Mode Remaining Control via Latent Geometry label:chap-traj-opt-geometry
This section introduces two different approaches to performing mode remaining trajectory optimisation.
They both exploit concepts from Riemannian geometry -- extended to probabilistic manifolds -- to
encode mode remaining behaviour.
The first approach in cref:sec-traj-opt-collocation resembles an indirect optimal control method citep:kirkOptimal2004
as it projects the trajectory optimisation problem onto
an \acrfull{ode} that implicitly encodes the mode remaining behaviour.
The second approach in cref:sec-traj-opt-collocation is a direct optimal control method that
resembles standard Gaussian process control methods
with the mode remaining behaviour encoded via a geometric objective function.

# cref:sec-geometry-recap introduces the relevant background on Riemannian geometry and its extension to
# probabilistic manifolds.
# cref:sec-traj-opt-collocation,sec-traj-opt-energy
# detail the two trajectory optimisation algorithms.
# sec-traj-opt-energy
*** Concepts from Riemannian Geometry label:sec-geometry-recap
**** intro :ignore:

#+BEGIN_EXPORT latex
\begin{figure}[h!]
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=0.98\textwidth]{./images/mode-opt/env/scenario_7/mosvgpe/mixing_probs_no_obs.pdf}
\subcaption{Mixing probabilities}
\label{eq-traj-opt-gating-network-prob-post}
\end{minipage}
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=0.98\textwidth]{./images/mode-opt/env/scenario_7/mosvgpe/desired_gating_gp_no_obs.pdf}
\subcaption{Gating function's GP posterior}
\label{eq-traj-opt-gating-network-gp-post}
\end{minipage}
\caption{
Visualisation of the gating network posterior after training \acrshort{mosvgpe}
on the state transition data set from the simulated version of the 2D quadcopter environment
in the illustrative example from \cref{illustrative_example}.
(\subref{eq-traj-opt-gating-network-prob-post}) shows the probability mass function over the expert indicator
variable and (\subref{eq-traj-opt-gating-network-gp-post}) shows the gating function's GP
posterior mean (left) and posterior variance (right).
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line)
and the subset of the environment which has not been observed (hashed box).}
\label{fig-traj-opt-gating-network-gp}
\end{figure}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{figure}[t!]
# \centering
# %\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/mosvgpe/desired_gating_gp_no_traj.pdf}
# \includegraphics[width=\textwidth]{./images/mode-opt/env/scenario_7/mosvgpe/desired_gating_gp_no_obs.pdf}
# \caption{
# Visualisation of the GP posterior mean (left) and posterior variance (right), associated with the
# desired mode's gating function after training \acrshort{mosvgpe}
# on the historical data set of state transitions from the 2D quadcopter environment
# in the illustrative example from \cref{illustrative_example}.
# The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line)
# and a subset of the environment which has not been observed (hashed box).}
# \label{fig-traj-opt-gating-network-gp}
# \end{figure}
# #+END_EXPORT
# cref:fig-traj-opt-gating-network-gp shows the GP posterior associated with the desired mode's gating function
# after training
# \acrshort{mosvgpe} on the historical data set of state transitions from the quadcopter experiment in cref:illustrative_example.
The \acrshort{mosvgpe} model correctly identifies the underlying dynamics modes and infers informative
latent spaces that can be used to encode mode remaining behaviour.
cref:fig-traj-opt-gating-network-gp shows the GP posterior associated with the desired mode's gating function
after training
\acrshort{mosvgpe} on the historical data set of state transitions from the quadcopter experiment in cref:illustrative_example.
The work in this chapter is based on the observation that
Goals 1 and 2 can be encoded as finding length minimising trajectories on the desired mode's gating function.
Intuitively, the length of a trajectory from $\mathbf{x}_0$ to $\mathbf{x}_f$ on the
manifold given by the desired mode's gating function,
increases when it passes over the contours; analogous to climbing a hill.
Given appropriate scaling of the gating function, shortest trajectories between two locations are
those that attempt to follow the contours, and as a result,
remain in a single mode by not climbing up or down any hills.
This section will review the relevant concepts from Riemannian geometry and show how they can be used
to encode Goals 1 and 2.
cref:sec-prob-geo then extends the concepts to probabilistic geometries in order to encode Goal 3.
The relevant concepts from Riemannian geometry, that are needed to formalise this intuition, are now introduced.

# Now that the relevant concepts from Riemannian geometry have been introduced, the objective function
# for the first trajectory optimisation can be detailed.
# The geometry of the latent gating functions can be used to formulate a cost term
# that favours trajectories remaining in a preferred dynamics mode.
# One approach is to obtaining mode remaining behaviour is to consider the geometry of the latent gating functions.
# Let us quickly recap how to calculate lengths on Riemannian manifolds.

**** Lengths in Euclidean Spaces :ignore:
\newline
*Lengths in Euclidean spaces*
The $l^2$ norm (aka Euclidean norm) provides an intuitive notion for the length of a
vector $\manifoldInput \in \manifoldDomain \subseteq \R^{\manifoldDomainDim}$
in a Euclidean space.
Overloading notation, a continuous time trajectory is also denoted $\trajectory$ and corresponds to
${\trajectory: [t_0, t_f] \rightarrow \manifoldDomain$.
Under the $l^2$ norm, the length of a trajectory $\trajectory$ is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-euclidean-length}
\text{Length}\left(\trajectory\right)
&= \int_{t_0}^{t_f}\left\|\dot{\manifoldInput}(t)\right\|_2 \mathrm{d}t,
\end{align}
#+END_EXPORT
where Newton's notation has been used to denote differentiation with respect to time $t$.
As a norm can be expressed for any space endowed with an inner product, it is possible to
calculate lengths of trajectories on manifolds endowed with an inner product.
# Just as the dot product is the inner product of the Euclidean space, it is possible to
# define inner products for Riemannian spaces (also known as Riemannian manifolds).
# Remembering that a norm can be expressed for any space endowed with an inner product,
# provides us with the tools to for calculating lengths on manifolds.

# Under the $l^2$ norm, the length of a trajectory
# ${\trajectory = \{\manifoldInput(t) \in \manifoldDomain \mid \forall t \in [t_0, t_f]\}}$ is given by,

**** Riemannian manifolds :ignore:
\newline
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Riemannian manifolds}
In this dissertation if suffices to consider manifolds $\manifold$ defined by a mapping,
%#+BEGIN_EXPORT latex
\begin{equation} \label{eq-manifold-function}
\manifoldFunction : \manifoldDomain \rightarrow \manifoldCodomain,
\end{equation}
%#+END_EXPORT
where $\manifoldDomain$ and $\manifoldCodomain$ are open subsets of Euclidean spaces.
The manifold $\manifold$ is given by $\manifold = \manifoldFunction(\manifoldDomain)$ and is said to
be immersed in the ambient space $\manifoldCodomain$.
The dimensionality of the surface is denoted $\manifoldDomainDim = \text{dim}(\manifoldDomain)$
whilst $\manifoldCodomainDim = \text{dim}(\manifoldCodomain)$ denotes the dimensionality of the ambient space.
Riemannian manifolds can intuitively be seen as $\manifoldCodomainDim\text{-dimensional}$
curved surfaces with a smoothly
varying positive-definite inner product, governed by the Riemannian metric $\metricTensor$ \citep{carmoRiemannian1992}.
\begin{definition}[Riemannian Metric]
A Riemannian metric $\metricTensor$,
on a manifold $\manifold$, is a smooth function
$\metricTensor : \manifoldDomain \rightarrow \R^{\manifoldDomainDim \times \manifoldDomainDim}$
that assigns a symmetric positive definite matrix to any point in $\manifoldDomain$.
%A Riemannian metric $\metricTensor$ on a
%manifold $\manifold$ is a symmetric and positive definite matrix which defines
%a smoothly varying inner product
%$\langle \dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \rangle_{\mathbf{x}} = \dot{\mathbf{x}}_a^T \mathbf{G}(\mathbf{x}) \dot{\mathbf{x}}_b$
%in the tangent space $T_{\mathbf{x}}\manifold$, for each point $\mathbf{x} \in \manifold$ and
%$\dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \in T_{\mathbf{x}}\manifold$.
\end{definition}
Intuitively, the metric forms a local inner product in $\manifoldDomain$ that informs how to measure lengths
on the manifold $\manifold$, locally in $\manifoldDomain$.
This is indicated in \cref{eq-manifold-norm}.
Riemannian manifolds locally resemble Euclidean spaces and have
globally defined differentiable structure.
\end{myquote}
#+END_EXPORT

**** Lengths on Riemannian Manifolds :ignore:
\newline

*Lengths on Riemannian manifolds*
The length of a trajectory $\trajectory$ on a manifold $\manifold$, can be calculated
by mapping it through the function $\manifoldFunction$ and
using cref:eq-euclidean-length,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-length}
\text{Length}\left(\manifoldFunction(\trajectory)\right)
&= \int_{t_0}^{t_f}\left\|\dot{\manifoldFunction}(\manifoldInput(t))\right\|_2 \mathrm{d}t.
\end{align}
#+END_EXPORT
Applying the chain-rule allows cref:eq-manifold-length to be expressed in terms of the Jacobian and the
velocity,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-length-chain}
\text{Length}\left(\manifoldFunction(\trajectory)\right)
&= \int_{t_0}^{t_f}\left\|\jacobian \dot{\manifoldInput}(t)\right\|_2 \mathrm{d}t, \\
\jacobian &= \frac{\partial \manifoldFunction}{\partial \manifoldInput(t)} \in \R^{1 \times \manifoldDomainDim}.
\end{align}
#+END_EXPORT
This implies that the length of a trajectory on the manifold $\manifold$,
can be calculated in the input space $\manifoldDomain$,
using a locally defined norm,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-norm}
\left\|\jacobian \dot{\manifoldInput}(t)\right\|_2
&= \sqrt{ \left( \jacobian \dot{\manifoldInput}(t) \right)^T
\jacobian \dot{\manifoldInput}(t)} \nonumber \\
&= \sqrt{\dot{\mathbf{x}}^T(t) \metricTensor_{\mathbf{x}_t} \dot{\manifoldInput}(t)}
\defeq \left\|\dot{\manifoldInput}(t)\right\|_{\metricTensor_{\manifoldInput_t}},
%= \left\|\dot{\mathbf{x}}(t)\right\|_{\mathbf{G}(\mathbf{x}(t))}.
\end{align}
#+END_EXPORT
where $\metricTensor_{\mathbf{x}_t} = \jacobian^T \jacobian}$
is a symmetric positive definite matrix
(akin to a local Mahalanobis distance measure), known as the natural Riemannian metric.
The length of a trajectory on a manifold $\manifold$, endowed with the metric $\metricTensor$,
can then be calculated with,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-length-G}
\text{Length}(\manifoldFunction(\trajectory))
&= \int_{t_0}^{t_f}\left\|\dot{\manifoldInput}(t)\right\|_{\metricTensor_{\mathbf{x}_t}} \mathrm{d}t.
\end{align}
#+END_EXPORT

cref:fig-traj-opt-gating-network-gp shows the GP posterior over the desired mode's gating function,
$\desiredGatingFunction : \stateDomain \times \controlDomain \rightarrow \gatingCodomain$.
Consider finding
length minimising trajectories on the manifold $\desiredManifold=\desiredGatingFunction(\gatingDomain)$
associated with the desired mode's gating function, where the metric is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-desired-metric-tensor}
\desiredMetricTensor &= \desiredJacobian^T \desiredJacobian, \\
\desiredJacobian &= \frac{\partial \desiredGatingFunction}{\partial \manifoldInput(t)} \in \R^{1 \times \GatingDim}.
\end{align}
#+END_EXPORT
These trajectories will attempt to remain in the desired dynamics mode $\desiredMode$, encoding Goal 1 and 2.
However, length minimising trajectories subject to this metric do not encode Goal 3.
That is, they will not avoid regions of the learned dynamics, which cannot be predicted confidently due to
high /epistemic uncertainty/.
Goal 3 can be encoded by observing that the metric tensor is actually a random variable and extending the concepts
of length minimising trajectories to probabilistic manifolds.
# For notational conciseness, only a single gating function
# $\manifoldFuncion$ is used to denote the desired mode's gating function in this chapter.

# Note, that the gating functions are assumed to only depend on the state,
# $\desiredGatingFunction : \gatingDomain \rightarrow \gatingCodomain$,
# where $\gatingDomain \in \R^{\GatingDim}$ and $\gatingCodomain \in \R$,
# so the metric is given by,

# That is, trajectories minimising cref:eq-manifold-length-G,
# subject to the metric $\desiredMetricTensor$,

# In this work the gating functions are assumed to only depend on the state,
# $\desiredGatingFunction : \gatingDomain \rightarrow \gatingCodomain$,
# where $\gatingDomain \in \R^{\GatingDim}$ and $\gatingCodomain \in \R$.
# That is, trajectories minimising cref:eq-manifold-length-G,
# subject to the metric $\desiredMetricTensor$,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-desired-metric-tensor}
# \desiredMetricTensor &= \desiredJacobian^T \desiredJacobian, \\
# \desiredJacobian &= \frac{\partial \desiredGatingFunction}{\partial \manifoldInput(t)} \in \R^{1 \times \GatingDim}.
# \end{align}
# #+END_EXPORT

# These trajectories will attempt to remain in the desired dynamics mode $\desiredMode$, i.e. encode Goals 1 and 2.
# However, this metric does encode Goal 3.
# We now address Goal 3 by observing that the metric tensor is actually stochastic.
# For notational conciseness, only a single gating function
# $\manifoldFuncion$ is used to denote the desired mode's gating function in this chapter.

**** Probabilistic Geometries label:sec-prob-geo
***** Extension to Probabilistic Geometries :ignore:
\newline

Following cite:tosiMetrics2014 this work formulates a metric tensor that captures the variance in the manifold
via a probability distribution.
First note that as the differential operator is linear, the derivative of a GP is also a GP,
assuming that the mean and covariance functions are differentiable.
#+BEGIN_EXPORT latex
\begin{assumption}[Differentiable Gaussian Process] \label{}
Let $\mu : \stateDomain \rightarrow \R$ and $k: \stateDomain \times \stateDomain \rightarrow \R$
denote the mean and covariance functions associated with a Gaussian process.
The Gaussian process is differentiable iff
$\exists \frac{\partial \mu(\state)}{\partial \state}, \frac{\partial^2 k(\state, \state')}{\partial \state \partial \state'} \quad \forall \state, \state' \in \stateDomain$.
\end{assumption}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo{should there be minus mean function in jac?}
\begin{myquote}
\textbf{Gaussian Process Jacobian}
As the differential operator is linear, a function $\manifoldFunction: \gatingDomain \rightarrow  \R$ distributed as a GP,
\begin{align} \label{eq-jacobian-gp-random-var}
\manifoldFunction(\cdot) \sim \mathcal{GP}\left(\mu(\cdot), k(\cdot, \cdot)\right),
\end{align}
%\begin{align} \label{eq-jacobian-gp-random-var}
%\manifoldFunction(\allInput) \sim \mathcal{GP}\left(\mu(\allInput), k(\allInput, \allInput)\right),
%\end{align}
where $\mu$ and $k$ represent the mean and covariance functions,
is jointly Gaussian with its Jacobian
at a new input location $\testInput \in \R^{1 \times \GatingDim}$,
\begin{align} \label{eq-jacobian-random-var} \testJac = \Jac(\testInput) = \frac{\partial \manifoldFunction}{ \partial \testInput} \in \R^{\GatingDim},
\end{align}
assuming that the mean and covariance functions are differentiable.
As such, the conditional distribution over the Jacobian
$\testJac$ can be obtained using the properties of multivariate Normals
and is given by,
%\begin{align} \label{eq-predictive-jacobian-dist}
%\Jac(\cdot)
%&\sim \mathcal{GP}\left(
%\underbrace{\partial \mu(\cdot) + \jacManifoldKerneldN \manifoldKernelNN^{-1} \manifoldFunction(\allInput)}_{\muJac},
%\underbrace{\hessManifoldKerneldd - \jacManifoldKerneldN \manifoldKernelNN^{-1} \jacManifoldKernelNd}_{\covJac}
%\right),
%\end{align}
\begin{align} \label{eq-predictive-jacobian-dist}
\testJac
&\sim \mathcal{N}\left(
\underbrace{\frac{\partial \mu}{\partial \testInput} + \jacManifoldKernelsN \manifoldKernelNN^{-1} \manifoldFunction(\allInput)}_{\muJac},
\underbrace{\hessManifoldKernel - \jacManifoldKernelsN \manifoldKernelNN^{-1} \jacManifoldKernelNs}_{\covJac}
\right),
\end{align}
%\begin{align} \label{eq-predictive-jacobian-dist}
%p\left(\testJac | \manifoldFunction(\allInput), \testInput, \allInput \right)
%&= \mathcal{N}\left(\testJac \mid \muJac, \covJac \right) \\
%\muJac &= \frac{\partial \mu}{\partial \testInput} + \jacManifoldKernelsM \manifoldKernelNN^{-1}
%\left(\manifoldFunction(\allInput) - \manifoldMeanFunc(\allInput) \right) \\
%\muJac &= \frac{\partial \mu}{\partial \testInput} + \jacManifoldKernelsM \manifoldKernelNN^{-1}
%\manifoldFunction(\allInput) \\
%\covJac &= \hessManifoldKernel - \jacManifoldKernelsN \manifoldKernelNN^{-1} \jacManifoldKernelNs
%\end{align}
where the covariance matrices are given by,
\begin{align} \label{eq}
\manifoldKernelNN &= k\left( \allInput, \allInput \right) \in \R^{\NumData \times \NumData} \\
\jacManifoldKernelsN&= \frac{\partial k\left(\testInput, \allInput\right)}{\partial \testInput} \in \R^{\GatingDim \times \NumData} \\
\hessManifoldKernel &= \frac{\partial^2 k\left(\testInput, \testInput \right)}{\partial \testInput \partial \testInput} \in \R^{\GatingDim \times \GatingDim}.
\end{align}
Eq. \ref{eq-predictive-jacobian-dist} is a $\GatingDim\text{-dimensional}$ multivariate Normal distribution.
\end{myquote}
#+END_EXPORT

Therefore, the metric tensor $\desiredMetricTensor$ in cref:eq-desired-metric-tensor is the outer product of
two Normally distributed random variables.
As such, the metric tensor $\desiredMetricTensor$ is also a random variable, following
a non-central Wishart distribution citep:andersonNonCentral1946,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-metric-dist}
  \desiredMetricTensor \sim
  \mathcal{W}_{\GatingDim}\left(P, \covJac, \mathbb{E}\left[\Jac^{T}\right] \mathbb{E}[\Jac]\right),
\end{align}
#+END_EXPORT
where $P$ is the number of degrees of freedom (always one in our case) and
$\E\left[\desiredJacobian\right]$ and $\covJac$ are the mean and covariance matrices
associated with the GP over the Jacobian.
The expected value of the metric tensor in cref:eq-metric-dist is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-metric}
  \E[\desiredMetricTensor] = \E[\mathbf{J}^T] \E[\mathbf{J}] + \covJac.
\end{align}
#+END_EXPORT
Importantly, this expected metric tensor includes a covariance term $\covJac$,
which implies that lengths on the manifold increase in areas of high covariance.
This is a desirable behaviour because it encourages length minimising trajectories
to avoid regions of the learned dynamics with high epistemic uncertainty,
encoding Goal 3.
To aid with user control the metric tensor in cref:eq-expected-metric is modified with
a weighting parameter $\lambda$ that enables the relevance of the covariance term to be adjusted,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-metric-weighting}
  \tilde{\mathbf{G}} = \E[\mathbf{J}^T] \E[\mathbf{J}] + \lambda \mathbf{\Sigma}_J.
\end{align}
#+END_EXPORT
Setting $\lambda$ to be small should find trajectories that prioritise staying in the desired mode,
whereas selecting a large $\lambda$ should find trajectories that prioritise avoiding regions
of the dynamics with high epistemic uncertainty.

**** Extension to Sparse Variational Gaussian Processes
The model in Chapter ref:chap-dynamics is built upon sparse GP approximations,
so the Jacobian in cref:eq-predictive-jacobian-dist must be extended for such approximations.
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Sparse Gaussian Process Jacobian}
To obtain the distribution over the Jacobian
in the sparse variational Gaussian process setting, we first condition on the inducing variables
$\manifoldInducingOutput  \in \R^{\NumInducing \times 1}$,
%\begin{equation} \label{eq-}
%\manifoldInducingOutput \sim \mathcal{GP}\left(\mu(\manifoldInducingInput),
%k(\manifoldInducingInput, \manifoldInducingInput)\right)
%\end{equation}
\begin{equation} \label{eq-}
\testJac \mid \manifoldInducingOutput \sim \mathcal{N}\left(
\frac{\partial \mu}{\partial \testInput} + \jacManifoldKernelsM \manifoldKernelMM^{-1}
\manifoldInducingOutput,
\hessManifoldKernel - \jacManifoldKernelsM \manifoldKernelMM^{-1}
\jacManifoldKernelMs \right).
\end{equation}
where the inducing variables' density is from the prior in \cref{eq-jacobian-gp-random-var},
so the covariance matrices are given by,
\begin{align} \label{eq-}
\manifoldKernelMM &= k\left( \manifoldInducingInput, \manifoldInducingInput \right) \in \R^{\NumInducing \times \NumInducing} \\
\jacManifoldKernelsM&= \frac{\partial k\left(\testInput, \manifoldInducingInput \right)}{\partial \testInput} \in \R^{\GatingDim \times \NumInducing}.
\end{align}
The distribution over the Jacobian is then obtained by marginalising the inducing variables
with respect to their variational density,
\begin{equation} \label{eq-}
\manifoldInducingOutput \sim \mathcal{N}(\manifoldInducingOutputMean, \manifoldInducingOutputCov).
\end{equation}
The distribution over the Jacobian is then obtained via a Gaussian convolution,
\begin{align}
%q(\testJac \mid \testInput)
\testJac &\sim \mathcal{N}\left(
\underbrace{\frac{\partial \mu}{\partial \testInput} + \jacManifoldKernelsM \manifoldKernelMM^{-1}
\manifoldInducingOutputMean}_{\muJac},
\underbrace{\hessManifoldKernel - \jacManifoldKernelsM \manifoldKernelMM^{-1}
\left( \manifoldKernelMM - \manifoldInducingOutputCov \right) \manifoldKernelMM^{-1} \jacManifoldKernelMs}_{\covJac}
\right).
\end{align}
%The distribution over the Jacobian is approximated as follows,
%\begin{align}
%p(\testJac \mid \testInput, \allOutput, \allInput) &=
%\int \int p(\testJac \mid \testInput, \manifoldFunction(\allInput), \manifoldInducingOutput)
%p(\manifoldFunction(\allInput), \manifoldInducingOutput \mid \allOutput, \allInput)
%\text{d} \manifoldFunction(\allInput) \text{d} \manifoldInducingOutput \nonumber \\
%&\approx \int \int p(\testJac \mid \testInput, \manifoldFunction(\allInput), \manifoldInducingOutput)
%p(\manifoldFunction(\allInput) \mid \manifoldInducingOutput) \manifoldInducingVariational
%\text{d} \manifoldFunction(\allInput) \text{d} \manifoldInducingOutput \nonumber \\
%&= \int  p(\testJac \mid \testInput, \manifoldInducingOutput)
%\manifoldInducingVariational
%\text{d} \manifoldInducingOutput
%\coloneqq q(\testJac \mid \testInput)
%\end{align}
%where the mean and covariance are given by,
%\begin{align} q(\testJac \mid \testInput) &= \mathcal{GP}\left(\muJac, \covJac \right) \\
%\muJac &= \frac{\partial \mu}{\partial \testInput} + \jacManifoldKernelsM \manifoldKernelMM^{-1}
%\left( \manifoldInducingOutputMean - \manifoldMeanFunc(\allInput) \right), \\
%\covJac &= \hessManifoldKernel - \jacManifoldKernelsM \manifoldKernelMM^{-1}
%\left( \manifoldKernelMM - \manifoldInducingOutputCov \right) \manifoldKernelMM^{-1} \jacManifoldKernelMs \right).
%\end{align}
\end{myquote}
#+END_EXPORT

*** Indirect Optimal Control via Latent Geodesics label:sec-traj-opt-collocation
**** intro :ignore:
This section presents a trajectory optimisation algorithm that exploits the fact that
length minimising trajectories on the manifold endowed with the expected metric from cref:eq-expected-metric,
encodes all of the goals.
As shortest lengths on a manifold are known as geodesics, we refer to them as geodesic trajectories.
The algorithm presented in this section
exploits a classic result of Riemannian geometry, that geodesic trajectories are solutions
to a 2^{nd} order \acrshort{ode}, known as the geodesic ODE $f_G$.
As solutions to this ODE encode the necessary conditions for finding length minimising trajectories,
the method presented in this section resembles an indirect optimal control method.

# This algorithm exploits the fact that geodesic trajectories are solutions to a 2^{nd} order \acrshort{ode} and projects
# the optimisation onto this \acrshort{ode}.

# Therefore, the geodesic ODE encodes the necessary conditions for minimising our objective function

# Therefore, the geodesic ODE encodes the necessary conditions for minimising our objective function
# and projects
# the optimisation onto this \acrshort{ode}.

# Soltuion The geodesic \acrshort{ode} can be considered an
# #+BEGIN_EXPORT latex
# \begin{remark}
# \end{remark}
# #+END_EXPORT

# The first control algorithm presented in this chapter is an indirect trajectory optimisation algorithm that
# exploits the fact that
# length minimising trajectories, on the manifold endowed with the expected metric from cref:eq-expected-metric,
# encodes both of our goals.
# Shortest lengths on a manifold are known as geodesics, so we refer to shortest
# trajectories as geodesic trajectories.

**** Geodesics :ignore:
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Geodesics}
Given the method for calculating lengths on Riemannian manifolds in \cref{eq-manifold-length},
the notion of a shortest trajectory, or geodesic trajectory, is defined as follows,
\begin{definition}[Geodesic]
Given two points $\manifoldInput_0, \manifoldInput_f \in
\manifold$, a Geodesic is a length minimising trajectory (curve)
$\trajectory_g$ connecting the points, such that,
\begin{subequations} \label{eq-geodesic}
\begin{align}
  \trajectory_{g} =\arg &\min_{\trajectory} \operatorname{Length}(\trajectory) \\
\text{s.t.} \quad  \manifoldInput(t_0)&=\manifoldInput_{0} \\
  \manifoldInput(t_f)&=\manifoldInput_{f}.
\end{align}
\end{subequations}
\end{definition}
\end{myquote}
#+END_EXPORT
*Geodesic \acrshort{ode}* An important observation from cite:carmoRiemannian1992, is that geodesics
satisfy a continuous-time $2^{\text{nd}}$ order \acrshort{ode}, given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-2ode}
 \ddot{\manifoldInput}(t)
&= \geodesicFunction(t, \manifoldInput, \dot{\manifoldInput}) \nonumber \\
&=-\frac{1}{2} \metricTensor^{-1}(\manifoldInput(t))\left[
\frac{\partial \operatorname{vec}[\metricTensor(\manifoldInput(t))]}{\partial \manifoldInput(t)}
\right]^{T}\left(\dot{\manifoldInput}(t) \otimes \dot{\manifoldInput}(t)\right),
\end{align}
#+END_EXPORT
where $\operatorname{vec}[\metricTensor(\manifoldInput(t)])$ stacks the columns of $\metricTensor(\manifoldInput(t))$
and $\otimes$ denotes the Kronecker product.
The implication of cref:eq-geodesic,eq-2ode, is that trajectories that are solutions
to the $2^{\text{nd}}$ order \acrshort{ode} in cref:eq-2ode, implicitly minimise the objective
in cref:eq-manifold-length-G.
Given this observation, computing geodesics involves finding a solution to cref:eq-2ode
with $\manifoldInput(t_0) = \manifoldInput_0$ and $\manifoldInput(t_f) = \manifoldInput_f$.
This is a boundary value problem (BVP) with a smooth solution so it can be solved using
any BVP solver, e.g. (multiple) shooting and collocation methods.
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-length-objective}
# \text{Length(\gatingFunc(\stateTraj))}
# = \int_{t_0}^{t_f}\left\|\dot{\manifoldInput}(t)\right\|_{\metricTensor(\mathbf{x}(t))} \mathrm{d}t.
# \end{align}
# #+END_EXPORT

**** Implicit Trajectory Optimisation
Solving the 2^{nd} order \acrshort{ode} in cref:eq-2ode with the expected metric from cref:eq-expected-metric-weighting,
is equivalent to solving our trajectory optimisation problem subject to the same boundary conditions.
This resembles an indirect optimal control method as it is based on an observation that the
necessary conditions for optimality are encoded via the geodesic \acrshort{ode}.
However, it is worth noting that solutions to the geodesic \acrshort{ode} are not guaranteed to satisfy the
dynamics constraints.


# The original trajectory optimisation problem can be converted to finding $\state(t)$
# for $t \in [t_0, t_f]$ subject to the boundary conditions and the geodesic \acrshort{ode},
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-diff-flat-ode}
# \ddot{\mathbf{x}}
# &= f_G(t, \mathbf{x}(t), \dot{\mathbf{x}}(t)).
# \end{align}
# #+END_EXPORT
*Collocation*
Since neither $\dot{\mathbf{x}}(t_0)$ nor $\dot{\mathbf{x}}(t_f)$ are known, cref:eq-2ode cannot
be solved with simple forward or backward integration.
Instead, the problem is transcribed using collocation.
Collocation methods are used to transcribe continuous-time trajectory optimisation problems into
nonlinear programs, i.e. constrained parameter optimisation citep:kellyIntroduction2017,fahrooDirect2000.
The expected metric in cref:eq-expected-metric is substituted into cref:eq-2ode and solved via collocation.
This work implements a Hermite-Simpson collocation method.
It parameterises the state trajectory using cubic polynomials and the dynamics equations (the geodesic ODE in this case)
are imposed as constraints at a set of collocation points.
The trajectory $[t_0, t_f]$ is discretised into $I$ intervals where the collocation points
are the mid points of the discretisation intervals.
The collocation states
$\bar{\stateCol} = \{\stateCol_{i+\frac{1}{2}}\}_{i=0}^{I-1}$
are obtained by interpolating the polynomials.
The derivative of the collocation states w.r.t. time
$\{\dot{\stateCol}_{i+\frac{1}{2}}, \ddot{\stateCol}_{i+\frac{1}{2}}\}_{i=0}^{I-1}$
are obtained algebraically via the polynomials.
The collocation constraints then enforce
the collocation state derivatives interpolated by the polynomials $\{\ddot{\stateCol}_{i+\frac{1}{2}} \}_{i=0}^{I-1}$,
to equal the geodesic \acrshort{ode} $f_G$ at the collocation points.
This is achieved through the collocation defects,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-defect}
# \state(t) = \coeff_{i,0} + \coeff_{i,1} t + \coeff_{i,2} t^2 + \coeff_{i,3} t^3
# \dot{\state}(t) = \coeff_{i,1}  + \coeff_{i,2} t + \coeff_{i,3} t^2
# \end{align}
# #+END_EXPORT
# where $\coeff_{i,0}, \coeff_{i,1}, \coeff_{i,2}, \coeff_{i,3}$ are the polynomial coefficients

# where $\ddot{\stateCol}_{i+\frac{1}{2}}, \dot{\stateCol}_{i+\frac{1}{2}}, \stateCol_{i+\frac{1}{2}}$

# no integrals need to be computed, all the functions are algebraic operations.


# uses Simpson's rule for integrating
# The polynomial is
# at the midpoints between a set of I

# enforces the state
# derivative interpolated by the polynomials to equal the geodesic \acrshort{ode} $f_G$
# at the midpoints between a set of I

# This work implements a Hermite-Simpson collocation

# at the midpoints between a set of I
# $\{\stateCol_i\}_{i=1}^I$ collocation points
# defined by their time index

# $I = \{0,\ldots, I\}$ collocation points,
# $\{\stateCol_i\}_{i=1}^I$.
# This is achieved through the collocation defects,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-defect}
\Delta\ddot{\stateCol}_{i+\frac{1}{2}} &= \ddot{\stateCol}_{i+\frac{1}{2}} - f_G(t_{i+\frac{1}{2}}, \stateCol_{i+\frac{1}{2}}, \dot{\stateCol}_{i+\frac{1}{2}}) = 0 \quad \forall i \in  \{ 0, \ldots, I-1\}
\end{align}
#+END_EXPORT
where $\ddot{\stateCol}_{i+\frac{1}{2}}, \dot{\stateCol}_{i+\frac{1}{2}}, \stateCol_{i+\frac{1}{2}}$
are obtained by interpolating between $i$ and $i+1$.
cref:eq-defect defines a set of constraints that ensure trajectories are solutions
to the geodesic \acrshort{ode} $f_G$.
The nonlinear program that this method solves is given by,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-collocation-problem}
\begin{align}
%\min_{\stateCol(t), \dot{\stateCol}(t)}& \int_{t_0}^{t_f}
%\costFunc(\stateCol(\timeInd), \dot{\stateCol}(\timeInd)) \text{d}t \\
\min_{\stateCol_0 \ldots \stateCol_I, \dot{\stateCol}_0 \ldots \dot{\stateCol}_I}& \sum_{i=0}^{I-1}
\costFunc(\stateCol_i, \dot{\stateCol}_i ) \\
\text{s.t.} \quad
\Delta\ddot{\stateCol}_{i+\frac{1}{2}} &= 0 \quad \forall i \in \{0, \ldots, I-1\} \\
\stateCol_0 &= \state_0 \\
\stateCol_I &= \targetState.
\end{align}
\end{subequations}
#+END_EXPORT
Notice that no integrals need to be computed as all of the functions are algebraic operations.
In practice, a quadratic cost function is used to regularise the state derivative $\dot{\stateCol}$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-cost-state-derivative}
\costFunc(\stateCol_i, \dot{\stateCol}_i)
&= \dot{\stateCol}_i^T \controlCostMatrix \dot{\stateCol}_i
= \left\| \dot{\stateCol}_i \right\|_{\controlCostMatrix},
\end{align}
#+END_EXPORT
where $\controlCostMatrix$ is a user defined, real symmetric positive definite weight matrix.
It is solved using Sequential Least Squares Programming (SLSQP) in SciPy citep:2020SciPy-NMeth.

# This nonlinear program returns a continuous-time state trajectory, however, it does not return the
# control trajectory.
*Latent variable controls*
This nonlinear program returns a collocation state trajectory $\bar{\stateCol}$ which parameterises a
continuous-time state trajectory (via the polynomials).
However, it does not return the control trajectory.
The control trajectory is recovered from the state trajectory by performing inference in the
probabilistic dynamics model.
In order to do this, the state trajectory is first discretised.
In practice, using the collocation states as the discretised state trajectory worked well, i.e.
$\stateTraj = \{\state_{\timeInd}\}_{\timeInd=0}^{I} = \{\stateCol_{i}\}_{i=0}^{I}$.
The state difference outputs $\stateDiffTraj = \{\stateDiff_{\timeInd}\}_{\timeInd=1}^{\TimeInd}$ are then
calculated from the state trajectory $\stateTraj$.
The control trajectory $\controlTraj$ is then inferred from the state trajectory by extending
the ELBO for the desired mode's SVGP expert with latent variable inputs.
Following cite:hensmanGaussian2013, the ELBO for a single SVGP expert is given by,
#+BEGIN_EXPORT latex
\begin{align}
\log p(\stateDiffTraj \mid \stateTraj, \controlTraj) \geq
\E_{q(\mode{\latentFunc}(\stateTraj, \controlTraj))} \left[ \nonumber
&\log p(\stateDiffTraj \mid \mode{\latentFunc}(\stateTraj, \controlTraj)) \right] \\
&- \text{KL}\left(q(\mode{\latentFunc}(\expertInducingInput)) \mid p(\mode{\latentFunc}(\expertInducingInput))) \right)
\coloneqq \mathcal{L}_{\text{SVGP}},
\end{align}
#+END_EXPORT
where the variational posterior is given by
$q(\mode{\latentFunc}(\stateTraj, \controlTraj)) &= \int p(\mode{\latentFunc}(\stateTraj, \controlTraj) \mid \mode{\latentFunc}(\expertInducingInput)) q(\mode{\latentFunc}(\expertInducingInput)) \text{d} \mode{\latentFunc}(\expertInducingInput)$.
The control inputs $\controlTraj$ are recovered by treating them as latent variables and extending the lower bound to,
#+BEGIN_EXPORT latex
\begin{align}  \label{eq-control-elbo-svgp}
\log p(\stateDiffTraj \mid \stateTraj) &= \log \int p(\stateDiffTraj \mid \stateTraj, \controlTraj) p(\controlTraj) \text{d}\controlTraj \\
&\geq \E_{q(\controlTraj)} \left[ \mathcal{L}_{\text{SVGP}} + \log p(\controlTraj) - \log q(\controlTraj) \right],
\label{eq-control-elbo}
\end{align}
#+END_EXPORT
where each time step of the latent control trajectory is assumed to be Normally distributed,
#+BEGIN_EXPORT latex
\begin{align}
p(\controlTraj) = \prod_{\timeInd=0}^{\TimeInd-1} \mathcal{N}(\control_{\timeInd} \mid \mathbf{0}, \mathbf{I}),
\end{align}
#+END_EXPORT
and its variational posterior is given by,
#+BEGIN_EXPORT latex
\begin{align}
q(\controlTraj) = \prod_{\timeInd=0}^{\TimeInd-1} \mathcal{N}(\control_{\timeInd} \mid \mathbf{m}_{\timeInd}, \mathbf{S}_{\timeInd}).
\end{align}
#+END_EXPORT
The posterior over the latent control trajectory $q(\controlTraj) \approx p(\controlTraj \mid \stateTraj, \stateDiffTraj)$
is obtained by finding the variational parameters
$\{\mathbf{m}_{\timeInd}, \mathbf{S}_{\timeInd}\}_{\timeInd=0}^{\TimeInd-1}$ that maximise the ELBO in
cref:eq-control-elbo.

# This objective regularises the control trajectory under the L2 norm.
# The meaning of minimum control effort depends upon the specific problem.
# It may refer to minimising the total expenditure of fuel, or to minimising the energy dissipated from the system.
# Regardless of the specific interpretation, it is usually desirable to regularise the controls.


# *** Results label:sec-traj-opt-results
# #+NAME: fig-geometric-traj-opt-over-svgp
# #+ATTR_LATEX: :width 1.1\textwidth :placement [!t] :center t
# #+caption: Contour plots showing the GP posterior mean (left) and variance (right) over the gating function associated with dynamics mode 1 after training on a subset of the quadcopter data set. The initial and optimised trajectories are overlayed to show the influence of the GP's mean and variance on the trajectory optimisation with different $\lambda$ settings.
# [[file:./images/geometric-traj-opt-over-svgp.pdf]]

**** Remarks :ignore:
Although this method provides an elegant solution to finding trajectories that satisfy Goals 1, 2 and 3,
it is not without its limitations.
First of all, this approach does not necessarily find trajectories that satisfy the dynamics constraints,
as it projects the problem onto the geodesic \acrshort{ode}.
#+BEGIN_EXPORT latex
\begin{remark}
Dynamics constraints not guaranteed to be satisfied.
\end{remark}
#+END_EXPORT
Secondly, it does not consider the full distribution over state-control trajectories.
Without the inclusion of the full probabilistic dynamics model, it is impossible
to consider the full distribution over state-control trajectories.
Although propagating uncertainty through a single dynamics GP is straightforward,
handling the collocation constraints is not.
This is because the geodesic \acrshort{ode} will become a Stochastic Differential Equation (SDE).
#+BEGIN_EXPORT latex
\begin{remark}
Ignores much of the stochasticity inherent in the problem.
\end{remark}
#+END_EXPORT

# The goal of this chapter is to find trajectories that are $\delta-\text{mode remaining}$ (cref:def-delta-mode-remaining).
# However, this method is not able to provide such guarantees as it ignores the state uncertainty accumulated
# over a trajectory.
# Consider calculating the probability that a single time step remains in the desired mode,
# #+BEGIN_EXPORT latex
# \small
# \begin{align} \label{eq-mode-chance-constraint-integral}
# \Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{0}, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd-1}=\desiredMode) =
# &\int \underbrace{\Pr(\modeVar_{\timeInd} = \desiredMode \mid \GatingFunc(\state_{\timeInd}))}_{\text{Bernoulli/softmax likelihood}} \nonumber \\
# &\int \underbrace{q(\GatingFunc(\state_{\timeInd}) \mid \state_{\timeInd})}_{\text{approx posterior}}
# \underbrace{p(\state_\timeInd \mid \state_{0}, \control_{0:\timeInd-1}, \bm\modeVar_{0:\timeInd-1}=\desiredMode)}_{\text{state dist}}
# \text{d} \state_{\timeInd}
# \text{d} \GatingFunc(\state_{\timeInd})
# \end{align}
# \normalsize
# #+END_EXPORT
# where $q(\GatingFunc(\state_{\timeInd}) \mid \state_{\timeInd})$ is the approximate posterior over the
# gating functions from cref:eq-predictive-gating.
# Note the dependence on the state input $\state_{\timeInd}$ is reintroduced here,
# as it becomes a random variable when making multi-step predictions.
# This probability will decrease as the uncertainty in the state increases.
# For example, when a trajectory passes through regions of the desired dynamics GP with high uncertainty.
# This is implied by the marginalisation over $\state_\timeInd$.
# It will also decrease when the trajectory passes through regions of the state space where the gating functions
# are uncertain, indicated by the marginalisation over $\GatingFunc(\state_{\timeInd})$.
# The method presented in this section does not quantify the state uncertainty over a trajectory and at
# best can calculate the
# following probability,
# $\Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{0:\timeInd}, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd-1}=\desiredMode)$.
# As such, this method cannot validate that trajectories are $\delta-\text{mode remaining}$.
# #+BEGIN_EXPORT latex
# \begin{remark}
# Cannot validate that trajectories are $\delta-\text{mode remaining}$.
# \end{remark}
# #+END_EXPORT
# # The trajectory optimisation algorithm presented in the next section attempts to address these issues.


# Firstly, this approach does do not necessarily find trajectories that satisfy the dynamics constraints,
# as it projects the problem onto the geodesic \acrshort{ode}.
# Secondly, this algorithm ignores much of the stochasticity inherent in the problem,
# as it does not consider the full distribution over state-control trajectories.
# Without the inclusion of the full probabilistic dynamics model, it is impossible
# to consider the full distribution over state-control trajectories.
# Although propagating uncertainty through a single GP dynamics mode is straight forward,
# how to handle the collocation constraints is not immediately clear.
# This is because the geodesic \acrshort{ode} will become a Stochastic Differential Equation (SDE).

# Perhaps the biggest limitation of this method, is its dependence on the
# differential flatness property, which is used to obtain the controls from the
# optimised state trajectory and geodesic \acrshort{ode}.
# #+BEGIN_EXPORT latex
# \begin{remark}
# Requires the state and control to be expressable in terms of a flat output.
# %Only applicable in systems where the state and control can be expressed in terms of a flat output.
# \end{remark}
# #+END_EXPORT
# This dependence makes this algorithm only applicable in systems where the state and control can be expressed in terms
# of a flat output.




# Thirdly, this algorithm does not consider the full stochastic optimal control problem in cref:mode-soc-problem.
# In reality, the geodesic \acrshort{ode} in cref:eq-2ode is stochastic because both the state and Jacobian are random
# variables.
# This algorithm assumes that the goals are encoded via the expected metric in cref:eq-expected-metric
# such that the full stochastic problem can be simplified to a deterministic \acrshort{ode}.
# In practice, this assumption appears to achieve both of our goals whilst offering an easier problem to solve.
**** Remarks :ignore:noexport:
Although this method provides an elegant solution to finding trajectories that satisfy Goals 1, 2 and 3,
it is not without its limitations.
First of all, this approach does not necessarily find trajectories that satisfy the dynamics constraints,
as it projects the problem onto the geodesic \acrshort{ode}.
#+BEGIN_EXPORT latex
\begin{remark}
Dynamics constraints not guaranteed to be satisfied.
\end{remark}
#+END_EXPORT
Secondly, it does not consider the full distribution over state-control trajectories.
Without the inclusion of the full probabilistic dynamics model, it is impossible
to consider the full distribution over state-control trajectories.
Although propagating uncertainty through a single dynamics GP is straightforward,
handling the collocation constraints is not.
This is because the geodesic \acrshort{ode} will become a Stochastic Differential Equation (SDE).
#+BEGIN_EXPORT latex
\begin{remark}
Ignores much of the stochasticity inherent in the problem.
\end{remark}
#+END_EXPORT

The goal of this chapter is to find trajectories that are $\delta-\text{mode remaining}$ (cref:def-delta-mode-remaining).
However, this method is not able to provide such guarantees as it ignores the state uncertainty accumulated
over a trajectory.
Consider calculating the probability that a single time step remains in the desired mode,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-mode-chance-constraint-integral}
\Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{0}, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd-1}=\desiredMode) =
&\int \underbrace{\Pr(\modeVar_{\timeInd} = \desiredMode \mid \GatingFunc(\state_{\timeInd}))}_{\text{Bernoulli/softmax likelihood}} \nonumber \\
&\int \underbrace{q(\GatingFunc(\state_{\timeInd}) \mid \state_{\timeInd})}_{\text{approx posterior}}
\underbrace{p(\state_\timeInd \mid \state_{0}, \control_{0:\timeInd-1}, \bm\modeVar_{0:\timeInd-1}=\desiredMode)}_{\text{state dist}}
\text{d} \state_{\timeInd}
\text{d} \GatingFunc(\state_{\timeInd})
\end{align}
\normalsize
#+END_EXPORT
where $q(\GatingFunc(\state_{\timeInd}) \mid \state_{\timeInd})$ is the approximate posterior over the
gating functions from cref:eq-predictive-gating.
Note the dependence on the state input $\state_{\timeInd}$ is reintroduced here,
as it becomes a random variable when making multi-step predictions.
This probability will decrease as the uncertainty in the state increases.
For example, when a trajectory passes through regions of the desired dynamics GP with high uncertainty.
This is implied by the marginalisation over $\state_\timeInd$.
It will also decrease when the trajectory passes through regions of the state space where the gating functions
are uncertain, indicated by the marginalisation over $\GatingFunc(\state_{\timeInd})$.
The method presented in this section does not quantify the state uncertainty over a trajectory and at
best can calculate the
following probability,
$\Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{0:\timeInd}, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd-1}=\desiredMode)$.
As such, this method cannot validate that trajectories are $\delta-\text{mode remaining}$.
#+BEGIN_EXPORT latex
\begin{remark}
Cannot validate that trajectories are $\delta-\text{mode remaining}$.
\end{remark}
#+END_EXPORT
# The trajectory optimisation algorithm presented in the next section attempts to address these issues.


# Firstly, this approach does do not necessarily find trajectories that satisfy the dynamics constraints,
# as it projects the problem onto the geodesic \acrshort{ode}.
# Secondly, this algorithm ignores much of the stochasticity inherent in the problem,
# as it does not consider the full distribution over state-control trajectories.
# Without the inclusion of the full probabilistic dynamics model, it is impossible
# to consider the full distribution over state-control trajectories.
# Although propagating uncertainty through a single GP dynamics mode is straight forward,
# how to handle the collocation constraints is not immediately clear.
# This is because the geodesic \acrshort{ode} will become a Stochastic Differential Equation (SDE).

# Perhaps the biggest limitation of this method, is its dependence on the
# differential flatness property, which is used to obtain the controls from the
# optimised state trajectory and geodesic \acrshort{ode}.
# #+BEGIN_EXPORT latex
# \begin{remark}
# Requires the state and control to be expressable in terms of a flat output.
# %Only applicable in systems where the state and control can be expressed in terms of a flat output.
# \end{remark}
# #+END_EXPORT
# This dependence makes this algorithm only applicable in systems where the state and control can be expressed in terms
# of a flat output.




# Thirdly, this algorithm does not consider the full stochastic optimal control problem in cref:mode-soc-problem.
# In reality, the geodesic \acrshort{ode} in cref:eq-2ode is stochastic because both the state and Jacobian are random
# variables.
# This algorithm assumes that the goals are encoded via the expected metric in cref:eq-expected-metric
# such that the full stochastic problem can be simplified to a deterministic \acrshort{ode}.
# In practice, this assumption appears to achieve both of our goals whilst offering an easier problem to solve.
*** Direct Optimal Control via Riemannian Energy label:sec-traj-opt-energy
**** intro :ignore:
This section details a direct optimal control approach which embeds the mode remaining behaviour directly into the
\acrfull{soc} problem, via a geometric objective function.
In contrast to the previous approach, this method:
1) enforces the dynamics constraints,
2) principally handles the uncertainty associated with the dynamics.
This approach is a shooting method that enforces the dynamics constraints through simulation, i.e.
the state trajectory is enforced to match the integral of the dynamics with respect to time.
# 3) can validate that trajectories are $\delta-\text{mode remaining}$.

Similar to the previous approach, this method builds on the observation that
length minimising trajectories on the Riemannian manifold $\manifold$,
associated with the desired mode's gating function $\manifoldFunction$, encodes the goals.
Further to this, this method exploits the fact that length minimising trajectories on a Riemannian manifold $\manifold$,
are also energy minimising trajectories citep:carmoRiemannian1992.
As such, mode remaining behaviour can be encoded through the minimisation of
Riemannian energy. Formally, this is given by,
# #+BEGIN_EXPORT latex
# \begin{subequations} \label{eq-mode-soc-problem-geometry}
# \begin{align}
# \min_{\controlTraj} \E\big[ c&(\stateTraj, \controlTraj)) \big] \label{eq-mode-soc-problem-geometry-cost} \\
# %\text{s.t.} \quad \state_{\timeInd+1} &= \dynamicsFunc(\state_\timeInd, \control_\timeInd) + {\epsilon},
# %\quad {\epsilon} \sim \mathcal{N}(\mathbf{0}, \Sigma_{\epsilon})
# \text{s.t.} \quad \state_{\timeInd+1} &\sim p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)
# \end{align}
# \end{subequations}
# #+END_EXPORT
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-mode-soc-problem-geometry}
\begin{align}
\min_{\controlTraj} \E_{\state_{\timeInd+1} \sim p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)}
\Big[
\underbrace{\text{Energy}(\manifoldFunction(\stateTraj))}_{\text{Riemannian energy}} +
\underbrace{(\terminalState - \targetState)^T \terminalStateCostMatrix (\terminalState - \targetState)}_{\text{terminal cost}}
+ \sum_{\timeInd=0}^{\TimeInd-1}
\underbrace{\control_{\timeInd}^T \controlCostMatrix \control_{\timeInd}}_{\text{control cost}}
\Big] \label{eq-mode-soc-problem-geometry-cost}
\end{align}
\end{subequations}
#+END_EXPORT
where $\terminalStateCostMatrix$ and $\controlCostMatrix$ are user defined,
real, symmetric, positive semi-definite and positive definite matrices respectively.
The energy of a trajectory on a Riemannian manifold, endowed with the metric $\metricTensor$, is given by,
# where the expectation is taken w.r.t. the distribution over state trajectories,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-trajectory-dist}
# %\controlledPolicyDist(\stateTraj \mid \state_0, \controlTraj) =
# p(\stateTraj \mid \state_0, \controlTraj) =
# \prod_{\timeInd=0}^{\TimeInd-1}
# p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd),
# \end{align}
# #+END_EXPORT
# and the mode remaining behaviour and the terminal state boundary condition are encoded via the
# cost function,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-quadratic-cost-with-energy}
# \costFunc(\stateTraj, \controlTraj) &=
# \underbrace{\text{Energy}(\manifoldFunction(\stateTraj))}_{\text{Riemannian energy}} +
# \underbrace{(\terminalState - \targetState)^T \terminalStateCostMatrix (\terminalState - \targetState)}_{\text{terminal cost}}
# + \sum_{\timeInd=0}^{\TimeInd-1}
# \underbrace{\control_{\timeInd}^T \controlCostMatrix \control_{\timeInd}}_{\text{control cost}},
# %\underbrace{\| \terminalState - \targetState \|_\terminalStateCostMatrix}_{\text{terminal state cost}} +
# %\underbrace{\sum_{\timeInd=1}^{\TimeInd}
# %\| \stateDiff_\timeInd \|_{\metricTensor_{\state_{\timeInd}}}}_{\text{Riemannian energy}} +
# %+ \underbrace{\sum_{\timeInd=0}^{\TimeInd-1}
# %\| \control_\timeInd \|_\controlCostMatrix}_{\text{control cost}}
# \end{align}
# #+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-trajectory-energy}
\text{Energy}(\manifoldFunction(\trajectory)) = \sum_{\timeInd=1}^{\TimeInd}
\stateDiff_{\timeInd}^T \metricTensor_{\state_{\timeInd}} \stateDiff_{\timeInd},
\end{align}
#+END_EXPORT
where $\stateDiff_{\timeInd} = \state_{\timeInd} - \state_{\timeInd-1$ is the state difference.
The mode remaining behaviour and the terminal state boundary condition are encoded via the objective function.
#+BEGIN_EXPORT latex
\begin{remark} \label{}
In contrast to the collocation solver in \cref{sec-traj-opt-collocation},
the terminal state boundary condition is encoded via the cost function, instead of being enforced by the solver.
\end{remark}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-trajectory-energy}
# \text{Energy}(\manifoldFunction(\trajectory))
# = \int \dot{\state}(\timeInd) \metricTensor(\state(\timeInd)) \dot{\state}(\timeInd) \text{d} \timeInd.
# \end{align}
# #+END_EXPORT

This may seem like an easy optimisation problem,
however, calculating the expected cost in cref:eq-mode-soc-problem-geometry-cost is not straightforward.
Given a starting state $\state_0$ and a control trajectory $\controlTraj$,
the expectation in cref:eq-mode-soc-problem-geometry-cost is taken with respect to the joint
state-metric distribution over a trajectory, $p(\stateTraj, \metricTensorTraj, \mid \state_0, \controlTraj)$.
Calculating this expectation is difficult as multi-step predictions in the MoSVSGPE dynamics model
cannot be calculated in closed-from.

This work adopts a two-stage approximation to obtain a closed-form expression for the expected cost.
First, multi-step dynamics predictions are approximated to obtain Normally distributed states at
each time step.
Given Normally distributed states, calculating the expected terminal and control cost terms in
cref:eq-mode-soc-problem-geometry-cost is straightforward.
However, the expected Riemannian energy in cref:eq-mode-soc-problem-geometry-cost has no closed-form expression,
due to the dependence of metric $\metricTensor$ on the state.
The second stage approximates the calculation of the expected Riemannian energy under Normally distributed states.

# The following issues arise when calculating this expectation:
# 1) Calculating multi-step predictions in the MoSVSGPE dynamics model cannot be calculated in closed-from,
#    - This is because the state difference after the first time step is a Gaussian mixture and
#      propagating Gaussian mixtures through Gaussian processes has no closed-form solution.
# 2) Unlike the quadratic terminal/control costs  in cref:eq-quadratic-cost-with-energy, the expected Riemannian energy in cref:eq-trajectory-energy has no closed-form expression under Normally distributed states.
# This work adopts a two-stage approximation to obtain a closed-form expression for the expected cost.
# First, multi-step state predictions under the dynamics are approximated to obtain Normally distributed at
# each time step.
# Given Normally distributed states, calculating the expected terminal/control cost terms in
# cref:eq-quadratic-cost-with-energy is trivial.
# However, the expected Riemannian energy in cref:eq-quadratic-cost-with-energy has no closed-form expression,
# due to the metric's $\metricTensor$ dependence on the state, via the Jacobian.
# The second stage approximates the calculation of the expected Riemannain energy under Normally distributed states.

# 1) Simulating the \acrshort{mosvgpe} dynamics model to obtain multi-step predictions is not trivial,

# This work obtains a closed-form expression for the expected cost  by first simplifying the .

**** Approximate Inference for Dynamics Predictions label:sec-dynamics-predictions
#+BEGIN_EXPORT latex
\renewcommand{\singleInput}{\ensuremath{\state_\timeInd, \control_\timeInd}}
\renewcommand{\singleInput}{\ensuremath{\hat{\state}_\timeInd}}
\renewcommand{\singleOutput}{\ensuremath{\Delta\state_{\timeInd+1}}}

\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput)}}
\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput)}}
%\newcommand{\expertInducingVariational}{\ensuremath{q(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\expertInducingVariational}{\ensuremath{q(\expertInducingOutput)}}
\newcommand{\expertsInducingVariational}{\ensuremath{q(\expertsInducingOutput)}}
\newcommand{\expertsVariational}{\ensuremath{q(\LatentFunc_\numData)}}
%\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \expertInducingOutput)}}
%\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))}}
\renewcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \singleInput, \expertInducingOutput)}}

\renewcommand{\expertVariational}{\ensuremath{q(\mode{\latentFunc}(\singleInput) \mid \singleInput)}}

\newcommand{\singleInputMean}{\ensuremath{\hat{\bm\mu}}}
\newcommand{\singleInputCov}{\ensuremath{\hat{\bm\Sigma}}}
#+END_EXPORT
# *Approximate Inference for Dynamics Predictions*
Multi-step predictions in the \acrshort{mosvgpe} dynamics model have no closed-form solution  because the state
difference after the first time step is a Gaussian mixture, and
propagating Gaussian mixtures through Gaussian processes has no closed-form solution.
Further to this, constructing approximate closed-form solutions is difficult,
due to the exponential growth in the number of Gaussian components.
#+BEGIN_EXPORT latex
\begin{myquote} \label{}
Consider assuming each of the $\ModeInd$ dynamics modes to be independent.
Recursively propagating the Gaussian components associated with the state, through
all of the modes, over a trajectory of length $\TimeInd$, would lead to the final state consisting of
$\ModeInd^{\TimeInd}$ Gaussian components.
\end{myquote}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{assumption}[Mode Remaining Dynamics] \label{}
# Let $\desiredMode$ denote a desired dynamics mode, defined by its state domain $\desiredStateDomain \subseteq \stateDomain$.
# Given an inital state $\state_0 \in \desiredStateDomain$, and a sequence of controls $\controlTraj$,
# the controlled system remains in the desired mode
# $\dynamicsFunc(\state_{\timeInd}, \control_{\timeInd}) &\in \desiredStateDomain \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd]$.
# \end{assumption}
# #+END_EXPORT
This work sidesteps this issue and obtains closed-form multi-step predictions
by enforcing that the controlled system remains in the desired dynamics mode.
Multi-step predictions can then be calculated in closed-form by cascading single-step predictions
using the desired dynamics GP, whose transition density is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-dynamics-gp}
\transitionDistK
&= \mathcal{N} \left( \mode{\latentFunc}(\singleInput) \mid
\mode{\mathbf{A}} \mode{\mathbf{m}},
\expertKernel(\singleInput, \singleInput)
+ (\mode{\mathbf{S}} - \expertKernel(\expertInducingInput,\expertInducingInput))
\mode{\mathbf{A}}^T
\right)
\end{align}
#+END_EXPORT
where
$\mode{\mathbf{A}} = \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}$.
Cascading single-step predictions requires recursively mapping uncertain state-control inputs through
the desired mode's dynamics GP, i.e. recursively calculating the following integral,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-state-unc-prop}
p(\state_{\timeInd+1} \mid \state_0, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd}=\desiredMode)
&= \int \transitionDistK p(\state_\timeInd \mid \state_{0}, \control_{0:\timeInd-1}, \bm\modeVar_{0:\timeInd-1}=\desiredMode) \text{d}\state_{\timeInd}
\end{align}
#+END_EXPORT
with $p(\state_0) = \delta(\state_0)$.
Approximate closed-form solutions exist for propagating Normally distributed states and controls
through GP models
citep:girardGaussian2003,kussGaussian2006,quinonero-candelaPropagation2003.
\todo{correct girard citation}
This work exploits the moment-matching approximation
implemented in the =uncertain_conditional= from GPflow citep:GPflow2017.
For more details on the approximation see Section 7.2.1 of cite:kussGaussian2006.

# However, this assumption should be enforced, i.e. the controlled system should remain in the desired mode.
# As the dynamics model is learned from observations, this work relaxes the requirement to finding
# mode remaining trajectories with high probability.
# #+BEGIN_EXPORT latex
# \begin{definition}[$\delta$-mode remaining] \label{def-delta-mode-remaining}
# Let $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ denote a multimodal dynamical system
# and $\desiredMode$ a desired dynamics mode defined by its state domain $\desiredStateDomain \subseteq \stateDomain$.
# Given an initial state $\state_0 \in \desiredStateDomain$ and $\delta \in (0,1]$,
# a system is said to be $\delta$-mode remaining under the controls $\controlTraj$ iff:
# \begin{align}
# \Pr( \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] : \dynamicsFunc(\state_{\timeInd},\control_{\timeInd}) &\in \desiredStateDomain,
# \control_{\timeInd} \in \controlDomain) \geq 1 - \delta
# \end{align}
# \end{definition}
# #+END_EXPORT
*$\delta-\text{mode remaining}$ chance constraints*
Enforcing the controlled system to remain in the desired dynamics mode simplifies
calculating multi-step predictions and the expected cost in cref:eq-mode-soc-problem-geometry-cost.
As the dynamics model is learned from observations, this work relaxes the requirement to ensuring that trajectories
are $\delta-\text{mode remaining}$ (cref:def-delta-mode-remaining).
The conditions to be $\delta-\text{mode remaining}$ can be enforced with chance constraints,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mode-chance-constraint}
\Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{0}, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd-1}=\desiredMode)
\geq 1-\delta \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd].
%\geq \satisfactionProb \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd].
\end{align}
#+END_EXPORT
These constraints enforce the system to remain in the desired dynamics mode with satisfaction probability
$\satisfactionProb=1-\delta$, at each time step.
As the \acrshort{mosvgpe} model assumes that the mode indicator variable $\modeVar$,
depends on the state via the gating function, this probability is calculated as follows,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-mode-chance-constraint-integral}
\Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{0}, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd-1}=\desiredMode) =
&\int \underbrace{\Pr(\modeVar_{\timeInd} = \desiredMode \mid \GatingFunc(\state_{\timeInd}))}_{\text{Bernoulli/softmax likelihood}} \nonumber \\
&\int \underbrace{q(\GatingFunc(\state_{\timeInd}) \mid \state_{\timeInd})}_{\text{approx posterior}}
\underbrace{p(\state_\timeInd \mid \state_{0}, \control_{0:\timeInd-1}, \bm\modeVar_{0:\timeInd-1}=\desiredMode)}_{\text{state dist}}
\text{d} \state_{\timeInd}
\text{d} \GatingFunc(\state_{\timeInd})
\end{align}
\normalsize
#+END_EXPORT
where $q(\GatingFunc(\state_{\timeInd}) \mid \state_{\timeInd})$ is the approximate posterior over the
gating functions from cref:eq-predictive-gating.
# Note the dependence on the state input $\state_{\timeInd}$ is reintroduced here,
# as it becomes a random variable when making multi-step predictions.
# This probability will decrease as the uncertainty in the state increases.
# For example, when a trajectory passes through regions of the desired dynamics GP with high uncertainty.
# This is implied by the marginalisation over $\state_\timeInd$.
# It will also decrease when the trajectory passes through regions of the state space where the gating functions
# are uncertain, indicated by the marginalisation over $\GatingFunc(\state_{\timeInd})$.

**** Approximate Riemannian Energy
Given this approach for simulating the \acrshort{mosvgpe} dynamics model, the state at each time step is Normally distributed.
Unlike the terminal and control cost terms in cref:eq-mode-soc-problem-geometry-cost, the expected Riemannian
energy,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-trajectory-riemannian-energy}
\E_{\stateTraj, \jacTraj} \left[ \text{Energy}(\stateTraj) \right]
= \sum_{\timeInd=1}^\TimeInd
\E_{\stateDist}\left[ \stateDiff_\timeInd^T
\E_{\Jac_{\state_{\timeInd}} \mid \state_{\timeInd}}\left[ \Jac_{\state_\timeInd} \Jac_{\state_\timeInd}^T \right]
 \stateDiff_\timeInd \right],
\end{align}
#+END_EXPORT
has no closed-form expression under Normally distributed states.
This is because the metric tensor $\metricTensor$ depends on the Jacobian, which depends on the state.
However, it is possible to approximate the expected energy to obtain a closed-form expression.

The distribution over the Jacobian when the input location in Normally distributed
$\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)$ can be calculated in closed-form when using the
Squared Exponential kernel.
However, this work simplifies the problem and calculates the Jacobian at the state mean of each time step
along a trajectory, i.e.
$\Jac_{\state_{\timeInd}} \approx \frac{\partial \manifoldFunction(\state_{\timeInd})}{\partial \stateMean}$.
The distribution over the Jacobian given deterministic inputs can be calculated using cref:eq-predictive-jacobian-dist.

Approximating the Jacobian to be independent of the state enables the expected metric tensor to be calculated in
closed-form with cref:eq-expected-metric-weighting.
Given this approximation, the Riemannian energy retains a quadratic form, so the expectation with respect to
$\stateDiff_{\timeInd} \sim \mathcal{N}(\stateDiffMean, \stateDiffCov)$ can be calculated with,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-approximate-trajectory-riemannian-energy}
\E \left[ \text{Energy}(\manifoldFunction(\stateTraj)) \right] &\approx
\sum_{\timeInd=1}^\TimeInd
\E_{\stateDiff_{\timeInd}}\left[ \stateDiff_\timeInd^T
\E_{\Jac_{\state_{\timeInd}}}\left[ \metricTensor_{\state_{\timeInd}} \right]
 \stateDiff_\timeInd \right] \nonumber \\
&= \sum_{\timeInd=1}^\TimeInd \stateDiffMean^T (\muJac \muJac^T + \covJac) \stateDiffMean
+ \text{tr}\left(
\left(\muJac \muJac^T + \covJac \right)
\stateDiffCov \right)
\end{align}
#+END_EXPORT
# It's a bit hacky but we could assumed the Jacobian distribution is given by marginalising the state
# and moment matching,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# p(\mathbf{J}_{\state_{\timeInd}}) =
# \int p(\mathbf{J}_{\state_{\timeInd}} \mid \state_\timeInd)
# \mathcal{N}(\state_{\timeInd} \mid \stateMean, \stateCov)
# \text{d}\state_{\timeInd}
# \approx \mathcal{N}(\mathbf{J}_{\state_{\timeInd}} \mid \muJac, \covJac)
# \end{align}
# #+END_EXPORT
#+BEGIN_EXPORT latex
\begin{myquote} \label{}
The expected metric tensor encourages trajectories to 1) follow contours on the desired mode's gating function,
hence avoiding changing modes,
and 2) avoid entering regions where it is uncertain which mode governs the dynamics.
The expectation over the state difference then encourages trajectories to remain in regions of the desired dynamics mode with
low uncertainty.
\end{myquote}
#+END_EXPORT
Given this approximation for the expected Riemannian energy, the expected cost in cref:eq-mode-soc-problem-geometry
can be calculated in closed-form with,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-quadratic-cost-with-energy}
\approxExpectedCost =
&\underbrace{
(\terminalStateMean - \targetState)^T \terminalStateCostMatrix (\terminalStateMean - \targetState)
+\text{tr}(\terminalStateCostMatrix \terminalStateCov)
}_{\text{expected terminal cost}} \\
&+ \underbrace{\sum_{\timeInd=1}^{\TimeInd} \stateDiffMean^T (\muJac \muJac^T + \covJac) \stateDiffMean
+ \text{tr}\left( \left(\muJac \muJac^T + \covJac \right)
\stateDiffCov \right)}_{\text{expected Riemannian energy}} \\
&+ \underbrace{\sum_{\timeInd=0}^{\TimeInd-1}
 \controlMean^T \controlCostMatrix \controlMean +
\text{tr}(\controlCostMatrix \controlCov) }_{\text{expected control cost}}.
\end{align}
#+END_EXPORT
This work then approximately solves the problem in cref:eq-mode-soc-problem by solving,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-mode-soc-problem-geometry-approx}
\begin{align}
\min_{\controlTraj} & \quad \approxExpectedCost \\
\text{s.t.}& \quad \text{\cref{eq-state-unc-prop,eq-mode-chance-constraint}}
%\text{s.t.}& \quad \state_{\timeInd+1} \sim
%p(\state_{\timeInd+1} \mid \state_0, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd}=\desiredMode) \\
%& \quad \Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{0}, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd-1}=\desiredMode)
%\geq 1-\delta \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd],
\end{align}
\end{subequations}
#+END_EXPORT
using Sequential Least Squares Programming (SLSQP) in SciPy citep:2020SciPy-NMeth.
This method obtains closed-form expressions for the expected cost in cref:eq-mode-soc-problem-geometry
by constraining the system to be $\delta-\text{mode remaining}$ (cref:def-delta-mode-remaining).
# Given this constraints on the dynamics, the mode remaining

**** Practical Implementation
An alternative approach to obtain mode remaining behaviour is to optimise subject to the chance constraints
in cref:eq-mode-chance-constraint alone, i.e. without the Riemannian energy cost term.
However, this constrained optimisation is often not able to converge in practice.
Experiments and intuition indicate that the geometry of the gating functions provide a much
better optimisation landscape.
This is because the gating functions vary gradually over the state domain, whilst the mixing probability changes
abruptly at the boundaries between dynamics modes.

Therefore in practice, the optimisation in cref:eq-mode-soc-problem-geometry-approx
is performed unconstrained, i.e. without enforcing the chance constraints at every iteration.
Instead, the chance constraints are used to validate trajectories found by the unconstrained
optimiser, before deploying them in the environment.
In most experiments, this strategy was far superior than constraining the optimisation at every iteration.

# Instead, the trajectory returned from the unconstrained optimisation is checked to ensure that
# it satisfies the chance constraints.

**** Cost Functions :ignore:noexport:
*Cost Functions*
This work primarily focuses on quadratic costs, as they are ubiquitous in control and lead
to closed-form expectations under Normally distributed states
${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$
and controls
${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$.
This work seeks to find trajectories between a start state $\state_0$ and a target state $\targetState$, at
time $\TimeInd$.
It is common to minimise the deviation of the final state $\state_\TimeInd$
from the desired target state $\targetState$.
It is also common to find trajectories that minimise the expenditure of control effort.
As such, this work adopts the following cost function,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-cost-control}
\costFunc(\stateTraj, \controlTraj) &=
\underbrace{(\terminalState - \targetState)^T \terminalStateCostMatrix (\terminalState - \targetState)}_{\text{terminal cost}}
+ \sum_{\timeInd=0}^{\TimeInd-1}
\underbrace{\control_{\timeInd}^T \controlCostMatrix \control_{\timeInd}}_{\text{control cost}}
\nonumber \\
&=
 \| \terminalState - \targetState \|_\terminalStateCostMatrix
+ \sum_{\timeInd=0}^{\TimeInd-1}
\| \control_\timeInd \|_\controlCostMatrix
\end{align}
#+END_EXPORT
where $\terminalStateCostMatrix$
and $\controlCostMatrix$ are user defined,
real, symmetric, positive semi-definite and positive definite matrices respectively.
Importantly, the expected value of this cost under Normally distributed states and controls can be
calculated in closed form with,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-quadratic-cost}
\E_{\stateTraj, \controlTraj} \left[ \costFunc(\stateTraj, \controlTraj) \right] =
\text{tr}(\terminalStateCostMatrix \terminalStateCov) +
(\terminalStateMean - \targetState)^T \terminalStateCostMatrix (\terminalStateMean - \targetState)
+ \sum_{\timeInd=1}^{\TimeInd-1}
\text{tr}(\controlCostMatrix \controlCov )
+ \controlMean^T \controlCostMatrix \controlMean.
\end{align}
#+END_EXPORT

** Mode Remaining Control as Probabilistic Inference label:chap-traj-opt-inference
*** Maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\startStateDist}{\ensuremath{p(\state_{1})}}
\newcommand{\transitionDist}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \modeVar_{\timeInd}=\desiredMode)}}
%\newcommand{\trajectoryDist}{\ensuremath{p(\stateControlTraj)}}
\renewcommand{\controlDist}{\ensuremath{\policy(\control_\timeInd \mid \state_\timeInd)}}


\renewcommand{\trajectoryVarDist}{\ensuremath{q(\stateTraj, \controlTraj \mid \state_0, \modeVarTraj)}}
\newcommand{\controlTrajVarDist}{\ensuremath{q(\control_{0:\TimeInd} \mid \state_{0:\TimeInd})}}
\newcommand{\controlVarDist}{\ensuremath{q(\control_{\timeInd})}}

\newcommand{\optimalVar}{\ensuremath{\mathcal{O}}}
\newcommand{\monotonicFunc}{\ensuremath{g}}
\newcommand{\temperature}{\ensuremath{\gamma}}

\newcommand{\optimalVarTraj}{\ensuremath{\bar{\bm{\optimalVar}}}}

\newcommand{\modeProb}{\ensuremath{\Pr(\modeVar_\timeInd = \modeInd \mid \state_\timeInd, \control_\timeInd)}}
\renewcommand{\optimalProb}{\ensuremath{\Pr(\optimalVar_\timeInd = 1 \mid \state_\timeInd, \control_\timeInd)}}
\newcommand{\optimalDist}{\ensuremath{P(\optimalVar_\timeInd \mid \state_\timeInd, \control_\timeInd)}}

\newcommand{\terminalCostDist}{\ensuremath{\Pr(\optimalVar_\TimeInd=1 \mid \state_\TimeInd)}}

\renewcommand{\marginalLikelihood}{\ensuremath{p(\optimalVarTraj, \modeVarTraj \mid \state_0)}}
%\newcommand{\jointDist}{\ensuremath{p(\optimalVar_{1:\TimeInd}=1, \modeVar_{1:\TimeInd}=\modeDes{\modeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})}}
%\newcommand{\jointDist}{\ensuremath{p(\optimalVar_{1:\TimeInd}, \modeVar_{1:\TimeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})}}
\renewcommand{\jointDist}{\ensuremath{p(\optimalVarTraj, \stateTraj, \controlTraj \mid \state_0})}}

\renewcommand{\trajectoryDist}{\ensuremath{p(\state_{1:\TimeInd}, \control_{0:\TimeInd} \mid \state_0)}}


\newcommand{\objective}{\ensuremath{J_{\text{quadratic}}}}

\renewcommand{\priorPolicy}{\ensuremath{\policy_0}}
\renewcommand{\priorPolicyDist}{\ensuremath{p_{\policy_0}}}
\renewcommand{\policyDist}{\ensuremath{p_{\policy}}}
#+END_EXPORT
*** intro :ignore:
This section presents an alternative approach to finding mode remaining trajectories.
In contrast to the previous section that
encoded mode remaining behaviour via the latent geometry of the \acrshort{mosvgpe}'s gating network,
this section unleashes the power of the probability mass function over the expert indicator variable.
As all \acrshort{mogpe} methods have a probability mass function over the expert indicator variable,
the method presented in this chapter is applicable in a wider range of \acrshort{mogpe} dynamics models.
cref:sec-inference-background recaps the necessary background and related work and
cref:sec-traj-opt-inference then details the trajectory optimisation algorithm.

*** Background and Related Work label:sec-inference-background
**** Cost Functions as Likelihoods :ignore:
This section firsts recaps the control-as-inference framework.
To formulate optimal control as probabilistic inference
it is first embedded into a graphical model (see Figure ref:fig-basic-control-graphical-model).
The joint probability model (over a trajectory) is augmented with an
additional variable to encode the notion of cost (or reward)
over the trajectory (see Figure ref:fig-augmented-control-graphical-model).
The new variable is a Bernoulli random variable $\optimalVar_\timeInd \in \{0, 1\}$, that indicates if
time step $\timeInd$ is /optimal/ $\optimalVar_\timeInd=1$, or not /optimal/ $\optimalVar_\timeInd=0$.
The likelihood distribution can be formulated by mapping the negative
cost through a monotonically increasing function $\monotonicFunc$,
giving the likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-monotonicOptimalityLikelihood}
\optimalProb &\coloneqq \monotonicFunc( -\costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
\todo{compare different fmons?}
# #+BEGIN_EXPORT latex
# \todo[inline]{add detials on Cross Entropy Method (CEM) cite:chuaDeep2018 and other methods}
# \begin{myquote}
# As shown in \cite{okadaVariational2020}, the choice of monotonic function,
# $\monotonicFunc$,
# leads to the inference algorithm resembling different well-known algorithms.
# For example, selecting $\monotonicFunc$ to be the exponential function,
# \begin{align} \label{eq-}
# \optimalProb &\propto \text{exp}( -\costFunc(\state_\timeInd, \control_\timeInd))
# \end{align}
# leads to the algorithm resembling Model Predictive Path Integral (MPPI)
# \cite{williamsModel2017,williamsInformation2017}, whilst
# selecting $\monotonicFunc$ to as,
# \begin{align} \label{eq-}
# \optimalProb &\propto \mathbb{I}\left[( \costFunc(\state_\timeInd, \control_\timeInd)) \leq \costFunc_{\text{thd}} \right]
# \end{align}
# recovers an algorithm resembling the Cross Entropy Method (CEM).
# \end{myquote}
# #+END_EXPORT
A common (and convenient) approach is to formulate
the likelihood using an exponential transform of
the cost. This results in a Boltzmann distribution where the
inverse temperature, $\temperature$, is used to scale the cost,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-exponentialOptimalityLikelihood}
\optimalProb &\propto \exp ( -\temperature\costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
The resulting negative log-likelihood,
for a single state-control trajectory \stateTraj, \controlTraj,
is an affine transformation of the cost,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-negative-log-likelihood-cost}
-\log \Pr(\optimalVarTraj \mid \stateTraj, \controlTraj)
&=  \temperature\costFunc(\stateTraj, \controlTraj),
\end{align}
#+END_EXPORT
which preserves convexity.
The set of optimal Bernoulli variables over a trajectory is denoted
$\optimalVarTraj = \{\optimalVar_{\timeInd}=1\}_{\timeInd=0}^{\TimeInd$.
When the inverse temperature parameter is set to $\temperature=1$ the maximum likelihood
trajectory coincides with classical optimal control citep:toussaintRobot2009.

***** Quadratic Cost Functions :noexport:
This work primarily focuses on quadratic costs, $\costFunc$, which are ubiquitous in control.

*Terminal Control Problems*
To find a trajectory between a start state, $\state_0$, and a target state, $\targetState$, at
time, $\TimeInd$, it is common to minimise the deviation of the final state, $\state_\TimeInd$,
from the desired target state $\targetState$.
To allow greater generality, a user defined, real symmetric positive definite matrix,
$\terminalStateCostMatrix$, can be introduced,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-terminal-cost}
J &= (\state_{\TimeInd} - \targetState)^T \terminalStateCostMatrix (\state_{\TimeInd} - \targetState) \nonumber \\
&= || \state_\TimeInd - \targetState ||_{\terminalStateCostMatrix}.
\end{align}
#+END_EXPORT
# When $\terminalStateCostMatrix$ is the identity matrix cref:eq-quadratic-terminal-cost-matrix-notation
# is recovered.
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-quadratic-terminal-cost-matrix-notation}
# J = (\state_{\TimeInd} - \targetState)^T(\state_{\TimeInd} - \targetState).
# \end{align}
# #+END_EXPORT
# This can also be written as,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-quadratic-terminal-cost-norm}
# J = || \state_\TimeInd - \targetState ||_2,
# \end{align}
# #+END_EXPORT
# where $|| \state_\TimeInd - \targetState ||_2$ denotes the L2 norm.

# *Minimum Control Effort Problems*
# It is also common to find trajectories that minimise the expenditure of control effort.
# The meaning of minimum control effort depends upon the specific problem.
# It may refer to minimising the total expenditure of fuel, or to minimising the energy dissipated from the system.
# Regardless of the specific interpretation, it is usually desirable to regularise the controls.
# Similar to the terminal cost, it is common to use a quadratic form,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-quadratic-cost-control}
# J &= \sum_{\timeInd=0}^{\TimeInd-1} \control_{\timeInd}^T \controlCostMatrix \control_{\timeInd} \nonumber \\
# &= || \control_\timeInd ||_\controlCostMatrix^2,
# \end{align}
# #+END_EXPORT
# where $\controlCostMatrix$ is a user defined, real symmetric positive definite weight matrix.
# This objective regularises the control trajectory under the L2 norm.
# It is worth noting that the control weight matrix can be a function of time, $\controlCostMatrix(t)$.

# #+BEGIN_EXPORT latex
# \todo{Add tracking/regulating state cost term? And length of trajectory with $\dot{x}^T\dot{x}$?}
# #+END_EXPORT

# *Tracking Problems*
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-quadratic-cost-state}
# J &= \state_{\timeInd}^T \stateCostMatrix \state_{\timeInd}
#   &= || \state_\timeInd ||_\stateCostMatrix^2
# \end{align}
# #+END_EXPORT
# where $\stateCostMatrix$ and $\controlCostMatrix$ are
# user defined, real symmetric positive semi-definite and real symmetric positive definite weight matrices respectively.
# It is worth noting that the state and control weight matrices can be functions of time, i.e.
# $\stateCostMatrix(t)$ and $\controlCostMatrix(t)$.
# This cost function regularises state
# This work seeks to control the terminal state and regularise the controls so combines these two objectives as
# follows,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-quadratic-cost-terminal-control}
# \objective =
# ||\state_{\TimeInd} - \targetState ||_{\terminalStateCostMatrix}
# + \sum_{\timeInd=0}^{\TimeInd-1}
# + ||\control_{\timeInd}||_{\controlCostMatrix}.
# \end{align}
# #+END_EXPORT
# Importantly, given a trajectory where each state and control are normally distributed,
# ${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$
# and
# ${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$,
# the expected cost for a trajectory can be calculated in closed-form,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-expected-quadratic-cost}
# %\E_{\stateTraj, \controlTraj} \left[ \costFunc_{\text{quadratic}}(\stateTraj, \controlTraj) \right]
# \E_{\stateTraj, \controlTraj} \left[ \objective \right] =
# \text{tr}(\terminalStateCostMatrix \terminalStateCov) +
# (\terminalStateMean - \targetState)^T \terminalStateCostMatrix (\terminalStateMean - \targetState)
# + \sum_{\timeInd=1}^{\TimeInd-1}
# %\text{tr}(\stateCostMatrix \stateCov_\timeInd)
# %+ \stateMean_{\timeInd}^T \stateCostMatrix \stateMean_{\timeInd}
# \text{tr}(\controlCostMatrix \controlCov )
# + \controlMean^T \controlCostMatrix \controlMean
# \end{align}
# #+END_EXPORT

It is worth noting that if a cost function is not quadratic then a quadratic approximation
can be obtained with a Taylor expansion.

**** graphical model :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \begin{subfigure}{.32\textwidth}
  \centering
   \resizebox{0.93\columnwidth}{!}{
 %  \resizebox{0.32\textwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_0$};
      \node[latent, right=of x1] (x2) {$\state_1$};
      \node[latent, right=of x2] (x3) {$\state_2$};
      \node[latent, right=of x3] (x4) {$\state_3$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_0$};
      \node[latent, right=of u1] (u2) {$\control_1$};
      \node[latent, right=of u2] (u3) {$\control_2$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);
    \end{tikzpicture}
    }
  \caption{Graphical model with states and controls.}
\label{fig-basic-control-graphical-model}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
  \centering
   \resizebox{0.93\columnwidth}{!}{
 %  \resizebox{0.32\textwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_0$};
      \node[latent, right=of x1] (x2) {$\state_1$};
      \node[latent, right=of x2] (x3) {$\state_2$};
      \node[latent, right=of x3] (x4) {$\state_3$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_0$};
      \node[latent, right=of u1] (u2) {$\control_1$};
      \node[latent, right=of u2] (u3) {$\control_2$};

      \node[obs, above=of u1] (o1) {$\optimalVar_0$};
      \node[obs, right=of o1] (o2) {$\optimalVar_1$};
      \node[obs, right=of o2] (o3) {$\optimalVar_2$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);
    \end{tikzpicture}
    }
  \caption{Graphical model with optimality variable.}
\label{fig-augmented-control-graphical-model}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
  \centering
   \resizebox{0.93\textwidth}{!}{
   %\resizebox{0.32\textwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_0$};
      \node[latent, right=of x1] (x2) {$\state_1$};
      \node[latent, right=of x2] (x3) {$\state_2$};
      \node[latent, right=of x3] (x4) {$\state_3$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_0$};
      \node[latent, right=of u1] (u2) {$\control_1$};
      \node[latent, right=of u2] (u3) {$\control_2$};

      \node[latent, above=of u1] (o1) {$\optimalVar_0$};
      \node[latent, right=of o1] (o2) {$\optimalVar_1$};
      \node[latent, right=of o2] (o3) {$\optimalVar_2$};

      \node[latent, below=of x1] (a1) {$\modeVar_0$};
      \node[latent, right=of a1] (a2) {$\modeVar_1$};
      \node[latent, right=of a2] (a3) {$\modeVar_2$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);

      \draw[->] (x1)--(a1);
      \draw[->] (u1) to [out=-90,in=30] (a1);
      \draw[->] (x2)--(a2);
      \draw[->] (u2) to [out=-90,in=30] (a2);
      \draw[->] (x3)--(a3);
      \draw[->] (u3) to [out=-90,in=30] (a3);
    \end{tikzpicture}
    }
  \caption{Graphical model with optimality and mode indicator variables.}
\label{fig-mode-augmented-control-graphical-model}
\end{subfigure}
\caption{Graphical models of control formulated as inference.}
\label{fig-control-graphical-model}
\end{figure}
#+END_EXPORT

**** graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \begin{subfigure}{.48\textwidth}
  \centering
   \resizebox{0.93\columnwidth}{!}{
 %  \resizebox{0.32\textwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_0$};
      \node[latent, right=of x1] (x2) {$\state_1$};
      \node[latent, right=of x2] (x3) {$\state_2$};
      \node[latent, right=of x3] (x4) {$\state_3$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_0$};
      \node[latent, right=of u1] (u2) {$\control_1$};
      \node[latent, right=of u2] (u3) {$\control_2$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);
    \end{tikzpicture}
    }
  \caption{Graphical model with states and controls.}
\label{fig-basic-control-graphical-model}
\end{subfigure}
\begin{subfigure}{.48\textwidth}
  \centering
   \resizebox{0.93\columnwidth}{!}{
 %  \resizebox{0.32\textwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_0$};
      \node[latent, right=of x1] (x2) {$\state_1$};
      \node[latent, right=of x2] (x3) {$\state_2$};
      \node[latent, right=of x3] (x4) {$\state_3$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_0$};
      \node[latent, right=of u1] (u2) {$\control_1$};
      \node[latent, right=of u2] (u3) {$\control_2$};

      \node[obs, above=of u1] (o1) {$\optimalVar_0$};
      \node[obs, right=of o1] (o2) {$\optimalVar_1$};
      \node[obs, right=of o2] (o3) {$\optimalVar_2$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);
    \end{tikzpicture}
    }
  \caption{Graphical model with optimality variable.}
\label{fig-augmented-control-graphical-model}
\end{subfigure}
\caption{Graphical models of control formulated as inference.}
\label{fig-control-graphical-model}
\end{figure}
#+END_EXPORT

**** Inference of Sequential Latent Variables
\newline

The joint probability for an optimal trajectory (i.e.
for $\optimalVar_\timeInd=1$ for all $\timeInd \in \{0,\ldots,\TimeInd\}$),
can be factorised using its Markovian structure,
# #+BEGIN_EXPORT latex
# \small
# \begin{align} \label{eq-traj-opt-joint-dist}
# \jointDist =
# \underbrace{\terminalCostDist}_{\text{terminal cost}}
# \prod_{\timeInd=0}^{\TimeInd-1}
# \underbrace{\optimalProb}_{\text{integral cost}}
# \underbrace{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)}_{\text{dynamics}}
# \underbrace{\controlDist}_{\text{controller}}
# \end{align}
# \normalsize
# #+END_EXPORT
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-traj-opt-joint-dist}
\jointDist =
\left[ \prod_{\timeInd=0}^{\TimeInd}
\underbrace{\optimalProb}_{\text{cost}} \right]
\left[ \prod_{\timeInd=0}^{\TimeInd-1}
\underbrace{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)}_{\text{dynamics}}
\underbrace{\controlDist}_{\text{controller}} \right]
\end{align}
\normalsize
#+END_EXPORT
where $\controlDist$ denotes the controller or policy.
cite:toussaintRobot2009 highlight that although the maximum liklihood trajectory coincides with the classical
optimal trajectory, taking expectations over trajectories
i.e. calculating $\log p(\optimalVarTraj \mid \state_0)$,
is not equivalent to expected cost minimisation.
cite:rawlikStochastic2013 extend the concepts from  cite:toussaintRobot2009 to show the general relation
to classical \acrshort{soc}.
For a given policy $\policy$, they introduce the posterior distribution over state-control trajectories as,
# They introduce a prior policy, $\priorPolicy$, and the distribution over state-control trajectories
# under this policy is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-prior-trajectory-dist}
%\priorPolicyDist(\stateTraj, \controlTraj) =
\policyDist(\stateTraj, \controlTraj \mid \optimalVarTraj, \state_0) =
Z^{-1}
\left[ \prod_{\timeInd=0}^\TimeInd \exp \left( - \temperature
\costFunc(\state_\timeInd,\control_\timeInd) \right) \right]
\left[ \prod_{\timeInd=0}^{\TimeInd-1}
p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)
\policy(\control_\timeInd \mid \state_\timeInd) \right]
\end{align}
#+END_EXPORT
# where the distribution over state-control trajectories under policy $\policy$, is given by,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-trajectory-dist}
# %p(\stateTraj, \controlTraj \mid \state_0) =
# \controlledPolicyDist(\stateTraj, \controlTraj) =
# \prod_{\timeInd=0}^{\TimeInd-1}
# p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)
# \policy(\control_\timeInd \mid \state_\timeInd).
# \end{align}
# #+END_EXPORT
where $Z=p(\optimalVarTraj \mid \state_0)$.
This distribution
$\policyDist(\stateTraj, \controlTraj \mid \optimalVarTraj, \state_0)$ is conditioned on the
optimality variable but generated by a potentially uniform policy $\policy$.

They then distinguish between a prior policy $\priorPolicy$ and an unknown control policy $\policy$.
The prior distribution over state-control trajectories under the control policy $\policy$, is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-trajectory-dist}
%p(\stateTraj, \controlTraj \mid \state_0) =
\controlledPolicyDist(\stateTraj, \controlTraj) =
\prod_{\timeInd=0}^{\TimeInd-1}
p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)
\policy(\control_\timeInd \mid \state_\timeInd).
\end{align}
#+END_EXPORT
Intuitively $\controlledPolicyDist(\stateTraj, \controlTraj)$, is thought of as the controlled process, which
is not conditioned on optimality and $\priorPolicyDist(\stateTraj, \controlTraj \mid \optimalVarTraj, \state_0)$
as the posterior process, conditioned on optimality
but generated by a potentially uniform policy, $\priorPolicy$.
The dual problem is then to find the control policy, $\policy$, where the controlled process,
$\controlledPolicyDist(\stateTraj, \controlTraj)$,
matches the posterior process,
$\priorPolicyDist(\stateTraj, \controlTraj \mid \optimalVarTraj, \state_0)$.
Given $\priorPolicy$ is an arbitrary stochastic policy and $\mathbb{D}$ is the set of deterministic
policies, the problem,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-kl-control}
\policy_* &= \argmin_{\policy \in \mathbb{D}}
\text{KL}(\controlledPolicyDist(\stateTraj, \controlTraj) \mid\mid \priorPolicyDist(\stateTraj, \controlTraj \mid \optimalVarTraj, \state_0)) \nonumber \\
&= \argmin_{\policy \in \mathbb{D}} Z + \temperature
\E_{\controlledPolicyDist(\stateTraj, \controlTraj)}
\left[ \costFunc(\stateTraj, \controlTraj) \right]
+ \E_{\controlledPolicyDist(\stateTraj)} \left[ \text{KL}(\policy \mid\mid \priorPolicy) \right] \nonumber \\
&= \argmin_{\policy \in \mathbb{D}}
\underbrace{Z}_{\text{constant}} +
\underbrace{\temperature \E_{\controlledPolicyDist(\stateTraj, \controlTraj)}
\left[ \costFunc(\stateTraj, \controlTraj) \right]}_{\text{expected costs}}
- \E_{\controlledPolicyDist(\stateTraj, \controlTraj)} \left[ \log \priorPolicy(\controlTraj \mid \stateTraj) \right]
- \underbrace{\E_{\controlledPolicyDist(\stateTraj)} \left[ H(\policy) \right]}_{\text{max entropy term}},
\end{align}
#+END_EXPORT
is equivalent to the \acrshort{soc} problem in cref:eq-mode-soc-problem, with a modified integral cost,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-cost-per-stage}
\hat{\costFunc}(\state_\timeInd, \control_\timeInd) = \costFunc(\state_\timeInd, \control_\timeInd)
- \frac{1}{\temperature} \log \priorPolicy(\control_\timeInd \mid \state_\timeInd)
\end{align}
#+END_EXPORT
and no mode remaining constraint.
The problem in cref:eq-kl-control finds trajectories that balance
minimising expected costs
$\E_{\controlledPolicyDist(\stateTraj, \controlTraj)} \left[ \costFunc(\stateTraj, \controlTraj) \right]$
and selecting a policy $\policy$ that is similar to the prior policy $\priorPolicy$.
If the prior policy $\priorPolicy$ is assumed to be uniform, then
$\E_{\controlledPolicyDist(\stateTraj, \controlTraj)} \left[ \log \priorPolicy(\controlTraj \mid \stateTraj) \right]$
becomes constant and the optimised policy, $\policy_*$, is a balance of minimising expected costs and
maximising the policy's entropy,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-kl-control-uniform}
\policy_* &= \argmin_{\policy \in \mathbb{D}}
\underbrace{\temperature \E_{\controlledPolicyDist(\stateTraj, \controlTraj)}
\left[ \costFunc(\stateTraj, \controlTraj) \right]}_{\text{expected costs}}
- \underbrace{\E_{\controlledPolicyDist(\stateTraj)} \left[ H(\policy) \right]}_{\text{max entropy term}}.
\end{align}
#+END_EXPORT

*Maximum entropy regularisation* Formulating trajectory optimisation in this way encodes the
maximum causal entropy principle, which is often used to achieve robustness,
in particular for inverse optimal control cite:ziebartModeling2010.
# \todo{add something on ME being questionable for stochastic dynamics}
# It is important to note that the maximum entropy framework is problematic for stochastic dynamics.
# In essence, it assumes that the agent is able to control the dynamics as well as the controls in order
# to obtain optimal trajectories.

*Other approaches* There are multiple approaches to performing inference in this graphical model.
Trading accuracy for computational complexity is often required for real-time control.
In this case, one approach is to approximate the dynamics with linear or
quadratic approximations, as is done in iLQR/iLQG and DDP respectively.
Given linear dynamics,
the full graphical model in cref:eq-traj-opt-joint-dist can be
computed using approximate Gaussian message passing, for which effective methods exist citep:loeligerFactor2007.
The inference problem can then be solved using the
expectation maximisation algorithm for dynamical system estimation
citep:shumwayAPPROACH1982,ghahramaniLearning1999,schonSystem2011, with input estimation citep:watsonStochastic2021.

# However, the novelty of the problem arises from the mode remaining behaviour,

# However, inference in nonlinear SSMs is

*** Mode Remaining Control as Inference label:sec-traj-opt-inference
**** maths :ignore:
#+BEGIN_EXPORT latex
%\renewcommand{\optimalProb}{\ensuremath{\Pr(\optimalVar_\timeInd = 1, \modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd)}}
%\renewcommand{\modeProb}{\ensuremath{\Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd)}}
%\renewcommand{\optimalVarTraj}{\ensuremath{\optimalVar_{0:\TimeInd}=1}}
\renewcommand{\modeVarTraj}{\ensuremath{\bar{\bm\modeVar}}}

\renewcommand{\modeVarK}{\ensuremath{\modeVar_{\timeInd} = \modeInd}}
\renewcommand{\modeProb}{\ensuremath{\Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd)}}
\renewcommand{\terminalCostDist}{\ensuremath{\Pr(\optimalVar_\TimeInd=1, \modeVar_\TimeInd=\desiredMode \mid \state_\TimeInd)}}
\renewcommand{\terminalModeProbDist}{\ensuremath{\Pr(\modeVar_\TimeInd=\desiredMode \mid \state_\TimeInd)}}
\renewcommand{\jointDist}{\ensuremath{p(\optimalVarTraj, \modeVarTraj, \stateTraj, \controlTraj \mid \state_0})}}

\renewcommand{\transitionDistOptimal}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \optimalVarTraj_{0:\timeInd}, \modeVarTraj_{0:\timeInd})}}
\renewcommand{\transitionDist}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)}}
\renewcommand{\transitionDistK}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \modeVarK)}}

\renewcommand{\transitionVarDistK}{\ensuremath{q(\state_{\timeInd+1} \mid \state_0, \modeVarTraj_{0:\timeInd})}}
#+END_EXPORT

**** Fig :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[h!]
\centering
\includegraphics[width=0.98\textwidth]{./images/mode-opt/env/scenario_7/mosvgpe/mixing_probs_no_obs.pdf}
\caption{
Visualisation of the probability mass function over the expert indicator variable after training \acrshort{mosvgpe}
on the state transition data set from the simulated version of the 2D quadcopter environment
in the illustrative example from \cref{illustrative_example}.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line)
and the subset of the environment which has not been observed (hashed box).}
\label{eq-traj-opt-gating-network-prob-post-inf}
\end{figure}
#+END_EXPORT
**** intro :ignore:
This section details how the control-as-inference framework can be extended to multimodal dynamical systems and
used to encode mode remaining behaviour.
cref:eq-traj-opt-gating-network-prob-post-inf shows the probability mass function over the expert indicator variable
after training \acrshort{mosvgpe} on the historical data set of state transitions from the
quadcopter experiment in cref:illustrative_example.
Intuitively, the goal is to find trajectories that remain in regions of the dynamics with high probability of
remaining in the desired dynamics mode.

# This method unleashes the power of the probability mass function over the expert indicator variable to encode
# mode remaining behaviour.

**** graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
   \resizebox{0.4\textwidth}{!}{
   %\resizebox{0.32\textwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_0$};
      \node[latent, right=of x1] (x2) {$\state_1$};
      \node[latent, right=of x2] (x3) {$\state_2$};
      \node[latent, right=of x3] (x4) {$\state_3$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_0$};
      \node[latent, right=of u1] (u2) {$\control_1$};
      \node[latent, right=of u2] (u3) {$\control_2$};

      \node[obs, above=of u1] (o1) {$\optimalVar_0$};
      \node[obs, right=of o1] (o2) {$\optimalVar_1$};
      \node[obs, right=of o2] (o3) {$\optimalVar_2$};

      \node[obs, below=of x1] (a1) {$\modeVar_0$};
      \node[obs, right=of a1] (a2) {$\modeVar_1$};
      \node[obs, right=of a2] (a3) {$\modeVar_2$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);

      \draw[->] (x1)--(a1);
      \draw[->] (u1) to [out=-90,in=30] (a1);
      \draw[->] (x2)--(a2);
      \draw[->] (u2) to [out=-90,in=30] (a2);
      \draw[->] (x3)--(a3);
      \draw[->] (u3) to [out=-90,in=30] (a3);
    \end{tikzpicture}
    }
  \caption{Graphical model with optimality and mode indicator variables.}
\label{fig-mode-augmented-control-graphical-model}
\end{figure}
#+END_EXPORT

**** after graphical model :ignore:
\newline

In order to find trajectories that remain in a desired
dynamics mode, this work further augments
the graphical model in cref:fig-control-graphical-model with the mode indicator variable $\modeVar \in \modeDomain$
from cref:eq-multimodal-dynamics-disc.
The resulting graphical model is shown in Figure ref:fig-mode-augmented-control-graphical-model,
where the evidence is that $\optimalVar_{\timeInd}=1$ and $\modeVar_{\timeInd}=\desiredMode$ for
all $\timeInd \in \{0,\ldots,\TimeInd\}$.
The joint probability model is then given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-traj-opt-joint-dist-mode}
\jointDist =
&\bigg[ \prod_{\timeInd=0}^{\TimeInd}
\underbrace{\optimalProb}_{\text{cost}}
\underbrace{\modeProb}_{\text{mode remaining term}} \bigg] \nonumber \\
&\bigg[ \prod_{\timeInd=0}^{\TimeInd-1}
\underbrace{\transitionDistK}_{\text{dynamics}}
\underbrace{\controlDist}_{\text{controller}} \bigg],
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-traj-opt-joint-dist-mode}
# \jointDist =
# &\underbrace{\terminalCostDist}_{\text{terminal cost}} \nonumber \\
# &\bigg[ \prod_{\timeInd=0}^{\TimeInd-1}
# \underbrace{\optimalProb}_{\text{integral cost}}
# \underbrace{\modeProb}_{\text{mode remaining term}} \bigg] \nonumber \\
# &\bigg[ \prod_{\timeInd=0}^{\TimeInd-1}
# \underbrace{\transitionDistK}_{\text{dynamics}}
# \underbrace{\controlDist}_{\text{controller}} \bigg],
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \small
# \begin{align} \label{eq-traj-opt-joint-dist-mode}
# \jointDist =
# &\underbrace{\terminalCostDist}_{\text{Terminal Cost}} \nonumber \\
# &\prod_{\timeInd=0}^{\TimeInd-1} \bigg[
# \underbrace{\optimalProb}_{\text{Integral Cost}}
# \underbrace{\modeProb}_{\text{Mode Remaining Term}} \nonumber \\
# &\underbrace{\transitionDist}_{\text{Dynamics}}
# \underbrace{\controlDist}_{\text{Controller}} \bigg].
# \end{align}
# \normalsize
# #+END_EXPORT
where $\modeVarTraj = \{\modeVar_{\timeInd}=\desiredMode \}_{\timeInd=0}^{\TimeInd}$
denotes every time step of a trajectory belonging to the desired dynamics mode $\desiredMode$.
cref:eq-traj-opt-joint-dist-mode says
that the probability of observing a trajectory is given by taking the product of its probability of occurring
according to the dynamics, with the exponential of the negative cost and the probability of remaining
in the desired dynamics mode.
Given deterministic dynamics, the trajectory with highest probability will be that with the lowest
cost and highest probability of remaining in the desired dynamics mode.

# Also note that the posterior is conditioned on the initial state $\state_0$.


# Intuitively, the posterior of interest is th
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# p(\state_{1:\TimeInd}, \control_{1:\TimeInd} \mid \optimalVar_{1:\timeInd}=1, \state_1) =
# \trajectoryDist
# \left[
# \prod_{\timeInd=1}^\TimeInd
# p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \optimalVar_{1:\TimeInd})
# \policy(\control_\timeInd \mid \state_\timeInd)
# p(\optimalVar_\timeInd=1 \mid \state_\timeInd, \control_\timeInd)
# \end{align}
# #+END_EXPORT

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# p(\state_{1:\TimeInd}, \control_{1:\TimeInd}\mid \optimalVar_{1:\TimeInd}=1, \state_1)
# =
# \prod_{\timeInd=1}^\TimeInd
# p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)
# p(\optimalVar_\timeInd=1 \mid \state_\timeInd, \control_\timeInd)
# p(\control_\timeInd \mid \state_\timeInd)
# \prod_{\timeInd=1}^\TimeInd
# \text{exp}\left( -\temperature \integralCostFunc(\state_\timeInd, \control_\timeInd) \right)
# \end{align}
# #+END_EXPORT

# In the RL setting the policy is parameterised with parameters, $\theta$, which are optimised and used
# to predict the controls.
# In contrast, the control setting is usually interested in optimising the states and controls directly.

# The object of interest is the distribution over the controls, $\controlTraj$,
# conditioned on the initial state, $\state_0$, all future time steps being optimal and in the
# desired dynamics mode,

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-optimal-control-message-passing}
# p(\controlTraj \mid \state_{0}, \optimalVar_{0:\TimeInd}=1, \modeVar_{0:\TimeInd}=\desiredMode) &=
# \frac{
# p(\optimalVar_{0:\TimeInd}=1 \mid \stateTraj, \controlTraj)
# p(\modeVar_{0:\TimeInd}=\desiredMode \mid \stateTraj, \controlTraj)
# p(\stateTraj, \controlTraj \mid \optimalVar_{0:\TimeInd}=1, \modeVar_{0:\TimeInd}=\desiredMode, \state_0)
# }{}
# \end{align}
# #+END_EXPORT

**** Variational Inference :ignore:

This work draws on the connection between KL-divergence control citep:rawlikStochastic2013
and structured variational inference.
Whilst the derivation shown here differs from cite:rawlikStochastic2013, the underlying framework and
objective are the same.

In variational inference the goal is to approximate a distribution $p(\mathbf{y})$
with another, potentially simpler distribution $q(\mathbf{y})$.
Typically this distribution $q(\mathbf{y})$ is selected to be a product of conditional distributions connected in a
chain or tree, which lends itself to exact inference.
In this work, the goal is to approximate the intractable distribution over optimal trajectories,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\stateTraj, \controlTraj \mid \state_0, \optimalVarTraj, \modeVarTraj)
= Z^{-1}
\Bigg[ \prod_{\timeInd=0}^{\TimeInd}
&\underbrace{\exp \left( - \gamma  \costFunc(\state_\timeInd, \control_\timeInd) \right)}_{\text{cost}}
\underbrace{\modeProb}_{\text{mode remaining term}}
\Bigg]
\nonumber \\
\Bigg[ \prod_{\timeInd=0}^{\TimeInd-1}
&\underbrace{ \transitionDistK}_{\text{dynamics}}
\underbrace{ \policy(\control_\timeInd \mid \state_\timeInd) }_{\text{controller}}
\Bigg]
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# p(\stateTraj, \controlTraj \mid \state_0, \optimalVarTraj, \modeVarTraj)
# = Z^{-1}
# \Bigg[ \prod_{\timeInd=0}^{\TimeInd-1}
# &\underbrace{ \transitionDistOptimal}_{\text{optimal dynamics}}
# \underbrace{ p(\control_\timeInd \mid \state_\timeInd, \optimalVarTraj_{0:\timeInd}, \modeVarTraj_{0:\timeInd}) }_{\text{optimal policy}}
# \Bigg] \nonumber \\
# \Bigg[ \prod_{\timeInd=0}^{\TimeInd}
# &\underbrace{\modeProb}_{\text{mode remaining}}
# \underbrace{\exp \left( - \gamma  \costFunc(\state_\timeInd, \control_\timeInd) \right)}_{\text{cost}} \Bigg],
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# p(\stateTraj, \controlTraj \mid \state_0, \optimalVarTraj, \modeVarTraj)
# = Z^{-1}
# &\underbrace{\terminalModeProbDist}_{\text{Mode Remaining}}
# \underbrace{\exp \left( - \gamma  \terminalCostFunc(\state_\TimeInd, \control_\TimeInd) \right)}_{\text{Terminal Cost}}
# \nonumber \\
# \prod_{\timeInd=0}^{\TimeInd-1} \Bigg[
# &\underbrace{ \transitionDistOptimal}_{\text{Optimal Dynamics}} \nonumber \\
# &\underbrace{p(\control_\timeInd \mid \state_\timeInd, \optimalVarTraj, \modeVarTraj) }_{\text{Optimal Policy}} \nonumber \\
# &\underbrace{\modeProb}_{\text{Mode Remaining}}
# \underbrace{\exp \left( - \gamma  \costFunc(\state_\timeInd, \control_\timeInd) \right)}_{\text{Cost}} \Bigg],
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# p(\stateTraj, \controlTraj \mid \state_0, \optimalVar_{0:\TimeInd}=1, \modeVar_{0:\TimeInd}=\desiredMode)
# = \underbrace{\left[ \prod_{\timeInd=0}^{\TimeInd-1} \transitionDist \policy(\control_\timeInd \mid \state_\timeInd) \right]}_{\text{controlled dynamics}}
# \exp \left( \sum_{\timeInd=0}^\TimeInd \costFunc(\state_\timeInd, \control_\timeInd) \right),
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# q(\stateTraj, \controlTraj \mid \state_0)
# = \prod_{\timeInd=0}^{\TimeInd-1} q(\state_{\timeInd+1} \mid \state_{0})
# %= \prod_{\timeInd=0}^{\TimeInd-1} q(\state_{\timeInd+1} \mid \state_{\timeInd}, \control_\timeInd)
# \controlVarDist.
# \end{align}
# #+END_EXPORT
# where $Z=p(\optimalVarTraj, \modeVarTraj \mid \state_0)$.
with the variational distribution $\trajectoryVarDist$.
Note that $Z=p(\optimalVarTraj, \modeVarTraj \mid \state_0)$.
In this work the variational distribution over the controller is assumed independent of the state.
That is, instead of parameterising the controller as a feedback policy, it is parameterised as a set
of open-loop controls q(\control_{\timeInd}).
Calculating $\trajectoryVarDist$ for a set of controls
$q(\controlTraj) = \prod_{\timeInd=0}^{\TimeInd-1} \controlVarDist$,
requires simulating the trajectory in the learned,
single-step dynamics model, i.e. making long-term predictions.
As this method is using a learned representation of the transition dynamics,
it suffices to assume that the dynamics are given by the desired mode's learned dynamics.

**** Approximate Inference for Dynamics Predictions

# *Approximate Inference for Dynamics Predictions*
# Following cref:sec-dynamics-predictions, this chapter sidesteps the issue of
# constructing approximate closed-form solutions based on the model in cref:chap-dynamics,
# by enforcing that the controlled system remains in the desired dynamics mode.

# *Approximate inference for dynamics predictions*
Constructing approximate closed-form solutions based on the model in cref:chap-dynamics
is difficult, due to the exponential growth in the number of Gaussian components.
Similar to the approach in cref:sec-dynamics-predictions, this chapter obtains multi-step
predictions by cascading single-step predictions through the desired mode's dynamics GP.
However, this approach extends the predictions to handle Normally distributed controls
$\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)$.
Multi-step predictions are then obtained by recursively calculating the following integral,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-state-control-unc-prop-inf}
q(\state_{\timeInd+1} \mid \state_0, \modeVarTraj_{0:\timeInd})
%q(\state_{\timeInd+1}) = q(\state_{\timeInd+1} \mid \state_0, \modeVarTraj_{0:\timeInd})
&= \int_{\controlDomain} \int_{\stateDomain}
\transitionDistK q(\state_\timeInd \mid \state_{0}, \modeVarTraj_{0:\timeInd-1})
q(\control_{\timeInd})
\text{d}\state_{\timeInd}
\text{d}\control_{\timeInd}
\end{align}
#+END_EXPORT
Note that $\modeVarTraj_{0:\timeInd}$ denotes the first $\timeInd$ elements of $\modeVarTraj$, i.e.
$\modeVarTraj_{0:\timeInd} = \{ \modeVar_i=\desiredMode\}_{i=0}^{\timeInd}$.
Given this method for making multi-step predictions, the variational distribution over state-control trajectories is
given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
%q(\stateTraj, \controlTraj \mid \state_0, \modeVarTraj)
\trajectoryVarDist
= \prod_{\timeInd=0}^{\TimeInd-1}
q(\state_{\timeInd+1} \mid \state_0, \modeVarTraj_{0:\timeInd})
%q(\state_{\timeInd})
\controlVarDist,
\end{align}
#+END_EXPORT
with $q(\state_0) = \delta(\state_0)$.
Note that the state and control at each time step are Normally distributed.
# For notational conciseness, the variational distribution is denoted
# $q(\stateTraj, \controlTraj) = q(\stateTraj, \controlTraj \mid \state_0)$,
# i.e. the dependence on $\state_0$ is dropped.

**** Variational Inference for Sequential Latent Variables
# *** Evidence Lower BOund :ignore:

# *Evidence Lower BOund*
Variational inference seeks to optimise $q(\controlTraj)$
w.r.t. the \acrfull{elbo}.
In this setup, the evidence is that $\optimalVar_{\timeInd}=1$ and $\modeVar_{\timeInd}=\desiredMode$ for
all $\timeInd \in \{0,\ldots,\TimeInd\}$.
Given this, the ELBO is given by,
# but we drop the dependence from here on out for notational conciseness.
# #+BEGIN_EXPORT latex
# \small
# \begin{align} \label{eq-}
# \text{log} \marginalLikelihood
# &= \text{log} \E_{\trajectoryVarDist} \left[ \frac{\jointDist}{\trajectoryVarDist} \right] \nonumber \\
# &= \text{log} \E_{\trajectoryVarDist} \left[
# \frac{\prod_{\timeInd=0}^{\TimeInd-1} \optimalProb
# \Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd)
# \controlDist}{\prod_{\timeInd=0}^{\TimeInd-1}  \controlVarDist}
# \right] \nonumber \\
# &\geq \E_{\trajectoryVarDist} \left[ \sum_{\timeInd=0}^{\TimeInd} - \temperature \costFunc(\state_\timeInd, \control_\timeInd) \right]
# + \E_{\trajectoryVarDist} \left[
# \sum_{\timeInd=0}^{\TimeInd}
# \log \Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd) \right] \nonumber \\
# &\quad - \sum_{\timeInd=0}^{\TimeInd}
# \text{KL}\left(\controlVarDist \mid\mid \policy(\control_\timeInd \mid \state_\timeInd) \right)  \coloneqq \mathcal{L},
# \end{align}
# \normalsize
# #+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\text{log} \marginalLikelihood
&= \text{log} \E_{\trajectoryVarDist} \left[ \frac{\jointDist}{\trajectoryVarDist} \right] \nonumber \\
&= \text{log} \E_{\trajectoryVarDist} \left[
\frac{\prod_{\timeInd=0}^{\TimeInd-1} \optimalProb
%\Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd)
\modeProb
\controlDist}{\prod_{\timeInd=0}^{\TimeInd-1}  \controlVarDist}
\right] \nonumber \\
&\geq - \sum_{\timeInd=0}^{\TimeInd}
\underbrace{\E_{q(\state_{\timeInd}, \control_{\timeInd} \mid \state_0, \modeVarTraj_{0:\timeInd-1})}
\left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]}_{\text{expected cost}}
+ \sum_{\timeInd=0}^{\TimeInd}
\underbrace{\E_{ q(\state_{\timeInd} \mid \state_0, \modeVarTraj_{0:\timeInd-1})}
\left[ \log \modeProb\right]}_{\text{mode remaining term}} \nonumber \\
&\quad - \sum_{\timeInd=0}^{\TimeInd-1}
\text{KL}\left(\controlVarDist \mid\mid \policy(\control_\timeInd \mid \state_\timeInd) \right)  \coloneqq \mathcal{L},
\end{align}
#+END_EXPORT
where for notational conciseness the terminal values have been omitted.
Assuming a uniform prior policy $\policy(\control_\timeInd \mid \state_\timeInd)$ leads to the $\text{KL}$
term reducing to an entropy term and a constant,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-traj-opt-elbo}
\mathcal{L} = &-\sum_{\timeInd=0}^{\TimeInd}
\underbrace{\E_{q(\state_{\timeInd} \mid \state_0, \modeVarTraj_{0:\timeInd-1}) \controlVarDist }
\left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]}_{\text{expected cost}}
+ \sum_{\timeInd=0}^{\TimeInd} \underbrace{\E_{ q(\state_{\timeInd} \mid \state_0, \modeVarTraj_{0:\timeInd-1})}
\left[
\log \modeProb \right]}_{\text{mode remaining term}} \nonumber \\
&+ \sum_{\timeInd=0}^{\TimeInd-1} \cancel{\underbrace{\E_{q(\state_{\timeInd} \mid \state_0, \modeVarTraj_{0:\timeInd-1}) \controlVarDist }
\left[\log\policy(\control_{\timeInd} \mid \state_\timeInd)\right]}_{\text{constant}}}
+ \sum_{\timeInd=0}^{\TimeInd-1} \underbrace{\mathcal{H}(\control_{\timeInd})}_{\text{entropy}}
\end{align}
#+END_EXPORT
The ELBO in cref:eq-traj-opt-elbo resembles the KL control objective in cref:eq-kl-control-uniform
but with an extra term encoding the mode remaining behaviour.
In practice, maximum entropy control is achieved by parameterising the control at each time step to
be Normally distributed ${\controlVarDist = \mathcal{N}(\control_{\timeInd} \mid \controlMean, \controlCov)}$.
The control dimensions are assumed independent so $\controlCov$ becomes diagonal.
The maximum entropy behaviour can be omitted by using deterministic controls, i.e.
parameterising them to follow a dirac delta distribution $\controlVarDist = \delta(\control_\timeInd)$.

***** Control problem :ignore:
The control problem is then given by,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-traj-opt-inference}
\begin{align}
\max_{\controlTraj} &-\sum_{\timeInd=0}^{\TimeInd}
\underbrace{\E_{q(\state_{\timeInd} \mid \state_0, \modeVarTraj_{0:\timeInd-1}) \controlVarDist} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]}_{\text{expected cost}}
&+ \sum_{\timeInd=0}^{\TimeInd} \underbrace{\E_{q(\state_{\timeInd} \mid \state_0, \modeVarTraj_{0:\timeInd-1})} \left[
\log \modeProb \right]}_{\text{mode remaining term}} \nonumber \\
 &+ \sum_{\timeInd=0}^{\TimeInd-1} \underbrace{\mathcal{H}(\control_{\timeInd})}_{\text{entropy}} \\
\text{s.t.}& \quad \cref{eq-state-control-unc-prop-inf} &
\end{align}
\end{subequations}
#+END_EXPORT
which encodes mode remaining behaviour alongside maximum entropy control.
The variational parameters $\controlMean$ (and $\controlCov$)
are found by maximising the \acrshort{elbo} using gradient-based optimisation.
At each iteration the \acrshort{elbo} is calculated by rolling out the control distribution in the
desired mode's GP dynamics model using cref:eq-state-control-unc-prop-inf.
The resulting state-control trajectory is then used to calculate the \acrshort{elbo}.
The cost function is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-cost-control}
\costFunc(\stateTraj, \controlTraj) &=
\underbrace{(\terminalState - \targetState)^T \terminalStateCostMatrix (\terminalState - \targetState)}_{\text{terminal cost}}
+ \sum_{\timeInd=0}^{\TimeInd-1}
\underbrace{\control_{\timeInd}^T \controlCostMatrix \control_{\timeInd}}_{\text{control cost}},
%\nonumber \\
%&= \| \terminalState - \targetState \|_\terminalStateCostMatrix + \sum_{\timeInd=0}^{\TimeInd-1}
%\| \control_\timeInd \|_\controlCostMatrix
\end{align}
#+END_EXPORT
where $\terminalStateCostMatrix$ and $\controlCostMatrix$ are user defined,
real, symmetric, positive semi-definite and positive definite matrices respectively.
This cost function encodes the terminal state boundary condition in cref:eq-mode-soc-problem via a
quadratic cost that minimises the deviation of the final state $\state_{\TimeInd}$ from the target
state $\targetState$.
Importantly, the expected value of this cost under Normally distributed states and controls can be
calculated in closed-form with,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-quadratic-cost}
\E_{\stateTraj, \controlTraj} \left[ \costFunc(\stateTraj, \controlTraj) \right] =
\text{tr}(\terminalStateCostMatrix \terminalStateCov) +
(\terminalStateMean - \targetState)^T \terminalStateCostMatrix (\terminalStateMean - \targetState)
+ \sum_{\timeInd=1}^{\TimeInd-1}
\text{tr}(\controlCostMatrix \controlCov )
+ \controlMean^T \controlCostMatrix \controlMean.
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{remark} \label{}
# In contrast to the collocation solver in \cref{sec-traj-opt-collocation},
# the terminal state boundary condition in \cref{eq-mode-soc-problem-geometry}
# is encoded via the cost function, instead of being enforced by the solver.
# \end{remark}
# #+END_EXPORT

**** Cost Functions :ignore:noexport:
*Cost Functions*
This work primarily focuses on quadratic costs, as they are ubiquitous in control and lead
to closed-form expectations under Normally distributed states
${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$
and controls
${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$.
This work seeks to find trajectories between a start state $\state_0$ and a target state $\targetState$, at
time $\TimeInd$.
It is common to minimise the deviation of the final state $\state_\TimeInd$
from the desired target state $\targetState$.
It is also common to find trajectories that minimise the expenditure of control effort.
As such, this work adopts the following cost function,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-cost-control}
\costFunc(\stateTraj, \controlTraj) &=
\underbrace{(\terminalState - \targetState)^T \terminalStateCostMatrix (\terminalState - \targetState)}_{\text{terminal cost}}
+ \sum_{\timeInd=0}^{\TimeInd-1}
\underbrace{\control_{\timeInd}^T \controlCostMatrix \control_{\timeInd}}_{\text{control cost}}
\nonumber \\
&=
 \| \terminalState - \targetState \|_\terminalStateCostMatrix
+ \sum_{\timeInd=0}^{\TimeInd-1}
\| \control_\timeInd \|_\controlCostMatrix
\end{align}
#+END_EXPORT
where $\terminalStateCostMatrix$
and $\controlCostMatrix$ are user defined,
real, symmetric, positive semi-definite and positive definite matrices respectively.
Importantly, the expected value of this cost under Normally distributed states and controls can be
calculated in closed form with,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-quadratic-cost}
\E_{\stateTraj, \controlTraj} \left[ \costFunc(\stateTraj, \controlTraj) \right] =
\text{tr}(\terminalStateCostMatrix \terminalStateCov) +
(\terminalStateMean - \targetState)^T \terminalStateCostMatrix (\terminalStateMean - \targetState)
+ \sum_{\timeInd=1}^{\TimeInd-1}
\text{tr}(\controlCostMatrix \controlCov )
+ \controlMean^T \controlCostMatrix \controlMean.
\end{align}
#+END_EXPORT

**** Approximate Inference for Dynamics Predictions :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\singleInput}{\ensuremath{\state_\timeInd, \control_\timeInd}}
\renewcommand{\singleInput}{\ensuremath{\hat{\state}_\timeInd}}
\renewcommand{\singleOutput}{\ensuremath{\Delta\state_{\timeInd+1}}}

\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput)}}
\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput)}}
%\newcommand{\expertInducingVariational}{\ensuremath{q(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\expertInducingVariational}{\ensuremath{q(\expertInducingOutput)}}
\newcommand{\expertsInducingVariational}{\ensuremath{q(\expertsInducingOutput)}}
\newcommand{\expertsVariational}{\ensuremath{q(\LatentFunc_\numData)}}
%\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \expertInducingOutput)}}
%\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))}}
\renewcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \singleInput, \expertInducingOutput)}}

\renewcommand{\expertVariational}{\ensuremath{q(\mode{\latentFunc}(\singleInput) \mid \singleInput)}}

\newcommand{\singleInputMean}{\ensuremath{\hat{\bm\mu}}}
\newcommand{\singleInputCov}{\ensuremath{\hat{\bm\Sigma}}}
#+END_EXPORT
*Approximate Inference for Dynamics Predictions*
Fortunately, this work is interested in remaining in a single dynamics mode $\desiredMode$,
which simplifies making long-term predictions.
The model in Chapter ref:chap-dynamics represents each mode's dynamics function as a sparse variational GP,
giving the transition density as,
#+BEGIN_EXPORT latex
\begin{align}
\transitionDistOptimal &= \expertVariational \nonumber \\
&=
\mathcal{N} \left( \mode{\latentFunc}(\singleInput) \mid
\mode{\mathbf{A}} \mode{\mathbf{m}},
\expertKernel(\singleInput, \singleInput)
+ (\mode{\mathbf{S}} - \expertKernel(\expertInducingInput,\expertInducingInput))
\mode{\mathbf{A}}^T
\right)
%\nonumber \\
%\mode{\mathbf{A}} &=
%\expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}.
\end{align}
#+END_EXPORT
where $\mode{\mathbf{A}} = \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}$.
To obtain long-term predictions, we cascade one-step predictions.
This requires mapping uncertain state-control inputs through the desired mode's GP dynamics model.
This prediction problem corresponds to recursively calculating the following integral,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-state-unc-prop}
q(\state_{\timeInd+1} \mid \state_0) = \int \int
\expertVariational
q(\control_\timeInd) q(\state_\timeInd)
\text{d}\state_{\timeInd} \text{d}\control_{\timeInd}
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-state-unc-prop}
# q(\state_{\timeInd+1} \mid \state_0) = \int \int
# \transitionDistK
# q(\control_\timeInd) q(\state_\timeInd)
# \text{d}\state_{\timeInd} \text{d}\control_{\timeInd}
# \end{align}
# #+END_EXPORT
with $q(\state_0) = \delta(\state_0)$.
This work considers Normally distributed controls
${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$ and
deterministic controls.
Approximate closed-form solutions exist for propagating Normally distributed states and controls
through GP models
citep:girardGaussian2003,kussGaussian2006,quinonero-candelaPropagation2003.
\todo{correct girard citation}
This work exploits the moment-matching approximation
implemented in the =uncertain_conditional= from GPflow citep:GPflow2017.
For more details on the approximation see Section 7.2.1 of citep:kussGaussian2006.

Given this method for making long term predictions, the distribution over state-control trajectories is
given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
q(\stateTraj, \controlTraj \mid \state_0)
= \prod_{\timeInd=0}^{\TimeInd-1} q(\state_{\timeInd+1} \mid \state_0) \controlVarDist.
\end{align}
#+END_EXPORT
The variational lower bound is then given by sums over each time step,
#+BEGIN_EXPORT latex
%\small
\begin{align} \label{eq-traj-opt-elbo}
\mathcal{L} = &- \sum_{\timeInd=0}^{\TimeInd}
\underbrace{\E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]}_{\text{Expected cost}}
+ \sum_{\timeInd=0}^{\TimeInd} \underbrace{\E_{\controlVarDist q(\state_{\timeInd})} \left[
\log \Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd) \right]}_{\text{Mode remaining term}} \nonumber \\
&+ \sum_{\timeInd=0}^{\TimeInd} \underbrace{\E_{\controlVarDist q(\state_{\timeInd})}
\left[\log\policy(\control_{\timeInd} \mid \state_\timeInd)\right]}_{\text{Constant}}
+ \sum_{\timeInd=0}^{\TimeInd} \underbrace{\mathcal{H}(\control_{\timeInd})}_{\text{Entropy}}.
\end{align}
%\normalsize
#+END_EXPORT

# This work
# Consider the problem of predicting the next state $\state_{\timeInd+1}$
# given multivariate Normally distributed states
# ${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$
# and controls
# ${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$,
# where $\desiredDynamicsFunc \sim \mathcal{GP}$.



# Approximate closed-form solutions exist for propagating uncertain inputs through GP models
# citep:girardGaussian2003,kussGaussian2006,quinonero-candelaPropagation2003.
# \todo{correct girard citation}
# This work exploits the moment-matching approximation
# implemented in the =uncertain_conditional= from GPflow citep:GPflow2017.
# For more details on the approximation see Section 7.2.1 of citep:kussGaussian2006.
# \todo{Add moment matching figure?}
# This work propagates model uncertainty forwards by cascading such one-step predictions.
# This results in a factorised variational posterior,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# q(\stateTraj, \controlTraj \mid \state_0)
# = \prod_{\timeInd=0}^{\TimeInd-1} q(\state_{\timeInd+1} \mid \state_0) \controlVarDist.
# \end{align}
# #+END_EXPORT
# where each state and control are Normally
# distributed

**** Approximate Inference for Dynamics Predictions :noexport:
#+BEGIN_EXPORT latex
\renewcommand{\singleInput}{\ensuremath{\state_\timeInd, \control_\timeInd}}
\renewcommand{\singleInput}{\ensuremath{\hat{\state}_\timeInd}}
\renewcommand{\singleOutput}{\ensuremath{\Delta\state_{\timeInd+1}}}

\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput)}}
\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput)}}
%\newcommand{\expertInducingVariational}{\ensuremath{q(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\expertInducingVariational}{\ensuremath{q(\expertInducingOutput)}}
\newcommand{\expertsInducingVariational}{\ensuremath{q(\expertsInducingOutput)}}
\newcommand{\expertsVariational}{\ensuremath{q(\LatentFunc_\numData)}}
%\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \expertInducingOutput)}}
%\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))}}
\renewcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \singleInput, \expertInducingOutput)}}

\renewcommand{\expertVariational}{\ensuremath{q(\mode{\latentFunc}(\singleInput) \mid \singleInput)}}

\newcommand{\singleInputMean}{\ensuremath{\hat{\bm\mu}}}
\newcommand{\singleInputCov}{\ensuremath{\hat{\bm\Sigma}}}

%\newpage
#+END_EXPORT
The aformentioned trajectory optimisation technique relies on the ability to simulate the controlled
system using the learned dynamics model.
Recursively estimating the state of a nonlinear dynamical system is a common problem.
Exact Bayesian solutions in closed-form can only be obtained for a few special cases.
For example, the Kalman filter for linear Gaussian systems citep:kalmanNew1960 is exact.
In the nonlinear case, approximate methods are required to obtain efficient closed-form solutions.
\todo{cite Extended Kalman filter (EKF), Unscented Kalman filter (UKF) etc?}

Constructing approximate closed-form solutions based on the model in Chapter ref:chap-dynamics
is especially difficult, due to the exponential growth in the number of Gaussian components.
\marginpar{multimodal exponential growth}
Consider approximating the dynamics modes to be independent over a trajectory of length, $N$,
and recursively propagating each component through both modes.
This approximation would lead to the distribution over the final state consisting of
$\ModeInd^N$ Gaussian components.

# After the initial time step the state distribution will be a mixture of $\ModeInd$ modes.
# Assuming that the $\ModeInd$ components are independent and that each of the dynamics GPs are independent,
# propagating the components would result in a
# mode would result in the next state distribution containing $\ModInd^2$ modes.

# This is due to the explosion in the number of Gaussian components that would need to be modelled.

Luckily, this work is interested in remaining in a single dynamics mode $\desiredMode$,
which simplifies the problem.
Assuming that the controlled system remains in this desired dynamics mode,
the state trajectory can be simulated using only a single dynamics function,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\state_{\timeInd+1} &= \mode{\latentFunc}(\state_\timeInd, \control_\timeInd) + \state_\timeInd +  \mode{\epsilon}.
\end{align}
#+END_EXPORT
The model in Chapter ref:chap-dynamics represents each mode's dynamics function as a sparse variational GP,
giving the transition density as,
#+BEGIN_EXPORT latex
\begin{subequations}
\label{eq-sparse-gp-dynamics}
\begin{align}
p(\Delta\state_{\timeInd+1} \mid \singleInput, \modeVarK) &=
\E_{\expertVariational} \left[\singleExpertLikelihood \right] \label{eq-sparse-gp-dynamics-0} \\
\expertVariational &=  \E_{ \expertInducingVariational} \left[  \singleLatentExpertGivenInducing \right] \label{eq-sparse-gp-dynamics-1} \\
\expertInducingVariational &= \mathcal{N}\left( \expertInducingOutput \mid \mode{\mathbf{m}}, \mode{\mathbf{S}} \right)
\label{eq-sparse-gp-dynamics-2}
\end{align}
\end{subequations}
#+END_EXPORT
where the functional form of \expertVariational is given by,
#+BEGIN_EXPORT latex
\begin{align}
\expertVariational &=
\mathcal{N} \left( \mode{\latentFunc}(\singleInput) \mid
\mode{\mathbf{A}} \mode{\mathbf{m}},
\expertKernel(\singleInput, \singleInput)
+ (\mode{\mathbf{S}} - \expertKernel(\expertInducingInput,\expertInducingInput))
\mode{\mathbf{A}}^T
\right) \\
\mode{\mathbf{A}} &=
\expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}.
\end{align}
#+END_EXPORT
Remember that the variational posterior $\expertVariational$ is an approximation,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\expertVariational &\approx p(\mode{\latentFunc}(\singleInput) \mid \singleInput, \allInput, \allOutput),
\end{align}
#+END_EXPORT
that captures the joint distribution in the data through the inducing variables $\expertInducingOutput$.
Given a deterministic state-control input, $\singleInput$, cref:eq-sparse-gp-dynamics can be used
to calculate the density over the next state $\state_{\timeInd+1}$.
However, as both the state and control at a given time step could also be Gaussian distributed,
${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$,
and
${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$,
the joint distribution of the state and control, $p(\singleInput)$, is also Gaussian distributed,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\singleInput) = p(\state_{\timeInd}, \control_\timeInd) = p(\state_{\timeInd}) p(\control_\timeInd).
\end{align}
#+END_EXPORT

***** Predictions with Uncertain Inputs
Consider the problem of predicting the next state  $\state_{\timeInd+1}$
given multivariate Normally distributed states,
${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$,
and controls,
${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$,
where $\desiredDynamicsFunc \sim \mathcal{GP}$.
This prediction problem corresponds to calculating the following integral,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-state-unc-prop}
p(\state_{\timeInd+1}) = \int \int
p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd) p(\control_\timeInd) p(\state_\timeInd)
\text{d}\state_{\timeInd} \text{d}\control_{\timeInd}
\end{align}
#+END_EXPORT
Approximate closed-form solutions exist for propagating uncertain inputs through GP models
cite:girardGaussian2003,kussGaussian2006,quinonero-candelaPropagation2003.
\todo{correct girard citation}
This work exploits the moment-matching approximation
implemented in the =uncertain_conditional= from GPflow cite:GPflow2017.
For more details on the approximation see Section 7.2.1 of cite:kussGaussian2006.
\todo{Add moment matching figure?}
This work propagates model uncertainty forwards by cascading such one-step predictions.

# The transition density $\transitionDist$ can be obtained
# by recursive moment-matching
# This is the case when using a GP model to simulate more than a single time step.

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# q(\desiredDynamicsFunc(\singleInput) ) = \int  \underbrace{q(\desiredDynamicsFunc(\singleInput) \mid \singleInput)}_{\text{GP predictive dist}}
# p(\singleInput) \text{d}\singleInput.
# \end{align}
# #+END_EXPORT



# Importantly, given a trajectory where each state and control are normally distributed,
# ${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$
# and
# ${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$,
# the expected cost for a trajectory can be calculated in closed-form,

** Conclusion
This chapter has presented three /mode remaining/ trajectory optimisation algorithms.
The first two have shown how the geometry of the \acrshort{mosvgpe} gating network
infers valuable information regarding how a multimodal dynamical system switches between its underlying dynamics modes.
Moreover, they have show how this latent geometry can be leveraged to encode /mode remaining/ behaviour into
two different control strategies.
Both of these control strategies introduce a user tunable parameter $\lambda$ that can be
tuned to either prioritise remaining in a desired dynamics mode or avoiding regions of high
epistemic uncertainty.
However, cref:chap-traj-opt-results will show that setting the $\lambda$ parameter is not straightforward in practice.
The third method presented in this chapter has shown how the probability mass function over the \acrshort{mogpe}'s
expert indicator variable can be used to encode mode remaining behaviour for trajectory optimisation.
In particular, it has show how the control-as-inference framework citep:toussaintProbabilistic2006,kappenOptimal2013
can be extended to multimodal dynamical systems and how
mode remaining behaviour can be encoded by conditioning on the mode indicator variable.

In cref:chap-traj-opt-results the methods presented in this chapter are evaluated and compared using the
quadcopter navigation problem from the illustrative example in cref:illustrative_example.
In turns out that in practice, the direct optimal control via Riemannian energy approach from  cref:sec-traj-opt-energy
and the mode remaining control as probabilistic inference approach from cref:chap-traj-opt-inference,
perform significantly better than the indirect optimal control via latent geodesics method.
# The mode remaining control as probabilistic inference approach from cref:chap-traj-opt-inference also performs
# well in practice

# This chapter has shown how the geometry of the \acrshort{mosvgpe} gating network
# infers valuable information regarding how a multimodal dynamical system switches between its underlying dynamics modes.
# Moreover, it has presented two different control strategies that leverage this latent geometry to encode mode
# remaining behaviour.
# The control strategies introduce a user tunable parameter $\lambda$ that can be
# tuned to either prioritise remaining in a desired dynamics mode or avoiding regions of high
# epistemic uncertainty.
# Both methods presented in this chapter are evaluated in cref:chap-traj-opt-results.

# This chapter has shown how the probability mass function over the \acrshort{mogpe}'s
# expert indicator variable can be used to encode mode remaining behaviour for trajectory optimisation.
# In particular, it has show how the control-as-inference framework citep:toussaintProbabilistic2006,kappenOptimal2013
# can be extended to multimodal dynamical systems and how
# mode remaining behaviour can be encoded by conditioning on the mode indicator variable.
# In cref:chap-traj-opt-results
# the method is evaluated and compared to the geometry-based methods from cref:chap-traj-opt-geometry.

# The first method is an indirect optimal control method that projects the trajectory optimisation onto the geodesic ODE
# and recovers the control trajectory using a variational inference with latent variable inputs.
# The second method is a direct method that obtains


# In future work it would be interesting
# to explore active learning techniques that can avoid entering
# the undesired dynamics modes during data collection.

# shown how this latent geometry can be leveraged to encode mode remaining behaviour into
# two different control strategies.
# Both methods presented in this chapter are evaluated in cref:chap-traj-opt-results.

# Given a start and end state, both of the methods presented in this chapter can be tuned to find trajectories that either
# prioritise remaining in a desired dynamics mode, or prioritise avoiding regions of the learned
# dynamics model with high epistemic uncertainty.

# The direct optimal control algorithm from cref:sec-traj-opt-energy and the mode chance constraints from
# cref:eq-mode-chance-constraint are used in cref:chap-active-learning

# The control algorithms find trajectories from an initial state
# $\state_0$, to a target state $\targetState$, whilst remaining in a desired dynamics mode.

# The methods leverage the \acrshort{mosvgpe} model from cref:chap-dynamics to first learn a single-step dynamics model.
# This chapter has shown that the geometry of the \acrshort{mosvgpe} gating network
# infers valuable information regarding how a system switches between its underlying modes.
# Moreover, it has shown how this latent geometry can be leveraged to encode both mode remaining
# and risk-sensitive behaviour into control strategies.

# This chapter has shown that the geometry of the gating network from the \acrshort{mosvgpe} model from cref:chap-dynamics
# infers valuable information regarding how a system switches between its underlying modes.
# Further to this, it has detailed two trajectory optimisation algorithms that encode the goals via this
# latent geometry.
# That is, the algorithms find trajectories that remain in a desired dynamics mode,
# whilst also avoiding regions of high epistemic uncertainty.

# This chapter has presented a method for performing trajectory optimisation in
# multimodal dynamical systems with the transition dynamics modelled as a
# \acrshort{mogpe} method.
# The trajectory optimisation is projected onto a probabilistic Riemannian
# manifold parameterised by the gating network of the \acrshort{mogpe} model.
# Given a start and end state, the trajectory optimisation can be tuned to find trajectories that either
# prioritise remaining in a desired dynamics mode or prioritise avoiding regions of the learned transition
# dynamics model with high epistemic uncertainty.

** Conclusion :noexport:
This chapter has shown how the geometry of the \acrshort{mosvgpe} gating network
infers valuable information regarding how a multimodal dynamical system switches between its underlying dynamics modes.
Moreover, it has shown how this latent geometry can be leveraged to encode mode remaining behaviour into
two different control strategies.
Both methods have been evaluated on a quadcopter navigation problem.
Overall, the direct optimal control approach offers superior performance, as it finds trajectories that do not violate
the dynamics constraints, whilst ensuring trajectories are $\delta-\text{mode remaining}$.

# Given a start and end state, both of the methods presented in this chapter can be tuned to find trajectories that either
# prioritise remaining in a desired dynamics mode, or prioritise avoiding regions of the learned
# dynamics model with high epistemic uncertainty.

# The direct optimal control algorithm from cref:sec-traj-opt-energy and the mode chance constraints from
# cref:eq-mode-chance-constraint are used in cref:chap-active-learning

# The control algorithms find trajectories from an initial state
# $\state_0$, to a target state $\targetState$, whilst remaining in a desired dynamics mode.

# The methods leverage the \acrshort{mosvgpe} model from cref:chap-dynamics to first learn a single-step dynamics model.
# This chapter has shown that the geometry of the \acrshort{mosvgpe} gating network
# infers valuable information regarding how a system switches between its underlying modes.
# Moreover, it has shown how this latent geometry can be leveraged to encode both mode remaining
# and risk-sensitive behaviour into control strategies.

# This chapter has shown that the geometry of the gating network from the \acrshort{mosvgpe} model from cref:chap-dynamics
# infers valuable information regarding how a system switches between its underlying modes.
# Further to this, it has detailed two trajectory optimisation algorithms that encode the goals via this
# latent geometry.
# That is, the algorithms find trajectories that remain in a desired dynamics mode,
# whilst also avoiding regions of high epistemic uncertainty.

# This chapter has presented a method for performing trajectory optimisation in
# multimodal dynamical systems with the transition dynamics modelled as a
# \acrshort{mogpe} method.
# The trajectory optimisation is projected onto a probabilistic Riemannian
# manifold parameterised by the gating network of the \acrshort{mogpe} model.
# Given a start and end state, the trajectory optimisation can be tuned to find trajectories that either
# prioritise remaining in a desired dynamics mode or prioritise avoiding regions of the learned transition
# dynamics model with high epistemic uncertainty.

* Quadcopter Experiments - Mode Remaining Trajectory Optimisation label:chap-traj-opt-results
** maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\deltaTime}{\ensuremath{\Delta \timeInd}}
\newcommand{\env}[1]{\ensuremath{\hat{#1}}}
\newcommand{\modeProbTraj}{\ensuremath{\Pr(\allModeVarK \mid \stateTraj)}}

\newcommand{\windDrift}[1]{\ensuremath{\bm\omega_{#1}}}
\newcommand{\windTurbulence}[1]{\ensuremath{\bm\epsilon_{#1}}}
\newcommand{\windTurbulenceNoise}[1]{\ensuremath{\bm\Sigma_{\windTurbulence{#1}}}}
#+END_EXPORT
** intro :ignore:
The three /mode remaining/ trajectory optimisation algorithms presented in
cref:chap-traj-opt-control are evaluated
on the illustrative example from cref:illustrative_example, i.e.
flying a velocity controlled quadcopter from an initial state $\state_0$, to a target
state $\state_{f}$, whilst avoiding the turbulent dynamics mode.
The methods are further evaluated on a velocity controlled quadcopter navigation problem in a second
environment.
See cref:fig-environments for a schematic of the two environments.
The turbulent dynamics modes are subject to higher drift due to the wind field created by the fan.
It is also subject to higher diffusion (aka process noise) resulting from the turbulence induced by the fan.
Although the exact turbulent dynamics are not known, they are believed to be difficult to control.
This is due to the high process noise which may lead to catastrophic failure.
It is therefore desirable to find trajectories that avoid entering this turbulent dynamics mode.

#+BEGIN_EXPORT latex
\begin{figure}[h!]
    \centering
    \begin{minipage}[r]{0.49\columnwidth}
        \includegraphics[width=\textwidth]{./images/quadcopter-domain-collocation-ppt.png}
        \subcaption{Environment 1}
        \label{fig-environment-1}
    \end{minipage}
    \begin{minipage}[r]{0.49\columnwidth}
        %\includegraphics[width=\textwidth]{./images/point-mass-problem-statement-scenario-4.pdf}
        \includegraphics[width=\textwidth]{./images/quadcopter-domain-environment-2.png}
        \subcaption{Environment 2}
        \label{fig-environment-2}
    \end{minipage}
    \caption{Visualisation of two quadcopter navigation problems in environments with different spatially
    varying modes.
    It shows the desired dynamics mode in blue (Mode 1) and
    the turbulent dynamics mode induced by a fan in green (Mode 2).
    The white box indicates a region of the environment which was not observed.
    The goal is to navigate from the start state $\state_0$, to the target state $\targetState$, whilst remaining
    in the desired dynamics mode (Mode 1).}
    \label{fig-environments}
\end{figure}
#+END_EXPORT

# #+BEGIN_EXPORT latex
# \begin{figure}[h!]
# \centering
# \includegraphics[width=0.7\textwidth]{./images/quadcopter-domain-collocation-ppt.png}
# \caption{Diagram showing the real-world quadcopter navigation problem.
# It shows the desired dynamics mode in blue (Mode 1) and
# the turbulent dynamics mode induced by the fan at the right hand side of the room, in green (Mode 2).
# The white box indicates a region of the environment which was not observed.
# The goal is to navigate from the start state $\state_0$, to the target state $\targetState$, whilst remaining
# in the desired dynamics mode.}
# \label{fig-real-world-schematic}
# \end{figure}
# #+END_EXPORT
The state-space of the velocity controlled quadcopter example consists of the 2D Cartesian coordinates $\state = (x, y)$.
The controls consist of the speed in each direction, given by $\control = (\velocity_x, \velocity_y)$.

The collocation solver from the indirect geodesic control method in cref:chap-traj-opt-geometry
was tested on a real-world quadcopter problem.
Due to COVID-19 and limited access to the laboratory, only a constant controls data set was collected.
As such, the controls could not be recovered from the full transition dynamics model, nor could the
direct control method from cref:chap-traj-opt-geometry or the control-as-inference method from
cref:chap-traj-opt-inference be tested.
Nevertheless, the results are a small step towards validating the methods applicability to real-world systems.
All three control methods were then tested in two simulated environments so that they could be compared.
To aid comparison with the real-world experiments, the layout of one of the simulated environments  (Environment 1)
was kept consistent with the real-world experiments.

** Real-World Quadcopter Experiments label:sec-traj-opt-results-brl
*** intro :ignore:
The indirect optimal control via latent geodesics method presented in cref:sec-traj-opt-collocation was evaluated
using data from the real-world quadcopter navigation problem detailed in cref:sec-brl-experiment.
However, a different subset of the environment was not observed
and the model was trained using an old variational inference scheme, not the one presented in cref:chap-dynamics.
cref:fig-environment-1 shows the environment and details the quadcopter navigation problem.
The controls were kept constant during data collection, reducing the dynamics to
$\Delta \state_{\timeInd+1} = \dynamicsFunc(\state_{\timeInd}; \control_{\timeInd}=\fixedControl)$.
See cref:sec-brl-experiment for more details on data collection and processing.
# #+BEGIN_EXPORT latex
# \begin{figure}[h!]
# \centering
# \includegraphics[width=0.7\textwidth]{./images/quadcopter_bimodal_domain.pdf}
# \caption{Diagram showing the real-world quadcopter navigation problem.
# It shows (Mode 1) the turbulent dynamics mode (blue) induced by
# the fan at the right hand side of the room and (Mode 2) the desired dynamics mode (red) everywhere else.
# The hashed box indicates a region of the environment which was not observed.
# The goal is to navigate from the start state $\state_0$, to the target state $\targetState$, whilst remaining
# in the desired dynamics mode.}
# \label{fig-real-world-schematic}
# \end{figure}
# #+END_EXPORT

*** results figs :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[!t]
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=0.7\textwidth]{./images/geometric-traj-opt-over-prob.pdf}
\subcaption{Desired mode's mixing probability.}
\label{fig-geometric-traj-opt-over-prob}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/geometric-traj-opt-over-svgp.pdf}
\subcaption{GP posterior mean (left) and variance (right) over the desired mode's gating function.}
\label{fig-geometric-traj-opt-over-svgp}
\end{minipage}
\caption{\textbf{Indirect geodesic control} Trajectory optimisation results after solving the nonlinear
program in \cref{eq-collocation-problem}
using the desired mode's ($\modeVar=1$) gating function from the \acrshort{mosvgpe} (with $\ModeInd=2$ experts)
after training on the real-world velocity controlled quadcopter data set.
The intial (cyan) and optimised trajectories' -- for two settings of $\lambda$ -- are
overlayed on the desired mode's (\subref{fig-geometric-traj-opt-over-prob}) mixing probability and
(\subref{fig-geometric-traj-opt-over-svgp}) gating function GP posterior.}
\label{fig-geometric-traj-opt}
\end{figure}
#+END_EXPORT
*** Model Learning
The model from cref:chap-dynamics was instantiated with $\ModeInd=2$ experts and trained on
the data collected from the velocity controlled quadcopter experiment.
Each mode's dynamics \acrshort{gp} used a Squared Exponential kernel with \acrfull{ard} and a constant mean function.
The gating network used a single gating function with a Bernoulli likelihood.
Its \acrshort{gp} prior utilised a Squared Exponential kernel with \acrshort{ard} and a zero mean function.

cref:fig-geometric-traj-opt shows the gating network posterior where the model has clearly learned
two dynamics modes, characterised by the drift and process noise induced by the fan.
Mode 1 represents the operable dynamics mode whilst Mode 2 represents the inoperable turbulent dynamics mode.
This is illustrated in cref:fig-geometric-traj-opt-over-prob which shows the probability that
the desired mode ($\modeVar = 1$) governs the dynamics over the domain.
cref:fig-geometric-traj-opt-over-svgp shows the \acrshort{gp} posterior mean (left) and variance (right) of the
gating function $\gatingFunc_1$ associated with the desired dynamics mode.
The mean is high where the model believes the desired mode is responsible for predicting, low where it
believes another mode is responsible and zero where it is uncertain.
The variance (right) has also clearly captured information regrading the epistemic uncertainty,
i.e. where the model is uncertain which mode governs the dynamics.

*** Trajectory Optimisation using Indirect Optimal Control via Latent Geodesics

#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/geometric-traj-opt-prob-vs-time.pdf}
\subcaption{Desired mode's (\modeVar=1) mixing probability over the trajectories.}
\label{fig-mixing_prob_vs_time}
\end{minipage}
\begin{minipage}[r]{0.49\columnwidth}
\includegraphics[width=\textwidth]{./images/geometric-traj-opt-epistemic-vs-time.pdf}
%\label{}
\subcaption{Posterior variance associated with the desired mode's (\modeVar=1) gating function over the trajectories.}
\label{fig-epistemic_var_vs_time}
\end{minipage}
\caption{\textbf{Indirect geodesic control} Comparision of the intial and optimised trajectories' performance -- for two
settings of $\lambda$ -- at a) staying in the desired mode and b) avoiding regions of the gating network
with high epistemic uncertainty.}
\label{fig-metric-vs-time}
\end{figure}
#+END_EXPORT
# The indirect geodesic control method projects the trajectory optimisation onto the gating function associated with
# the desired mode.
# These experiments adopted a quadratic cost function to regularise the controls,
# Experiments with the indirect geodesic control method adopted a quadratic cost function to regularise the controls,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-quadratic-cost-control}
# \costFunc(\stateTraj, \controlTraj)
# &= \sum_{\timeInd=0}^{\TimeInd-1} \control_{\timeInd}^T \controlCostMatrix \control_{\timeInd}
# = \sum_{\timeInd=0}^{\TimeInd-1} \left\| \control_\timeInd \right\|_{\controlCostMatrix},
# \end{align}
# #+END_EXPORT
# where $\controlCostMatrix$ is a user defined, real symmetric positive definite weight matrix.
# It is common to use this cost function as it encodes minimising the expenditure of control effort.
# For example, minimising the total expenditure of fuel, or minimising the energy dissipated from the system.
*** Table :ignore:
#+LABEL: tab-results
#+CAPTION: *Indirect geodesic control* Comparison of performance with different settings of $\lambda$.
#+CAPTION: The performance measures are summed over collocation points.
#+attr_latex: :placement [!t]
|--------------------------+--------------------------------------------------+--------------------------------------------------------|
| Trajectory               | Mixing Probability                               | Epistemic Uncertainty                                  |
|                          | $\sum_{i=1}^{I} \Pr(\modeVar_i=1 \mid \state_i)$ | $\sum_{i=1}^{I} \mathbb{V}[\gatingFunc_{1}(\state_i)]$ |
|--------------------------+--------------------------------------------------+--------------------------------------------------------|
| Initial                  | $7.480$                                          | $1.345$                                                |
| Optimised $\lambda=20.0$ | $6.091$                                          | $\mathbf{1.274}$                                       |
| Optimised $\lambda=0.5$  | $\mathbf{8.118}$                                 | $1.437}$                                               |

*** After Table :ignore:
\newline

The initial (cyan) trajectory in cref:fig-geometric-traj-opt-over-svgp was initialised as a straight line with
$10$ collocation points, indicated by the crosses.
The collocation solver guarantees that trajectories end at the target state.
However, trajectories are not guaranteed to remain in the desired dynamics mode, nor are they guaranteed to
satisfy the systems dynamics.
cref:tab-results compares the initial trajectory to the results obtained
with two settings of the $\lambda$ parameter from cref:eq-expected-metric-weighting.

The higher probability of remaining in the desired dynamics mode indicates that
the lower setting of $\lambda=0.5$, exhibits more mode remaining behaviour.
This can be seen visually in cref:fig-geometric-traj-opt-over-prob, where the trajectory with $\lambda=0.5$
remains in regions of the model with high probability.
The right hand plot in cref:fig-geometric-traj-opt-over-svgp
shows that this trajectory favours remaining in the desired mode at the cost of entering the region
of the gating network with high epistemic uncertainty.
This is quantified in cref:tab-results, which shows it accumulates more gating function variance than both the
initial trajectory and the trajectory found with $\lambda=20.0$.

In contrast, the trajectory found with $\lambda=20.0$, initially remains in the desired mode
but then enters the turbulent mode in favour of avoiding the area of high epistemic uncertainty.
This is confirmed in cref:tab-results as the trajectory found with $\lambda=20.0$ accumulates the least gating
function variance over the trajectory.
This is further visualised in cref:fig-epistemic_var_vs_time,fig-mixing_prob_vs_time.
These results align with the intended behaviour of the user tunable $\lambda$ parameter.
However, in this experiment, increasing the relevance of the covariance term in the expected Riemannian
metric has no benefit.
This is because it pushes the trajectory into the turbulent dynamics mode.

These results indicate that the nonlinear program in cref:eq-collocation-problem,
i.e. solving the geodesic ODE in cref:eq-2ode via collocation, is capable of finding state
trajectories from $\state_0$ to $\targetState$ that exhibit mode remaining behaviour.
However, these experiments have not validated the methods ability to recover the controls from the state trajectory
using cref:eq-control-elbo.
This is because a full transition dynamics model has not been learned so the control trajectory cannot be recovered.
In cref:sec-traj-opt-results-simulated the method is evaluated in a simulated environment
where its ability to recover the controls is tested.

It is worth noting here that the mode remaining behaviour is
sensitive to the tolerance on the collocation constraints.
Setting the tolerance too small can result in the solver failing to converge, whilst setting the tolerance too
large often omits any mode remaining behaviour.
# In practice, this severely limits the practicality of the method.

# These experiments do not test the methods ability to recover the controls using cref:eq-control-elbo.
# This is because a full transition dynamics model has not been learned so the control trajectory cannot be recovered.
# In cref:sec-traj-opt-results-energy the method is evaluated in a simulated environment
# where its ability to recover the controls is tested.


# remain
# navigates to the left, away from the fan, and as a res


# is clear from cref:fig-geometric-traj-opt,fig-metric-vs-time
# that for $\lambda=0.5$,
# trajectories favour remaining in the desired mode at the cost of entering regions of the gating network
# with high epistemic uncertainty.


# It is clear from cref:fig-geometric-traj-opt,fig-metric-vs-time
# that for $\lambda=0.5$,
# trajectories favour remaining in the desired mode at the cost of entering regions of the gating network
# with high epistemic uncertainty.
# For $\lambda=20$, the trajectory initially remains in the desired mode
# but then sacrifices entering the turbulent mode in favour of avoiding the area of high epistemic
# uncertainty.
# These results align with the intended behaviour of the user tunable $\lambda$ parameter.
# However, in this experiment, increasing the relevance of the covariance term in the expected Riemannian
# metric has no benefit.
# This is because it pushes the trajectory into the turbulent dynamics mode.
# It is also worth noting here that the mode remaining behaviour is
# sensitive to the tolerance on the collocation constraints.
# Setting the tolerance too small can result in the solver failing to converge, whilst setting the tolerance too
# large often omits any mode remaining behaviour.

** Simulated Quadcopter Experiments label:sec-traj-opt-results-simulated
*** intro :ignore:
# The constant control data set in cref:sec-brl-experiment cannot be used for the
# method in cref:sec-traj-opt-energy as it requires a data set with controls.
# Due to COVID-19 and limited access to the laboratory, the direct optimal control method in cref:sec-traj-opt-energy
# was tested in two simulated environments.
All of the control methods were then tested in two simulated environments so that they could be compared.
This section first details the simulation environments before presenting the results for each
environment.
# To aid comparison with the real-world experiments, the layout of Environment 1 was kept consistent
# with the real-world experiments.


# This is because the data set in cref:sec-brl-experiment was collected with constant controls and
# the method in cref:sec-traj-opt-energy requires a data set with controls.

# To aid with comparison, the two control methods are tested and evaluated in a simulated environment.
# The simulation environment represents the velocity controlled quadcopter in an environment subject to spatially
# varying wind, constituting both drift and diffusion components.

*** Simulator Setup
The simulated environments have two dynamics modes, $\modeVar \in \{1, 2\}$, whose
transition dynamics are given by simple Newtonian dynamics (velocity times time).
The modes are induced by different wind fields which are characterised by their
drift $\windDrift{\modeInd}$ and process noise $\windTurbulence{\modeInd}$ terms.
Each mode's dynamics are given by,
#+BEGIN_EXPORT latex
\begin{align}
\mode{\dynamicsFunc}(\state_\timeInd, \control_\timeInd; \Delta\timeInd=0.25)
&= \control_{\timeInd} \times \Delta\timeInd + \windDrift{\modeInd} + \windTurbulence{\modeInd} \\
%\quad \text{if} \quad \modeVar_\timeInd = \modeInd \\
\windTurbulence{\modeInd} &\sim \mathcal{N}(\bm0, \windTurbulenceNoise{\modeInd}).
\end{align}
#+END_EXPORT
The state domain is constrained to $x, y \in [-3, 3]$ and min/max controls are implemented by constraining the
control domain to $\velocity_x, \velocity_y \in [-5, 5]$.


# The modes have the same $y$ drift but Mode 2 has 100 times higher drift in the negative $x$.
# Further to this, Mode 2 is subject to much higher process noise.
# As such, Mode 1 represents the operable, desired dynamics mode and Mode 2 represents the turbulent dynamics mode.

# with
# $\windDrift{1} = \{0.02, 0.4\}$, $\windDrift{2} = \{-2.0, -0.4\}$,
# $\windTurbulenceNoise{1} = \diag\left([0.0001, 0.0002]\right)$ and
# $\windTurbulenceNoise{2} = \diag\left([0.2, 0.05]\right)$.
# The modes have the same $y$ drift but Mode 2 has 100 times higher drift in the negative $x$.
# Further to this, Mode 2 is subject to much higher process noise.
# As such, Mode 1 represents the operable, desired dynamics mode and Mode 2 represents the turbulent dynamics mode.

# The mode indicator variable $\modeVar \in \{1, 2\}$, determines where each mode governs the dynamics.
# #+BEGIN_EXPORT latex
# \begin{figure}
# \centering
# \begin{minipage}[r]{\columnwidth}
# \centering
# \includegraphics[width=0.7\textwidth]{./images/mode-opt/env/scenario_7/env_with_dataset_start_end_pos.pdf}
# \subcaption{\textbf{Environment 1} - $\windDrift{1} = \{0.02, -0.4\}$, $\windDrift{2} = \{-2.0, -0.4\}$,
# $\windTurbulenceNoise{1} = \diag\left([0.0002, 0.0001]\right)$ and
# $\windTurbulenceNoise{2} = \diag\left([0.2, 0.05]\right)$}
# \label{fig-dataset-scenario-7}
# \end{minipage}
# \begin{minipage}[r]{\columnwidth}
# \centering
# \includegraphics[width=0.7\textwidth]{./images/mode-opt/env/scenario_5/env_with_dataset_start_end_pos.pdf}
# \subcaption{\textbf{Environment 2} - $\windDrift{1} = \{0.4, 0.02\}$, $\windDrift{2} = \{0.4, -2.0\}$,
# $\windTurbulenceNoise{1} = \diag\left([0.0001, 0.0002]\right)$ and
# $\windTurbulenceNoise{2} = \diag\left([0.05, 0.2]\right)$}
# \label{fig-dataset-scenario-5}
# \end{minipage}
# \caption{Visualisation of two multimodal environments ($\ModeInd=2$), with mode boundaries indicated
# by the purple lines.
# A state transition data set has been sampled from each environment and is visualised by the quiver plots.
# The initial $\state_0$ and target states $\targetState$ for the trajectory optimisation are overlayed.}
# \label{fig-dataset-scenario}
# \end{figure}
# #+END_EXPORT
$4000$ state transitions were sampled from each environment with $\Delta \timeInd = 0.25s$.
A subset of the state transitions were then removed to induce a region of high epistemic uncertainty in the
learned dynamics model. This enable the control methods ability to avoid regions of high epistemic uncertainty
to be tested.
# cref:fig-dataset-scenario shows the state transition data sets and also indicates the separation of the modes.

# The simulated environment is instantiated with the gating mask in cref:fig-dataset-scenrio-7,
# where the red region indicates the desired (operable) dynamics mode $\modeVar=1$ and the white region
# indicates the turbulent dynamics mode $\modeVar=2$.
# #+BEGIN_EXPORT latex
# \begin{figure}[h!]
# \centering
# \includegraphics[width=0.9\textwidth]{./images/mode-opt/env/scenario_7/env_with_dataset_start_end_pos.pdf}
# \caption{\textbf{State transition data set} visualised as a quiver plot with
# the start/end states and the mode boundary overlayed.}
# \label{fig-dataset-scenrio-7}
# \end{figure}
# #+END_EXPORT

*** Model Learning
# To begin with, $\NumData$ state transitions were sampled from the simulator with $\Delta \timeInd = 0.25s$.
Following the experiments in cref:sec-traj-opt-results-brl,
the model from cref:chap-dynamics was instantiated with $\ModeInd=2$ experts, one to
represent the desired dynamics mode and one to represent the turbulent dynamics mode.
Each mode's dynamics GP used a Squared Exponential kernel with \acrshort{ard} and a zero mean function.
The gating network used a single gating function with
a Squared Exponential kernel with \acrshort{ard} and a zero mean function.
In contrast to the real-world experiments, the simulated experiments presented here learn the full transition
dynamics model, i.e. they do not assume constant controls.

*** Performance Indicators
This section evaluates the performance of trajectories using four performance indicators:
1. Probability of remaining in the desired mode with the model's uncertainty marginalised,
   calculated using cref:eq-mode-chance-constraint-integral,
   #+BEGIN_EXPORT latex
   \begin{align} \label{eq-mode-probability-all-unc}
   \sum_{\timeInd=1}^{\TimeInd}
   \Pr(\modeVar_\timeInd=\desiredMode \mid \state_0, \control_{0:\timeInd}, \modeVar_{0:\timeInd-1}=\desiredMode).
   \end{align}
   #+END_EXPORT
   This probability will decrease when a trajectory leaves the desired mode and when it
   passes through regions of the learned dynamics with high uncertainty.
2. Probability of remaining in the desired mode without the model's uncertainty marginalised,
   #+BEGIN_EXPORT latex
   \begin{align} \label{eq-mode-probability-no-unc}
   \sum_{\timeInd=0}^{\TimeInd} \Pr(\modeVar_\timeInd=\desiredMode \mid \gatingFunc(\state_{0:\timeInd}), \state_{0:\timeInd}, \control_{0:\timeInd}, \modeVar_{0:\timeInd-1}=\desiredMode)
   \end{align}
   #+END_EXPORT
   This probability will only decrease when a trajectory leaves the desired mode.
   # #+BEGIN_EXPORT latex
   # \begin{align} \label{eq-mode-probability-all-unc}
   # \sum_{\timeInd=0}^{\TimeInd} \Pr(\modeVar_\timeInd=\desiredMode \mid \state_0, \control_{0:\timeInd})
   # \end{align}
   # #+END_EXPORT
   # - The indirect optimal control method calculates the probability of being in the desired dynamics mode under the
   #   MoSVGPE model,
   #  #+BEGIN_EXPORT latex
   #  \begin{align}
   #  \sum_{\timeInd=1}^{\TimeInd}
   #   \Pr(\modeVar_\timeInd=\desiredMode \mid \state_{0:\timeInd}, \control_{0:\timeInd}, \modeVar_{0:\timeInd-1}=\desiredMode)
   #  \end{align}
   #  #+END_EXPORT
   # - The direct optimal control method also calculates the probability of being in the desired dynamics mode
   #   but it also marginalises the state uncertainty along the trajectory,
3. The state variance accumulated from cascading single-step predictions,
   #+BEGIN_EXPORT latex
    \begin{align} \label{eq-metric-state-var}
    \sum_{\timeInd=1}^{\TimeInd} \mathbb{V}[\state_{\timeInd}].
    \end{align}
   #+END_EXPORT
   This will increase when a trajectory passes through regions of the desired dynamics mode with high uncertainty.
4. The gating function variance accumulated from cascading single-step predictions,
   #+BEGIN_EXPORT latex
    \begin{align} \label{eq-metric-gating-var}
    \sum_{\timeInd=1}^{\TimeInd}
    \mathbb{V}[\gatingFunc_{\desiredMode}(\state_{\timeInd})].
    \end{align}
   #+END_EXPORT
   This will increase when a trajectory passes through regions of the gating network with high uncertainty.
Intuitively, the goal is to maximise the probability of being in the desired mode,
whilst minimising the variance accumulated over a trajectory, i.e. maximise cref:eq-mode-probability-all-unc.
To test each methods ability at avoiding regions of high epistemic uncertainty,
a subset of the environment was intentionally not observed.

# This section evaluates the performance of the two algorithms at achieving the goals:
# - Goal 1 :: Remain in the operable, desired dynamics mode $\desiredMode$,
# - Goal 2 :: Avoid regions of the learned dynamics with high /epistemic uncertainty/,
#   - in the desired dynamic mode.
#   - in the gating network,

# The indirect geodesic control method from cref:sec-traj-opt-collocation is evaluated on the real-world
# quadcopter data set detailed in cref:sec-brl-experiment.
# Due to coronavirus and limited access to the laboratory, the direct optimal control method in cref:sec-traj-opt-energy
# is tested in a simulated version of the illustrative example from cref:illustrative_example.
# This is because the data set in cref:sec-brl-experiment was collected with constant controls and
# the method in cref:sec-traj-opt-energy requires a data set with controls.
# Nevertheless, results on the real-world quadcopter are included to demonstrate the applicability of this method
# to real-world robotic systems.

\todo{Add section looking at GP Jacobian and metric tensor for one of the experiments?}

*** Environment 1
**** Simulator :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{./images/mode-opt/env/scenario_7/env_with_dataset_start_end_pos.pdf}
\caption{\textbf{Environment 1} - Visualisation of Environment 1 with its mode boundary
indicated by the purple lines.
A state transition data set has been sampled from each environment and is visualised by the quiver plots.
The initial $\state_0$ and target states $\targetState$ for the trajectory optimisation are overlayed.
$\windDrift{1} = \{0.02, -0.4\}$, $\windDrift{2} = \{-2.0, -0.4\}$,
$\windTurbulenceNoise{1} = \diag\left([0.0002, 0.0001]\right)$ and
$\windTurbulenceNoise{2} = \diag\left([0.2, 0.05]\right)$}
\label{fig-dataset-scenario-7}
\end{figure}
#+END_EXPORT
**** Learned model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[h!]
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=0.98\textwidth]{./images/mode-opt/env/scenario_7/mosvgpe/mixing_probs_no_obs.pdf}
\subcaption{Mixing probabilities}
\label{eq-traj-opt-gating-network-prob-post-7}
\end{minipage}
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=0.98\textwidth]{./images/mode-opt/env/scenario_7/mosvgpe/desired_gating_gp_no_obs.pdf}
\subcaption{Gating function's GP posterior}
\label{eq-traj-opt-gating-network-gp-post-7}
\end{minipage}
\caption{\textbf{Environment 1} Visualisation of the gating network posterior after training \acrshort{mosvgpe}
on the state transition data set from Environment 1.
(\subref{eq-traj-opt-gating-network-prob-post-7}) shows the probability mass function over the expert indicator
variable and (\subref{eq-traj-opt-gating-network-gp-post-7}) shows the gating function's GP
posterior mean (left) and posterior variance (right).
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line)
and the subset of the environment which has not been observed (hashed box).}
\label{fig-traj-opt-gating-network-7}
\end{figure}
#+END_EXPORT

**** Blah :ignore:
The three control methods were first tested in a simulated representation of the real-world environment, named
Environment 1.
cref:fig-dataset-scenario-7 shows the state transition data set that was sampled from Environment 1 and
used to train the \acrshort{mosvgpe} dynamics model.
cref:fig-traj-opt-gating-network-7 shows the gating network posterior after performing inference.
Expert 1 represents the turbulent dynamics mode and Expert 2 represents the operable, desired dynamics mode.
This results from Expert 2 learning much higher drift and process noise than Expert 1.


Three settings of the tunable $\lambda$ parameter --
which determines the relevance of the covariance term in the expected Riemannian metric -- were tested for
each of the geometry-based methods.
The control-as-inference method in cref:sec-traj-opt-inference is tested with and without maximum entropy behaviour,
i.e. with Gaussian and deterministic controls.
All experiments in Environment 1 used a $\TimeInd=20$ step horizon.
cref:fig-geodesic-low-traj-opt-7,fig-geodesic-traj-opt-7,fig-geodesic-high-traj-opt-7
show the results using the indirect geodesic control method in cref:sec-traj-opt-collocation,
cref:fig-riemannian-energy-low-traj-opt-7,fig-riemannian-energy-traj-opt-7,fig-riemannian-energy-high-traj-opt-7
show the results using the direct optimal control method in cref:sec-traj-opt-energy and
cref:fig-mode-conditioning-max-entropy-traj-opt-7,fig-mode-conditioning-no-max-entropy-traj-opt-7
show the results using  the control-as-inference method in cref:sec-traj-opt-inference.
The top row of each figure shows the optimised trajectories overlayed on each mode's mixing probability.
This is useful for seeing how well trajectories remain in regions of the learned dynamics model with high
probability of being in the desired mode.
The bottom row of each figure shows the trajectories overlayed on the gating function's posterior
mean (left) and posterior variance (right).
As the geometry-based methods in this chapter are based on finding shortest trajectories on this manifold,
the posterior mean plot is useful for observing contour following behaviour.
Similarly, as the methods should find trajectories that avoid regions of the gating network with
high epistemic uncertainty, overlaying the trajectories on the posterior variance is useful for seeing
this behaviour.

To further help visualise the results, the optimised controls are rolled out in
the desired mode's dynamics GP (magenta) and in the environment (cyan).
Deviation between these two trajectories is undesirable as it indicates that the controls,
1. drive the system out of the desired mode, or,
2. drive the system into regions of the learned model with high epistemic uncertainty.
For the indirect geodesic control experiments, the state trajectory found by the collocation solver
is also overlayed (yellow).
cref:tab-results-sim-envs summarises all of the results from the simulated experiments.

# In the indirect geodesic control experiments, the yellow trajectory represents the state trajectory found by the
# collocation solver on the geodesic ODE.

**** Table :ignore:noexport:

#+begin_table
#+LATEX: \caption{\textbf{Results in simulated environments} Comparison of the direct geodesic control method from \cref{sec-traj-opt-collocation} and the  indirect optimal control method from \cref{sec-traj-opt-energy}, in the two simulated environments, when using different settings of $\lambda$. The performance measures are summed over the collocation points for the indirect geodesic control method and over each time step for the direct optimal control method. The "Mode prob" column calculates the probability at each time step without marginalising the state and gating function uncertainty whereas the "Mode prob with uncertainty" marginalises both the state and gating function uncertainty at each time step.}
#+LATEX: \label{tab-results-sim-envs}
#+LATEX: \resizebox{\textwidth}{!}{
#+ATTR_LATEX: :center t :placement [!b] :align clcccccc
|-----+---------------------------+-----------+---------------------------------+----------------------------------+--------------------------+------------------------------------------------------+--------------------------------------------------+---|
| Env | Method                    | $\lambda$ | Mode prob                       | Mode prob with uncertainty       | State uncertainty        | Gating uncertainty                                   | Riemannian energy                                |   |
|     |                           |           | cref:eq-mode-probability-no-unc | cref:eq-mode-probability-all-unc | $\mathbb{V}[\stateTraj]$ | $\mathbb{V}[\gatingFunc_{\desiredMode}(\stateTraj)]$ | cref:eq-approximate-trajectory-riemannian-energy |   |
|-----+---------------------------+-----------+---------------------------------+----------------------------------+--------------------------+------------------------------------------------------+--------------------------------------------------+---|
|     | Direct                    | $0.5$     | $16.96$                         | $16.22$                          | $8.69$                   | $346.62$                                             | $238.29$                                         |   |
|     | Direct                    | $1.0$     | $18.51$                         | $16.98$                          | $8.70$                   | $367.16$                                             | $222.07$                                         |   |
|   1 | Direct                    | $20.0$    | $15.32$                         | $14.62$                          | $\mathbf{8.66}$          | $201.45$                                             | $627.74$                                         |   |
|     | Indirect                  | $0.5$     | $20.91$                         | $16.77$                          | $8.70$                   | $404.51$                                             | $180.10$                                         |   |
|     | Indirect                  | $1.0$     | $\mathbf{20.98}$                | $\mathbf{18.10}$                 | $8.72$                   | $388.51$                                             | $\mathbf{173.84}$                                |   |
|     | Indirect                  | $20.0$    | $12.64$                         | $13.00$                          | $8.69$                   | $\mathbf{157.83}$                                    | $349.08$                                         |   |
|-----+---------------------------+-----------+---------------------------------+----------------------------------+--------------------------+------------------------------------------------------+--------------------------------------------------+---|
|     | Indirect                  | $0.01$    | $\mathbf{20.98}$                | $\mathbf{20.80}$                 | $1.79$                   | $497.47$                                             | $275.00$                                         |   |
|     | Indirect                  | $0.5$     | $20.96$                         | $20.34$                          | $1.42$                   | $367.47$                                             | $182.06$                                         |   |
|   2 | Indirect                  | $1.0$     | $20.98$                         | $20.68$                          | $\mathbf{1.28}$          | $\mathbf{334.47}$                                    | $144.16$                                         |   |
|     | Indirect                  | $5.0$     | $20.81$                         | $20.13$                          | $1.37$                   | $374.13$                                             | $\mathbf{134.33}$                                |   |
#+LATEX: }
#+end_table

**** Table :ignore:
#+begin_table
#+LATEX: \caption{\textbf{Results in simulated environments} Comparison of the  indirect optimal control method from \cref{sec-traj-opt-collocation}, the direct geodesic control method from \cref{sec-traj-opt-energy} and the control-as-inference method from \cref{sec-traj-opt-inference}. All methods are evaluated in the two simulated environments. The performance measures are summed over the collocation points for the indirect geodesic control method and over each time step for the direct optimal control method and the control-as-inference method. The "Mode prob" column calculates the probability at each time step without marginalising the state and gating function uncertainty whereas the "Mode prob with uncertainty" marginalises both the state and gating function uncertainty at each time step.}
#+LATEX: \label{tab-results-sim-envs}
#+LATEX: \resizebox{\textwidth}{!}{
#+ATTR_LATEX: :center t :placement [!b] :align clcccccc
|-----+---------------------------+-----------+---------------------------------+----------------------------------+--------------------------+------------------------------------------------------+--------------------------------------------------+---|
| Env | Method                    | $\lambda$ | Mode prob                       | Mode prob with uncertainty       | State uncertainty        | Gating uncertainty                                   | Riemannian energy                                |   |
|     |                           |           | cref:eq-mode-probability-no-unc | cref:eq-mode-probability-all-unc | $\mathbb{V}[\stateTraj]$ | $\mathbb{V}[\gatingFunc_{\desiredMode}(\stateTraj)]$ | cref:eq-approximate-trajectory-riemannian-energy |   |
|-----+---------------------------+-----------+---------------------------------+----------------------------------+--------------------------+------------------------------------------------------+--------------------------------------------------+---|
|     | Direct                    | $0.5$     | $16.96$                         | $16.22$                          | $8.69$                   | $346.62$                                             | $238.29$                                         |   |
|     | Direct                    | $1.0$     | $18.51$                         | $16.98$                          | $8.70$                   | $367.16$                                             | $222.07$                                         |   |
|   1 | Direct                    | $20.0$    | $15.32$                         | $14.62$                          | $\mathbf{8.66}$          | $201.45$                                             | $627.74$                                         |   |
|     | Indirect                  | $0.5$     | $20.91$                         | $16.77$                          | $8.70$                   | $404.51$                                             | $180.10$                                         |   |
|     | Indirect                  | $1.0$     | $\mathbf{20.98}$                | $18.10$                          | $8.72$                   | $388.51$                                             | $\mathbf{173.84}$                                |   |
|     | Indirect                  | $20.0$    | $12.64$                         | $13.00$                          | $8.69$                   | $\mathbf{157.83}$                                    | $349.08$                                         |   |
|     | Inference                 | N/A       | $20.94$                         | $\mathbf{19.99}$                 | $8.74$                   | $208.62$                                             | $287.52$                                         |   |
|     | Inference (deterministic) | N/A       | $20.96$                         | $19.58$                          | $8.73$                   | $258.27$                                             | $237.32$                                         |   |
|-----+---------------------------+-----------+---------------------------------+----------------------------------+--------------------------+------------------------------------------------------+--------------------------------------------------+---|
|     | Indirect                  | $0.01$    | $\mathbf{20.98}$                | $\mathbf{20.80}$                 | $1.79$                   | $497.47$                                             | $275.00$                                         |   |
|     | Indirect                  | $0.5$     | $20.96$                         | $20.34$                          | $1.42$                   | $367.47$                                             | $182.06$                                         |   |
|   2 | Indirect                  | $1.0$     | $\mathbf{20.98}$                | $20.68$                          | $\mathbf{1.28}$          | $\mathbf{334.47}$                                    | $144.16$                                         |   |
|     | Indirect                  | $5.0$     | $20.81$                         | $20.13$                          | $1.37$                   | $374.13$                                             | $\mathbf{134.33}$                                |   |
|     | Inference                 | N/A       | $\mathbf{20.98}$                | $20.72$                          | $1.75$                   | $596.67$                                             | $442.88$                                         |   |
|     | Inference (deterministic) | N/A       | $\mathbf{20.98}$                | $20.61$                          | $1.56$                   | $644.35$                                             | $534.39$                                         |   |
#+LATEX: }
#+end_table

**** Geodesic collocation 0.5 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/geodesic-collocation-low/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-geodesic-low-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/geodesic-collocation-low/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-geodesic-low-traj-opt-over-svgp-7}
\end{minipage}
\caption{\textbf{Indirect geodesic control} $\bm\lambda\mathbf{=0.5}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$, using the indirect geodesic control method from \cref{sec-traj-opt-collocation}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The optimised state trajectory from the collocation solver is indicated in yellow.
The trajectories are overlayed on (\subref{fig-geodesic-low-traj-opt-over-prob-7})
each mode's mixing probability
and (\subref{fig-geodesic-low-traj-opt-over-svgp-7})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.}
\label{fig-geodesic-low-traj-opt-7}
\end{figure}
#+END_EXPORT

**** Geodesic collocation 1.0 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/geodesic-collocation/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-geodesic-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/geodesic-collocation/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-geodesic-traj-opt-over-svgp-7}
\end{minipage}
\caption{\textbf{Indirect geodesic control} $\bm\lambda\mathbf{=1.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$, using the indirect geodesic control method from \cref{sec-traj-opt-collocation}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The optimised state trajectory from the collocation solver is indicated in yellow.
The trajectories are overlayed on (\subref{fig-geodesic-traj-opt-over-prob-7})
each mode's mixing probability
and (\subref{fig-geodesic-traj-opt-over-svgp-7})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.}
\label{fig-geodesic-traj-opt-7}
\end{figure}
#+END_EXPORT

**** Geodesic collocation 20 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/geodesic-collocation-high/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-geodesic-high-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/geodesic-collocation-high/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-geodesic-high-traj-opt-over-svgp-7}
\end{minipage}
\caption{\textbf{Indirect geodesic control} $\bm\lambda\mathbf{=20.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$, using the indirect geodesic control method from \cref{sec-traj-opt-collocation}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The optimised state trajectory from the collocation solver is indicated in yellow.
The trajectories are overlayed on (\subref{fig-geodesic-high-traj-opt-over-prob-7})
each mode's mixing probability
and (\subref{fig-geodesic-high-traj-opt-over-svgp-7})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.}
\label{fig-geodesic-high-traj-opt-7}
\end{figure}
#+END_EXPORT

**** Riemannian energy 0.5 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/riemannian-energy-low/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/riemannian-energy-low/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-traj-opt-over-svgp-7}
\end{minipage}
\caption{\textbf{Riemannian energy} $\bm\lambda\mathbf{=0.5}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the Riemannian energy cost function from \cref{eq-approximate-trajectory-riemannian-energy}
with $\lambda=0.5$
and deterministic controls (no maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-riemannian-energy-low-traj-opt-over-prob-7})
each mode's mixing probability
and (\subref{fig-riemannian-energy-low-traj-opt-over-svgp-7})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-riemannian-energy-low-traj-opt-7}
\end{figure}
#+END_EXPORT

**** Riemannian energy 1 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/riemannian-energy/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-riemannian-energy-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/riemannian-energy/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-riemannian-energy-traj-opt-over-svgp-7}
\end{minipage}
\caption{\textbf{Riemannian energy} $\bm\lambda\mathbf{=1.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the approximate Riemannian energy cost function from \cref{eq-approximate-trajectory-riemannian-energy}
with $\lambda=1.0$.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-riemannian-energy-traj-opt-over-prob-7})
each mode's mixing probability
and (\subref{fig-riemannian-energy-traj-opt-over-svgp-7})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-riemannian-energy-traj-opt-7}
\end{figure}
#+END_EXPORT

**** Riemannian energy 20 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/riemannian-energy-high/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/riemannian-energy-high/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-riemannian-energy-high-traj-opt-over-svgp-7}
\end{minipage}
\caption{\textbf{Riemannian energy} $\bm\lambda\mathbf{=20.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the Riemannian energy cost function from \cref{eq-approximate-trajectory-riemannian-energy}
with $\lambda=20.0$
and deterministic controls (no maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-riemannian-energy-high-traj-opt-over-prob-7})
each mode's mixing probability
and (\subref{fig-riemannian-energy-high-traj-opt-over-svgp-7})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-riemannian-energy-high-traj-opt-7}
\end{figure}
#+END_EXPORT

**** Mode Conditioning max entropy :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/control-as-inference/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-mode-conditioning-max-entropy-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{0.99\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/control-as-inference/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-mode-conditioning-max-entropy-traj-opt-over-svgp-7}
\end{minipage}
\caption{\textbf{Mode conditioning ELBO with maximum entropy control}
Trajectory optimisation results finding trajectories from the start state $\state_0$,
to the target state $\targetState$, after solving \cref{eq-traj-opt-inference} with Gaussian controls
(maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The trajectories are overlayed on (\subref{fig-mode-conditioning-max-entropy-traj-opt-over-prob-7})
each mode's mixing probability
and (\subref{fig-mode-conditioning-max-entropy-traj-opt-over-svgp-7})
the posterior mean (left) and posterior variance (right) associated with desired mode's gating function.}
\label{fig-mode-conditioning-max-entropy-traj-opt-7}
\end{figure}
#+END_EXPORT

**** Mode Conditioning NO max entropy :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/control-as-inference-deterministic/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-mode-conditioning-no-max-entropy-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{0.99\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_7/control-as-inference-deterministic/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-mode-conditioning-no-max-entropy-traj-opt-over-svgp-7}
\end{minipage}
\caption{\textbf{Mode conditioning ELBO without maximum entropy control}
Trajectory optimisation results finding trajectories from the start state $\state_0$,
to the target state $\targetState$, after solving \cref{eq-traj-opt-inference} with deterministic controls
(no maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-mode-conditioning-no-max-entropy-traj-opt-over-prob-7})
each mode's mixing probability
and (\subref{fig-mode-conditioning-no-max-entropy-traj-opt-over-svgp-7})
the posterior mean (left) and posterior variance (right) associated with desired mode's gating function.}
\label{fig-mode-conditioning-no-max-entropy-traj-opt-7}
\end{figure}
#+END_EXPORT

**** target state :ignore:
\newline

*Target state*
The methods are first evaluated at their ability to navigate to the target state.
All experiments in Environment 1 were able to find trajectories that navigate to the target state under the
desired mode's GP dynamics.
This is indicated by the magenta trajectory in
cref:fig-riemannian-energy-low-traj-opt-7,fig-riemannian-energy-traj-opt-7,fig-riemannian-energy-high-traj-opt-7,fig-geodesic-low-traj-opt-7,fig-geodesic-traj-opt-7,fig-geodesic-high-traj-opt-7,fig-mode-conditioning-max-entropy-traj-opt-7,fig-mode-conditioning-no-max-entropy-traj-opt-7
successfully navigating to the target state $\targetState$.
The state trajectory found by the collocation solver is guaranteed to satisfy the boundary conditions
in the indirect geodesic control experiments
(cref:fig-geodesic-low-traj-opt-7,fig-geodesic-traj-opt-7,fig-geodesic-high-traj-opt-7).
This is because it is a property of the collocation solver.
The inference strategy has then successfully recovered controls that drive the system along the
collocation solver's state trajectory (yellow),
indicated by the dynamics trajectory (magenta) following the collocation trajectory (yellow).

All of the direct optimal control and control-as-inference experiments shown in
cref:fig-riemannian-energy-low-traj-opt-7,fig-riemannian-energy-traj-opt-7,fig-riemannian-energy-high-traj-opt-7,fig-mode-conditioning-max-entropy-traj-opt-7,fig-mode-conditioning-no-max-entropy-traj-opt-7
were also able to find trajectories that navigate to the target state under the learned dynamics (magenta).
Although these methods do not guarantee that trajectories will satisfy the boundary conditions,
in practice, setting the terminal state cost matrix $\terminalStateCostMatrix$ to be very high,
appears to work well.

Although all of the dynamics trajectories (magenta) navigate to the target state, not all of
their corresponding environment trajectories (cyan) do.
This is due to the trajectories in
cref:fig-riemannian-energy-high-traj-opt-7,fig-geodesic-low-traj-opt-7,fig-geodesic-traj-opt-7,fig-geodesic-high-traj-opt-7
leaving the desired dynamics mode and becoming subject to the turbulent dynamics.

**** mode remaining :ignore:
\newline

*Mode remaining*
The experiments are now evaluated at their ability to remain in the desired dynamics mode.
None of the indirect geodesic control experiments
(cref:fig-geodesic-low-traj-opt-7,fig-geodesic-traj-opt-7,fig-geodesic-high-traj-opt-7)
were able to successfully remain in the desired dynamics mode.
This is shown by the dynamics trajectories (magenta) passing over the mode boundary into the turbulent dynamics mode.
This is further emphasised by the environment trajectory (cyan) deviating from the
dynamics trajectory (magenta) when it passes into the turbulent dynamics mode.
This is due to the strong wind field blowing the quadcopter in the negative $x$ direction.
Although none of the indirect geodesic control experiments successfully remained in the desired dynamics mode,
they have exhibited mode remaining behaviour.
That is, the trajectories with $\lambda=0.5$ in cref:fig-geodesic-low-traj-opt-7
and with $\lambda=1.0$ in cref:fig-geodesic-traj-opt-7,
navigate to the left and almost reach the mode boundary.
# This is further indicated in cref:tab-results-sim-envs, where the probability of remaining in the desired
# dynamics mode is higher than the experiments with $\lambda=20.0$.

In practice, setting the lower and upper bounds on the collocation constraints in SciPy, is extremely fiddly,
and impacts how much mode remaining behaviour is exhibited.
Setting the bounds too loose results in trajectories not satisfying the geodesic ODE and
setting them to tight leads to the constrained optimisation exiting with an error.
These issues could potentially be resolved by casting the constrained optimisation into an unconstrained optimisation
using Lagrange multipliers. However, this is left for future work.

The two direct optimal control experiments in cref:fig-riemannian-energy-low-traj-opt-7,fig-riemannian-energy-traj-opt-7,
with $\lambda =0.5$ and $\lambda=1.0$ respectively,
were able to successfully remain in the desired dynamics mode, and thus,
navigate to the target state in the environment (cyan).
This is demonstrated by their dynamics trajectories (magenta) and their environment trajectories
(cyan), not passing over the mode boundary into the turbulent dynamics mode.
These results are confirmed by cref:tab-results-sim-envs where the trajectory found with $\lambda=1.0$ obtained
the highest probability of remaining in the desired dynamics mode for Environment 1.
From visual inspection, the trajectory found with $\lambda=1.0$ also has more clearance from the turbulent dynamics
mode than for $\lambda=0.5$ and $\lambda=20.0$.
This appears to be due to the covariance term pushing the trajectory away
from the region of high epistemic uncertainty in the gating network.
These results confirm that the covariance term in the expected Riemannian metric encodes the notion of avoiding
regions of high epistemic uncertainty in the gating network.
# Further, this suggests that avoiding regions of high epistemic uncertainty in the gating network can improve
# the mode remaining behaviour.

# The lower relevance of the covariance term in the expected Riemannian metric with $\lambda=0.5$
# has allowed the trajectory to pass
# through regions of the gating network with high epistemic uncertainty.
# This is quantified in cref:tab-results-sim-envs as it is the trajectory with the highest accumulated gating
# variance.


# Interestingly, the trajectory found with $\lambda=0.5$ obtained the second highest probability
# when not considering the epistemic from the learned dynamics model (cref:eq-mode-probability-no-unc),
# but only the third highest when considering the epistemic uncertainty (cref:eq-mode-probability-all-unc).
# This is due to the lower relevance of the covariance term in the expected Riemannian metric
# This can be seen in the bottom right plot of cref:fig-riemannian-energy-low-traj-opt-7 where the trajectory
# passes straight over the region of highest gating function variance in the middle of the environment.
# This is quantified in cref:tab-results-sim-envs as it is the trajectory with the highest accumulated gating
# variance.
# From visual inspection, the trajectory found with $\lambda=1.0$ has more clearance from the turbulent dynamics
# mode.
# This is due to the higher relevance of the covariance term pushing the trajectory away from the region of high
# epistemic uncertainty in the gating network.
# These results confirm that the covariance term in the expected Riemannian metric encodes the notion of avoiding
# regions of high epistemic uncertainty in the gating network.

However, the trajectory found with $\lambda=20.0$ in cref:fig-riemannian-energy-high-traj-opt-7
did not successfully remain in the desired dynamics mode.
This indicates that setting the relevance of the covariance term too high can have a negative impact on performance.
This is likely due to the optimisation landscape favouring trajectories that
avoid the region of high posterior variance in the gating network more than following a true geodesic path
on the mean of the gating function.
As such, these results suggest that care should be taken when adjusting the value of $\lambda$.
# This is most likely due to the trajectory optimiser finding a local optima when $\lambda=20.0$.

It is clear from
cref:fig-mode-conditioning-no-max-entropy-traj-opt-7,fig-mode-conditioning-max-entropy-traj-opt-7,
that all of the control-as-inference experiments found trajectories that
remained in the desired dynamics mode.
Visual inspection of
cref:fig-mode-conditioning-no-max-entropy-traj-opt-7,fig-mode-conditioning-max-entropy-traj-opt-7
show that the maximum entropy control term resulted in a trajectory with more clearance from the
mode boundary.
This is a desirable behaviour that is expected from the maximum entropy control term.
This observation is confirmed in cref:tab-results-sim-envs,
where the experiments with maximum entropy
control obtained higher probabilities of remaining in the desired dynamics mode when considering the
model's epistemic uncertainty.

Overall, the control-as-inference experiments achieved the best mode remaining behaviour.
This is indicated by them obtaining the highest probabilities of remaining in the desired
dynamics mode when considering the model's epistemic uncertainty and the second and third highest probabilities
when not.

# Both of the control-as-inference experiments obtained high probabilities of
# remaining in the desired dynamics mode when not marginalising the model's epistemic uncertainty.
# This indicates that the maximum entropy control term has little impact on this probability.
# However, the maximum entropy control experiment obtained a
# higher probability of remaining in the desired dynamics mode when marginalising the models epistemic uncertainty.

# This is because for $\lambda=20.0$ (cref:fig-riemannian-energy-high-traj-opt-7) the loss landscape favours
# avoiding the region of high posterior variance in the gating network, over remaining the desired dynamics mode.
# In this setting, increasing $\lambda$ has a negative impact on performance

# Two of the direct optimal control experiments (with $\lambda =0.5$ and $\lambda=1.0$),
# (cref:fig-riemannian-energy-low-traj-opt-7,fig-riemannian-energy-traj-opt-7)
# were able to successfully navigate to the target state in the environment.
# This is because for $\lambda=20.0$ (cref:fig-riemannian-energy-high-traj-opt-7) the loss landscape favours
# avoiding the region of high posterior variance in the gating network, over remaining the desired dynamics mode.
# In this setting, increasing $\lambda$ has a negative impact on performance

**** epistemic uncertainty :ignore:
\newline

*Epistemic uncertainty*
Finally, the methods are evaluated at their ability to avoid regions of the dynamics with high epistemic uncertainty.
cref:tab-results-sim-envs shows the state variance and the gating function variance accumulated over
each trajectory.
In all experiments, the state variance accumulated over the trajectory (from cascading single-step predictions
via moment matching) were fairly similar.
This is due to the dynamics of the simulator being simple enough that the desired mode's GP
can confidently interpolate into the turbulent dynamics mode and the region which has not been observed.
In the latter case, it is up to the gating network to model the epistemic uncertainty arising from limited
training data.
# In this case, the dynamics model explains epistemic uncertainty is explained by the gating network
# In this case, the complexity of the dynamics arises from the multmodaility which the model explains by
# assigning the second dynamics mode to a second expert.
# regions of the

In the indirect geodesic control experiments with $\lambda=1.0$ and $\lambda=0.5$, the trajectories navigate
directly over the region of high posterior variance associated with the gating function.
This can be seen in the bottom right plots of cref:fig-geodesic-low-traj-opt-7,fig-geodesic-traj-opt-7.
In contrast, the indirect geodesic control experiment with $\lambda=20.0$ (in cref:fig-geodesic-high-traj-opt-7)
avoided this region.
However, it should be noted that
this uncertainty avoiding behaviour came at the cost of passing straight through the turbulent dynamics mode.

Similarly for the direct optimal control experiments, setting $\lambda=20.0$ resulted in the trajectory passing
straight through the turbulent dynamics mode.
However, the benefit of the covariance term in the expected Riemannian metric
is demonstrated in the results with $\lambda=0.5$ in cref:fig-riemannian-energy-low-traj-opt-7 and with
$\lambda=1.0$ in cref:fig-riemannian-energy-traj-opt-7.
As mentioned earlier, this resulted in the trajectory having more clearance from the turbulent dynamics mode,
because the covariance term pushed the trajectory to the left of the region of high variance in the gating
network.
This is confirmed in cref:tab-results-sim-envs where the trajectory found with $\lambda=1.0$ accumulated
less gating function variance than the trajectory found with $\lambda=0.5$.

These results indicate that the covariance term -- in the expected Riemannian metric -- plays an important role
at keeping trajectories in the desired dynamics mode.
However, they also indicate that $\lambda$ alters both the contour following term and the covariance term,
in a complex manner.
As such, it is not always clear how the value of $\lambda$ should be set.
This is especially the case in this environment,
where the inoperable dynamics mode intersects a region of high epistemic uncertainty.
In this case, the effect of adjusting the tunable $\lambda$ parameter is complex enough that it may
be best to just leave it at $\lambda=1.0$.
# It is worth noting here that the experiments in Environment 2 do demonstrate a
# scenario where it is beneficial to adjust the $\lambda$ parameter.

The control-as-inference experiments in
cref:fig-mode-conditioning-no-max-entropy-traj-opt-over-svgp-7,fig-mode-conditioning-max-entropy-traj-opt-over-svgp-7
also navigate to the left of the region high epistemic uncertainty in the gating network.
This uncertainty avoiding behaviour is confirmed in cref:tab-results-sim-envs
where they have the third and fourth lowest amounts of accumulated gating function variance.
Only the experiments with $\lambda=20.0$ obtained lower values.
This shows that the ELBO in cref:cref:eq-traj-opt-elbo encodes uncertainty avoiding behaviour
via the marginalsation over the latent variables.
# Albeit as a consequence


# *Overall* Visual inspection combined with the results in cref:tab-results-sim-envs,
# indicate that the direct optimal control method with $\lambda=1.0$ was the highest performing experiment in Environment 1.
# Not only does the trajectory avoid regions of the learned dynamics

# In cref:fig-riemannian-energy-low-traj-opt-7 the optimised trajectory only just remained in the desired mode.
# That is, the trajectory passes very close to the boundary between the dynamics modes.
# This is because with $\lambda=0.5$ the method is starting to ignore the uncertainty in the gating network.
# In contrast, the optimised trajectory with $\lambda=1.0$ follows the contours of the gating function's
# posterior variance (cref:fig-riemannian-energy-low-traj-opt-over-svgp-7 right).
# As a result, the trajectory is remaining in the desired mode by following the contours of the
# desired mode's gating function
# (cref:fig-riemannian-energy-low-traj-opt-over-svgp-7 left)
# and avoiding regions of the gating network which it cannot predict confidently
# (cref:fig-riemannian-energy-low-traj-opt-over-svgp-7 right).


# This is because it obtained the highest probability of remaining in the desired mode over the trajectory.
# That is, it exhibited the best mode remaining behaviour under the \acrshort{mosvgpe} dynamics model.
# Further to this, it obtained the lowest accumulation of gating function variance over the trajectory,
# indicating that it introduced the least amount of epistemic uncertainty from the gating network.
# This is an interesting result as I originally hypothesised that the gating function variance would
# decrease with higher $\lambda$ values.
# This is most likely due to the trajectory optimiser finding a local optima when $\lambda=20.0$.
# In contrast, the loss landscape non

*** Environment 2
**** Simulator :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{./images/mode-opt/env/scenario_5/env_with_dataset_start_end_pos.pdf}
\caption{\textbf{Environment 2} - Visualisation of Environment 2 with its mode boundary
indicated by the purple lines.
A state transition data set has been sampled from each environment and is visualised by the quiver plots.
The initial $\state_0$ and target states $\targetState$ for the trajectory optimisation are overlayed.
$\windDrift{1} = \{0.4, 0.02\}$, $\windDrift{2} = \{0.4, -2.0\}$,
$\windTurbulenceNoise{1} = \diag\left([0.0001, 0.0002]\right)$ and
$\windTurbulenceNoise{2} = \diag\left([0.05, 0.2]\right)$}
\label{fig-dataset-scenario-5}
\end{figure}
#+END_EXPORT
**** Learned model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[h!]
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=0.98\textwidth]{./images/mode-opt/env/scenario_5/mosvgpe/mixing_probs_no_obs.pdf}
\subcaption{Mixing probabilities}
\label{eq-traj-opt-gating-network-prob-post-5}
\end{minipage}
\begin{minipage}{1.0\textwidth}
\centering
\includegraphics[width=0.98\textwidth]{./images/mode-opt/env/scenario_5/mosvgpe/desired_gating_gp_no_obs.pdf}
\subcaption{Gating function's GP posterior}
\label{eq-traj-opt-gating-network-gp-post-5}
\end{minipage}
\caption{\textbf{Environment 2} Visualisation of the gating network posterior after training \acrshort{mosvgpe}
on the state transition data set from Environment 2.
(\subref{eq-traj-opt-gating-network-prob-post-5}) shows the probability mass function over the expert indicator
variable and (\subref{eq-traj-opt-gating-network-gp-post-5}) shows the gating function's GP
posterior mean (left) and posterior variance (right).
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line)
and the subset of the environment which has not been observed (hashed box).}
\label{fig-traj-opt-gating-network-5}
\end{figure}
#+END_EXPORT

# In both environments, Expert 1 represents the turbulent dynamics mode and Expert 2 represents
# the operable, desired dynamics mode.
# This results from Expert 2 having much higher drift and process noise than Expert 1.
# cref:fig-dataset-scenario illustrates the two environments,
# where the goal is to navigate from the initial state
# $\state_0$, to the target state $\targetState$, given the state transition data sets indicated by the quiver plots.
**** blah :ignore:
\newline

# The direct optimal control method was tested in a second simulated environment.
The methods were then tested in a second simulated environment (Environment 2).
In this environment, the turbulent dynamics mode and the unobserved regions are in different locations to
Environment 1.
cref:fig-dataset-scenario-5 shows the state transition data set that was sampled from Environment 2 and
used to train the \acrshort{mosvgpe} dynamics model.
cref:fig-traj-opt-gating-network-5 shows the gating network posterior after training on this data set.
Expert 1 represents the turbulent dynamics mode and Expert 2 represents the operable, desired dynamics mode.
This results from Expert 2 having much higher drift and process noise than Expert 1.


All of the direct optimal control and control-as-inference experiments used a horizon of $\TimeInd=20$ time steps and
all of the indirect optimal control experiments were initialised with $I=20$ collocation points.
Further to this, all of the indirect optimal control experiments were initialised
with straight line trajectories between $\state_0$ and
$\targetState$, except for the experiment in cref:fig-geodesic-mid-point-traj-opt-5, which was initialised
with two straight lines, between $\state_0$, $[-2.0, -2.0]$ and $\targetState$.
This is because the indirect geodesic control experiments struggled to find solutions in this environment
-- due to local optima -- without adjusting the initial solution.

Four different settings of the $\lambda$ parameter were tested for the
direct control method and three setting were tested for the indirect optimal control method.
This was due to the indirect optimal control method's collocation solver failing with $\lambda=0.01$.
cref:tab-results-sim-envs summarises the direct optimal control and the control-as-inference results in Environment 2.
The experiments for the indirect optimal control method are not included in cref:tab-results-sim-envs
because none of them found trajectories to the target state.

**** Geodesic collocation 0.5 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/geodesic-collocation-low/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-geodesic-low-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/geodesic-collocation-low/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-geodesic-low-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Indirect geodesic control} $\bm\lambda\mathbf{=0.5}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$, using the indirect geodesic control method from \cref{sec-traj-opt-collocation}.
The geodesic state trajectory was initialised as a straight line between $\state_0$ and $\targetState$.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The optimised state trajectory from the collocation solver is indicated in yellow.
The trajectories are overlayed on (\subref{fig-geodesic-low-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-geodesic-low-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.}
\label{fig-geodesic-low-traj-opt-5}
\end{figure}
#+END_EXPORT

**** Geodesic collocation 1.0 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/geodesic-collocation/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-geodesic-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/geodesic-collocation/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-geodesic-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Indirect geodesic control} $\bm\lambda\mathbf{=1.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$, using the indirect geodesic control method from \cref{sec-traj-opt-collocation}.
The geodesic state trajectory was initialised as a straight line between $\state_0$ and $\targetState$.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The optimised state trajectory from the collocation solver is indicated in yellow.
The trajectories are overlayed on (\subref{fig-geodesic-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-geodesic-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.}
\label{fig-geodesic-traj-opt-5}
\end{figure}
#+END_EXPORT

**** Geodesic collocation 1.0 mid point :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/geodesic-collocation-mid-point/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-geodesic-mid-point-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/geodesic-collocation-mid-point/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-geodesic-mid-point-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Indirect geodesic control} $\bm\lambda\mathbf{=1.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$, using the indirect geodesic control method from \cref{sec-traj-opt-collocation}.
The geodesic state trajectory was initialised as two straight lines between $\state_0$, $[-2.0, -2.0]$ and $\targetState$.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The optimised state trajectory from the collocation solver is indicated in yellow.
The trajectories are overlayed on (\subref{fig-geodesic-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-geodesic-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.}
\label{fig-geodesic-mid-point-traj-opt-5}
\end{figure}
#+END_EXPORT

**** Geodesic collocation 5 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/geodesic-collocation-high/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-geodesic-high-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/geodesic-collocation-high/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-geodesic-high-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Indirect geodesic control} $\bm\lambda\mathbf{=5.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$, using the indirect geodesic control method from \cref{sec-traj-opt-collocation}.
The geodesic state trajectory was initialised as a straight line between $\state_0$ and $\targetState$.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The optimised state trajectory from the collocation solver is indicated in yellow.
The trajectories are overlayed on (\subref{fig-geodesic-high-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-geodesic-high-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.}
\label{fig-geodesic-high-traj-opt-5}
\end{figure}
#+END_EXPORT

**** Riemannian energy 0.01 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy-low/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy-low/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Riemannian energy} $\bm\lambda\mathbf{=0.01}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the Riemannian energy cost function from \cref{eq-approximate-trajectory-riemannian-energy}
with $\lambda=0.01$
and deterministic controls (no maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-riemannian-energy-low-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-riemannian-energy-low-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-riemannian-energy-low-traj-opt-5}
\end{figure}
#+END_EXPORT

**** Riemannian energy 0.5 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy-low-2/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-2-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy-low-2/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-2-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Riemannian energy} $\bm\lambda\mathbf{=0.5}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the Riemannian energy cost function from \cref{eq-approximate-trajectory-riemannian-energy}
with $\lambda=0.5$
and deterministic controls (no maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-riemannian-energy-low-2-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-riemannian-energy-low-2-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-riemannian-energy-low-2-traj-opt-5}
\end{figure}
#+END_EXPORT

**** Riemannian energy 1 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-riemannian-energy-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-riemannian-energy-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Riemannian energy} $\bm\lambda\mathbf{=1.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the approximate Riemannian energy cost function from \cref{eq-approximate-trajectory-riemannian-energy}
with $\lambda=1.0$.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-riemannian-energy-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-riemannian-energy-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-riemannian-energy-traj-opt-5}
\end{figure}
#+END_EXPORT

**** Riemannian energy 5 :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy-high/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-riemannian-energy-low-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/riemannian-energy-high/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-riemannian-energy-high-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Riemannian energy} $\bm\lambda\mathbf{=5.0}$
Trajectory optimisation results after finding trajectories from the start state $\state_0$,
to the target state $\targetState$,
with the Riemannian energy cost function from \cref{eq-approximate-trajectory-riemannian-energy}
with $\lambda=5.0$
and deterministic controls (no maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-riemannian-energy-high-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-riemannian-energy-high-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-riemannian-energy-high-traj-opt-5}
\end{figure}
#+END_EXPORT

**** Mode Conditioning max entropy :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/control-as-inference/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-mode-conditioning-max-entropy-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{0.99\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/control-as-inference/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-mode-conditioning-max-entropy-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Mode conditioning ELBO with maximum entropy control}
Trajectory optimisation results finding trajectories from the start state $\state_0$,
to the target state $\targetState$, after solving \cref{eq-traj-opt-inference} with Gaussian controls
(maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The trajectories are overlayed on (\subref{fig-mode-conditioning-max-entropy-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-mode-conditioning-max-entropy-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with desired mode's gating function.}
\label{fig-mode-conditioning-max-entropy-traj-opt-5}
\end{figure}
#+END_EXPORT

**** Mode Conditioning NO max entropy :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/control-as-inference-deterministic/trajectories_over_mixing_probs.pdf}
\subcaption{}
\label{fig-mode-conditioning-no-max-entropy-traj-opt-over-prob-5}
\end{minipage}
\begin{minipage}[r]{0.99\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/trajectory_optimisation/scenario_5/control-as-inference-deterministic/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{fig-mode-conditioning-no-max-entropy-traj-opt-over-svgp-5}
\end{minipage}
\caption{\textbf{Mode conditioning ELBO without maximum entropy control}
Trajectory optimisation results finding trajectories from the start state $\state_0$,
to the target state $\targetState$, after solving \cref{eq-traj-opt-inference} with deterministic controls
(no maximum entropy control).
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-mode-conditioning-no-max-entropy-traj-opt-over-prob-5})
each mode's mixing probability
and (\subref{fig-mode-conditioning-no-max-entropy-traj-opt-over-svgp-5})
the posterior mean (left) and posterior variance (right) associated with desired mode's gating function.}
\label{fig-mode-conditioning-no-max-entropy-traj-opt-5}
\end{figure}
#+END_EXPORT

**** target state :ignore:
\newline
*Target state*
The collocation trajectories (yellow) in
cref:fig-geodesic-traj-opt-5,fig-geodesic-high-traj-opt-5,fig-geodesic-low-traj-opt-5,fig-geodesic-mid-point-traj-opt-5
indicate that the indirect optimal control experiments were able to find state trajectories to the target state.
This is because it is a property of the collocation solver.
However, none of the experiments were able to reach the target state under the desired mode's dynamics (magenta).
This is because the variational inference strategy was not able to recover the controls from the collocation solver's
state trajectory.
It is likely that the ELBO in cref:eq-control-elbo-svgp was not able to recover the controls due to a local optima.
The collocation state trajectories (yellow) in the experiments in
cref:fig-geodesic-traj-opt-5,fig-geodesic-high-traj-opt-5,fig-geodesic-low-traj-opt-5
pass through the turbulent dynamics mode.
As this region belongs to the turbulent dynamics mode, the desired mode's dynamics GP has high epistemic uncertainty
in this region.
This high uncertainty results in the ELBO being low when trajectories pass through this region and
it is well known that gradient based methods struggle to get out of such local minima.
Further to this, the experiment in cref:fig-geodesic-mid-point-traj-opt-5
was not able to recover the correct controls, even though the collocation state trajectory (yellow)
does not pass through the turbulent dynamics mode.
Monitoring the optimisation of the control trajectory showed that the control trajectory
hit the local optima attributed to the turbulent dynamics mode during optimisation.
Although not tested, this could likely be overcome with random restarts, i.e. different initial
control trajectories.
# as some initialisations will not pass through the local optima during optimisation.


The magenta trajectories in
cref:fig-riemannian-energy-low-2-traj-opt-5,fig-riemannian-energy-low-traj-opt-5,fig-riemannian-energy-traj-opt-5,fig-riemannian-energy-high-traj-opt-5,fig-mode-conditioning-max-entropy-traj-opt-5,fig-mode-conditioning-no-max-entropy-traj-opt-5
indicate that all of the direct optimal control and control-as-inference
experiments in Environment 2 were able to find trajectories
that navigate to the target state $\targetState$ under the desired mode's GP dynamics.
Further to this, the cyan trajectories indicate that all of the experiments found controls that
successfully navigate to the target state in the environment.
Similarly to the results for Environment 1, setting the terminal state cost matrix $\terminalStateCostMatrix$
to be very high, ensured trajectories successfully reached the target state.
# All of their corresponding environment trajectories (cyan) also navigate to the target state $\targetState$.
# The small error between the environment trajectories (cyan) and dynamics trajectories (magenta) is due to
# the accumulation of process noise.

**** mode remaining :ignore:
\newline

*Mode remaining*
None of the indirect optimal control experiments in
cref:fig-geodesic-traj-opt-5,fig-geodesic-high-traj-opt-5,fig-geodesic-low-traj-opt-5,fig-geodesic-mid-point-traj-opt-5
were able to successfully remain in the desired dynamics mode.
In the experiments in
cref:fig-geodesic-traj-opt-5,fig-geodesic-high-traj-opt-5,fig-geodesic-low-traj-opt-5,
which were initialised with straight line trajectories, the collocation solver was not able to escape the
local optima induced by the straight line trajectories.
This is indicated by the collocation state trajectories (yellow) passing directly through the turbulent dynamics
mode.
Issues with local optima (in the collocation solver) were overcome for $\lambda=1.0$, by initialising the state
trajectory to pass through $[-2.0, -2.0]$.
With this initalisation the collocation state trajectory (yellow) in  cref:fig-geodesic-mid-point-traj-opt-5
was able to remain in the desired dynamics mode.
However, it should be noted that the need to initialise the collocation solver in this way
limits the direct optimal control method's applicability.
For example, how should the initial solution be set in environments where the state-space cannot
be visualised as easily?
In contrast, the direct optimal control method requires little to no human input to initalise the optimisation.

All of the direct optimal control experiments were able to successfully remain in the desired dynamics mode.
This is indicated by none of the dynamics trajectories (magenta) passing over the mode boundary into
the turbulent dynamics mode in
cref:fig-riemannian-energy-low-2-traj-opt-5,fig-riemannian-energy-low-traj-opt-5,fig-riemannian-energy-traj-opt-5,fig-riemannian-energy-high-traj-opt-5.
cref:tab-results-sim-envs shows that the
trajectories found with $\lambda=0.01$ and $\lambda=1.0$ in
cref:fig-riemannian-energy-low-traj-opt-5
and
cref:fig-riemannian-energy-traj-opt-5
respectively, obtained the highest probability of remaining in the desired dynamics mode.
From visual inspection of
cref:fig-riemannian-energy-low-2-traj-opt-5,fig-riemannian-energy-low-traj-opt-5,fig-riemannian-energy-traj-opt-5,fig-riemannian-energy-high-traj-opt-5
it is clear that the trajectory found with $\lambda=0.01$ achieved a lower probability
because it has the most clearance from the desired dynamics mode.
Interestingly, although the trajectory found with $\lambda=1.0$
passed through less regions of high epistemic uncertainty, indicated by lower
state and gating variance in cref:tab-results-sim-envs,
it obtained a lower probability of remaining in the desired dynamics mode
when considering the epistemic uncertainty (cref:eq-mode-probability-all-unc).
Further to this, the trajectory found with $\lambda=0.01$ obtained the lowest probability even though it
accumulated the most state and gating variance in comparison to the other $\lambda$ values.
In contrast to the experiments in Environment 1,
this indicates that avoiding regions of high epistemic uncertainty does not necessarily increase the
probability of remaining in the desired dynamics mode under cref:eq-mode-probability-all-unc.
This is because the mode probability is calculated using both the gating function's mean and variance.
In these experiments, the influence of the gating function mean must have had a higher impact on
the mode probability than the variance.
# Intuitively, this suggests that if the model's interpolation/extrapolation believes a region belongs
# Although the model is uncertain, if its interpolation/extrapolation suggest that the


The trajectory found with $\lambda=5.0$ obtained the lowest probability of remaining in the desired dynamics mode.
Increasing the relevance of the covariance term in the expected Riemannian metric
is equivalent to decreasing the relevance of the mode remaining term.
As a result, the optimisation has favoured avoiding regions of high epistemic uncertainty
over remaining in the desired dynamics mode.
Again, the low probability of remaining in the desired dynamics mode
when considering the epistemic uncertainty (cref:eq-mode-probability-all-unc), indicates that avoiding regions
of high epistemic uncertainty does not necessarily lead to higher probabilities.
# Quantitatively, this indicates that adjusting $\lambda$ can have a negative impact on performance.
# This highlights that the mode probability is
# further indicates that although

# The left hand plot of cref:fig-riemannian-energy-low-2-traj-opt-over-svgp-5 shows the
# contour following that achieves the mode remaining behaviour.
The second half of the trajectories in the left hand plots of
cref:fig-riemannian-energy-low-2-traj-opt-over-svgp-5,fig-riemannian-energy-low-traj-opt-over-svgp-5,fig-riemannian-energy-traj-opt-over-svgp-5,fig-riemannian-energy-high-traj-opt-over-svgp-5
show the contour following that achieves the mode remaining behaviour.
It can also be seen that
as the value of $\lambda$ increases, the trajectories exhibit less contour following on the gating function's
mean.
This is because increasing the relevance of the covariance term in the Riemannian metric tensor
alters the optimisation landscape, which in turn reduces the contour following behaviour.

# It is worth noting that the trajectory found with $\lambda=0.01$ does not follow a smooth state trajectory.
# This is shown by the sixth and seventh time steps changing direction abruptly.
# Although this behaviour can be alleviated by increasing the control regularisation,
# complex interactions between the control cost term and the Riemannian energy cost term make it difficult to set.

It is clear from
cref:fig-mode-conditioning-no-max-entropy-traj-opt-5,fig-mode-conditioning-max-entropy-traj-opt-5
that both of the control-as-inference experiments found discrete-time
trajectories that remained in the desired dynamics mode.
This is indicated by none of the trajectories time steps being in the turbulent dynamics mode.
However, when interpolating the discrete-time trajectory for the experiment without
maximum entropy control, shown in
cref:fig-mode-conditioning-no-max-entropy-traj-opt-5,
the trajectory crosses the mode boundary.
This is an undesirable behaviour, as in a non simulated environment this would correspond to the trajectory
entering the turbulent dynamics mode.
Visual inspection of
cref:fig-mode-conditioning-no-max-entropy-traj-opt-5,fig-mode-conditioning-max-entropy-traj-opt-5
show that the maximum entropy control term resulted in a trajectory with more clearance from the
mode boundary.
This is a desirable behaviour that is expected from the maximum entropy control term.
This observation is confirmed in cref:tab-results-sim-envs-inf, where the experiment with maximum entropy
control obtained a higher probability of remaining in the desired dynamics mode when considering the
model's epistemic uncertainty.

# Both experiments in each environment obtained equal (or almost equal) probabilities of
# remaining in the desired dynamics mode when not marginalising the model's epistemic uncertainty.
# This indicates that the maximum entropy control term has little impact on this probability.
# However, in both environments, the maximum entropy control experiment obtained a
# higher probability of remaining in the desired
# dynamics mode when  marginalising the models epistemic uncertainty.
# This indicates that the maximum entropy term

**** epistemic uncertainty :ignore:
\newline


*Epistemic uncertainty*
Each experiment's ability to avoid regions of high epistemic uncertainty is now evaluated.
Because the indirect optimal control experiments failed to remain in the desired dynamics mode they are not considered here.
In contrast to the experiments in Environment 1, the state variance accumulated over each trajectory does vary
between experiments.
However, the results in  cref:tab-results-sim-envs suggest that the state variance does not have a large impact
on the mode remaining behaviour.
This is indicated by no correlation between the mode probability and the state variance,
which is likely due to the state variance being extremely low.
The trajectory found with $\lambda=1.0$ accumulated the least state and gating function variance.
In contrast, the result for $\lambda=0.01$ accumulated the highest state and gating variance, whilst also
obtaining the highest probability of remaining in the desired dynamics mode.
As mentioned earlier, this result suggests that avoiding regions of high epistemic uncertainty is not the most
important factor for obtaining the highest probability of remaining in the desired dynamics mode.

The control-as-inference experiment with maximum entropy control,
shown in cref:fig-mode-conditioning-max-entropy-over-svgp-traj-opt-5,
obtained a lower accumulation of gating function variance than the experiment without maximum entropy control.
Relative to the geometry-based methods, the trajectories found with the control-as-inference method, shown in
cref:fig-mode-conditioning-max-entropy-traj-opt-5,fig-mode-conditioning-no-max-entropy-traj-opt-5,
do not exhibit as much uncertainty avoiding behaviour in the gating network.
In fact, they obtained higher accumulations of gating function variance by quite a margin.
In contrast, in Environment 1, the trajectories found with the control-as-inference method
obtained some of the most uncertainty avoiding behaviour, indicated by some of the
lowest accumulated gating function variance.
This change in behaviour is due to the relevance of the uncertainty avoiding behaviour being automatically
handled by the marginalisation of the gating function in cref:eq-traj-opt-elbo.

Although marginalisation is a principled way of handing uncertainty, in this scenario,
it can be conjectured otherwise.
First note that the quantitative performance measures do not tell the full story.
This is because in the region with no observations, it is perfectly plausible that there
is another region belonging to the turbulent dynamics mode, or a different dynamics mode altogether.
In this case, the trajectory found with $\lambda=5.0$ is most likely to avoid entering the turbulent dynamics mode.
This is because it avoids the region of high epistemic uncertainty associated with no observations.
The probability of remaining in the desired dynamics mode does not capture this notion.
There are two reasons for this.
Firstly, the gating network is overconfident in this region due to learning a long lengthscale.
This could be overcome by fixing the lengthscale a priori.
However, there is a second issue due to the fundamental modelling approximation made by GPs.
The GP-based gating network is not aware that the region with no observations is so large that it could fit
another turbulent dynamics mode inside it.
This is due to the Squared Exponential kernel function relying on point-wise calculations to build the covariance
matrix.
Future work could explore kernel functions that capture structure at a mode level (shape etc),
as opposed to simple point-wise structure.
# \todo[inline]{is this spatial awareness? What to call this? More than point-wise structure,
# Such a kernel could learn the shape of a dynamics mode, for example, the turbulent region induced by a fan.
# Such a kernel would be aware if a region is large enough for another
# This problem could be
# That is, the kernel function is not able to detect if

\todo[inline]{Carl you mentioned something about the kernel using pointwise calculations which make it impossible to model this. Not sure if what I've written here fully captures what you meant?}
# This overconfidence arises due to the gating network learning a long lengthscale.
# One approach to mitigate this behaviour, is to fix the lengthscale.
# However, it may not be obvious how to set this value a priori.

# This overconfidence in the gating network arises from the gating network evaluating the covariance between
# each new point
# the
# However, when adopting a kernel function that cannot capture this behaviour, increasing the relevance of
# the covariance term in the expected Riemannian metric appears to

The results for $\lambda=0.01$, $\lambda=0.5$ and $\lambda=1.0$ in cref:tab-results-sim-envs,
follow the results for Environment 1, suggesting
that increasing $\lambda$ leads to more uncertainty avoiding behaviour in the gating network.
The result for $\lambda=5.0$ in cref:tab-results-sim-envs does not follow this trend.
However, visual inspection of the right hand plots of
cref:fig-riemannian-energy-traj-opt-over-svgp-5,fig-riemannian-energy-high-traj-opt-over-svgp-5
show that the trajectories for $\lambda=1.0$ and $\lambda=5.0$, are initially similar whilst they
avoid the region with high epistemic uncertainty in the gating network.
The increased relevance of the covariance term with $\lambda=5.0$ then appears to have had a negative impact on
the trajectories ability to remain in the desired dynamics mode.
This can be seen in cref:fig-riemannian-energy-high-traj-opt-over-svgp-5,
where the trajectory passes very close to the mode boundary.
Again, this further emphasises the difficulty in setting $\lambda$ due to complex interactions between
the mode remaining behaviour and the uncertainty avoiding behaviour.


# Without decoupling the uncertainty avoiding behaviour from the mode remaining behaviour,
# it is not possible to find trajectories which avoid entering the region of high epistemic uncertainty in the
# gating network.
# However, as mentioned previously, it is not always clear how to balance these two behaviours.
# As shown in cref:fig-riemannian-energy-high-traj-opt-5,
# increasing the uncertainty avoiding behaviour can have a detrimental impact on the mode remaining behaviour.
# In contrast, the control-as-inference method does not provide a mechanism for adjusting the relevance
# of each behaviour.
# However, the marginalisation of uncertainty in the \acrshort{elbo}
# (cref:eq-traj-opt-inference) is principled and requires no human input.

** Conclusion
*** intro :ignore:
This section details the main findings and insights obtained from the experiments and then discusses
the methods and future work.

*Geometry-based methods*
The indirect optimal control method's collocation solver is susceptible to local optima around its initial state trajectory.
Although this can be overcome by engineering the initial solution, in practice, it makes it difficult to
get the method working.
On top of this, the variational inference strategy used to recover the controls cannot always recover the correct controls.
This may be due to the collocation solver's state trajectory not satisfying the dynamics constraints, or,
due to the variational inference getting stuck in local optima.
Regardless, this is a big limitation that makes the indirect optimal control method fail in some environments.

Overall, the direct optimal control approach performed significantly better than the indirect optimal control method in the experiments.
Firstly, two of the direct optimal control experiments in Environment 1 were able to remain in the desired dynamics mode,
whilst the indirect geodesic control experiments could not.
Further to this, the direct optimal control approach worked in Environment 2, where it was not possible to get the
indirect control method working, without engineering the initial solution.
Not only did the direct optimal control approach perform better in the experiments, but it is also significantly easier
to configure.
That is, setting the optimisation parameters is straightforward.
In contrast, setting the parameters for the indirect control method is not.
In particular, setting the upper and lower bounds for the collocation constraints is pernickety.
In conclusion, the direct optimal control method is the preferred geometry-based method for finding mode remaining
trajectories.

*How to set $\lambda$?*
Although increasing $\lambda$ generally leads to more uncertainty avoiding behaviour in the gating network,
this is not necessarily the case.
Further to this,
avoiding regions of high epistemic uncertainty in the gating network does not always lead to higher probabilities
of remaining in the desired dynamics mode under cref:eq-mode-probability-all-unc.
As a result, care should be taken if/when adjusting $\lambda$.

The quantitative results indicate that good performance is generally achieved with $\lambda=1.0$.
That is, not modifying the expected Riemannian metric tensor.
Although some benefits can be obtained via setting $\lambda$,
for example, encoding a notion of risk-sensitivity for avoiding the region with no observations,
it is not always clear how it should be set.
Further to this, in many realistic scenarios, the state-space will not be easy to visualise like in
the 2D quadcopter experiments.
As such, visual inspection of trajectories may not be possible.
For these reasons, I conclude that it is best to air on the side of caution when setting $\lambda$.
That is, $\lambda$ should remain at $1.0$ unless it is immediately clear how it should be set.

# - Although increasing $\lambda$ generally leads to more uncertainty avoiding behaviour in the gating network, it
#    does not necessarily,
#    - As a result, care should be taken if/when adjusting $\lambda$.
# - Avoiding regions of high epistemic uncertainty in the gating network does not always lead to higher probabilities
#    of remaining in the desired dynamics mode under cref:eq-mode-probability-all-unc,
#    - This further suggests that care should be taken if/when adjusting $\lambda$.
# - The indirect control method's collocation solver is susceptible to local optima around its initial state trajectory,
#    - Although this can be overcome by engineering the initial solution, in practice, it makes it difficult to
#      get the method working.
# - The variational inference strategy used to recover the controls for the indirect control method cannot always
#    recover the correct controls,
#    - This is a big limitation that makes the method fail in some environments.
#    # - This may be due to the collocation solver's state trajectory not satisfying the dynamics constraints, or,
#    # - due to the variational inference getting stuck in local optima.


*Control-as-inference*
The control-as-inference method also performed well in all of the experiments.
The experiments showed that the maximum entropy control obtained from using Normally distributed controls
improves the mode remaining behaviour and works well in practice.
Both the direct optimal control and the control-as-inference methods are competitive approaches for finding
mode remaining trajectories.

*** Discussion & Future Work
This section compares the three control methods presented in
cref:sec-traj-opt-collocation,sec-traj-opt-energy,chap-traj-opt-inference
and suggests future work to address their limitations.
cref:tab-geometry-control-comparison offers a succinct comparison of the three approaches.
# The main drawback of the indirect control method is that it does not enforce the dynamics constraints.


#+begin_table
#+LATEX: \caption{Comparison of the indirect control via latent geodesics method from \cref{sec-traj-opt-collocation}, the direct optimal control via Riemannian energy method from \cref{sec-traj-opt-energy} and the control-as-inference method from \cref{chap-traj-opt-inference}.}
#+LATEX: \label{tab-geometry-control-comparison}
#+LATEX: \resizebox{\textwidth}{!}{
#+ATTR_LATEX: :center t :placement [!t] :align lccc
|                                                    | Control as Inference | Indirect Control | direct optimal control |
|----------------------------------------------------+----------------------+------------------+----------------|
| Dynamics constraints guaranteed?                   | \checkmark           | \times           | \checkmark     |
| Considers epistemic uncertainty in dynamics?       | \checkmark           | \times           | \checkmark     |
| Considers epistemic uncertainty in gating network? | \checkmark           | \checkmark       | \checkmark     |
| Can remain in *multiple* modes?                    | \checkmark           | \times           | \times         |
| Boundary conditions guaranteed?                    | \times               | \checkmark       | \times         |
| $\delta-\text{mode remaining}$?                    | \checkmark           | \times           | \checkmark     |
| Continuous time trajectory?                        | \times               | \checkmark       | \times         |
# | Decouples goals?                                   | \checkmark       | \checkmark     |
#+LATEX: }
#+end_table

*Dynamics constraints*
Both the control-as-inference method presented in cref:chap-traj-opt-inference
and the direct Riemannian energy approach presented in
cref:sec-traj-opt-energy find trajectories that satisfy the dynamics constraints, i.e. the learned dynamics.
They achieve this by enforcing the distribution over state trajectories,
to match the distribution from cascading single-step predictions through the desired mode's dynamics \acrshort{gp}.
This can be seen as a method for approximating the integration of the controlled dynamics with respect to time, whilst
considering the epistemic uncertainty associated with learning from observations.
The chance constraints ensure the controlled system is $\delta-\text{mode remaining}$, which
makes the approximate dynamics integration valid.
In contrast, the indirect control method does not find trajectories that are guaranteed to satisfy the dynamics
constraints.
This is a limiting factor that makes the indirect control method less appealing.
Therefore, methods for incorporating the dynamics constraints into the indirect method are an interesting
direction for future work.
# However, future work could attempt to enforce the dynamics constraints in the indirect method.

# Following the same approach as the direct optimal control method,
# the control-as-inference method ensures that trajectories satisfy the dynamics constraints.
# Further to this, the maximum entropy control behaviour


# Future work could consider extending the indirect control method to enforce the dynamics constraints.
# Further to this,
# *Uncertainty aware*
*Decision-making under uncertainty*
The combination of the approximate dynamics integration and the chance constraints
in the direct optimal control method, leads to a closed-form expression for the expected cost.
This expression considers the epistemic uncertainty associated with the learned dynamics model,
both in the desired dynamics mode and in the gating network.
Similarly for the control-as-inference method, the \acrshort{elbo} principally handles the uncertainty
in both the desired dynamics mode and gating network.
In contrast, the indirect control method ignores the uncertainty accumulated by cascading single-step predictions through
the learned dynamics model.
That is, it considers the uncertainty associated with the gating network and ignores any uncertainty
in the desired dynamics mode.
Although this did not have a massive impact in the simulated quadcopter experiments, it may have a
larger impact in real-world systems with more complex dynamics modes.
That is, systems whose underlying dynamics result in each mode's dynamics GP being more uncertain.
In the simulated quadcopter experiments, the GPs representing the underlying dynamics modes were fairly certain --
due to simple dynamics -- so most of the epistemic uncertainty was captured by the gating network.
# It models the state at each time step to be deterministic and approximates the full geodesic \acrshort{sde}
# to be deterministic.
# As as result, the cost function will not keep the state trajectory in regions of low uncertainty.

*Uncertainty propagation*
This work only considered the moment matching approximation for propagating uncertainty through the probabilistic
dynamics model.
cite:chuaDeep2018 test different uncertainty propagation schemes in Bayesian neural networks.
It would be interesting to test sampling-based approaches for the direct optimal control approach.
For example, to see the impact of calculating the expected Riemannian energy over a trajectory without approximations.
# In particular, calculating the expected Riemannian metric would require less assumptions.
# In particular, calculating the expected Riemannian metric would require less assumptions.

# Moment matching cannot represent multimodal distributions as it forces a unimodal distribution at each time step.
# cite:mcallisterImproving2016 hypothesise that this effect may be caused due to smoothing of the loss surface and
# implicitly penalising multimodal distributions, which often occur in uncontrolled systems.

# Note that the uncertainty in the gating network may be low in these regions.
# This is undesirable behaviour in a risk-averse setting.
# It approximates the geodesic \acrshort{sde} by replacing the random variable associated with the Riemannian metric
# tensor with its expected value.
# In combination with the deterministic state approximation, this reduces the geodesic \acrshort{sde} to an \acrshort{ode}.

# As as result, trajectories may pass through regions of the desired dynamics mode which are uncertain.
# Note that the uncertainty in the gating network may be low in these regions.
# This is undesirable behaviour in a risk-averse setting.


*Decouple goals*
Without decoupling the uncertainty avoiding behaviour from the mode remaining behaviour,
it is not possible to find trajectories which avoid entering the region of high epistemic uncertainty in the
gating network.
The geometry-based approaches provide a mechanism to set the relevance of the covariance term, i.e. decouple the goals.
This is achieved by augmenting the expected Riemannian metric tensor with the user-tunable weight
parameter $\lambda$, that can be adjusted to determine the relevance of the covariance term.
The experiments show that although adjusting $\lambda$ can be beneficial in some scenarios, it is not necessarily straight
forward to set.
In particular, when regions of high epistemic uncertainty intersect with mode boundaries.
Therefore, care should be taken when setting $\lambda$.
The control-as-inference method does not provide a mechanism for adjusting the relevance of each behaviour.
Instead, it relies on the marginalisation of the latent variables in the \acrshort{elbo}
(cref:eq-traj-opt-inference) to automatically handle it, without requiring human input.
This works well in practice whilst also making the method easier to configure.

# However, as mentioned previously, it is not always clear how to balance these two behaviours.
# As shown in cref:fig-riemannian-energy-high-traj-opt-5,
# increasing the uncertainty avoiding behaviour can have a detrimental impact on the mode remaining behaviour.


*Multiple desired modes*
Although not tested, the approaches are theoretically sound and should be applicable in systems with more
than two dynamics modes.
However, it is interesting to consider if this is even necessary given the goals.
For example, in the quadcopter experiment, the transition dynamics model was intentionally instantiated
with two dynamics modes, even though there could be more in reality.
The desired dynamics mode was engineered to have a noise variance that was deemed operable.
The other dynamics mode was then used to explain away all of the inoperable modes.
In most scenarios, a similar approach could be followed.
Nevertheless, it is interesting to consider systems with more than two modes.
This is because the main goal is to avoid entering the inoperable dynamics mode, not just remain in a desired
dynamics mode.
Although not tested here, the control-as-inference method should be able to remain
in more than one desired dynamics modes.
This can be achieved by slightly altering the mode conditioning to condition on a set of modes.
In contrast, the geometry-based methods in cref:chap-traj-opt-geometry are only capable of remaining in a single
dynamics mode.

*Mode remaining guarantee*
None of the method provide theoretical guarantees that the trajectories will remain in the desired dynamics mode.
However, the chance constraints can be evaluated to check if a trajectory is $\delta-\text{mode remaining}$.
Recent work by
cite:berkenkampSafe2019,berkenkampSafe2017,kollerLearningBased2018
consider the use of invariant sets to synthesise closed-loop controllers/policies, that are guaranteed to
remain in a given set of states.
Their work considers safe sets defined by Lyapanov stability but it would be interesting to extend their
concepts to synthesise $\delta-\text{mode remaining}$ controllers
$\pi_{\desiredMode} : \desiredStateDomain  \rightarrow \controlDomain$,
with theoretical guarantees of the form,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-invariant-set-controller}
\Pr( \dynamicsFunc(\state_\timeInd, \pi_{\desiredMode}(\state_\timeInd)) \in \desiredStateDomain, \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd]  ) \ge 1 - \delta.
\end{align}
#+END_EXPORT

*Continuous-time trajectories*
The direct optimal control and control-as-inference methods required control regularisation to prevent state transitions that
"jump" over the undesired mode.
Although the discrete time steps of the trajectory appear to satisfy the constraints and minimise the cost,
in reality, the continuous-time trajectory may pass through the undesired mode.
This is a general problem that arises when solving continuous-time problems in discrete-time.
In contrast, the indirect control method uses a collocation solver where the cost can be evaluated at arbitrary
points along the continuous-time trajectory.
An interesting direction for future work is to extend the direct optimal control and control-as-inference methods
to find trajectories in continuous-time.
For example, cite:mukadamContinuoustime2018,dongMotion2016  use \acrshort{gp}s for continuous-time trajectories
when solving motion planning via probabilistic inference.
# This is because they can be used to enforce the dynamics and interpolate the cost with higher resolution.
# Stochastic collocation methods may offer

*State-control dependent modes*
This chapter assumed that the dynamics modes were separated according to their disjoint
state domains, i.e.
$\stateDomain_{i} \cap \stateDomain_{j} = \emptyset$ for distinct $i, j \in \{1, \ldots, \ModeInd\}$.
It would be interesting to extend this work to systems where the modes are governed by both state and control.
For example, flying at high speed through a wind field may be deemed an operable dynamics mode, whilst
flying at low speed may not.
# For example, flying at high speed through a wind field may belong to the operable dynamics mode with low drift and
# process noise.

*Dynamics models*
The control-as-inference method can be deployed in a wider range of dynamics models than the geometry-based
methods.
First of all, it is not limited to differentiable mean and covariance functions for the gating function GPs.
Secondly, it can be deployed in dynamics models learned with any \acrshort{mogpe} method.
This is because all \acrshort{mogpe} methods consist of a probability mass function over the expert indicator
variable.
In contrast, the geometry-based methods are limited to the \acrshort{mosvgpe} method because of it's GP-based
gating network.
Further to this, the control-as-inference method is applicable in systems where the dynamics mode's are not
necessarily separated according to their state domains $\mode{\stateDomain}$.
This is because it does not rely on the state dependent gating functions.

# *Maximum entropy control*
# The maximum entropy control that arises from using Normally distributed controls in the control-as-inference method
# improved the mode remaining behaviour in all experiments.

# the single-step dynamics models time step.

# This is an artifact of solving the trajectory optimisation problem in discrete time and can be solved by decreasing
# the single-step dynamics models time step.
# Alternatively, an interesting direction for future work is to consider methods that optimise continuous time controls
# that can be used to interpolate cost over trajectory to get higher resolution.
# For example a Gaussian process over controls instead of Gaussian.
# Or stochastic collocation.

*Real-time control*
Whilst real-time control requires efficient inference algorithms,
"offline" trajectory optimisation can trade in computational cost for greater accuracy.
This work is primarily interested in finding trajectories that attempt to remain in a
desired dynamics mode.
For the sake of simplicity, it has considered the "offline" trajectory optimisation setting.
The increased computational time may hinder
its suitability to obtain a closed-loop controller via MPC citep:eduardof.Model2007.
However, it can be used "offline" to generate reference trajectories for a tracking controller,
or for guided policy search in a model-based RL setting citep:levineGuided2013.
Alternatively, future work could investigate approximate inference methods for efficient state estimation
to aid with real-time control, e.g. iLQG/DDP.

*Infeasible trajectories*
It is worth noting that it might not be possible to find a trajectory
between a given start state $\state_0$, and a target state $\targetState$, that satisfies the chance constraints.
This may be due to either the desired dynamics mode being uncertain, or the gating network being uncertain.
In this scenario, it is desirable to explore the environment and update the learned dynamics mode with this new experience.
This will reduce the epistemic uncertainty in the model, increasing the likelihood of being able to find
trajectories that satisfy the chance constraints.
This motivates the work in cref:chap-active-learning, which addresses exploration of
multimodal dynamical systems, whilst attempting to remain in a desired dynamics mode.

# moment matching leads to closed-form gradients

*** Summary
This chapter has evaluated and compared the indirect geodesic control method from cref:sec-traj-opt-collocation,
the direct optimal control method from cref:sec-traj-opt-energy and the control-as-inference method from
cref:chap-traj-opt-inference.
The methods' abilities to navigate to a target state, whilst remaining in a desired dynamics mode,
were evaluated on two velocity controlled quadcopter navigation problems.
The results in this chapter have verified that the latent geometry of the \acrshort{mosvgpe} gating
network can be used to encode mode remaining behaviour into control strategies.
However, the results indicate that the indirect geodesic control method is not only the lowest performing
method but is also the hardest method to configure.
In contrast, the direct optimal control and control-as-inference methods work well in practice,
whilst being much easier to configure.

Although the geometry-based methods have a tunable parameter $\lambda$ for balancing the mode remaining
and the uncertainty avoiding behaviour, it does not work well in practice.
In contrast, the control-as-inference method automatically balances the behaviours by marginalising the uncertainty
in the control-as-inference's \acrshort{elbo}.

From the experiments in this chapter, it can be concluded that both the direct optimal control
and the control-as-inference methods are competitive approaches for finding mode remaining trajectories.
They find trajectories that navigate to the target state, satisfy the dynamics constraints and attempt to remain in the
desired dynamics mode.
Further to this, it is easy to verify that they are $\delta-\text{mode remaining}$ by evaluating
the mode chance constraints.

# Overall, both the direct optimal control and the control-as-inference methods performed well in the experiments.

# that do not violate the dynamics constraints,
# whilst ensuring trajectories are $\delta-\text{mode remaining}$.

# Overall, the direct optimal control approach offers superior performance, as it finds trajectories that do not violate
# the dynamics constraints, whilst ensuring trajectories are $\delta-\text{mode remaining}$.

# Given a start and end state, both of the methods presented in this chapter can be tuned to find trajectories that either
# prioritise remaining in a desired dynamics mode, or prioritise avoiding regions of the learned
# dynamics model with high epistemic uncertainty.

# The direct optimal control algorithm from cref:sec-traj-opt-energy and the mode chance constraints from
# cref:eq-mode-chance-constraint are used in cref:chap-active-learning

# The control algorithms find trajectories from an initial state
# $\state_0$, to a target state $\targetState$, whilst remaining in a desired dynamics mode.

# The methods leverage the \acrshort{mosvgpe} model from cref:chap-dynamics to first learn a single-step dynamics model.
# This chapter has shown that the geometry of the \acrshort{mosvgpe} gating network
# infers valuable information regarding how a system switches between its underlying modes.
# Moreover, it has shown how this latent geometry can be leveraged to encode both mode remaining
# and risk-sensitive behaviour into control strategies.

# This chapter has shown that the geometry of the gating network from the \acrshort{mosvgpe} model from cref:chap-dynamics
# infers valuable information regarding how a system switches between its underlying modes.
# Further to this, it has detailed two trajectory optimisation algorithms that encode the goals via this
# latent geometry.
# That is, the algorithms find trajectories that remain in a desired dynamics mode,
# whilst also avoiding regions of high epistemic uncertainty.

# This chapter has presented a method for performing trajectory optimisation in
# multimodal dynamical systems with the transition dynamics modelled as a
# \acrshort{mogpe} method.
# The trajectory optimisation is projected onto a probabilistic Riemannian
# manifold parameterised by the gating network of the \acrshort{mogpe} model.
# Given a start and end state, the trajectory optimisation can be tuned to find trajectories that either
# prioritise remaining in a desired dynamics mode or prioritise avoiding regions of the learned transition
# dynamics model with high epistemic uncertainty.

* Mode Remaining Exploration for Model-Based Reinforcement Learning label:chap-active-learning
#+begin_export latex
\epigraph{Real knowledge is to know the extent of ones ignorance.}{\textit{Confucius (Philosopher, 551479BC).}}
\renewcommand{\modeOpt}{ModeOpt }
#+end_export
** maths :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\explorativeController}{\ensuremath{\pi_{\text{explore}}}}
\renewcommand{\modeController}{\ensuremath{\pi_{\text{mode}}}}
\renewcommand{\dynamicsModel}{\ensuremath{p_{\theta}}}

\renewcommand{\desiredGatingKernel}{\ensuremath{\hat{k}_{\desiredMode}}}
\renewcommand{\desiredGatingVariance}{\ensuremath{\hat{\sigma}^2_{\timeInd}}}
\renewcommand{\desiredGatingMean}{\ensuremath{\hat{\mu}_{\timeInd}}}
\renewcommand{\desiredGatingCovFunc}{\ensuremath{\bm\Sigma^2_{\desiredMode}}}
\renewcommand{\desiredGatingMeanFunc}{\ensuremath{\bm\mu_{\desiredMode}}}

\newcommand{\initialStateDomain}{\ensuremath{\stateDomain_0}}

\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{\MakeUppercase{\timeInd}}}
\newcommand{\stateTraj}{\ensuremath{\bar{\state}}}
\newcommand{\controlTraj}{\ensuremath{\bar{\control}}}
\newcommand{\nominalStateTraj}{\ensuremath{\stateTraj_*}}
\newcommand{\nominalControlTraj}{\ensuremath{\controlTraj_*}}

\newcommand{\utraj}{\ensuremath{\bar{\control}}}
\newcommand{\policies}{\ensuremath{\Pi}}
\newcommand{\policy}{\ensuremath{\pi_{\theta}}}


\renewcommand{\dataset}{\ensuremath{\mathcal{D}}}
\renewcommand{\params}{\ensuremath{\bm\theta}}
\renewcommand{\input}{\ensuremath{\hat{\state}}}
\renewcommand{\output}{\ensuremath{\modeVar}}
\renewcommand{\inputDomain}{\ensuremath{\controlDomain}}
\renewcommand{\outputDomain}{\ensuremath{\mathcal{A}}}

\newcommand{\outputGivenInputParams}{\ensuremath{p(\output \mid \input, \params)}}
\newcommand{\outputGivenInputData}{\ensuremath{p(\output \mid \input, \mathcal{D})}}
#+END_EXPORT
** Intro :ignore:
Similarly to cref:chap-traj-opt-control, this chapter is concerned with navigating from an initial
state $\state_0$  in a desired dynamics mode $\desiredMode$  to a target state $\targetState$, whilst guaranteeing
that the controlled system remains in the desired dynamics mode. Moreover, it
considers environments where both the underlying dynamics modes and how the
system switches between them, are not fully known a priori. However, in contrast
to previous chapters, it does not assume prior access to the environment. That is,
it considers the more realistic scenario, where the agent must iteratively explore its
environment, collect data and update its dynamics model  whilst remaining in
the desired dynamics mode  until it can confidently navigate to the target state
$\targetState$. Following previous chapters, this chapter also considers model-based approaches
where the dynamics model is learned from observations.

The main contribution of this chapter is a mode remaining exploration algorithm
that can explore multimodal environments with high probability of remaining in a
desired dynamics mode.

cref:problem-statement-explore formally states the control problem.
Section 7.2 then details the

This chapter seeks to solve this navigation problem by combining all of the work in this dissertation.
To this end, it first hypothesises how the work can be combined to solve the navigation problem.
It then uses the work to develop exploration strategies with high probability of remaining in the desired dynamics mode.
This chapter presents some initial results in the simulated quadcopter environment and details future work.
The work in this chapter is implemented in TensorFlow/GPflow and is available on GitHub
citep:tensorflow2015-whitepaper,GPflow2017[fn:2:Code accompanying this Chapter can be found @ [[https://github.com/aidanscannell/ModeOpt][\icon{\faGithub}/aidanscannell/ModeOpt.]]]

# This chapter is concerned with navigating from an initial state $\state_0$ -- in a desired dynamics mode $\desiredMode$
# -- to the target state $\targetState$, whilst guaranteeing that the controlled system remains in the desired dynamics mode.
# Moreover, it considers environments where both the underlying dynamics modes and how the system switches between them,
# are /not fully known a priori/.
# Following previous chapters, this chapter also considers model-based approaches where the dynamics model
# is learned from observations.
# However, in contrast to previous chapters, it does not assume prior access to the environment where it can collect
# a data set of state transitions.
# That is, it considers the more realistic scenario, where the agent must iteratively explore its environment --
# whilst remaining in the desired dynamics mode -- until it can confidently navigate to the target state $\targetState$.

** Problem Statement label:problem-statement-explore
*** intro :ignore:
Following previous chapters, this work considers nonlinear, stochastic, multimodal dynamical systems,
#+BEGIN_EXPORT latex
\begin{subequations}
\begin{align}
\state_{\timeInd+1} &= \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \bm\epsilon \\
&= \mode{\dynamicsFunc}(\state_\timeInd, \control_\timeInd) + \mode{\bm\epsilon}
\quad \text{if} \quad \modeVar(\state_{\timeInd})=\modeInd \nonumber \\
\mode{\bm\epsilon} &\sim \mathcal{N}(\mathbf{0}, \Sigma_{\mode{\bm\epsilon}}),
\end{align}
\end{subequations}
#+END_EXPORT
with states $\state \in \stateDomain$, controls $\control \in \controlDomain$ and a discrete time index $\timeInd$.
The discrete mode indicator variable
$\alpha_{\timeInd} \in \modeDomain = \mathbb{Z} \cap [1,\ModeInd]$,
indicates which of the $\ModeInd$ underlying dynamics modes
$\{\mode{\latentFunc} : \mode{\stateDomain} \times \controlDomain \rightarrow \stateDomain \}_{\modeInd=1}^\ModeInd$,
and associated noise models
$\mode{\bm\epsilon} &\sim \mathcal{N}\left(\mathbf{0}, \bm\Sigma_{\mode{\bm\epsilon}} \right)$, where
$\bm\Sigma_{\bm\mode{\epsilon}} = \text{diag}\left[ \sigma_{\modeInd,1}^{2}, \ldots, \sigma_{\modeInd,\StateDim}^2 \right]$,
governs the system at a given time step $\timeInd$.

# #+BEGIN_EXPORT latex
# \begin{definition}[Mode Remaining] \label{def-mode-remaining-explore}
# Given an inital state $\state_0$ in dynamics mode $\desiredMode$
# and a sequence of controls $\controlTraj=\control_{0:\TimeInd-1}$,
# the controlled system is said to be mode-remaining iff:
# \begin{align} \label{eq-mode-remaining-def-explore}
# \modeVar_{\timeInd} = \desiredMode \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd]
# %\dynamicsFunc(\state_{\timeInd}, \control_{\timeInd}) &\in \mode{\stateDomain} \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd]
# \end{align}
# \end{definition}
# #+END_EXPORT
The overall goal is to navigate from an initial state $\state_0$ -- in a desired dynamics mode $\desiredMode$
-- to the target state $\targetState$, whilst guaranteeing that the controlled
system remains in the desired dynamics mode.
Formally, a mode remaining controlled system is defined as follows.
#+BEGIN_EXPORT latex
\begin{definition}[Mode Remaining] \label{def-mode-remaining-explore}
Let $\modeInd$ denote a dynamics mode, defined by its state domain $\mode{\stateDomain} \subseteq \stateDomain$.
Given an inital state $\state_0 \in \mode{\stateDomain}$, and a controller $\policy \in \Pi$,
the controlled system is said to be mode-remaining iff:
\begin{align} \label{eq-mode-remaining-def-explore}
\dynamicsFunc(\state_{\timeInd}, \policy(\state_{\timeInd}, \timeInd))
&\in \mode{\stateDomain} \quad \forall \timeInd
\end{align}
\end{definition}
#+END_EXPORT
Notice that in contrast to previous chapters, the controller
$\policy : \stateDomain \times \mathbb{Z} \rightarrow \controlDomain$ is defined
generally. That is, it may take the form of a deterministic feedback policy or an
open-loop set of controls without state feedback.
It is assumed that $\Pi$ is the set of deterministic policies.

Given this definition of a mode remaining controlled system, the problem that this work seeks to solve
can be formally stated as,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-mode-explore-problem}
\begin{align}
\min_{\policy \in \Pi} \quad &J(\policy) \\
\text{s.t.} \quad &\state_{\timeInd+1} = \mode{\dynamicsFunc}(\state_\timeInd, \policy(\state_\timeInd, \timeInd)) + \mode{\epsilon},
\quad \modeVar(\state_{\timeInd}) = \modeInd \\
%&\text{\cref{eq-mode-remaining-def-explore}} \\
&\dynamicsFunc(\state_{\timeInd}, \policy(\state_{\timeInd}, \timeInd))
\in \desiredStateDomain \quad \forall \timeInd \in \{0, \ldots, \TimeInd-1 \} \\
%\modeVar_{\timeInd} = \desiredMode \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] \\
%&\state_{\timeInd} \in \desiredStateDomain \quad \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] \\
&\state_{0} = \state_0 \\
&\state_\TimeInd = \targetState,
\end{align}
\end{subequations}
#+END_EXPORT
where $J : \Pi \rightarrow \R$ denotes the objective function.
Note that the objective function
is not of primary importance in this work. It may be used to balance behaviours
such as speed and efficiency.
cref:eq-mode-explore-problem resembles the problem statement from cref:chap-traj-opt-control,
except that it does not assume a priori access to the environment. As
such, it does not have access to a historical data set of state transitions which it can
use to learn a predictive dynamics model. Instead, it must incrementally explore
the environment, without violating the mode remaining constraints. However, given
that the underlying dynamics modes and how the system switches between them are
not fully known a priori,
it is not possible to guarantee that the controlled system remains in the desired dynamics mode.

For two reasons, this work relaxes the requirement to keeping the system in the desired mode with high probability.
Firstly, given the dynamics model is learned from observations, it is not possible to find trajectories that
are mode remaining according to  cref:def-mode-remaining-explore.
Secondly, the agent will not be able to explore the environment if it is
mode remaining according to  cref:def-mode-remaining-explore
#+BEGIN_EXPORT latex
\begin{definition}[$\delta$-mode remaining] \label{def-delta-mode-remaining}
Let $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ denote a multimodal dynamical system
and $\desiredMode$ a desired dynamics mode defined by its state domain $\desiredStateDomain \subseteq \stateDomain$.
Given an initial state $\state_0 \in \desiredStateDomain$ and $\delta \in (0,1]$,
a controlled system is said to be $\delta$-mode remaining under the controller $\policy$ iff:
\begin{align}
%\Pr( \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] : \modeVar_{\timeInd}) = \desiredMode,
%\control_{\timeInd} \in \controlDomain) \geq 1 - \delta
\Pr( \forall \timeInd \in \mathbb{Z} \cap [0,\TimeInd] : \dynamicsFunc(\state_{\timeInd}, \policy(\state_{\timeInd}, \timeInd))
&\in \desiredStateDomain,
\policy(\state_{\timeInd}, \timeInd) \in \controlDomain) \geq 1 - \delta
\end{align}
\end{definition}
#+END_EXPORT
Intuitively, the more the $\delta-\text{mode remaining}$ constraint is relaxed, the more the controller can explore.
However, this will also increase the controllers chance of leaving the desired dynamics mode.
Thus, the algorithm should consider two important properties:
- Expand the region that is known to belong to the desired dynamics mode without violating the mode constraints,
- Find the optimal controller parameters.

*** Initial Model :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{0}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-initial}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-initial}
\end{minipage}
\caption{\textbf{Initial gating network} Gating network posterior after training on the initial data set $\dataset_0$
shown by the blue crosses.
The goal is to incrementally explore the environment until $\delta-\text{mode remaining}$ trajectories
under the learned dynamics model, can be found from the start state $\state_0$, to the target state $\targetState$.
The data set is overlayed on
(\subref{fig-explorative-data-over-prob-7-initial})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-initial})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-initial}
\end{figure}
#+END_EXPORT

*** Initial Mode remaining controller :ignore:

*Initial mode remaining controller*
In robotics applications, an initial set of poor performing controllers can normally be obtained via
simulation or domain knowledge.
This works assumes access to an initial data set of state transitions
$\dataset_0 = \{(\state_n, \control_n), \Delta \state_n\}_{n=1}^{N_0}$ collected from around the start state $\state_0$.
#+BEGIN_EXPORT latex
\begin{assumption} \label{}
An initial region of the state space $\initialStateDomain \subseteq \desiredStateDomain$ is known to belong to the desired
dynamics mode $\desiredMode$.
As such, a state transition data set can be collected
$\dataset_0 = \{(\state_n, \control_n), \Delta \state_n\}_{n=1}^{N_0}$
such that it contains the start state $((\state_0, \cdot), \cdot) \in \dataset_0$.
\end{assumption}
#+END_EXPORT
This data set is used to learn a predictive dynamics model $\dynamicsModel$
which is locally accurate around the start state $\state_0$.
cref:fig-explorative-traj-opt-7-initial shows the \acrshort{mosvgpe}'s gating network posterior after training on the
initial data set $\dataset_0$ for the quadcopter navigation problem in the illustrative example from
cref:illustrative_example.
cref:fig-explorative-data-over-prob-7-initial illustrates that the desired dynamics mode is $\desiredMode=2$
and cref:fig-explorative-data-over-svgp-7-initial shows that the \acrshort{gp} posterior over the gating function
is uncertain away from the initial data set $\dataset_0$, indicated by high posterior variance.
This is further demonstrated by the probability mass function over the mode indicator variable,
in cref:fig-explorative-data-over-prob-7-initial,
tending to a uniform distribution, i.e. maximum entropy.
Although this model can be used to learn an
initial controller, it may not work outside of the initial state domain $\initialStateDomain$ and may
not be able to find $\delta-\text{mode remaining}$ trajectories to the target state $\targetState$, due to
the models epistemic uncertainty.

** Mode Optimisation
*** Flowchart :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[!t]
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=2cm, minimum height=1cm,text centered, draw=black, fill=violet!30]
\tikzstyle{decision} = [diamond, rounded corners, minimum width=1cm, maximum height=1cm,text centered, draw=black, fill=violet!30]
%\tikzstyle{decision} = [diamond, minimum width=2cm, minimum height=0.5cm, text centered, draw=black, fill=violet!30]
\tikzstyle{arrow} = [thick,->,>=stealth]
% \begin{tikzpicture}[node distance=1.8cm, style={align=center}]]
\centering
\begin{tikzpicture}[every text node part/.style={align=center}, node distance=1.5cm]
\node (start) [startstop] {Learn dynamics model $\dynamicsModel$};
\node (trajopt) [startstop, below of=start, left of=start, xshift=-2cm] {Find trajectory to $\state_f$};
\node (constraints) [decision, below of=trajopt, yshift=-1cm] {$\delta-\text{mode}$\\ remaining?};
%# \node (constraints) [startstop, below of=trajopt] {$\delta-\text{mode remaining}$?};
\node (execute) [startstop, below of=constraints, yshift=-1cm] {Execute trajectory in environment};
\node (explore) [startstop, right of=constraints, xshift=6cm] {$\delta-\text{mode exploration}$ with $\explorativeController$};

\draw [arrow] (start) -| (trajopt);
\draw [arrow] (trajopt) -- (constraints);
\draw [arrow] (constraints) -- node[anchor=east] {yes} (execute);
\draw [arrow] (constraints) -- node[anchor=south] {no} (explore);
\draw [arrow] (explore) |- node[anchor=west] {Experience} (start);
\end{tikzpicture}
\caption{Flowchart showing the sequence of steps and processes needed to control a system
from an initial state $\state_0$, to a target state $\targetState$, with $\delta-\text{mode remaining}$ guarantee,
when the underlying dynamics modes and how
the system switches between them are \textit{not fully known a priori}.}
\label{fig-mode-opt-loop}
\end{figure}
#+END_EXPORT
*** After flowchart :ignore:

This section details the method for solving the mode remaining navigation problem that combines all of
the work in this dissertation.
The method is named Mode Optimisation or \modeOpt for short.
At its core, \modeOpt learns a single-step dynamics model $\dynamicsModel$ using the \acrshort{mosvgpe}
model from cref:chap-dynamics.
Given this model, the mode remaining control methods in cref:chap-traj-opt-control
can be used to find trajectories to the target state $\targetState$.
The mode chance constraints from cref:eq-mode-chance-constraint
can then be used to check if these trajectories are $\delta-\text{mode remaining}$ under the learned dynamics
model $\dynamicsModel$.
Initially, it will not be possible to find
$\delta-\text{mode remaining}$ trajectories under the learned dynamics model, due to high
epistemic uncertainty.
In this case, the agent must explore the environment, collect  data and update its dynamics model.
Eventually the agent will reduce the model's epistemic uncertainty such that
it can find $\delta-\text{mode remaining}$ trajectories to the target state $\targetState$.
Ideally, the exploration strategy will have some guarantee on remaining in the desired dynamics mode.
cref:fig-mode-opt-loop illustrates this process in a flowchart.
# and cref:alg-mode-opt details the algorithm presented in this chapter.
# The algorithm presented in this chapter is detailed in cref:alg-mode-opt.

*$\delta-\text{mode remaining}$ exploration*
In order to implement such an algorithm, this chapter first proposes a method for exploring the environment
with mode remaining guarantees.
That is, a controller that will explore the environment whilst ensuring the controlled system remains in the
desired dynamics mode with high probability, i.e. satisfy cref:eq-mode-remaining-def-explore.

*** MBRL Alg :ignore:
#+BEGIN_EXPORT latex
\begin{algorithm}[!t]
\caption{Model-based reinforcement learning}\label{alg-mbrl}
\begin{algorithmic}[1]
\Require{Policy/controller $\policy_0$, dynamics model $\dynamicsModel}$, start state $\state_0$}
\For{$i = 0, 1,  \ldots $}
    \State Select $\pi_i$ using exploration strategy, e.g. \cref{eq-greedy-exploration,eq-thompson-exploration,eq-active-exploration}
    \State Collect environment data set $\dataset_i$ using $\pi_i$; add to dataset $\dataset_{0:i} = \{\dataset_{i} \cup \dataset_{0:i-1}\}$
    \State Update dynamics model $\dynamicsModel}$ using $\dataset_{0:i}$.
\EndFor
\end{algorithmic}
\end{algorithm}
#+END_EXPORT
*** Exploration Strategies
The performance of \modeOpt depends on its ability to explore the environment.
This section recaps some relevant exploration strategies that fit into the general
\acrfull{mbrl} procedure shown in cref:alg-mbrl.

*Greedy exploitation*
One of the most commonly used exploration strategies is to select the controller that maximises
the expected performance under the learned dynamics model $\dynamicsModel(f \mid \mathcal{D}_{0:i-1})$.
Note that $i$ denotes the iteration/episode number of the \acrshort{mbrl} loop.
This greedy strategy is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-greedy-exploration}
\pi_i^{\text{greedy}} = \arg \max_{\pi \in \Pi} \E_{f \sim \dynamicsModel(f \mid \mathcal{D}_{0:i-1})} \left[ J(f, \pi) \right].
\end{align}
#+END_EXPORT
This approach is used in \acrshort{pilco} citep:deisenrothPILCO2011 and GP-MPC citep:kamtheDataEfficient2018
where the dynamics are represented using \acrshort{gp}s and the moment matching approximation is used to cascade
single-step predictions.
cite:parmasPIPPS2018 propose \acrshort{pipps},
a similar approach to \acrshort{pilco}, except that they use Monte Carlo methods
to integrate the dynamics forward it time, instead of using the moment matching approximation.
Similarly, \acrshort{pets} citep:chuaDeep2018 uses this exploration strategy but represents the dynamics
using ensembles of probabilistic neural networks.
This strategy initially favours exploring regions of the environment where the learned dynamics
model is not confident, i.e. has high epistemic uncertainty.
Once it has gathered knowledge of the environment and the models
epistemic uncertainty has been reduced, it favours maximising the objective function $J(f, \pi)$.
See cite:curiEfficient2020 for a recap of some commonly used MBRL exploration strategies.

In contrast to standard MBRL, the method presented in this chapter side-steps the
challenging exploration-exploitation trade-off. That is, \modeOpt does not require
an objective that changes its exploration-exploitation balance as it gathers more
knowledge of the environment. This is because it uses the $\delta-\text{mode remaining}$ chance
constraints to separate the exploitative mode remaining controller $\modeController$ from the
explorative controller $\explorativeController$. However, the explorative controller $\explorativeController$ should
seek to reduce the epistemic uncertainty in the learned dynamics model $\dynamicsModel$ such that
$\delta-\text{mode remaining}$ trajectories to the target state $\targetState$ can eventually be found.
That is, the exploration should be targeted towards the target state $\targetState$.

# *Thompson sampling*
# An alternative and theoretically grounded strategy is Thompson sampling.
# This approach samples a single model  $f_i \sim \dynamicsModel(f \mid \mathcal{D}_{0:i})$ at every iteration $i$
# and uses the sampled model to optimise the controller.
# This is given by,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-thompson-exploration}
# \pi_i^{\text{thompson}} = \arg \max_{\pi \in \Pi} J(f_i, \pi) \quad \text{s.t.} \quad f_i \sim \dynamicsModel(f \mid \mathcal{D}_{0:i}).
# \end{align}
# #+END_EXPORT
# # This contrasts the greedy approach that averages over the
# In general it is intractable to sample from $\dynamicsModel(f \mid \mathcal{D}_{0:i})$, although methods do exist
# for both \acrshort{gp} models and ensembles.
# Note that after the sampling step this problem is equivalent to greedy exploitation.


*Active learning*
Active learning is another class of exploration algorithms.
The goal of information theoretic active learning is to reduce the number of
possible hypothesis as fast as possible, i.e. minimise the uncertainty associated
with the parameters using Shannon's entropy citep:coverElements2006,
#+BEGIN_EXPORT latex
\newcommand{\crv}{\ensuremath{X}}
\newcommand{\density}{\ensuremath{p(\crv)}}
\begin{myquote}
\textbf{Differential Entropy}
Let $\crv$ be a continuos random variable, with a probability density
$\density$,
whose support is a set $\mathcal{X}$.
The differential entropy $H(\crv)$ is then defined as,
\begin{align} \label{eq-differential-entropy}
H[\crv] = - \int_{\mathcal{X}} \density \text{log} \density \text{d} \crv.
\end{align}
\end{myquote}
#+END_EXPORT
In contrast to the previous approaches, active learning does not seek to maximise a
black-box objective. Instead, it is only interested in exploration. There are many
approaches to active learning in the static setting, i.e. in systems where an arbitrary
state $\state$ can be sampled. In contrast, dynamical system must be steered to $\state$
through the unknown dynamics $f$ through a sequence of controls $\controlTraj$. Thus, there is
information gain along the trajectory which must also be considered. As highlighted
by cite:buisson-fenetActively2020, information gain in dynamical systems is therefore
fundamentally different to the static problem addressed by cite:krauseNearOptimal2008  and
cite:houlsbyBayesian2011.
The goal here is to pick the most informative control trajectory
$\controlTraj$ whilst observing $\stateTraj$.

Recent work has addressed active learning in GP dynamics models. cite:schreiterSafe2015
propose a greedy entropy-based strategy that considers the entropy of the
next state. cite:buisson-fenetActively2020 also propose a greedy entropy-based strategy
except that they consider the entropy accumulated over a trajectory. In contrast,
cite:caponeLocalized2020,yuActive2021 propose to use the mutual information.
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Mutual Information}
Given two sets of random variables, $\mathbf{X}$ and $\mathbf{F}$, with joint density $p(\mathbf{X}, \mathbf{F})$ the
mutual information \cite{coverElements2006} is given by,
\begin{align} \label{eq-mutual-information}
I(\mathbf{X};\mathbf{F}) = \int p(\mathbf{X},\mathbf{F}) \text{log}\frac{p(\mathbf{X},\mathbf{F})}{p(\mathbf{X})p(\mathbf{F})}
\text{d}\mathb{X} \text{d}\mathb{F},
\end{align}
and its well known relationship to differential entropy $H(\cdot)$ is given by,
\begin{align} \label{eq-mutual-information-entropy}
I(\mathbf{X};\mathbf{F}) =  H(\mathbf{X}) - H(\mathbf{X} \mid \mathbf{F}).
\end{align}
\end{myquote}
#+END_EXPORT
cite:caponeLocalized2020 find the most informative state as the one that minimises the
mutual information between it and a set of reference states (a discretisation of the
domain). They then find a set of controls to drive the system to this most informative state.
Given a fixed number of time steps, their method yields a better model
than the greedy entropy-based strategies.
cite:yuActive2021  propose an alternative
approach that leverages their \acrfull{gpssm} inference scheme to estimate the mutual
information between all the variables in time
$I \left[\mathbf{y}_{1:t}, \hat{\mathbf{y}}_{t+1} ; \mathbf{f}_{1:t+1} \right]$.
Here $\mathbf{y}_{1:t}$ denotes the set of observed outputs and $\hat{\mathbf{y}}_{t+1}$ denotes
the output predicted by the \acrshort{gpssm}. This contrasts other approaches, which tend
to study the latest mutual information $I[\hat{\mathbf{y}}_{t+1} ; \mathbf{f}_{t+1}]$.

*Myopic active learning*
In reinforcement learning and control it is standard to
consider objectives over a (potentially infinite) horizon. Should active learning in
dynamical systems be any different? This work build on the hypothesis that it must
be important to consider the information gain over a (potentially infinite) horizon, as
opposed to myopically selecting the next input, i.e. only considering the next query
point. The mutual information approaches in
cite:caponeLocalized2020,yuActive2021
fall into this myopic category as they only maximise the information gain at the next
time step. In contrast, the greedy entropy-based strategy in
cite:buisson-fenetActively2020 considers the entropy over a horizon.
The strategy presented in this chapter
relieves the myopia of active learning by leveraging a similar greedy entropy-based
strategy over a finite horizon.

It is worth noting here that the work by cite:schreiterSafe2015  is similar to the
method presented in this chapter. They consider safe exploration for active learning
where they distinguish safe and unsafe regions with a binary GP classifier, which is
learned separately to the dynamics model. Their exploration strategy then considers
the differential entropy of the dynamics GP and they use the GP classifier to define
a set of safety constraints. The work in this chapter similarly learns a GP classifier
that defines a set of constraints. However, the classifier takes the form of the latent
gating network of the \acrshort{mosvgpe} dynamics model. The work in this chapter further
differs as it leverages an exploration strategy that considers the uncertainty in the
gating network, as opposed to the uncertainty in the dynamics GP.



# In contrast to the previous approaches, active learning does not seek to maximise a black-box objective.
# Instead, it is only interested in exploration.
# This is given by,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-active-exploration}
# \pi_i^{\text{active}} = \arg \max_{\pi \in \Pi} \matahbb{H} \left[ f \mid \mathcal{D}_{0:i} \right].
# \end{align}
# #+END_EXPORT

# In dynamical systems an arbitrary state $\state$ cannot be sampled, so instead, the system must be steered
# to $\state$ through the unknown dynamics $\dynamicsFunc$ through a sequence of controls $\control$.
# Thus, there is information gain along the trajectory which must also be
# considered.
# As highlighted by cite:buisson-fenetActively2020, information gain in dynamical systems is therefore
# fundamentally different to the /static/ problem addressed by cite:krauseNearOptimal2008
# and cite:houlsbyBayesian2011.
# The goal here is to pick the most informative control trajectory $\controlTraj$ whilst observing $\stateTraj$.

# Recent work has addressed active learning in \acrshort{gp} dynamics models
# citep:buisson-fenetActively2020 and \acrshort{gp} state-space models citep:caponeLocalized2020.
# cite:schreiterSafe2015 consider safe exploration for active learning where they distinguish safe and unsafe regions
# with a binary \acrshort{gp} classifier, which is learned separately to the dynamics model.
# Their exploration strategy considers the differential entropy of the dynamics \acrshort{gp} and they
# use the \acrshort{gp} classifier to define a set of safety constraints.

# This chapter resembles cite:schreiterSafe2015
# except that it exploits the coupled learning of the dynamics modes and the gating network, instead of learning
# a separate \acrshort{gp} classifier to distinguish safe and unsafe regions.
# The exploration strategy is then projected onto the posterior \acrshort{gp}(s) associated with the gating network.
# The coupled learning provides well-calibrated uncertainty estimates in the gating network, making it a
# good choice for information-based exploration.


# *Maximum entropy reinforcement learning*

*** Mode Remaining Exploration label:sec-exploration
The exploration strategy presented in this chapter is primarily interested
in exploring a single desired dynamics mode whilst avoiding entering any of the other modes.
It is worth noting that this is a challenging problem because the agent must observe regions
outside of the desired dynamics mode
in order to know that a particular region does not belong to the desired mode.
To the best of our knowledge, there is no previous work addressing exploration of
a single dynamics mode in multimodal dynamical systems.
# cite:schreiterSafe2015 use a GP classifier to identify safe and unsafe regions
# when learning GP dynamics models in an active learning setting.
# However, they assume that they can directly observe whether a particular
# data point from the environment belongs to either the safe or unsafe regions.
# In contrast, this chapter is concerned with scenarios where the mode cannot be directly observed from the
# environment, but instead, is inferred by a probabilistic dynamics model.

*** fig :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{0.48\columnwidth}
\centering
%\includegraphics[width=0.78\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\adjincludegraphics[trim={0 0 {.45\width} 0},clip, width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{State difference $J(f, \pi) = - \sum_{\timeInd=1}^{\TimeInd-1}
\E \left[ (\state_{\timeInd} - \targetState)^T \mathbf{Q} (\state_{\timeInd} - \targetState) \right]$}
\label{fig-explorative-state-diff-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{0.48\columnwidth}
\centering
\adjincludegraphics[trim={0 0 {.45\width} 0},clip, width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{Joint entropy $J(f, \pi) = H \left[ \desiredGatingFunction(\stateTraj) \mid \stateTraj, \dataset_{0:i-1} \right]$}
\label{fig-explorative-mvn-full-cov-traj-opt-over-prob-7}
\end{minipage}
\begin{minipage}[r]{0.48\columnwidth}
\centering
\adjincludegraphics[trim={0 0 {.45\width} 0},clip, width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{Factorised entropy $J(f, \pi) = \sum_{\timeInd=1}^{\TimeInd}
H \left[ \desiredGatingFunction(\state_{\timeInd}) \mid \state_{\timeInd}, \dataset_{0:i-1} \right]$}
\label{fig-explorative-mvn-diag-traj-opt-over-prob-7}
\end{minipage}
\caption{\textbf{Comparison of explorative objectives}
Comparison of solving the constrainted optimisation in \cref{eq-explorative-traj-opt} with different objectives.
(\subref{fig-explorative-state-diff-traj-opt-over-prob-7})
shows the results found when using only the state difference term.
It shows the iteration where the optimisation gets stuck and no longer explores.
(\subref{fig-explorative-mvn-full-cov-traj-opt-over-prob-7}) shows the results when
using the joint entropy over a trajectory and the state difference term.
(\subref{fig-explorative-mvn-diag-traj-opt-over-prob-7}) then shows
the results when summing the entropy at each time step, instead of considering the full
joint distribution over the trajectory.
The trajectory collapses onto a single location of high entropy.
The optimised controls are rolled out in the desired modes
GP dynamics (magenta) and in the environment (cyan) and are overlayed on each modes mixing probability.}
\label{fig-explorative-comparison-traj-opt-7}
\end{figure}
#+END_EXPORT

*** exploration :ignore:
*$\delta-\text{mode remaining}$ exploration*
The exploration strategy presented here is primarily interested in exploring
regions of the gating network that are uncertain.
It relieves the myopia of active learning by considering trajectory optimisation with
a greedy entropy-based strategy over a finite horizon. Further to this,
it principally ensures solutions are $\delta-\text{mode remaining}$ via the chance constraints
in cref:eq-mode-chance-constraint-explore. The objective combines greedy entropy-based exploration of
the gating network with the main goal of navigating to the target state $\targetState$. The
exploration strategy is given by,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-explorative-traj-opt}
\begin{align}
%\arg \max_{\controlTraj} &\sum_{\timeInd=0}^{\TimeInd}
%\arg \max_{\pi \in \Pi} \sum_{\timeInd=0}^{\TimeInd}
%&\underbrace{\mathcal{H} \left[
%\desiredGatingFunction(\state_{\timeInd}) \mid \stateTraj_{\neg\timeInd}, \desiredGatingFunction(\stateTraj_{\neg\timeInd}), \dataset \right]}_{\text{conditional gating entropy}} \\
\arg \max_{\pi \in \Pi}
&\underbrace{\mathcal{H} \left[
\desiredGatingFunction(\stateTraj) \mid \stateTraj, \dataset_{0:i-1} \right]}_{\text{joint gating entropy}} \\
&+ \sum_{\timeInd=1}^{\TimeInd-1}
\E_{\dynamicsModel(\state_{\timeInd+1} \mid \state_{\timeInd}, \pi(\state_{\timeInd}, \timeInd))}
\left[ \underbrace{- (\state_{\timeInd} - \targetState)^T \mathbf{Q}
(\state_{\timeInd} - \targetState)}_{\text{state difference}}
- \underbrace{\control_{\timeInd}^{\TimeInd} \mathbf{R} \control_{\timeInd}}_{\text{control cost}}
\right] \\
%+ \sum_{\timeInd=1}^{\TimeInd-1} \underbrace{\control_{\timeInd}^T \mathbf{R} \control_{\timeInd}}_{\text{control cost}} \right] \\
\text{s.t.} \quad &\Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{0}, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd-1}=\desiredMode)
\geq 1-\delta \quad \forall \timeInd \in \{ 0, \ldots, \TimeInd \} \\
&\control_{\timeInd} = \pi(\state_{\timeInd}, \timeInd) \in \controlDomain \quad \forall \{0, \ldots, \TimeInd-1\},
\end{align}
\end{subequations}
#+END_EXPORT
where $\stateCostMatrix$ and $\controlCostMatrix$ are user defined, real, symmetric, positive semi-definite and
positive definite matrices respectively.
Intuitively, the first term (joint gating entropy) seeks to find trajectories
that explore regions of the gating network with high epistemic uncertainty and the second (state difference) term
targets exploration towards the target state $\targetState$.
The state difference term favours trajectories whose centre
of mass is closer to the target state $\targetState$.
However, using only the state difference term with the mode chance constraints leads to the optimisation getting
stuck in local optima and never exploring to the target state.
This is shown in cref:fig-explorative-state-diff-traj-opt-over-prob-7, which shows the final iteration of the
optimisation where the trajectory is stuck at the mode boundary.
The result in cref:fig-explorative-mvn-full-cov-traj-opt-over-prob-7
suggests that using only the joint gating entropy term
with the mode chance constraints prevents exploration towards the target state. This
happens when there is a mode boundary between the start state $\state_0$ and the target
state $\targetState$. In this case, the objective explores away form the mode boundary and
does not explore towards the target state $\targetState$.

However, when these two terms are combined,
the objective favours maximum entropy trajectories whose center of mass is closest
to the target state $\targetState$.
cref:fig-explorative-mvn-full-cov-traj-opt-over-prob-7 shows that
it is capable of exploring to the target state $\targetState$ without getting stuck in a local minima.

# This first term resembles a greedy entropy-based active learning objective.
# cref:fig-explorative-mvn-diag-traj-opt-over-prob-7
# compares solving cref:eq-explorative-traj-opt with the joint gating entropy and with a factorised gating entropy.
# The results show that the joint entropy over a trajectory ensures the
# objective maximises the information gain over the entire trajectory.
# This prevents trajectories from collapsing onto a single location of maximum entropy and favours
# trajectories spreading over the state space in order to maximise the entropy of the
# entire trajectory.



# It then compares solving  with both the joint gating entropy and a factorise joint gating entropy.
# The results show that the joint entropy over a trajectory ensures the
# objective maximises the information gain over the entire trajectory.
# This prevents
# trajectories from collapsing onto a single location of maximum entropy and favours
# trajectories spreading over the state space in order to maximise the entropy of the
# entire trajectory.

# This first term resembles a greedy entropy-based active learning
# objective.
# However, initial experiments suggest that using this entropy term alone
# with the chance constraints can prevent exploration towards the target state. This
# happens when there is a mode boundary between the start state $\state_0$ and the target
# state $\targetState$. In this case, the objective explores away form the mode boundary and
# does not explore towards the target state $\targetState$.
# Experiments suggest that the state
# difference term is effective for targeting exploration towards the direction of the
# target state $\targetState$.
# Intuitively, the state difference term favours trajectories whose centre
# of mass is closer to the target state $\targetState$.
# Overall, when these two terms are balanced,
# the objective favours maximum entropy trajectories whose center of mass is closest
# to the target state $\targetState$.


# Intuitively, the first term seeks to find trajectories that explore regions of the gating network with
# high epistemic uncertainty.
# Conditioning the entropy at each time step on all of the other time steps ensures the objective maximises
# the information gain over the entire trajectory.
# This prevents trajectories from collapsing onto a single location of maximum entropy and favours trajectories
# spreading over the state space in order to maximise the entropy of the entire trajectory.
# This entropy term resembles an active learning objective.
# However, initial experiments suggest that using this entropy term alone with the chance constraints
# can prevent exploration towards the target state.
# This appears to happen when there is a mode boundary between the start state $\state_0$ and the target state
# $\targetState$.
# In this scenario, the objective explores away form the mode boundary and thus the target state $\targetState$.
# Experiments further suggest that the second term is effective for targeting exploration
# towards the direction of the target state $\targetState$.
# Intuitively, the state difference term favours trajectories whose centre of mass is closer to the target
# state $\targetState$.
# Overall, when these two terms are balanced, the objective favours maximum entropy trajectories whose center
# of mass is closest to the target state $\targetState$.

# The second term targets exploration towards the direction of the target state $\targetState$.

*Chance constraints*
To ensure that the controlled system remains in the desired mode with high probability, i.e. trajectories are
$\delta-\text{mode remaining}$, this method deploys the chance constraints from cref:eq-mode-chance-constraint.
They are restated here for convenience,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mode-chance-constraint-explore}
\Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{0}, \control_{0:\timeInd}, \bm\modeVar_{0:\timeInd-1}=\desiredMode)
\geq 1-\delta \quad \forall \timeInd \in \{0, \ldots, \TimeInd \}.
\end{align}
#+END_EXPORT
In practice, the constrained optimisation fails if the initial trajectory does not satisfy the constraints.
As such, this work deploys a simple strategy to ensure the initial
trajectory satisfies the constraints. It does this by sampling a fake target state from
the initial data set $\dataset_0$ and optimising the initial trajectory using the cost function
in cref:eq-quadratic-cost-control. Experiments show that this procedure finds trajectories where
all of time steps are in the desired mode with high probability.

# *Gating network entropy*
# The distribution over the desired mode's gating function at $\state_{\timeInd}$ conditioned on the rest of the
# trajectory is given by,
*Gating network entropy*
The conditional entropy of the gating network over a trajectory
$\mathcal{H}\left[\desiredGatingFunction(\state_{\timeInd}) \mid \stateTraj, \desiredGatingFunction(\stateTraj_{\neg\timeInd}), \dataset \right]$
can be calculated by assuming that the unobserved gating function values $\desiredGatingFunction(\stateTraj)$
are jointly Gaussian with the gating function values at the training inputs $\desiredGatingFunction(\allInput)$.
The distribution over the desired mode's gating function at $\state_{\timeInd}$ conditioned on the rest of the trajectory
$p(\desiredGatingFunction(\state_{\timeInd}) \mid \stateTraj_{\neg\timeInd}, \desiredGatingFunction(\stateTraj_{\neg\timeInd}), \dataset)$
can be calculated using the properties of multivariate Normal distributions,
#+BEGIN_EXPORT latex
\begin{align}
p(\desiredGatingFunction(\state_{\timeInd}) \mid \stateTraj_{\neg\timeInd}, \desiredGatingFunction(\stateTraj_{\neg\timeInd}), \dataset)
&= \mathcal{N}\left( \desiredGatingMean, \desiredGatingVariance \right),
\end{align}
#+END_EXPORT
where the mean and variance are given by,
#+BEGIN_EXPORT latex
\begin{align}
\desiredGatingMean &=
\desiredGatingCovFunc(\state_{\timeInd},\stateTraj_{\neg\timeInd})
\desiredGatingCovFunc(\stateTraj_{\neg\timeInd},\stateTraj_{\neg\timeInd})^{-1}
\desiredGatingMeanFunc(\stateTraj_{\neg\timeInd}) \\
\desiredGatingVariance &=
\desiredGatingCovFunc(\state_{\timeInd}, \state_{\timeInd}) - \desiredGatingCovFunc(\state_{\timeInd},\stateTraj_{\neg\timeInd})
\desiredGatingCovFunc(\stateTraj_{\neg\timeInd},\stateTraj_{\neg\timeInd})^{-1}
\desiredGatingCovFunc(\stateTraj_{\neg\timeInd}, \state_{\timeInd}).
\end{align}
#+END_EXPORT
where $\desiredGatingMeanFunc(\cdot)$ and $\desiredGatingCovFunc(\cdot, \cdot)$ are
sparse \acrshort{gp} mean and covariance functions, given by,
#+BEGIN_EXPORT latex
\begin{align}
q(\desiredGatingFunction(\state_{\timeInd}))
&= \mathcal{N} \left(\desiredGatingFunction(\state_{\timeInd}) \mid \desiredGatingMeanFunc(\state),
\desiredGatingCovFunc(\state, \state')) \\
\desiredGatingMeanFunc(\state) &=
\desiredGatingKernel(\state, \gatingInducingInput)
\desiredGatingKernel(\gatingInducingInput, \gatingInducingInput)^{-1}
\hat{\mathbf{m}}_{\desiredMode}  \\
\desiredGatingCovFunc(\state, \state') &=
\desiredGatingKernel(\state, \state')
+ \desiredGatingKernel(\state, \gatingInducingInput)
\desiredGatingKernel(\gatingInducingInput, \gatingInducingInput)^{-1}
\left( \hat{\mathbf{S}}_{\desiredMode}
- \desiredGatingKernel(\gatingInducingInput, \gatingInducingInput) \right)
\desiredGatingKernel(\gatingInducingInput, \gatingInducingInput)^{-1}
\desiredGatingKernel(\gatingInducingInput, \state'),
\end{align}
#+END_EXPORT
where $\desiredGatingKernel$ and
$\gatingInducingInput$ are the kernel and inducing inputs
associated with the desired mode's gating function respectively.
The sparse \acrshort{gp} term arises because the \acrshort{mosvgpe} model uses sparse \acrshort{gp}s and
approximates the posterior
$p(\desiredGatingFunction(\state_{\timeInd}) \mid \dataset_{0:i-1}) \approx q(\desiredGatingFunction(\state_{\timeInd}))$,
where,
#+BEGIN_EXPORT latex
\begin{align}
q(\desiredGatingFunction(\gatingInducingInput)) = \mathcal{N}\left( \desiredGatingFunction(\gatingInducingInput
\mid \hat{\mathbf{m}}_{\desiredMode}, \hat{\mathbf{S}}_{\desiredMode} \right).
\end{align}
#+END_EXPORT



# Note that the variational distribution over the inducing inputs is given by
# $q(\desiredGatingFunction(\gatingInducingInput)) = \mathcal{N} \left(\desiredGatingFunction(\gatingInducingInput) \mid \hat{\mathbf{m}}_{\desiredMode}, \hat{\mathbf{S}}_{\desiredMode} \right)$.


# The \acrshort{mosvgpe} model assumes that the mode indicator variable is factorised over data points, i.e.
# along the trajectory.
# As such, it is not possible to consider the


# The entropy of the gating network is calculated by considering the joint distribution over the gating
# gating function values $\desiredGatingFunction(\stateTraj)$ over the trajectory.
# Under the

# The distribution over the desired mode's gating function at $\state_{\timeInd}$ conditioned on the rest of the
# trajectory
# $p(\desiredGatingFunction(\state_{\timeInd}) \mid \stateTraj_{\neg\timeInd}, \desiredGatingFunction(\stateTraj_{\neg\timeInd}), \dataset_{0:i})$
# can be calculated by assuming that the unobserved gating function values $\desiredGatingFunction(\stateTraj)$
# are from the gating function's GP.
# Making this assumptions leads to the


# It is worth noting that in contrast to normal \acrshort{gp} methods, the \acrshort{mosvgpe} model
# approximates the posterior p(\desiredGatingFunction(\state_{\timeInd}) \mid \dataset) with
# $q(\desiredGatingFunction(\state_{\timeInd})) \approx p(\desiredGatingFunction(\state_{\timeInd}) \mid \dataset)$.
# As such,  the mean and covariance functions $\desiredGatingMeanFunc$ and $\desiredGatingCovFunc$
# represent the




# Do not violate the a priori unknown, but learned safety constraints with high probability.
# Actively learn about the constraints

# - $\delta$ mode remaining exploration
#   + via model predictive control
#   + via policy optimisation


# #+BEGIN_EXPORT latex
# \begin{theorem}[definition] \label{}
# Given a desired dynamics mode $\desiredMode$ with state space
# $\desiredStateDomain = \{ \state \in \stateDomain \mid \modeVar = \desiredMode \}$
# controller $\policy_{\desiredMode}$ and the
# The controller satisfies the constraints
# \end{theorem}
# #+END_EXPORT

*** Alg :ignore:
#+BEGIN_EXPORT latex
\begin{algorithm}[!t]
\caption{ModeOpt}\label{alg-mode-opt}
\begin{algorithmic}[1]
\Require{Start state $\state_0$, target state $\targetState$, desired dynamics mode $\desiredMode$, initial data set $\dataset_0$, explorative controller $\explorativeController$, mode remaining controller $\modeController$, dynamics model $\dynamicsModel}$}
\For{$i = 0, 1, \ldots $}
    \State Train model $\dynamicsModel}$ on $\dataset_i$ using \acrshort{elbo} from \cref{eq-lower-bound-further}
    %\State Find trajectory $\controlTraj$ to $\targetState$ using \cref{eq-}
    \State Optimise mode remaining controller $\modeController$ using learned model $\dynamicsModel$
    \If{$\state_{\TimeInd}$ = $\targetState$ and $\modeController$ is $\delta-\text{mode remaining}$}
        \State Execute $\modeController$ in environment
        \State \textbf{break}
    \EndIf
    \State Optimise explorative controller $\explorativeController$ using learned model $\dynamicsModel}$
    \State Collect environment data set $\dataset_{i+1}$ using $\explorativeController$; add to data set$\{\dataset_{0:i+1} = \dataset_{i+1} \cup \dataset_{0:i} \}$
\EndFor
\end{algorithmic}
\end{algorithm}
#+END_EXPORT
*** ModeOpt
This section now details the complete \modeOpt procedure which is shown in cref:alg-mode-opt.
The algorithm is initialised with a start state $\state_0$, target state $\targetState$,
a desired dynamics mode $\desiredMode$ and
data set of state transitions $\dataset_0$ from the desired dynamics mode and a calibrated
dynamics model $\dyamicsModel$.

*Dynamics model $\dynamicsModel$*
\modeOpt learns a factorised representation of the underlying
dynamics modes using the \acrshort{mosvgpe} model from cref:chap-dynamics.
Importantly, it learns a single-step dynamics model $\dynamicsModel$, given by,
#+BEGIN_EXPORT latex
\begin{align}
\state_{\timeInd+1} \sim
\dynamicsModel(\state_{\timeInd+1} \mid \state_{\timeInd}, \control_{\timeInd}),
\end{align}
#+END_EXPORT
which also infers valuable information regarding how the system switches between
the modes.

*Mode remaining controller $\modeController$*
The mode remaining control methods from cref:chap-traj-opt-geometry,chap-traj-opt-inference
are used to construct a mode remaining controller $\modeController$,
#+BEGIN_EXPORT latex
\begin{align}
\control_{\timeInd} = \modeController(t) \quad \forall \timeInd \in \{0,\ldots, \TimeInd-1\}
\end{align}
#+END_EXPORT
It uses the learned dynamics model $\dynamicsModel$ to find trajectories to the target state $\targetState$.
The mode chance constraints from cref:eq-mode-chance-constraint are then be used to check if these
trajectories are $\delta-\text{mode remaining}$.

*Explorative controller $\explorativeController$*
When $\delta-\text{mode remaining}$ trajectories to the target state $\targetState$
cannot be found, \modeOpt relies on an explorative controller $\explorativeController$ to
explore the environment. This work uses the exploration strategy from cref:sec-exploration
to construct such an explorative controller,
#+BEGIN_EXPORT latex
\begin{align}
\control_{\timeInd} = \explorativeController(t) \quad \forall \timeInd \in \{0,\ldots, \TimeInd-1\}
\end{align}
#+END_EXPORT
The goal of this controller is to explore the environment whilst remaining in the desired dynamics mode.
It is worth noting that this is also an open-loop trajectory optimisation algorithm.
It should be noted that extending the method in cref:sec-exploration
to use a feedback policy should not be too difficult. For example, the trajectory
optimisation algorithm could be used for guided policy search  cite:levineGuided2013.
It should also be straightforward to check that trajectories under a feedback
policy are $\delta-\text{mode remaining}$, under the dynamics, however, this is left as future work.




# The steps of the algorithm presnted in this chapter can be summarised as follows,
# 1. Learn a single-step dynamics model using \acrshort{mosvgpe} from cref:chap-dynamics,
# 2. Find trajectories to $\targetState$ under the learned model, using the methods from cref:chap-traj-opt-geometry,chap-traj-opt-inference,
# 3. Check if trajectories are $\delta-\text{mode remaining}$ using mode chance constraints from cref:eq-mode-chance-constraint,
# 4. Explore the environment, whilst ensuring the controlled system is $\delta-\text{mode remaining}$.

** Preliminary Results :noexport:
*** Environment 1
**** Data Step 0 Epoch 1494 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{0}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 1 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{1}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 1 Epoch 0 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{1}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
%\caption{\textbf{Exploration - step \stepNum, epoch \epochNum}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 1 Epoch 1494 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{1}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 2 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{2}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 2 Epoch 0 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{2}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 2 Epoch 1494 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{2}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 3 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{3}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 3 Epoch 0 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{3}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 3 Epoch 1494 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{3}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 4 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{4}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 4 Epoch 0 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{4}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 4 Epoch 1494 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{4}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 5 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{5}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 5 Epoch 0 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{5}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 5 Epoch 1494 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{5}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 6 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{6}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 6 Epoch 0 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{6}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 6 Epoch 1494 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{6}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 7 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{7}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 7 Epoch 0 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{7}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 7 Epoch 1494 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{7}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 8 :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{8}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 8 Epoch 0 :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{8}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 8 Epoch 1494 :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{8}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum, epoch \epochNum}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 9 :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{9}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 9 Epoch 0 :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{9}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 9 Epoch 1494 :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{9}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 10 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{10}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 10 Epoch 0 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{10}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 10 Epoch 1494 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{10}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 11 :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{11}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 11 Epoch 0 :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{11}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 11 Epoch 1494 :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{11}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 12 :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{12}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 12 Epoch 0 :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{12}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 12 Epoch 1494 :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{12}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 15 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{15}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 15 Epoch 0 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{15}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 15 Epoch 1494 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{15}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 18 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{18}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 18 Epoch 0 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{18}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 18 Epoch 1494 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{18}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 20 :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{20}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 20 Epoch 0 :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{20}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 20 Epoch 1494 :ignore:noexport:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{20}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 21 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{21}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 21 Epoch 0 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{21}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 21 Epoch 1494 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{21}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Trajectory Step 22 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{22}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_mixing_probs_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-prob-7-step-\stepNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/trajectories_over_desired_gating_gp_step_\stepNum.pdf}
\subcaption{}
\label{fig-explorative-traj-opt-over-svgp-7-step-\stepNum}
\end{minipage}
\caption{\textbf{Exploration - step \stepNum}
Trajectory optimisation results after solving \cref{eq-explorative-traj-opt}.
The optimised controls are rolled out in the desired mode's GP dynamics (magenta) and in the environment (cyan).
The state trajectories are overlayed on (\subref{fig-explorative-traj-opt-over-prob-7-step-\stepNum})
each mode's mixing probability
and (\subref{fig-explorative-traj-opt-over-svgp-7-step-\stepNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function.
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-\stepNum}
\end{figure}
#+END_EXPORT

**** Data Step 22 Epoch 0 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{22}
\renewcommand{\epochNum}{0}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{Before training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

**** Data Step 22 Epoch 1494 :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\stepNum}{22}
\renewcommand{\epochNum}{1494}
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_mixing_probs_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/mode-opt/exploration/data_over_desired_gating_gp_step_\stepNum_epoch_\epochNum.pdf}
\subcaption{}
\label{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum}
\end{minipage}
\caption{\textbf{After training on $\dataset_{\stepNum}$}
Data set $\dataset_{\stepNum}$ (blue crosses)  overlayed on
(\subref{fig-explorative-data-over-prob-7-step-\stepNum-epoch-\epochNum})
each mode's mixing probability
and (\subref{fig-explorative-data-over-svgp-7-step-\stepNum-epoch-\epochNum})
the posterior mean (left) and posterior variance (right) associated with the desired mode's gating function,
after training for $\epochNum$ epochs.
The start state $\state_0$ and target state $\targetState$ are overlayed along with the mode boundary (purple line).
All plots show a slice of the input space with constant zero controls.}
\label{fig-explorative-traj-opt-7-step-\stepNum-epoch-\epochNum}
\end{figure}
#+END_EXPORT

** Discussion & Future Work
*Exploration-exploitation trade-off*
The method presented in this chapter side-
steps the exploration-exploitation trade-off which is common in MBRL. It dos so by
separating the exploration into the explorative controller $\explorativeController$  and the exploitation
into the mode remaining controller $\modeController$ and selecting the relevant strategy by
evaluating the mode chance constraints.

*Entropy of what?*
Greedy entropy-based exploration in $\modeVar$ is less good.
Tries to search where $\Pr(modeVar=\desiredMode \state= 0.5$ (uniform) even though the gating network may
by certain, i.e. have a low posterior variance.
In this case the \acrfull{bald} objective  from cite:houlsbyBayesian2011 may be a better option

*$\delta\text{mode remaining}*

*Exploration guarantees*
The method presented in this chapter has no exploration
guarantees. That is, given enough time, it is not guaranteed to have explored enough
such that $\delta-\text{mode remaining}$ trajectories to the target state can be found. Further
to this, some experiments showed that the constrained optimisation can get stuck
exploring away from the target state. As such, an interesting direction for future
work is to study exploration guarantees, for example, through regret bounds. Such
analysis may lead to an automatic method for selecting the smallest $\delta$ that can
guarantee exploration. A more simple (empirical) approach may quantify the rate
at which the gating networks epistemic uncertainty is reducing and use it to relax
$\delta$ if the agent has stopped exploring.

In practice it is straightforward to tune the parameters by visual inspection. This
is because the method assumes access to an initial dynamics model trained on the
initial data set .

The objective used in this chapter is concerned with exploration. It resembles the

() uncertainty

It contains a term targeting exploration towards the target state and a term favour-
ing exploration in regions of the state space where the gating network is uncertain.
Using the term targeting exploration towards the target state by in conjunction with
the $\delta-\text{mode remaining}$ chance constraints may lead to the strategy getting stuck
in local optima.

Using the state d

It contains a term favouring uncertainty exploration The entropy term It should be
noted that in some environments this behaviour may get the strategy stuck in local
optima. Add parameter to balance entropy and state difference terms to get sub
linear regret?

*Myopic active learning*
This work has shown that relieving the myopia of active
learning in dynamical systems  by conditioning the entropy on previous time steps
has a positive impact on performance. It is worth noting that this was only possible
because the gating network is based on GPs. Therefore, this method is not widely
applicable in all MoGPE dynamics models.

*Inducing points*
The method presented in this chapter initialised each of the sparse
GPs in the MoSVGPE dynamics model with a fixed number of inducing points. Although this
approach worked well in the experiments, it is unlikely that this will
always be the case. For example, consider exploring much larger environments, i.e.
environments with larger state domains. In these environments, the MoSVGPEs
ability to accurately model an ever increasing data set will decrease. This is due
to the sparse approximations ability to model the true non-parametric model deteriorating as
the number of data points increases. See cite:burtRates2019 for details
on rates of convergence for sparse GPs. As such, an interesting direction for future
work is to study methods for dynamically adding new inducing points to each GP.

*Fix gating network parameters during training?*
During its initial iterations,
\modeOpt only explores the desired dynamics mode and does not observe any state
transitions belonging to another mode. As a result, the lengthscale of the gating network GPs increases.
This often results in the gating network become overconfident,
almost as if the gating network believes that only a single dynamics mode exists over
the entire domain. When this happens, the $\delta-\text{mode remaining}$ chance constraints
expand the explorable region significantly further than the observed data. This is
undesirable because it leads to trajectories leaving the desired dynamics mode significantly more.
In practice, fixing the kernels hyperparameters (e.g. lengthscale
and signal variance) after training on the initial data set , appeared to alleviate this
issue. However, it is left to future work to study this in more depth.



The trajectory optimiastion algorithm in this chapter is only concerned with
the aforementioned exploration and is not intended to run fast enough for
real-time MPC.
Although not explored here, the algorithm could be used for guided policy
search in a model-based reinforcement learning setting
cite:levineGuided2013,levineVariational2013,okadaVariational2020.
Alternatively, one could explore approximations enabling the approach to
work for real time MPC e.g. linearising the dynamics and/or a second order
Taylor expansion of the cost function.
\todo{should this be cost of value function??}


This chapter is concerned with exploration in multimodal dynamical systems
where the underlying dynamics modes and how the system switches between them are /not fully known a priori/.
In particular, it is interested in exploring a single desired dynamics
mode whilst avoiding entering any of the other modes.
This is a challenging problem because the agent must observe regions outside of the desired dynamics mode
in order to know that a particular region does not belong to the desired mode.
Based on this logic, this chapter utilises the model from Chapter ref:chap-dynamics
to design an information theoretic trajectory optimisation algorithm
that


To the best of our knowledge, there is no previous work addressing exploration of
a single dynamics mode in multimodal dynamical systems.
cite:schreiterSafe2015 use a GP classifier to identify safe and unsafe regions
when learning GP dynamics models in an active learning setting.
However, they assume that they can directly observe whether a particular
data point from the environment belongs to either the safe or unsafe regions.
In contrast, this chapter is concerned with scenarios where the mode cannot be directly observed from the
environment, but instead, is inferred by a probabilistic dynamics model.

Utilising mutual information for active learning has been well motivated by, for example,
cite:krauseNearOptimal2008.
They highlight that mutual information may lead to a more accurate model than differential entropy.
It has also been shown that minimising the mutual information  is the same as minimising the expected posterior
uncertainty (conditional entropy) in the model cite:ertinMaximum2003.

It should be noted that in some environments this behaviour may get the strategy stuck in local
optima.
Add parameter to balance entropy and state difference terms to get sub linear regret?

- provable regret
- relax constraint if not improving
- guarantess on exloration by relaxing $\delta$ and/or other params - like UCB

** Conclusion
* Active Learning
** Bayesian Information Theoretic Active Learning
Following cite:houlsbyBayesian2011 this section introduces Bayesian
information theoretic active learning.
The Bayesian framework assumes that there are some latent parameters $\params$,
that control the dependence between the inputs $\input \in \inputDomain$
and the outputs $p(\output \mid \input, \params)$.
Given observations of the system
$\mathcal{D} = \{(\input_\timeInd, \output_\timeInd)\}_{\timeInd=1}^\TimeInd$,
it is assumed that the posterior over the parameters given the data
$p(\params \mid \dataset)$
has been inferred.
The goal of information theoretic active learning is to reduce the number of
possible hypothesis as fast as possible, i.e. minimise the uncertainty associated
with the parameters using Shannon's entropy citep:coverElements2006.
#+BEGIN_EXPORT latex
\newcommand{\crv}{\ensuremath{X}}
\newcommand{\density}{\ensuremath{p(\crv)}}
\begin{myquote}
\textbf{Differential Entropy}
Let $\crv$ be a continuos random variable, with a probability density
$\density$,
whose support is a set $\mathcal{X}$.
The differential entropy $H(\crv)$ is then defined as,
\begin{align} \label{eq-differential-entropy}
H(\crv) = - \int_{\mathcal{X}} \density \text{log} \density \text{d} \crv.
\end{align}
\end{myquote}
#+END_EXPORT
Data points $\dataset^*$ should thus be selected according to,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-posterior-entropy}
\text{arg}\underset{\dataset^*}{\min}
H(\params \mid \dataset^*)
= \text{arg}\underset{\dataset^*}{\min}
- \int p(\params \mid \dataset^*) \text{log} p(\params \mid \dataset^*) \text{d} \params.
\end{align}
#+END_EXPORT
In general, solving this problem is NP-hard,
so it is common to approximate it with a myopic greedy approximation.
cite:krauseNearOptimal2008,dasguptaAnalysis2005 show that a myopic policy can perform near-optimally.
The objective is then to seek the input $\input$ that maximises the decrease in
expected posterior entropy,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-decrease-entropy}
\text{arg}\underset{\control}{\max}
H(\params \mid \dataset)
- \E_{\output \sim \outputGivenInputData}
\left[ H(\params \mid \output, \input, \dataset) \right].
\end{align}
#+END_EXPORT
# The expectation requires the unseen output $\output$ and many works have
An important insight is that minimising cref:eq-expected-decrease-entropy
is equivalent to minimising the conditional mutual information between
the output and the parameters $I(\output, \params \mid \input, \dataset$.
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Mutual Information}
Given two sets of random variables, $\mathbf{X}$ and $\mathbf{F}$, with joint density $p(\mathbf{X}, \mathbf{F})$ the
mutual information \cite{coverElements2006} is given by,
\begin{align} \label{eq-mutual-information}
I(\mathbf{X};\mathbf{F}) = \int p(\mathbf{X},\mathbf{F}) \text{log}\frac{p(\mathbf{X},\mathbf{F})}{p(\mathbf{X})p(\mathbf{F})}
\text{d}\mathb{X} \text{d}\mathb{F},
\end{align}
#+END_EXPORT
and its well known relationship to differential entropy $H(\cdot)$ is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information-entropy}
I(\mathbf{X};\mathbf{F}) =  H(\mathbf{X}) - H(\mathbf{X} \mid \mathbf{F}).
\end{align}
\end{myquote}
#+END_EXPORT
An equivalent objective function can therefore be formulated in the output $\output$
space as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-output-space-entropy}
\text{arg}\underset{\input}{\max}
H(\output \mid \input, \dataset)
- \E_{\params \sim p(\params \mid\dataset)} \left[ H(\output \mid \input, \params) \right].
\end{align}
#+END_EXPORT
The entropy terms in cref:eq-output-space-entropy are now calculated
in a (usually) low dimensional output space.
In the binary classification setting, cref:eq-output-space-entropy simply
involves calculating the entropy of Bernoulli random variables.
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Entropy of Bernoulli Variable}
Consider a Bernoulli random varialbe $\output$ with probability $p$.
The entropy of such a Bernoulli random variable is given by,
\begin{align} \label{eq-entropy-bernoulli}
H(\alpha) &= h_{\text{Bern}}(p) \\
h_{\text{Bern}}(p) &= -p \text{log}p - (1-p) \text{log}(1-p),
\end{align}
\end{myquote}
#+END_EXPORT
Intuitively, the first term in cref:eq-output-space-entropy
($H(\output \mid \input, \dataset)$) seeks to
find the input $\input$ where the model is marginally most uncertain about
its corresponding output $\output$, whilst the
second term
$(- \E_{\params \sim p(\params \mid\dataset)} \left[ H(\output \mid \input, \params) \right])$
prefers inputs whose parameter settings are confident.
As highlighted by cite:houlsbyBayesian2011, this can be interpreted as finding the
input $\input$ for which the parameters under the posterior, disagree about
the output $\output$ the most.

*** Active Learning for GP Classification
cite:houlsbyBayesian2011 formulate Bayesian active learning in the GP
binary classification model,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-binary-gp-classification}
\gatingFunc &\sim \mathcal{GP}(\mu(\cdot), k(\cdot,\cdot)) \\
\output \mid \gatingFunc, \input &\sim \text{Bernoulli}(\Phi(\gatingFunc(\input)))
\end{align}
#+END_EXPORT
where $\Phi$ represents the Gaussian CDF and $\Phi(\gatingFunc(\input))$ is
the probability of the (Bernoulli) output variable
$\alpha$ cite:nickischApproximations2008.
In this context,  cref:eq-output-space-entropy can be re-written as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-output-space-entropy-gp-classification}
\text{arg}\underset{\input}{\max}
H(\output \mid \input, \dataset)
- \E_{\gatingFunc \sim p(\gatingFunc \mid\dataset)} \left[ H(\output \mid \input, \gatingFunc) \right].
\end{align}
#+END_EXPORT
Their approach introduces approximations to the two terms in
cref:eq-output-space-entropy-gp-classification.
They assume that the posterior over the latent function $\gatingFunc$
is approximated to be Gaussian,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-classification-posterior}
\gatingFunc \mid \input, \mathcal{D} &\sim \mathcal{N}(\gatingFunc(\input) \mid \mu_{\input,\dataset}, \sigma^2_{\input,\dataset}).
%\mu_{\input,\dataset} &= k(\input, \allInput) k(\allInput, \allInput)^{-1} \bm\alpha \\
%\sigma^2_{\input,\dataset}) &= k(\input, \input)
%- k(\input, \allInput) k(\allInput, \allInput)^{-1} k(\allInput, \input)
\end{align}
#+END_EXPORT
The $\overset{1}{\approx}$ symbol will be used to indicate when such an approximation
has been exploited.
The first term in cref:eq-output-space-entropy-gp-classification is approximated as follows,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-entropy--data}
H(\output \mid \input, \dataset) &\overset{1}{\approx}
h_{\text{Bern}} \left(\int \Phi(\gatingFunc(\input)) \mathcal{N}(\gatingFunc(\input) \mid \mu_{\input,\dataset}, \sigma^2_{\input,\dataset}) \text{d} \gatingFunc(\input) \right) \\
&= h_{\text{Bern}} \left( \Phi \left( \frac{\mu_{\input,\dataset}}{\sqrt{\sigma^2_{\input,\dataset} + 1}} \right) \right).
\end{align}
#+END_EXPORT
The second term is approximated as follows,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-entropy-expected-term}
\E_{\gatingFunc \sim p(\gatingFunc \mid \dataset)}
\left[H(\output \mid \input, \gatingFunc) \right]
&\overset{1}{\approx} \int h_{\text{Bern}}(\Phi(\gatingFunc(\input)))
\mathcal{N}(\gatingFunc(\input) \mid \mu_{\input,\dataset}, \sigma^2_{\input,\dataset})
\text{d} \gatingFunc(\input) \\
&\overset{2}{\approx}
\int \text{exp}\left( -\frac{\gatingFunc(\input)^2}{\pi \text{ln}2} \right)
\mathcal{N}(\gatingFunc(\input) \mid \mu_{\input,\dataset}, \sigma^2_{\input,\dataset})
\text{d} \gatingFunc(\input) \\
&= \frac{C}{\sqrt{\sigma_{\input,\dataset}^2 + C^2}}
\text{exp}\left( -\frac{\mu_{\input,\dataset}^2}{2\left( \sigma_{\input,\dataset}^2 + C^2 \right)} \right) \label{eq-entropy-expected-term-last}
\end{align}
#+END_EXPORT
where $C = \sqrt{\frac{\pi \text{ln}2}{2}}$.
This approximation exploits a Taylor expansion of $\text{ln} h_{\text{Bern}}(\Phi(\gatingFunc(\input)))$
(see supplementary material of cite:houlsbyBayesian2011)
allowing it to be represented up to $\mathcal{O}(\gatingFunc(\input)^4)$ by a squared
exponential curve ($\text{exp}(-\gatingFunc(\input)^2/\pi\text{ln}2)$).
This approximation is referred to as $\overset{2}{\approx}$.
A simple Gaussian convolution then gives the closed form expression in Eq.
ref:eq-entropy-expected-term-last.
The objective in cref:eq-output-space-entropy-gp-classification can then be approximated as,
#+BEGIN_EXPORT latex
\newcommand{\approxEntropy}{\ensuremath{\hat{H}}}
\begin{align} \label{eq-approximate-output-space-entropy-obj}
\text{arg}\max_{\input}
\approxEntropy(\output)
\end{align}
#+END_EXPORT
where,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-approximate-output-space-entropy}
\approxEntropy(\output)
\coloneqq
h_{\text{Bern}} \left( \Phi \left( \frac{\mu_{\input,\dataset}}{\sqrt{\sigma^2_{\input,\dataset} + 1}} \right) \right)
+ \frac{C}{\sqrt{\sigma_{\input,\dataset}^2 + C^2}}
\text{exp}\left( -\frac{\mu_{\input,\dataset}^2}{2\left( \sigma_{\input,\dataset}^2 + C^2 \right)} \right)
\end{align}
#+END_EXPORT
The objective is smooth and differentiable for most practically relevant kernels of $\input$, so
gradient-based optimisation can be used to find the maximally informative $\input$.


# #+BEGIN_EXPORT latex
# \begin{myquote}
# \textbf{Entropy of Bernoulli Variable}
# Consider the probit case, where the value of the output $\output$,
# given latent gating function $\gatingFunc(\input)$, takes a Bernoulli
# distribution with probability $\Phi(\gatingFunc(\input))$,
# where $\Phi$ represents the Gaussian CDF.
# The entropy of such a Bernoulli random variable is given by,
# \begin{align} \label{eq-entropy-bernoulli}
# H(\alpha \mid \control, \gatingFunc) &= H_{\mathcal{Bern}}(\Phi(\gatingFunc(\control))) \\
# H_{\mathcal{Bern}}(p) &= -p \text{log}p - (1-p) \text{log}(1-p),
# \end{align}
# where the probability $\Phi(\gatingFunc(\input))$ has been denoted $p$.
# \end{myquote}
# #+END_EXPORT

** Bayesian Information Theoretic Exploration Strategy
#+BEGIN_EXPORT latex
\newcommand{\posterior}{\ensuremath{p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \control_{\timeInd})}}
#+END_EXPORT

# The /static/ active learning problem addressed previously is fundamentally different
# to the problem in dynamical systems.
# In the /static/ problem, it is possible to query any point in the input domain.
# As such, it is open to a clean information-theoretic treatment.
# In the /dynamic/ problem, the system must be steered through the
# unknown dynamics $\dynamics$ by a sequence of controls $\controlTraj$.
In dynamical systems an arbitrary state $\state$ cannot be sampled, so instead, the dynamics must be steered
to $\state$ through the unknown dynamics $\dynamicsFunc$ through a sequence of controls $\control$.
Thus, there is information gain along the trajectory which must also be
considered.
As highlighted by cite:buisson-fenetActively2020, information gain in dynamical systems is therefore
fundamentally different to the /static/ problem addressed by cite:krauseNearOptimal2008
and cite:houlsbyBayesian2011.
The goal here is to pick the most informative control trajectory $\controlTraj$ whilst observing $\stateTraj$.

Recent work has addressed active learning in (unimodal) GP dynamics models
citep:buisson-fenetActively2020 and (unimodal)
GP state-space models citep:caponeLocalized2020.
cite:schreiterSafe2015 consider safe exploration for active learning where they distinguish safe and unsafe regions
with a binary GP classifier, which is learned separately to the dynamics model. Their exploration strategy considers
the differential entropy of the posterior GP associated with the dynamics model and they use the GP classifier to
define a set of constraints.

This chapter resembles cite:schreiterSafe2015
except that it exploits the coupled learning of the dynamics modes and the gating network, instead of learning
a separate \acrshort{gp} classifier to distinguish safe and unsafe regions.
The exploration strategy is then projected onto the posterior GP(s) associated with the gating network.
The coupled learning provides well-calibrated uncertainty estimates in the gating network, making it a
good choice for information-based exploration.

In order to calculate the approximate entropy objective in the output space $\approxEntropy(\output_{\timeInd})$
(cref:eq-approximate-output-space-entropy-obj)
at a given time step $\timeInd$, the posterior $\posterior$ over the gating function is required.
After the first time step this distribution is calculated as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-posterior-0}
p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \control_{\timeInd})&=
p(\state_0)
\int p(\gatingFunc(\state_{1}, \control_{1}) \mid \state_{0}, \control_{0})
p(\state_{1} \mid \state_{0}, \control_{0}) p(\control_{0}) \text{d}\state_0
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \control_{\timeInd})&=
p(\state_0) \prod_{\timeInd=1}^{\timeInd}
\int p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \state_{\timeInd}, \control_{\timeInd})
p(\state_{\timeInd} \mid \state_{\timeInd-1}, \control_{\timeInd-1}) p(\control_{\timeInd}) \text{d}\state_\timeInd
\end{align}
#+END_EXPORT


#+BEGIN_EXPORT latex
\newcommand{\gatingOutput}{\ensuremath{h(\singleInput)}}
\newcommand{\gatingTransition}{\ensuremath{p(\gatingOutput \mid \state_\timeInd, \control_\timeInd)}}
\begin{align} \label{eq-mutual-information}
\underset{\stateTraj \in \stateDomain^\TimeInd, \controlTraj \in \controlDomain^\TimeInd}{\text{argmax}} \approxEntropy(\stateTraj, \controlTraj) &=
\underset{\state \in \stateDomain, \control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd \approxEntropy_{\timeInd}(\state_{\timeInd}, \control_{\timeInd})
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-classification-posterior}
\gatingFunc \mid \input, \mathcal{D} &\sim \mathcal{N}(\gatingFunc(\input) \mid \mu_{\input,\dataset}, \sigma^2_{\input,\dataset}).
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\newcommand{\gatingOutput}{\ensuremath{h(\singleInput)}}
\newcommand{\gatingTransition}{\ensuremath{p(\gatingOutput \mid \state_\timeInd, \control_\timeInd)}}
\begin{align} \label{eq-mutual-information}
\underset{\stateTraj \in \stateDomain^\TimeInd, \controlTraj \in \controlDomain^\TimeInd}{\text{argmax}} H(\stateTraj, \controlTraj) &=
\underset{\state \in \stateDomain, \control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd H_{\timeInd}(\state_{\timeInd}, \control_{\timeInd}) \\
&= \underset{\state \in \stateDomain, \control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd - \int \gatingTransition
\text{log} \gatingTransition \text{d} \gatingOutput
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
\underset{\control \in \controlDomain}{\text{argmax}} H(\stateTraj, \controlTraj) &=
\underset{\control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd H_{\timeInd}(\state_{\timeInd}, \control_{\timeInd}) \\
&= \underset{\control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd - \int \gatingTransition
\text{log} \gatingTransition \text{d} \gatingOutput \\
\end{align}
#+END_EXPORT


The exploration strategy introduced in this chapter builds on the approximate
entropy objective in cref:eq-approximate-output-space-entropy.


is a selective
sampling approach based on the differential entropy of the GP posterior
over the desired modes gating function.
The
Considering a full discriminative model,
discover the dependence of the mode indicator variable $\modeVar \in \modeDomain$
on the state-control input $\input \in \inputDomain$





prediction with uncertain input


We consider a fully discriminative model where the goal of active learning is to discover the dependence of some variable y  Y on an input variable x  X . The key idea in active learning is that the learner chooses the input queries xi  X and observes the systems response yi, rather than passively receiving
(xiyi) pairs.




#+BEGIN_EXPORT latex
\begin{align} \label{eq--entropy-model}
\E_{\gatingFunc \sim p(\gatingFunc \mid \mathcal{D})} \left[ H(\alpha \mid \control, \gatingFunc) \right]
&\approx \int h \left( \Phi(\gatingFunc(\state, \control)) \mathcal{N}(\gatingFunc(\state, \control) \mid \mu_\gatingFunc, \sigma^2_\gatingFunc) \text{d} \gatingFunc(\state, \control)
\end{align}
#+END_EXPORT

An approximated mutual information exploration criterion is used

entropy

#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
\underset{\control \in \controlDomain}{\text{argmax}} H(\gatingFunc(\stateTraj, \controlTraj) \mid \controlTraj) &=
\underset{\control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd H_{\timeInd}(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \control_\timeInd)
\end{align}
#+END_EXPORT
where
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
p(\gatingFunc(\state_{1:\TimeInd}, \control_{1:\TimeInd}) \mid \control_{1:\TimeInd})&=
p(\state_0) \prod_{\timeInd=1}^{\TimeInd-1}
\int p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd} \mid \state_{\timeInd}, \control_{\timeInd})
p(\state_{\timeInd} \mid \state_{\timeInd-1}, \control_{\timeInd-1}) p(\control_{\timeInd}) \text{d}\state_\timeInd
\end{align}
#+END_EXPORT

* Conclusion
* Appendix
** Multivariate Normals
** Derivatives of a Gaussian Process
** Results
*** Velocity Controlled Quadcopter Experiments
Table ref:tab-params-quadcopter contains the optimisation settings and initial values for the optimisable parameters
that were used to train the model on the velocity controlled quadcopter data set in  Section ref:sec-brl-experiment.

*Experts* Both expert's GP priors were initialised with constant mean functions (with
a learnable parameter $c_{\modeInd}$) and separate independent squared exponential
kernels (with Automatic Relevance Determination) on each output dimension.
Table ref:tab-params-quadcopter shows a single set of kernel parameters $\sigma_f, l$ for each expert, as
the kernel associated with each output dimension was initialised with the same initial values.
Each experts likelihood was initialised with diagonal covariance matrices $\Sigma_{\epsilon_{\modeInd}}$.

*Gating Network* The gating network GP was initialised with a zero mean function, and a squared exponential kernel
with ARD.

#+begin_table
#+LATEX: \caption{Optimiser settings and initial values for optimisable parameters before training on the DJI Tello quadcopter data set.}
#+LATEX: \label{tab-params-quadcopter}
#+LATEX: \resizebox{\textwidth}{!}{
#+ATTR_LATEX: :center nil
|                 | Description                | Symbol                    | Value                                                                                   |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Num data                   | $\NumData$                | $1129$                                                                                  |
|                 | Batch size                 | $\NumData_b$              | $64$                                                                                    |
| Optimiser       | Num epochs                 | N/A                       | $6000$                                                                                  |
|                 | Num samples                | $S$                       | $1$                                                                                     |
|                 | Learning rate              | N/A                       | $0.01$                                                                                  |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Constant mean function     | $c_1$                     | $[0,0]$                                                                                 |
|                 | Kernel variance            | $\sigma_f$                | $0.1$                                                                                   |
|                 | Kernel lengthscales        | $l$                       | $[10, 10]$                                                                              |
|                 | Likelihood variance        | $\Sigma_{\epsilon_1}$     | $\diag([0.0011, 0.0011])$                                                               |
| Expert 1        | Num inducing points        | $\NumInducing$            | $100$                                                                                   |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Constant mean function     | $c_2$                     | $[0,0]$                                                                                 |
|                 | Kernel variance            | $\sigma_f$                | $20$                                                                                    |
|                 | Kernel lengthscales        | $l$                       | $[0.5, 0.5]$                                                                            |
|                 | Likelihood variance        | $\Sigma_{\epsilon_2}$     | $\diag([1.9,1.9])$                                                                      |
| Expert 2        | Num inducing points        | $\NumInducing$            | $100$                                                                                   |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel variance            | $\sigma_f$                | $0.6$                                                                                   |
|                 | Kernel lengthscales        | $l$                       | $[1.5, 1.5]$                                                                            |
| Gating function | Num inducing points        | $\NumInducing$            | $100$                                                                                   |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | $2 \times$ ones plus Gaussian noise                                                     |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
#+LATEX: }
#+end_table

*** Motorcycle Experiments

**** Two Experts :ignore:
\newline
*Model Training*
Table ref:tab-params-quadcopter contains the initial values for all of the trainable parameters in the model.

#+begin_table
#+LATEX: \caption{Initial parameter settings before training on motorcycle data set with two experts.}
#+LATEX: \label{tab-params-quadcopter}
#+LATEX: \resizebox{\textwidth}{!}{
#+ATTR_LATEX: :center nil
|                 | Description                | Symbol                    | Value                                                                                   |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Num data                   | $\NumData$                | $133$                                                                                   |
|                 | Batch size                 | $\NumData_b$              | $30$                                                                                    |
| Optimiser       | Epochs                     | N/A                       | $8000$                                                                                  |
|                 | Num samples                | $S$                       | $1$                                                                                     |
|                 | Learning rate              | N/A                       | $0.001$                                                                                 |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $0.1$                                                                                   |
|                 | Kernel Lengthscales        | $l$                       | $10$                                                                                    |
|                 | Likelihood Variance        | $\sigma_n$                | $0.0011$                                                                                |
| Expert 1        | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $20$                                                                                    |
|                 | Kernel Lengthscales        | $l$                       | $0.5$                                                                                   |
|                 | Likelihood Variance        | $\sigma_n$                | $0.9$                                                                                   |
| Expert 2        | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $3$                                                                                     |
|                 | Kernel Lengthscales        | $l$                       | $0.5$                                                                                   |
| Gating Function | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | $10 \times$ ones plus Gaussian noise                                                    |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
#+LATEX: }
#+end_table

**** Three Experts :ignore:
\newline
*Model Training*
Table ref:tab-params-quadcopter contains the initial values for all of the trainable parameters in the model.

#+begin_table
#+LATEX: \caption{Initial parameter settings before training on motorcycle data set with two experts.}
#+LATEX: \label{tab-params-quadcopter}
#+LATEX: \resizebox{\textwidth}{!}{
#+ATTR_LATEX: :center nil
|                 | Description                | Symbol                    | Value                                                                                   |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Num data                   | $\NumData$                | $133$                                                                                   |
|                 | Batch size                 | $\NumData_b$              | $30$                                                                                    |
| Optimiser       | Epochs                     | N/A                       | $8000$                                                                                  |
|                 | Num samples                | $S$                       | $1$                                                                                     |
|                 | Learning rate              | N/A                       | $0.001$                                                                                 |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $0.1$                                                                                   |
|                 | Kernel Lengthscales        | $l$                       | $10$                                                                                    |
|                 | Likelihood Variance        | $\sigma_n$                | $0.0011$                                                                                |
| Expert 1        | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $20$                                                                                    |
|                 | Kernel Lengthscales        | $l$                       | $0.5$                                                                                   |
|                 | Likelihood Variance        | $\sigma_n$                | $0.9$                                                                                   |
| Expert 2        | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $3$                                                                                     |
|                 | Kernel Lengthscales        | $l$                       | $0.5$                                                                                   |
| Gating Function | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | $10 \times$ ones plus Gaussian noise                                                    |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
#+LATEX: }
#+end_table

* Back Matter :ignore:
** Bibliography :ignore:

#+BEGIN_EXPORT latex
% \begingroup
% \sloppy
% \setstretch{1}
% \setlength\bibitemsep{3pt}
\printbibliography
% \endgroup
#+END_EXPORT

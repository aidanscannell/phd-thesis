* Config :ignore:
#+latex_class: mimosis
#+begin_src emacs-lisp :exports none  :results none
(unless (boundp 'org-latex-classes)
  (setq org-latex-classes nil))
(add-to-list 'org-latex-classes
             '("memoir"
               "\\documentclass{memoir}
    [NO-DEFAULT-PACKAGES]
    [PACKAGES]
    [EXTRA]
    \\newcommand{\\mboxparagraph}[1]{\\paragraph{#1}\\mbox{}\\\\}
    \\newcommand{\\mboxsubparagraph}[1]{\\subparagraph{#1}\\mbox{}\\\\}"
               ("\\chapter{%s}" . "\\chapter*{%s}")
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
               ;; ("\\mboxparagraph{%s}" . "\\mboxparagraph*{%s}")
               ;; ("\\mboxsubparagraph{%s}" . "\\mboxsubparagraph*{%s}")))
(add-to-list 'org-latex-classes
             '("mimosis"
               "\\documentclass{mimosis-class/mimosis}
  [NO-DEFAULT-PACKAGES]
  [PACKAGES]
  [EXTRA]"
               ("\\chapter{%s}" . "\\addchap{%s}")
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}\\newline" . "\\paragraph*{%s}\\newline")
               ("\\subparagraph{%s}\\newline" . "\\subparagraph*{%s}\\newline")))
#+end_src
# #+EXPORT_FILE_NAME: ./tmp/thesis.pdf
** Org Mode Export Options :noexport:
#+EXCLUDE_TAGS: journal noexport
#+OPTIONS: title:nil toc:nil date:nil author:nil H:6

** Macros :ignore:
# #+MACRO: acronym #+latex_header: \newacronym[description={$1}]{$2}{$2}{$3}
#+MACRO: glossaryentry #+latex_header: \newglossaryentry{$1}{name={$2},description={$3},sort={$4}}
#+MACRO: acronym #+latex_header: \newacronym{$1}{$2}{$3}
# #+MACRO: newline @@latex:\hspace{0pt}\\@@ @@html:<br>@@
# #+MACRO: fourstar @@latex:\bigskip{\centering\color{BrickRed}\FourStar\par}\bigskip@@
# #+MACRO: clearpage @@latex:\clearpage@@@@odt:<text:p text:style-name="PageBreak"/>@@

** LaTeX Export Headers and Options :noexport:
*** Packages :ignore:
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{amsmath,amssymb,amsfonts}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{todonotes}
*** Font Awesome icons
#+LATEX_HEADER: \usepackage{fontawesome}
*** Maths cancel
#+LATEX_HEADER: \usepackage[makeroom]{cancel}
*** Footnotes
#+LATEX_HEADER: \usepackage{footnote}
*** Tensor indexing (pre subscripts)
#+LATEX_HEADER: \usepackage{tensor}

*** Epigraph (chapter quotes)
#+LATEX_HEADER: \usepackage{epigraph}
*** Grey box for block quotes
#+LATEX_HEADER: \usepackage[most]{tcolorbox}
#+LATEX_HEADER: \definecolor{block-gray}{gray}{0.85}
#+LATEX_HEADER: \newtcolorbox{myquote}{colback=block-gray,boxrule=0pt,boxsep=0pt,breakable}
# #+LATEX_HEADER: \newtcolorbox{myquote}{colback=block-gray,grow to right by=-10mm,grow to left by=-10mm, boxrule=0pt,boxsep=0pt,breakable}
*** Acronym and Glossary :ignore:
#+latex_header: \usepackage[acronym]{glossaries}
#+latex_header: \makeglossaries

*** Equation Definitions

#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \newcommand{\defeq}{\vcentcolon=}

*** Create a Definition theorem
#+LATEX_HEADER: \newtheorem{definition}{Definition}[section]
#+LATEX_HEADER: \newtheorem{theorem}{Theorem}[section]
#+LATEX_HEADER: \newtheorem{lemma}{Lemma}[section]
*** Floating images configuration

By default,  if a figure consumes 60% of the page it will get its own float-page. To change that we have to adjust the value of the floatpagefraction derivative.
#+latex_header: \renewcommand{\floatpagefraction}{.8}%

See more information [[https://tex.stackexchange.com/questions/68516/avoid-that-figure-gets-its-own-page][here]].

*** Hyperref
Self-explanatory.
#+latex_header: \usepackage[colorlinks=true, citecolor=BrickRed, linkcolor=BrickRed, urlcolor=BrickRed]{hyperref}

*** Cleverref
#+latex_header: \usepackage[capitalise,noabbrev]{cleveref}
*** Bookmarks

The bookmark package implements a new bookmark (outline) organisation for package hyperref. This lets us change the "tree-navigation" associated with the generated pdf and constrain the menu only to H:2.
#+latex_header: \usepackage{bookmark}
#+latex_header: \bookmarksetup{depth=2}

*** BBding

Symbols such as diamond suit, which can be used for aesthetically separating paragraphs, could be added with the package =fdsymbol=. I'll use bbding which offers the more visually appealing =\FourStar=. I took this idea from seeing the thesis of the mimosis package author.
#+latex_header: \usepackage{bbding}

*** CS Quotes
The [[https://ctan.org/pkg/csquotes][csquotes]] package offers context sensitive quotation facilities, improving the typesetting of inline quotes.

Already imported by mimosis class.
# #+latex_header: \usepackage{csquotes}

To enclose quote environments with quotes from csquotes, see [[https://tex.stackexchange.com/questions/365231/enclose-a-custom-quote-environment-in-quotes-from-csquotes][the following TeX SE thread]].

#+latex_header: \def\signed #1{{\leavevmode\unskip\nobreak\hfil\penalty50\hskip1em
#+latex_header:   \hbox{}\nobreak\hfill #1%
#+latex_header:   \parfillskip=0pt \finalhyphendemerits=0 \endgraf}}

#+latex_header: \newsavebox\mybox
#+latex_header: \newenvironment{aquote}[1]
#+latex_header: {\savebox\mybox{#1}\begin{quote}\openautoquote\hspace*{-.7ex}}
#+latex_header:    {\unskip\closeautoquote\vspace*{1mm}\signed{\usebox\mybox}\end{quote}}

And then use quotes as:
#+begin_example
# The options derivative adds text after the environment. We use it to add the author.
#+ATTR_LATEX: :options {\cite{Frahm1994}}
#+begin_aquote
/Current (fMRI) applications often rely on "effects" or "statistically significant differences", rather than on a proper analysis of the relationship between neuronal activity, haemodynamic consequences, and MRI physics./
#+end_aquote
#+end_example

Note that org-ref links won't work here because the attr latex will be pasted as-is in the .tex file.

*** Date Time

The date time package allows us to specify a "formatted" date object, which will print different formats according to the current locale & language. I use this in my title page.
#+latex_header: \usepackage[level]{datetime}

*** Bibliography
General configuration.
# #+latex_header: \usepackage[autocite=plain, backend=biber, doi=true, url=true, hyperref=true,uniquename=false, maxbibnames=99, maxcitenames=2, sortcites=true, style=authoryear-comp]{biblatex}
# #+LATEX_HEADER: \usepackage[citestyle=authoryear-comp, maxcitenames=2, maxbibnames=99, doi=false, isbn=false, eprint=false, backend=bibtex, hyperref=true, url=false, natbib=true, style=authoryear-comp]{biblatex}
#+LATEX_HEADER: \usepackage[citestyle=authoryear-comp, maxcitenames=2, maxbibnames=99, doi=false, isbn=false, eprint=false, backend=bibtex, hyperref=true, url=false, natbib=true, style=authoryear-comp]{biblatex}
# #+LATEX_HEADER: \addbibresource{~/Dropbox/org/ref/mendeley/library.bib}
#+LATEX_HEADER: \addbibresource{~/Dropbox/org/ref/zotero-library.bib}

Improvements provided with the Mimosis class.
# #+latex_header: \input{bibliography-mimosis}

# And fix the andothers to show et al in English as well:
# #+latex_header: \DefineBibliographyStrings{english}{andothers={\textit{et\, al\adddot}}} 
# #+latex_header:\DefineBibliographyStrings{english}{and={\textit{and}}}


Remove ISSN, DOI and URL to shorten the bibliography.
#+latex_header: \AtEveryBibitem{%
#+latex_header:   \clearfield{urlyear}
#+latex_header:   \clearfield{urlmonth}
#+latex_header:   \clearfield{note}
#+latex_header:  \clearfield{issn} % Remove issn
#+latex_header:  \clearfield{doi} % Remove doi
#+latex_header: \ifentrytype{online}{}{% Remove url except for @online
#+latex_header:   \clearfield{url}
#+latex_header: }
#+latex_header: }

And increase the spacing between the entries, as per default they are too small.
#+latex_header: \setlength\bibitemsep{1.1\itemsep}

Also reduce the font-size
#+latex_header: \renewcommand*{\bibfont}{\footnotesize}

*** Improve chapter font colors and font size
The following commands make chapter numbers BrickRed, which look like the Donders color.
#+latex_header: \makeatletter
#+latex_header: \renewcommand*{\chapterformat}{  \mbox{\chapappifchapterprefix{\nobreakspace}{\color{BrickRed}\fontsize{40}{45}\selectfont\thechapter}\autodot\enskip}}
#+latex_header: \renewcommand\@seccntformat[1]{\color{BrickRed} {\csname the#1\endcsname}\hspace{0.3em}}
#+latex_header: \makeatother

*** Setspace for controlling line spacing

Already imported when using mimosis.
# #+latex_header: \usepackage{setspace}
#+latex_header: \setstretch{1.25} 

*** Parskip

Fine tuning of spacing between paragraphs. See [[https://tex.stackexchange.com/questions/161254/smaller-parskip-than-half-for-koma-script][thread here]].

#+latex_header: \setparsizes{0em}{0.1\baselineskip plus .1\baselineskip}{1em plus 1fil}

*** Table of Contents improvements

# TOC only the chapters, not their content.
# #+latex_header: \setcounter{tocdepth}{1}
#+latex_header: \setcounter{tocdepth}{2}

*** Possible Equation improvements

Make the equation numbers follow the chapter, not the whole thesis.
#+latex_header: \numberwithin{equation}{chapter}

*** TikZ and bayesnet for graphical models
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{bayesnet}

*** Notes in margins
# #+LATEX_HEADER: \usepackage{geometry}
#+LATEX_HEADER: \setlength{\marginparwidth}{3cm}
# #+LATEX_HEADER: \xdef\marginnotetextwidth{\textwidth}
#+LATEX_HEADER: \usepackage{marginnote}
# #+LATEX_HEADER: \renewcommand*{\marginfont}{\footnotesize}
# #+LATEX_HEADER: \newcommand{\parmarginnote}[1]{\hspace{\z@}\marginnote{#1}\ignorespaces}
#+LATEX_HEADER: \newcommand{\parmarginnote}[1]{\marginnote{#1}}
*** Captions
# #+LATEX_HEADER: \usepackage{caption}
# #+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \ifCLASSOPTIONcompsoc \usepackage[caption=false,font=footnotesize,labelfon
#+LATEX_HEADER: t=it,textfont=it]{subfig} \else
#+LATEX_HEADER: \usepackage[caption=false,font=footnotesize]{subfig}
#+LATEX_HEADER: \fi
#+LATEX_HEADER: \usepackage[format=plain,labelfont={bf},textfont=it]{caption} % make captions italic
*** Maths diag
#+LATEX_HEADER: \newcommand{\diag}{\mathop{\mathrm{diag}}}
** Text Variables :noexport:
#+latex_header: \newcommand{\ThesisTitle}{{Probabilistic Inference for Learning \& Control in Multimodal Dynamical Systems}}
# #+latex_header: \newcommand{\ThesisTitle}{{Data Efficient Learning for Control in Multimodal Dynamical Systems}}
#+latex_header: \newcommand{\ThesisSubTitle}{Synergising Bayesian Inference and Riemannian Geometry for Control}
#+latex_header: \newcommand{\FormattedThesisDefenseDate}{\mbox{\formatdate{1}{1}{2100}}}
#+latex_header: \newcommand{\FormattedAuthorDateOfBirth}{\mbox{\formatdate{1}{1}{2000}}}
#+latex_header: \newcommand{\FormattedThesisDefenseTime}{\mbox{10:00}}
#+latex_header: \newcommand{\AuthorShortName}{\mbox{Aidan Scannell}}
#+latex_header: \newcommand{\AuthorFullName}{\mbox{Aidan J. Scannell}}
#+latex_header: \newcommand{\ThesisISBN}{\mbox{}}

** Math Variables :noexport:
#+LATEX_HEADER: \DeclareMathOperator{\R}{\mathbb{R}}
#+LATEX_HEADER: \DeclareMathOperator{\E}{\mathbb{E}}
#+LATEX_HEADER: \DeclareMathOperator{\V}{\mathbb{V}}
#+LATEX_HEADER: \DeclareMathOperator{\K}{\mathbf{K}}

*** Num Data / Mode / State Dimension / Control Dimension (k, d, t/n)
#+LATEX_HEADER: \newcommand{\numData}{\ensuremath{t}}
# #+LATEX_HEADER: \newcommand{\numData}{\ensuremath{n}}
#+LATEX_HEADER: \newcommand{\numEpisodes}{\ensuremath{e}}
#+LATEX_HEADER: \newcommand{\numTimesteps}{\ensuremath{t}}
#+LATEX_HEADER: \newcommand{\numInd}{\ensuremath{m}}
#+LATEX_HEADER: \newcommand{\stateDim}{\ensuremath{d}}
#+LATEX_HEADER: \newcommand{\controlDim}{\ensuremath{f}}
#+LATEX_HEADER: \newcommand{\modeInd}{\ensuremath{k}}
#+LATEX_HEADER: \newcommand{\modeDesInd}{\ensuremath{\text{des}}}
#+LATEX_HEADER: \newcommand{\testInd}{\ensuremath{*}}
#+LATEX_HEADER: \newcommand{\NumData}{\ensuremath{\MakeUppercase{\numData}}}
#+LATEX_HEADER: \newcommand{\NumInd}{\ensuremath{\MakeUppercase{\numInd}}}
# #+LATEX_HEADER: \newcommand{\StateDim}{\ensuremath{\MakeUppercase{\stateDim}}}
# #+LATEX_HEADER: \newcommand{\ControlDim}{\ensuremath{\MakeUppercase{\controlDim}}}
#+LATEX_HEADER: \newcommand{\StateDim}{\ensuremath{{D_x}}}
#+LATEX_HEADER: \newcommand{\ControlDim}{\ensuremath{{D_u}}}
#+LATEX_HEADER: \newcommand{\ModeInd}{\ensuremath{\MakeUppercase{\modeInd}}}
#+LATEX_HEADER: \newcommand{\NumEpisodes}{\MakeUppercase{\numEpisodes}}
#+LATEX_HEADER: \newcommand{\NumTimesteps}{\MakeUppercase{\numTimesteps}}

# Macros for single/all data notation
#+LATEX_HEADER: \newcommand{\singleData}[1]{\ensuremath{#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\allData}[1]{\ensuremath{\MakeUppercase{#1}}}
# #+LATEX_HEADER: \newcommand{\singleData}[1]{\ensuremath{#1_{\numData}}}
# #+LATEX_HEADER: \newcommand{\allData}[1]{\ensuremath{#1}}
# #+LATEX_HEADER: \newcommand{\allData}[1]{\ensuremath{#1_{1:\NumData}}}

# Macros for data dimensions
# #+LATEX_HEADER: \newcommand{\singleDataDim}[1]{\ensuremath{#1_{\stateDim, \numData}}}
#+LATEX_HEADER: \newcommand{\singleDataDim}[1]{\ensuremath{_{\stateDim}#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\singleDim}[1]{\ensuremath{#1_{\stateDim}}}
# #+LATEX_HEADER: \newcommand{\singleDim}[1]{\ensuremath{_{\stateDim}#1}}
# #+LATEX_HEADER: \newcommand{\singleDimi}[2]{\ensuremath{\tensor*[_{#2}]{#1}{}}}
#+LATEX_HEADER: \newcommand{\singleDim}[1]{\ensuremath{\singleDimi{#1}{\stateDim}}}

# Macros for mode k notation
# #+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{#1^{(\modeInd)}}}
# #+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{#1^{\modeInd}}}
# #+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{\tensor*[^{\modeInd}]{#1}{}}}
#+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{#1_{\modeInd}}}
#+LATEX_HEADER: \newcommand{\modeDes}[1]{\ensuremath{#1^{\modeDesInd}}}

#+LATEX_HEADER: \newcommand{\singleDimiMode}[2]{\ensuremath{\tensor*[_#2^\modeInd]{#1}{}}}
#+LATEX_HEADER: \newcommand{\singleDimMode}[1]{\ensuremath{\singleDimiMode{#1}{\stateDim}}}
#+LATEX_HEADER: \newcommand{\singleDimModeData}[1]{\ensuremath{\tensor*[_\stateDim^\modeInd]{#1}{_\numData}}}

*** Data set
# Dataset/inputs/outputs
#+LATEX_HEADER: \newcommand{\state}{\ensuremath{\mathbf{x}}}
#+LATEX_HEADER: \newcommand{\control}{\ensuremath{\mathbf{u}}}
# #+LATEX_HEADER: \newcommand{\control}{\ensuremath{\mathbf{a}}}

#+LATEX_HEADER: \newcommand{\x}{\ensuremath{\mathbf{x}}}
# #+LATEX_HEADER: \newcommand{\y}{\ensuremath{\mathbf{y}}}
#+LATEX_HEADER: \newcommand{\y}{\ensuremath{y}}
# #+LATEX_HEADER: \newcommand{\x}{\ensuremath{\hat{\state}}}
# #+LATEX_HEADER: \newcommand{\y}{\ensuremath{\Delta\state}}
#+LATEX_HEADER: \newcommand{\dataset}{\ensuremath{\mathcal{D}}}

# Single/all input/output notation
# #+LATEX_HEADER: \newcommand{\singleInput}{\ensuremath{\singleData{\x}}}
#+LATEX_HEADER: \newcommand{\singleInput}{\ensuremath{\x_{\numData-1}}}
#+LATEX_HEADER: \newcommand{\singleOutput}{\ensuremath{\singleData{\y}}}
#+LATEX_HEADER: \newcommand{\allInput}{\ensuremath{\allData{\x}}}
#+LATEX_HEADER: \newcommand{\allOutput}{\ensuremath{\allData{\y}}}

# Single/all state/control notation
#+LATEX_HEADER: \newcommand{\singleState}{\ensuremath{\state_{\numData-1}}}
#+LATEX_HEADER: \newcommand{\singleControl}{\ensuremath{\control_{\numData-1}}}
#+LATEX_HEADER: \newcommand{\allState}{\ensuremath{\allData{\state}}}
#+LATEX_HEADER: \newcommand{\allControl}{\ensuremath{\allData{\control}}}

*** Noise Vars
#+LATEX_HEADER: \newcommand{\noiseVar}{\ensuremath{\sigma}}
#+LATEX_HEADER: \newcommand{\noiseVarK}{\ensuremath{\mode{\noiseVar}}}
#+LATEX_HEADER: \newcommand{\noiseVarOneK}{\ensuremath{\singleDimiMode{\noiseVar}{1}}}
#+LATEX_HEADER: \newcommand{\noiseVarDK}{\ensuremath{\singleDimiMode{\noiseVar}{\StateDim}}}
#+LATEX_HEADER: \newcommand{\noiseVardK}{\ensuremath{\singleDimMode{\noiseVar}}}
# #+LATEX_HEADER: \newcommand{\noiseVarOneK}{\ensuremath{\noiseVarK_{1}}}
# #+LATEX_HEADER: \newcommand{\noiseVarDK}{\ensuremath{\noiseVarK_{\StateDim}}}
# #+LATEX_HEADER: \newcommand{\noiseVardK}{\ensuremath{\noiseVarK_{\stateDim}}}
# #+LATEX_HEADER: \newcommand{\noiseVardK2}{\ensuremath{\left(\noiseVardK\right)^2}}

*** Mode Indicator Variable
#+LATEX_HEADER: \newcommand{\modeVar}{\ensuremath{\alpha}}
#+LATEX_HEADER: \newcommand{\modeVarn}{\ensuremath{\singleData{\modeVar}}}
#+LATEX_HEADER: \newcommand{\ModeVar}{\ensuremath{\bm{\modeVar}}}
# #+LATEX_HEADER: \newcommand{\ModeVar}{\ensuremath{\allData{\bm{\modeVar}}}}
#+LATEX_HEADER: \newcommand{\modeVarK}{\ensuremath{\modeVarn=\modeInd}}
# #+LATEX_HEADER: \newcommand{\ModeVarK}{\ensuremath{\mode{\bm{\modeVar}}}}
#+LATEX_HEADER: \newcommand{\ModeVarK}{\ensuremath{\ModeVar_{\modeInd}}}

*** Tensor Indexing
# Experts indexing
#+LATEX_HEADER: \newcommand{\nkd}[1]{\ensuremath{#1_{\numData,\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\nkD}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\NkD}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\nKD}[1]{\ensuremath{#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\Nkd}[1]{\ensuremath{#1_{:,\modeInd,\stateDim}}}

# Gating function indexing
#+LATEX_HEADER: \newcommand{\nk}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\Nk}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\nK}[1]{\ensuremath{#1_{\numData}}}

# Experts Inducing indexing
#+LATEX_HEADER: \newcommand{\mkd}[1]{\ensuremath{#1_{\numData,\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\mkD}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\MkD}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\mKD}[1]{\ensuremath{#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\Mkd}[1]{\ensuremath{#1_{:,\modeInd,\stateDim}}}

# Gating Inducing indexing
#+LATEX_HEADER: \newcommand{\mk}[1]{\ensuremath{#1_{\numData,\modeInd}}}
#+LATEX_HEADER: \newcommand{\Mk}[1]{\ensuremath{#1_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\mK}[1]{\ensuremath{#1_{\numData}}}

# Desired Mode Gating indexing
#+LATEX_HEADER: \newcommand{\MDes}[1]{\ensuremath{#1_{:, k^*}}}

*** Gating Network New
# Function notation
#+LATEX_HEADER: \newcommand{\gatingFunc}{\ensuremath{h}}
#+LATEX_HEADER: \newcommand{\hk}{\ensuremath{\mode{\gatingFunc}}}

# Single data notation
#+LATEX_HEADER: \newcommand{\hkn}{\ensuremath{\nk{\gatingFunc}}}
#+LATEX_HEADER: \newcommand{\hn}{\ensuremath{\nK{\mathbf{\gatingFunc}}}}

# All inputs set/vector/tensor notation
#+LATEX_HEADER: \newcommand{\GatingFunc}{\ensuremath{\mathbf{\gatingFunc}}}
#+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\MakeUppercase\GatingFunc}}}}
#+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\Nk{\GatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\allData{\GatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\allData{\mode{\GatingFunc}}}}

*** Experts New
# Function notation
#+LATEX_HEADER: \newcommand{\latentFunc}{\ensuremath{f}}
#+LATEX_HEADER: \newcommand{\LatentFunc}{\ensuremath{\mathbf{\latentFunc}}}
#+LATEX_HEADER: \newcommand{\fkd}{\ensuremath{\latentFunc_{\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\fk}{\ensuremath{\mathbf{\latentFunc}_{\modeInd}}}
# #+LATEX_HEADER: \newcommand{\fk}{\ensuremath{\latentFunc_{:,\modeInd}}}
#+LATEX_HEADER: \newcommand{\f}{\ensuremath{\mathbf{f}}}

# Vector/Matrix/Tensor notation
#+LATEX_HEADER: \newcommand{\F}{\ensuremath{\MakeUppercase{\mathbf{\latentFunc}}}}
# #+LATEX_HEADER: \newcommand{\Fnkd}{\ensuremath{\latentFunc_{\numData, \modeInd, \stateDim}}}
# #+LATEX_HEADER: \newcommand{\Fnk}{\ensuremath{\mathbf{\latentFunc}_{\numData, \modeInd}}}
# #+LATEX_HEADER: \newcommand{\Fk}{\ensuremath{\F_{:,\modeInd}}}
# #+LATEX_HEADER: \newcommand{\Fn}{\ensuremath{\F_{\numData}}}
#+LATEX_HEADER: \newcommand{\Fnkd}{\ensuremath{\nkd{\latentFunc}}}
#+LATEX_HEADER: \newcommand{\Fnk}{\ensuremath{\nkD{\mathbf{\latentFunc}}}}
#+LATEX_HEADER: \newcommand{\Fk}{\ensuremath{\NkD{\F}}}
#+LATEX_HEADER: \newcommand{\Fn}{\ensuremath{\nKD{\F}}}
#+LATEX_HEADER: \newcommand{\F}{\ensuremath{\F}}

# #+LATEX_HEADER: \newcommand{\Fdk}{\ensuremath{\mathbf{\latentFunc}_{:,\modeInd,\stateDim}}}
#+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\Nkd{\mathbf{\latentFunc}}}}

# Single input notation
#+LATEX_HEADER: \newcommand{\fn}{\ensuremath{\Fn}}
#+LATEX_HEADER: \newcommand{\fkn}{\ensuremath{\Fnk}}
#+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\Fnkd}}

# All inputs set/vector/tensor notation
# #+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\Fdk}}

*** Params
#+LATEX_HEADER: \newcommand{\gatingParams}{\ensuremath{\bm\phi}}
#+LATEX_HEADER: \newcommand{\expertParams}{\ensuremath{\bm\theta}}
#+LATEX_HEADER: \newcommand{\gatingParamsK}{\ensuremath{\mode{\bm\phi}}}
#+LATEX_HEADER: \newcommand{\expertParamsK}{\ensuremath{\mode{\bm\theta}}}
*** Sparse GPs
**** Experts
***** Variables
#+LATEX_HEADER: \newcommand{\uf}{\ensuremath{u}}
#+LATEX_HEADER: \newcommand{\uFkd}{\ensuremath{\Mkd{\mathbf{\uf}}}}
#+LATEX_HEADER: \newcommand{\uFk}{\ensuremath{\MkD{\MakeUppercase{\mathbf{\uf}}}}}
#+LATEX_HEADER: \newcommand{\uF}{\ensuremath{\MakeUppercase{\mathbf{\uf}}}}

***** Inputs
# #+LATEX_HEADER: \newcommand{\zf}{\ensuremath{\bm{\zeta}}}
# #+LATEX_HEADER: \newcommand{\zFkd}{\ensuremath{\Mkd{\zf}}}
# #+LATEX_HEADER: \newcommand{\zFk}{\ensuremath{\MkD{\zf}}}
# #+LATEX_HEADER: \newcommand{\zF}{\ensuremath{\MKD{\zf}}}
#+LATEX_HEADER: \newcommand{\zf}{\ensuremath{\mathbf{Z}}}
#+LATEX_HEADER: \newcommand{\zFkd}{\ensuremath{\Mkd{\zf}}}
#+LATEX_HEADER: \newcommand{\zFk}{\ensuremath{\MkD{\zf}}}
#+LATEX_HEADER: \newcommand{\zF}{\ensuremath{\MKD{\zf}}}

**** Gating
***** Variables
#+LATEX_HEADER: \newcommand{\uh}{\ensuremath{U}}
#+LATEX_HEADER: \newcommand{\uHk}{\ensuremath{\Mk{\hat{\mathbf{\uh}}}}}
#+LATEX_HEADER: \newcommand{\uH}{\ensuremath{\hat{\MakeUppercase{\mathbf{\uh}}}}}

#+LATEX_HEADER: \newcommand{\hu}{\ensuremath{\uh}}
#+LATEX_HEADER: \newcommand{\Hku}{\ensuremath{\uHk}}
#+LATEX_HEADER: \newcommand{\Hu}{\ensuremath{\uH}}

***** Inputs
# #+LATEX_HEADER: \newcommand{\zh}{\ensuremath{\bm{\xi}}}
# #+LATEX_HEADER: \newcommand{\zHk}{\ensuremath{\Mk{\zh}}}
# #+LATEX_HEADER: \newcommand{\zH}{\ensuremath{\MK{\zh}}}
#+LATEX_HEADER: \newcommand{\zh}{\ensuremath{\hat{\mathbf{Z}}}}
#+LATEX_HEADER: \newcommand{\zHk}{\ensuremath{\Mk{\zh}}}
#+LATEX_HEADER: \newcommand{\zH}{\ensuremath{\MK{\zh}}}

# #+LATEX_HEADER: \newcommand{\zHDes}{\ensuremath{\zH_{:, k^*}}}
#+LATEX_HEADER: \newcommand{\zHDes}{\ensuremath{\MDes{\zH}}}

**** Misc
#+LATEX_HEADER: \newcommand{\Z}{\ensuremath{\mathbf{Z}}}
**** Old
# Sparse GP macro
# #+LATEX_HEADER: \newcommand{\inducing}[1]{\ensuremath{\hat{#1}}}

# #+LATEX_HEADER: \newcommand{\fu}{\ensuremath{\inducing{\latentFunc}}}
# #+LATEX_HEADER: \newcommand{\Fu}{\ensuremath{\inducing{\mathbf{\latentFunc}}}}
# #+LATEX_HEADER: \newcommand{\Fku}{\ensuremath{\mode{\inducing{\mathbf{\latentFunc}}}}}
# #+LATEX_HEADER: \newcommand{\Fkdu}{\ensuremath{\singleDim{\Fku}}}
# #+LATEX_HEADER: \newcommand{\hu}{\ensuremath{\inducing{\gatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hu}{\ensuremath{\inducing{\mathbf{\gatingFunc}}}}
# #+LATEX_HEADER: \newcommand{\Hku}{\ensuremath{\mode{\inducing{\mathbf{\gatingFunc}}}}}

# #+LATEX_HEADER: \newcommand{\Zfk}{\ensuremath{\mode{\mathbf{Z}}_{\latentFunc}}}
# #+LATEX_HEADER: \newcommand{\Zfk}{\ensuremath{\mode{\bm{\zeta}}}}
# #+LATEX_HEADER: \newcommand{\Zf}{\ensuremath{\mathbf{Z}}}
# #+LATEX_HEADER: \newcommand{\Zf}{\ensuremath{\mathbf{Z}_{\latentFunc}}}

# #+LATEX_HEADER: \newcommand{\Zhk}{\ensuremath{\mode{\mathbf{Z}}_{\gatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Zh}{\ensuremath{\bm{\xi}}}
# #+LATEX_HEADER: \newcommand{\Zhk}{\ensuremath{\mode{\Zh}}}

# #+LATEX_HEADER: \newcommand{\ZhDes}{\ensuremath{\modeDes{\zH}}}

*** Continuous
#+LATEX_HEADER: \newcommand{\derivative}[1]{\ensuremath{\dot{#1}}}
#+LATEX_HEADER: \newcommand{\stateDerivative}{\ensuremath{\derivative{\state}}}
# #+LATEX_HEADER: \newcommand{\stateDerivative}{\ensuremath{\dot{\mathbf{x}}}}

*** Prob Dists New
#+LATEX_HEADER: \newcommand{\pFkd}{\ensuremath{p\left(\Fkd \right)}}
*** Prob Dists
#+LATEX_HEADER: \newcommand{\pFkd}{\ensuremath{p\left(\Fkd \mid \allInput \right)}}
#+LATEX_HEADER: \newcommand{\pFk}{\ensuremath{p\left(\Fk \mid \allInput, \expertParams\right)}}

#+LATEX_HEADER: \newcommand{\pF}{\ensuremath{p\left(\F \mid \allInput, \expertParams\right)}}
#+LATEX_HEADER: \newcommand{\pfk}{\ensuremath{p\left(\fk \mid \allInput, \expertParamsK \right)}}
#+LATEX_HEADER: \newcommand{\pfknd}{\ensuremath{p\left(\fknd \mid \allInput\right)}}

#+LATEX_HEADER: \newcommand{\pFkGivenUk}{\ensuremath{p\left(\Fk \mid \uFk \right)}}
# #+LATEX_HEADER: \newcommand{\pYkGivenUk}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pYkGivenFku}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \uFk \right)}}

#+LATEX_HEADER: \newcommand{\qF}{\ensuremath{q\left(\F \right)}}
#+LATEX_HEADER: \newcommand{\qFu}{\ensuremath{q\left(\uF \right)}}
#+LATEX_HEADER: \newcommand{\qFku}{\ensuremath{q\left(\uFk \right)}}
#+LATEX_HEADER: \newcommand{\pFku}{\ensuremath{p\left(\uFk \mid \zFk \right)}}
#+LATEX_HEADER: \newcommand{\pFkuGivenX}{\ensuremath{p\left(\uFk \mid \zFk \right)}}
#+LATEX_HEADER: \newcommand{\pFuGivenX}{\ensuremath{p\left(\uF \mid \zF \right)}}
#+LATEX_HEADER: \newcommand{\qFk}{\ensuremath{q\left(\Fk \right)}}
#+LATEX_HEADER: \newcommand{\qfk}{\ensuremath{q\left(\fk \right)}}
#+LATEX_HEADER: \newcommand{\qfkn}{\ensuremath{q\left(\fkn \right)}}
#+LATEX_HEADER: \newcommand{\qfn}{\ensuremath{q\left(\fn \right)}}
#+LATEX_HEADER: \newcommand{\pFkGivenFku}{\ensuremath{p\left(\Fk \mid \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pfkGivenFku}{\ensuremath{p\left(\fkn \mid \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenFku}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pYGivenUX}{\ensuremath{p\left(\allOutput \mid \uF, \allInput \right)}}
#+LATEX_HEADER: \newcommand{\pYGivenU}{\ensuremath{p\left(\allOutput \mid \uF \right)}}


#+LATEX_HEADER: \newcommand{\pY}{\ensuremath{p\left(\allOutput \right)}}
# #+LATEX_HEADER: \newcommand{\pykGivenfk}{\ensuremath{p\left(\singleOutputK \mid \fkn \right)}}
# #+LATEX_HEADER: \newcommand{\pYkGivenFk}{\ensuremath{p\left(\allOutputK \mid \Fk \right)}}
# #+LATEX_HEADER: \newcommand{\pYkGivenX}{\ensuremath{p(\allOutputK \mid \allInput)}}
#+LATEX_HEADER: \newcommand{\pykGivenx}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenxNegF}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \singleInput, \neg\Fk \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenfk}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \fkn \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenfkd}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \fknd \right)}}
#+LATEX_HEADER: \newcommand{\pYkGivenFk}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \Fk \right)}}
#+LATEX_HEADER: \newcommand{\pYkGivenX}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \allInput \right)}}
#+LATEX_HEADER: \newcommand{\pYGivenX}{\ensuremath{p\left(\allOutput \mid \allInput \right)}}

**** Gating network
#+LATEX_HEADER: \newcommand{\PrA}{\ensuremath{\Pr\left(\ModeVarK \right)}}
#+LATEX_HEADER: \newcommand{\Pra}{\ensuremath{\Pr\left(\modeVarK \right)}}
#+LATEX_HEADER: \newcommand{\PaGivenhx}{\ensuremath{P\left(\modeVarn \mid \hn, \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\PraGivenx}{\ensuremath{\Pr\left(\modeVarn \mid \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\PraGivenhx}{\ensuremath{\Pr\left(\modeVarK \mid \hn, \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\PraGivenxNegH}{\ensuremath{\Pr\left(\modeVarK \mid \singleInput, \neg\Hall \right)}}
#+LATEX_HEADER: \newcommand{\PrAGivenX}{\ensuremath{\Pr\left(\ModeVarK \mid \allInput \right)}}

#+LATEX_HEADER: \newcommand{\pHGivenX}{\ensuremath{p\left(\Hall \mid \allInput\right)}}
#+LATEX_HEADER: \newcommand{\pHkGivenX}{\ensuremath{p\left(\Hk \mid \allInput\right)}}

*** Kernels
# #+LATEX_HEADER: \newcommand{\Kkxx}{\mode{\mathbf{K}}_{\allInput\allInput}}
#+LATEX_HEADER: \newcommand{\Kkxx}{\mode{\mathbf{K}}_{d, \allInput\allInput}}

# TO derivative kernels
#+LATEX_HEADER: \newcommand{\ddK}{\ensuremath{\partial^2\K_{**}}}
#+LATEX_HEADER: \newcommand{\dK}{\ensuremath{\partial\K_{*}}}
#+LATEX_HEADER: \newcommand{\Kxx}{\ensuremath{\K_{}}}
#+LATEX_HEADER: \newcommand{\iKxx}{\ensuremath{\Kxx^{-1}}}

#+LATEX_HEADER: \newcommand{\dKz}{\ensuremath{\partial\K_{*\zH}}}
#+LATEX_HEADER: \newcommand{\Kzz}{\ensuremath{\K_{\zH\zH}}}
#+LATEX_HEADER: \newcommand{\iKzz}{\ensuremath{\Kzz^{-1}}}
*** Desired Mode
# Function notation
#+LATEX_HEADER: \newcommand{\HDes}{\ensuremath{\MDes{\GatingFunc}}}
#+LATEX_HEADER: \newcommand{\uHDes}{\ensuremath{\MDes{\uH}}}

# Inducing points
#+LATEX_HEADER: \newcommand{\pDes}{\ensuremath{p\left( \uHDes \mid \zHDes \right)}}
#+LATEX_HEADER: \newcommand{\qDes}{\ensuremath{q\left( \uHDes \right)}}
#+LATEX_HEADER: \newcommand{\mDes}{\ensuremath{\MDes{\mathbf{m}}}}
#+LATEX_HEADER: \newcommand{\SDes}{\ensuremath{\MDes{\mathbf{S}}}}

*** Jacobian
# Single data notation
#+LATEX_HEADER: \newcommand{\singleTest}[1]{\ensuremath{#1_{\testInd}}}
#+LATEX_HEADER: \newcommand{\testInput}{\ensuremath{\singleTest{\state}}}

# Jacobian notation
#+LATEX_HEADER: \newcommand{\Jac}{\ensuremath{\mathbf{J}}}
#+LATEX_HEADER: \newcommand{\testJac}{\ensuremath{\singleTest{\Jac}}}
#+LATEX_HEADER: \newcommand{\muJac}{\ensuremath{\mu_{\Jac}}}
#+LATEX_HEADER: \newcommand{\covJac}{\ensuremath{\Sigma_{\Jac}}}

*** Old
**** Gating Network Old
# # Function notation
# #+LATEX_HEADER: \newcommand{\gatingFunc}{\ensuremath{h}}
# #+LATEX_HEADER: \newcommand{\hk}{\ensuremath{\mode{\gatingFunc}}}

# # Single data notation
# #+LATEX_HEADER: \newcommand{\hkn}{\ensuremath{\singleData{\hk}}}
# #+LATEX_HEADER: \newcommand{\hn}{\ensuremath{\singleData{\mathbf{\gatingFunc}}}}

# # All inputs set/vector/tensor notation
# #+LATEX_HEADER: \newcommand{\GatingFunc}{\ensuremath{\mathbf{\gatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\GatingFunc}}
# #+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\mode{\GatingFunc}}}
# # #+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\allData{\GatingFunc}}}
# # #+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\allData{\mode{\GatingFunc}}}}
**** Desired Mode Old
# #+LATEX_HEADER: \newcommand{\HDes}{\ensuremath{\modeDes{\GatingFunc}}}
# #+LATEX_HEADER: \newcommand{\HuDes}{\ensuremath{\modeDes{\Hu}}}
# #+LATEX_HEADER: \newcommand{\mDes}{\ensuremath{\modeDes{\mathbf{m}}}}
# #+LATEX_HEADER: \newcommand{\SDes}{\ensuremath{\modeDes{\mathbf{S}}}}

**** Experts Old
# # Function notation
# #+LATEX_HEADER: \newcommand{\latentFunc}{\ensuremath{f}}
# #+LATEX_HEADER: \newcommand{\f}{\ensuremath{f}}
# #+LATEX_HEADER: \newcommand{\fk}{\ensuremath{\mode{\latentFunc}}}
# # #+LATEX_HEADER: \newcommand{\fkd}{\ensuremath{\singleDim{\fk}}}
# #+LATEX_HEADER: \newcommand{\fkd}{\ensuremath{\singleDimMode{\f}}}

# # Single input notation
# #+LATEX_HEADER: \newcommand{\fn}{\ensuremath{\singleData{\mathbf{\latentFunc}}}}
# #+LATEX_HEADER: \newcommand{\fkn}{\ensuremath{\singleData{\mode{\mathbf{\latentFunc}}}}}
# # #+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\singleDim{\singleData{\fk}}}}
# # #+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\singleDimMode{\singleData{\f}}}}
# #+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\singleDimModeData{\f}}}

# # All inputs set/vector/tensor notation
# # #+LATEX_HEADER: \newcommand{\F}{\ensuremath{\allData{\mathbf{\f}}}}
# #+LATEX_HEADER: \newcommand{\F}{\ensuremath{\mathbf{\f}}}
# #+LATEX_HEADER: \newcommand{\Fk}{\ensuremath{\mode{\F}}}
# # #+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\singleDim{\Fk}}}
# #+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\singleDimMode{\F}}}

#+LATEX_HEADER: \newcommand{\allOutputK}{\ensuremath{\mode{\allOutput}}}
#+LATEX_HEADER: \newcommand{\singleOutputK}{\ensuremath{\mode{\singleOutput}}}

** Acronyms :noexport:
{{{glossaryentry(LaTeX,\LaTeX,A document preparation system,LaTeX)}}}
{{{acronym(mogpe,MoGPE,Mixtures of Gaussian Process Experts)}}}
{{{acronym(gp,GP,Gaussian process)}}}
{{{acronym(mdp,MDP,Markov decision process)}}}
{{{glossaryentry(Real Numbers,$\real$,The set of Real numbers,Real Numbers)}}}
* Frontmatter :ignore:
#+BEGIN_EXPORT latex
\frontmatter
#+END_EXPORT
** Title Page :ignore:noexport:

  #+BEGIN_EXPORT latex
  \begin{titlepage}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % First page: Thesis Title and Author Name
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    % Uncomment when adding the background figure to the cover.
    \BgThispage

    \cleardoublepage
    \pagestyle{empty}

    \begin{center}
      \null\vfill
      {\huge{\bfseries \ThesisTitle}\par}
      \vspace{\stretch{0.5}}
      {\large \ThesisSubTitle \par}
      \vspace{\stretch{2}}
      \vspace{\baselineskip}
      {\large By \AuthorFullName\par}
      \vspace{\stretch{2}}
      %\vspace{\baselineskip}
      %\vspace{\baselineskip}
      \vspace{\baselineskip}
      \includegraphics[scale=0.6]{./logos/bristolcrest_colour}
      \hspace{5mm}
      \includegraphics[scale=0.35]{./logos/UWE_insignia.png}\\
      \vspace{10mm}
      {\large Department of Aerospace Engineering\\
       \textsc{University of Bristol}}
       \\
       \&
       \\
       {\large Department of Engineering Design and Mathematics\\
       \textsc{University of the West of England}}\\

      %{\large Faculty of Engineering\\
      %\textsc{University of Bristol}}\\
      %\vspace{6mm}
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \begin{minipage}{10cm}
        A dissertation submitted to the University of Bristol and the University of the West of England in accordance with the requirements of the degree of \textsc{Doctor of Philosophy} in the Faculty of Engineering.
      \end{minipage}\\
       \vspace{\baselineskip}
      % \vspace{\stretch{1}}
      \vspace{\baselineskip}
      \vspace{\stretch{1}}
      \noindent
      \begin{tabular}{@{}l@{\hspace{22pt}}ll}
        \textbf{Supervisors}:          & Prof.\ Arthur Richards\\
                                       & Dr.\ Carl Henrik Ek\\
      \end{tabular} \\
      %\vspace{\stretch{1}}
      %\vspace{\baselineskip}
      %\vspace{\baselineskip}
      \vspace{9mm}
      {\large\textsc{January 2022}}
      \vspace{12mm}
      \vfill
    \end{center}

    \cleardoublepage
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % End Titlepage
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \end{titlepage}
  #+END_EXPORT

** Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:
#+BEGIN_EXPORT latex
\begin{SingleSpace}
%\initial{R}einforcement learning and data-driven control have seen significant advances over the last decade,
%especially in simulated environments.
%Real world systems are often highly nonlinear, exhibit stochasticity and multimodalities,
%are expensive to run (slow, energy intensive, subject to wear and tear) and
%must be controlled subject to constraints (for safety, efficiency, etc).

%From robotics, to industrial processing, to finanace, learning-based approaches to control
%help alleviate the dependence on domain exerts for system identification and controller design.
This dissertation is concerned with \textit{learning} and \textit{control}
in unknown, (or partially unknown), multimodal dynamical systems.
It is motivated by controlling robotic systems in uncertain environments,
where both the underlying dynamics modes,
and how the dyanmics switches between them, are \textit{not fully known a priori}.

%For example, controlling a quadrotor subject to unoperatable dynamics modes that are
%induced via spatially varying turbulence
%i.e. fly a quadrotor to a target location, whilst remaining in the operatable (non turbulent) dynamics mode.

%This dissertation is concerned with \textbf{learning} and \textbf{control}
%in unknown, (or partially unknown), multimodal dynamical systems.
%It is motivated by controlling a quadrotor with unoperatable dynamics modes that are
%induced via spatially varying turbulence.
%The operatable mode corresponds to regions of the state space subject to \textbf{low turbulence}, and the
%unoperatable mode(s) corresponds to regions subject to \textbf{high turbulence}.
%The goal is to fly the quadrotor from an initial state in the desired (operatable) dynamics mode,
%to a target state, whilst remaining in the desired dynamics mode.

This dissertation first considers learning representations of multimodal dynamical systems, assuming
access to a historial data set of state transitions.
\todo{add comment about MoGPE vs SVGP}
The model resembles the Mixture of Gaussian Process Experts model with a gating network based on Gaussian processes.
Motivated by synergising model learning and control,
this model infers latent \textit{geometric} structure in the gating network,
that is later exploited by a geometry inspired control algorithm.
Well-calibrated uncertainty estimates and scalability are obtained via
stochastic variational inference.
%variational lower bound based on sparse approximations, that can be optimised with
%stochastic gradient methods.
%A novel variational lower bound based on sparse approximations, that can be optimised with
%stochastic gradient methods, is derived.
%It provides scalability as well as well-calibrated uncertainty estimates.

%Secondly, this work considers trajectory optimisation algorithms,
%that exploit the learned dynamics model to achieve the aformentioned goal.
%In a \textbf{risk-averse setting}, it is also desirable to avoid entering regions of a learned dynamics model with
%high \textit{epistemic uncertainty}.
%This is because the state-control trajectory cannot be predicted confidently, and thus,
%constraints may be violated i.e. the system may enter unoperatable dynamics modes.
%Still assuming access to a historical data set, the first approach presented in this dissertation
%exploits concepts of Riemannian geometry (extended to probabilistic geometries) to encode the trajectory optimisation
%goals into an objective function.
%A second, alternative approach, formulates the control problem as probabilistic inference
%in a graphical model by conditioning on a mode assignment variable.
%Both methods are evaluated via experiments on a simulated quadrotor, as well as a data set of a
%DJI Tello quadrotor flying in the Bristol Robotics Laboratory.

Secondly, this dissertation considers driving a dynamical system from an initial state (in a desired dynamics mode),
to a target state, whilst remaining in the desired dynamics mode.
For example, consider controlling a quadrotor in an environment subject to two dynamics modes: 1) a turbulent
dynamics mode in front of a fan, and 2) a non turbulent dynamics mode everywhere else.
The goal in this scenario is to fly the quadrotor to a target location,
whilst remaining in the operatable (non turbulent) dynamics mode.

In a \textbf{risk-averse setting}, it is desirable to avoid entering regions of a learned dynamics model with
high \textit{epistemic uncertainty}, as well as remaining in the desired dynamics mode.
This is because the trajectory cannot be predicted confidently, and may leave the operatable dynamics mode.
Given a partially learned dynamics model, this dissertation develops two trajectory optimisation algorithms
aimed at solving this risk-averse setting.
The first approach
exploits concepts of Riemannian geometry (extended to probabilistic geometries) to encode both of the goals
into a geometry inspired objective function.
The second approach formulates the control problem as probabilistic inference
in a graphical model, and encodes the goals by conditioning on a mode assignment variable.
Both methods are evaluated via experiments on a simulated quadrotor, as well as a data set collected onboard
a DJI Tello quadrotor.
%A second, alternative approach is also presented.
%Instead of exploiting the geometry of the learned model, it formulates the control problem as probabilistic inference
%in a graphical model by conditioning on a mode assignment variable.
%Based on these two goals, this dissertation develops two trajectory optimisation algorithms that exploit
%the learned dyanmics to achieve them.

Finally, this dissertation considers the active learning setting, where it does
not assume access to a historical data set.
To achieve this goal, a constrained exploration algorithm is introduced.
The algorithm exploits the \textit{epistemic uncertainty} associated with the learned model, to guide
exploration into regions of the dynamics that have not previously been observed.
This experience can then be used to update the model and reduce the associated \textit{epistemic uncertainty}.
Exploration is subject to chance constraints that prevent the system from leaving the desired dynamics
mode, resulting in an overconstrained problem.
Loosening the chance constraints enables the algorithm to incrementally explore the environment,
becoming more confident in the dynamics,
until it can find a trajectory to the target state that does not violate the chance constraints.



\end{SingleSpace}
#+END_EXPORT

** Declaration :noexport:
:PROPERTIES:
:UNNUMBERED: t
:END:
#+BEGIN_EXPORT latex
\begin{SingleSpace}
\begin{quote}
\initial{I} declare that the work in this dissertation was carried out in accordance with the requirements of  the University's Regulations and Code of Practice for Research Degree Programmes and that it  has not been submitted for any other academic award. Except where indicated by specific  reference in the text, the work is the candidate's own work. Work done in collaboration with, or with the assistance of, others, is indicated as such. Any views expressed in the dissertation are those of the author.

\vspace{1.5cm}
\noindent
\hspace{-0.75cm}\textsc{SIGNED: .................................................... DATE: ..........................................}
\end{quote}
\end{SingleSpace}
#+END_EXPORT

** Acknowledgements :noexport:
:PROPERTIES:
:UNNUMBERED: t
:END:
#+BEGIN_EXPORT latex
\begin{SingleSpace}
\initial{H}ere goes the dedication.
\end{SingleSpace}
#+END_EXPORT
* TOC and Mainmatter :ignore:
#+BEGIN_EXPORT latex
\tableofcontents
% This ensures that the subsequent sections are being included as root
% items in the bookmark structure of your PDF reader.
\begingroup
    \let\clearpage\relax
    \glsaddall
    \printglossary[type=\acronymtype]
    \newpage
    \printglossary
\endgroup
\printindex

\mainmatter
#+END_EXPORT

* Testing Maths Variables :noexport:
** Tables :ignore:
#+CAPTION: Variables
| Name                    | Symbol     | Equation                                                   |
|-------------------------+------------+------------------------------------------------------------|
| State                   | $\state$   | $\R^{\StateDim}$                                           |
| Control                 | $\control$ | $\R^{\ControlDim}$                                         |
| Time                    | $t$        | $\R$                                                       |
| State-action input      | $\x$       | $(\state, \control) \in \R^{\StateDim \times \ControlDim}$ |
| State difference        | $\y$       | $\state_{t} - \state_{t-1} \in \R^{\StateDim}$             |
| Mode indicator variable | $\modeVar$ | $\{1,\ldots,\ModeInd\}$                                    |
|                         |            |                                                            |

#+CAPTION: Variables at single data points
| Name                    | Symbol           | Equation                                                          |
|-------------------------+------------------+-------------------------------------------------------------------|
| State                   | $\singleState$   | $\R^{\StateDim}$                                                  |
| Control                 | $\singleControl$ | $\R^{\ControlDim}$                                                |
| State-Action input      | $\singleInput$   | $(\singleState, \singleControl) \in \R^{\StateDim + \ControlDim}$ |
| State Difference        | $\singleOutput$  | $\R^{\StateDim}$                                                  |
| Mode indicator variable | $\modeVarn$      | $\{1,\ldots,\ModeInd\}$                                           |

#+CAPTION: Variables at all data points
| Name                    | Symbol        | Equation                                                                      |
|-------------------------+---------------+-------------------------------------------------------------------------------|
| State                   | $\allState$   | $\R^{\NumData \times \StateDim}$                                              |
| Control                 | $\allControl$ | $\R^{\NumData \times \ControlDim}$                                            |
| State-Action input      | $\allInput$   | $(\allState, \allControl) \in \R^{\NumData \times (\StateDim + \ControlDim)}$ |
| State Difference        | $\allOutput$  | $\R^{\NumData \times \StateDim}$                                              |
| Mode indicator variable | $\ModeVarK$   | $\{\singleData{\modeVar}=k\}_{\numData=1}^{\NumData}$                         |

#+CAPTION: Gating network notation
|                | Name                                   | Symbol        | Equation                                                                         |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| Function       | Gating function k                      | $\hk$         | $\hk : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R$                    |
|                | Gating function                        | $\gatingFunc$ | $\gatingFunc : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R^{\ModeInd}$ |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| $\singleInput$ | Gating function k at $\singleInput$    | $\hkn$        | $\hk(\singleInput) \in \R$                                                       |
|                | Gating function at $\singleInput$      | $\hn$         | $\gatingFunc(\singleInput) \in \R^{\ModeInd}$                                    |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| $\allInput$    | Gating function k                      | $\Hk$         | $\hk(\allInput) \in \R^{\NumData}$                                               |
|                | Gating functions                       | $\Hall$       | $\gatingFunc(\allInput) \in \R^{\NumData \times \ModeInd}$                       |
|----------------+----------------------------------------+---------------+----------------------------------------------------------------------------------|
| $\zH$          | Inducing variables - gating function k | $\uHk$        | $\hk(\zHk) \in \R^{\NumInd}$                                                     |
|                | Inducing variables - gating functions | $\uH$         | $\h(\zH) \in \R^{\NumInd \times \ModeInd}$                                       |


#+CAPTION: Transition dynamics function notation
|                | Name                                    | Symbol  | Equation                                                                                 |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Dimension d of mode k                   | $\fkd$  | $\fkd : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R$                           |
| Function       | Mode k                                  | $\fk$   | $\fk : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R^{\StateDim}$                |
|                | All modes function                                          | $\f$    | $\f : \R^{\StateDim} \times \R^{\ControlDim} \rightarrow \R^{\ModeInd \times \StateDim}$ |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Dimension d mode k                      | $\fknd$ | $\fkd(\singleInput) \in \R$                                                              |
| $\singleInput$ | Mode k                                  | $\fkn$  | $\fk(\singleInput) \in \R^{\StateDim}$                                                   |
|                | All modes                               | $\fn$   | $\f(\singleInput) \in \R^{\ModeInd \times \StateDim}$                                    |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Dimension d mode k                      | $\Fkd$  | $\fkd(\allInput) \in \R^{\NumData}$                                                      |
| $\allInput$    | Mode k                                  | $\Fk$   | $\fk(\allInput) \in \R^{\NumData \times \StateDim}$                                      |
|                | All modes                               | $\F$    | $\f(\allInput) \in \R^{\NumData \times \ModeInd \times \StateDim}$                       |
|----------------+-----------------------------------------+---------+------------------------------------------------------------------------------------------|
|                | Inducing variables - dimension d mode k | $\uFkd$ | $\fkd(\zFkd) \in \R^{\NumInd}$                                                           |
| $\zF$          | Inducing variables - mode k             | $\uFk$  | $\fk(\zFk) \in \R^{\NumInd \times \StateDim}$                                            |
|                | Inducing variables - all modes          | $\uF$   | $\f(\zF) \in \R^{\NumInd \times \ModeInd \times \StateDim}$                              |
** Experts
GP prior over each output dimension $d$ for each dynamics mode $k$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior-single-dim}
p\left(\Fkd \mid \allInput \right) &= \mathcal{N}\left( \Fkd \mid \singleDimMode{\mu}(\allInput), \singleDimMode{k}(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
Assume each output dimension is independent,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior}
\pFk &= \prod_{\stateDim=1}^{\StateDim} \pFkd
\end{align}
#+END_EXPORT
Assume each dynamics mode $k$ is independent,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-prior}
\pF &= \prod_{k=1}^{K} \pFk
\end{align}
#+END_EXPORT
The process noise in each mode is modelled as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-likelihood}
\pYkGivenFk = \prod_{\numData=1}^{\NumData} \pykGivenfk &= \prod_{\numData=1}^{\NumData} \mathcal{N}\left( \singleOutput \mid \fkn, \text{diag}\left[ \left(\noiseVarOneK\right)^{2}, \ldots, \left( \noiseVarDK \right)^{2} \right]  \right)
\end{align}
#+END_EXPORT
where $\noiseVardK$ represents the noise variance associated with the $d$^{\text{th}} dimension of the $k$^{\text{th}} mode.

Each expert is then given by marginalising its associated latent function values,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert}
\pYkGivenX = \int  \pYkGivenFk \pFk \text{d} \Fk
\end{align}
#+END_EXPORT

The dynamics modes are combined via a distribution over the mode indicator variable $\modeVar$.
The resulting marginal likelihood is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-prior}
\pYGivenX = \sum_{\modeInd=1}^{\ModeInd} \Pr(\ModeVarK) \pYkGivenX
\end{align}
#+END_EXPORT

** Mixture of Experts
Mixture model marginal likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixture-marginal-likelihood}
\pYGivenX = \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \Pr(\modeVarK) p(\singleOutput \mid \modeVarn=\modeInd, \singleInput)
\end{align}
#+END_EXPORT
Mixture of experts marginal likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-moe-marginal-likelihood}
\pYGivenX &= \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \PraGivenx \pykGivenx
\end{align}
#+END_EXPORT

** Gating Network
This work is interested in transition dynamics where the governing mode varies over the input domain

This work specifies a probability mass function over the mode indicator variable that is governed by a set of input-dependent
latent functions. These model how the transition dynamics switch between modes over the input domain.
In the literature they are commonly referred to as gating functions.
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mode-indicator-dist}
\PaGivenhx = \prod_{\modeInd=1}^{\ModeInd} \PraGivenhx^{[\modeVarn = \modeInd]},
\end{align}
#+END_EXPORT
The probabilities $\Pr(\modeVarn=\modeInd \mid \hn )$ are obtained by normalising the outputs of all the gating functions, e.g.
$\text{softmax}(\hn)$.
Following a Bayesian formulation independent GP priors are placed on each of the gating functions,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-funcs-prior}
\pHGivenX = \prod_{\modeInd=1}^{\ModeInd} \pHkGivenX
= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \Hk \mid \mode{\mu}(\allInput), \mode{k}(\allInput, \allInput) \right).
\end{align}
#+END_EXPORT
Each GP models the epistemic uncertainty associated with its gating function.
The probabilities $\PraGivenx$ associated with the probability mass function over
the mode indicator variable are then obtained by marginalising the
latent gating functions,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-indicator-mult}
\PraGivenxNegH
&= \int \text{softmax}_k(\hn) p(\hn \mid \singleInput, \neg\Hall) \text{d} \mathbf{h}_t.
\end{align}
#+END_EXPORT
This equation integrates out the uncertainty associated with the gating functions.
High variance in the gating function GPs tends the distribution over the mode indicator variable
to a uniform distribution.

** Marginal Likelihood
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-expert}
\pykGivenxNegF = \pyk
\end{align*}
#+END_EXPORT

Our marginal likelihood can be written with the same factorisation as the \acrshort{moe}
marginal likelihood in Equation ref:eq-moe-marginal-likelihood,

#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood}
\pYGivenX &= \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \underset{\text{Mixing probability}}{\PraGivenxNegH} \underset{\text{Dynamics mode } k}{\pykGivenxNegF}
\end{align}
#+END_EXPORT
The
$\PraGivenxNegH$ contains $\ModeInd$ GP conditionals with complexity

$\pykGivenxNegF$ contains a GP conditional with complexity

** Inference

* Introduction
** Intro :ignore:
# *Dynamical Systems*
# The modern world is pervaded with dynamical systems that we seek to control to achieve a desired behaviour.
# Examples include autonomous vehicles, aircraft, robotic manipulators, financial markets and energy management systems.
# In the last decade, learning-based control citep:hewingLearningBased2020,sutton2018reinforcement has become
# a popular paradigm for controlling dynamical systems.
# This can be accounted to significant improvements in sensing and computational capabilities as well as
# recent successes in machine learning.

# with the /hope/ of addressing some of the shortcomings associated with pure control theoretic approaches?
# Perhaps with the /hope/ of addressing some of the shortcomings associated with pure control theoretic approaches.
# This can be accounted to recent successes in machine learning and the hope of
# and significant improvements in sensing and computational capabilities.

# Modern artificial intelligence seeks solutions that allow machines to /understand/ and /learn/.
# In the field of machine learning, these challenges are often solved using probabilistic models.
# In this setting, /understanding/ is achieved by separating signal from noise and removing redundancies in
# complex and noisy data.
# Leveraging learned models for control requires the models to extrapolate beyond training observations.
# That is, they must enable the machine to /reason/ about previously unseen inputs.
# In the field of machine learning, /understanding/ is achieved by separating signal from noise and removing redundancies in
# complex and noisy data.
# Leveraging learned models for control requires the models to extrapolate beyond training observations.
# That is, they must be able to /reason/ about previously unseen inputs.



# 1. Learning the system dynamics: model based control strategies rely on suitable and sufficiently accurate model
#    representations of the system dynamics. A promising approach is to learn /unknown/, or /partially unknown/
#    dynamics from observations. This enables control in previously uncontrollable systems, and can improve control
#    by learning (and accounting) for any model errors.
# 2. Learning the controller design:

# (coming from both the reinforcement learning
# cite:sutton2018reinforcement and control theory \todo{cite control theory book?} communities),



The modern world is pervaded with dynamical systems that we seek to control to achieve a desired behaviour.
Examples include autonomous vehicles, aircraft, robotic manipulators, financial markets and energy management systems.
In the last decade, reinforcement learning, and learning-based
control in general, have become
popular paradigms for controlling dynamical systems citep:hewingLearningBased2020,sutton2018reinforcement.
This can be accounted to significant improvements in sensing and computational capabilities, as well as
recent successes in machine learning.
# From robotics, to industrial processing, to finance, learning-based control
# offers promise of solving problems that could not previously be solved with purely control
# theoretic approaches.

This growing interest in learning-based control
\parmarginnote{real-world systems}
has emphasised the importance for real-world considerations.
Real world systems are often highly /nonlinear/, exhibit /stochasticity/ and /multimodalities/,
are expensive to run (energy intensive, subject to wear and tear) and
must be controlled subject to /constraints/ (for safety, efficiency, and so on).
In contrast to simulation, the control of physical systems also has real-world consequences:
components may get damaged, the system may damage its environment, or the system may catastrophically fail.
As such, any learning-based control strategy deployed in the real-world, should handle both the uncertainty inherent
to the environment, and the uncertainty introduced by learning from observations.
# As such, any control strategy deployed in the real-world should ensure the safety of itself and
# its surrounding environment.
# Therefore, any control strategy deployed in the real-world should ensure the safety of itself and
# its surrounding environment.

Many dynamical systems exhibit multimodalities (in their transition dynamics), where some dynamics modes
\parmarginnote{multimodal systems}
are believed to be /unoperatable/ or /undesirable/.
These multimodalities may be due to spatially varying model parameters, for example,
process noise terms modelling aircraft turbulence, or friction coefficients modelling
surface-tyre interactions for ground vehicles over different terrain.
In these systems, it is desirable to avoid entering specific dynamics mode that are believed to be /unoperatable/.
Perhaps they are hard to control due to stability, or the mode switch itself is hard to control.
Or perhaps it is desirable to avoid specific modes for /efficiency/ or /performance/ reasons.
Given these motivations, this dissertation is interested in control techniques for
driving dynamical systems from an initial state,
to a target state, whilst avoiding specific dynamics modes.

# In particular, it is primarily interested in controlling a quadrotor in an environment
# subject to two dynamics modes: 1) a turbulent
# dynamics mode in front of a fan, and 2) a non turbulent dynamics mode everywhere else.
# The objective in this environment is to control the quadrotor, whilst navigating to a target location,
# and remaining in the operatable (non turbulent) dynamics mode.

Model-based control comprises a powerful set of techniques for finding controls of /constrained/ dynamical
\parmarginnote{model-based control}
systems, given a transition dynamics model describing the evolution of the controlled system.
It is commonly used for controlling aircraft, robotic manipulators, and walking
robots cite:vonstrykDirect1992,bettsSurvey1998,gargUnified2010.
One caveat is that it requires a relatively accurate mathematical model of the system.
Traditionally, these mathematical models are built using first principles based on physics.
However, accurately modelling the underlying transition dynamics can be challenging and
lead to the introduction of model errors.
For example, incorrectly specifying model parameters, or
models themselves (modelling a nonlinear system to be linear, or a multimodal system to be unimodal).
Incorrectly specifying these model parameters (and their associated uncertainty)
can have a detrimental impact on controller performance,
and is an active area of research in the robust
and stochastic optimal control communities cite:freemanRobust1996,stengelStochastic1986.
# For example, model parameters might be,
# 1) hard to specify accurately, e.g. friction coefficients associated with surfaces,
# 2) assumed constant when in reality they vary spatially, e.g. process noise terms modelling the effect of
#    turbulence on aircraft,
# 3) assumed constant when in reality they vary with time, e.g. mass reducing due to fuel consumption.

The difficulties associated with constructing mathematical representations of dynamical systems
\parmarginnote{learning dynamics models}
can be overcome by learning from observations cite:ljungSystem1999.
Learning dynamics models has the added benefit that it
alleviates the dependence on domain experts for specifying accurate models, in turn making it easier to
deploy more general techniques.
However, learning dynamics models for control introduces other difficulties.
For example, it is important to know
where the model cannot predict confidently due to a lack of training observations.
This concept is known as /epistemic uncertainty/ and is reduced in the limit of infinite data.
Correctly quantifying uncertainty is crucial for intelligent decision-making.

In a *risk-averse setting*, it is desirable for control strategies
to avoid entering regions of a learned dynamics model with high /epistemic uncertainty/.
\parmarginnote{decision-making under uncertainty}
This is because the trajectory cannot be predicted confidently, and thus, if any
/constraints/ have been violated, i.e.
if the trajectory will avoid the undesired dynamics mode.
Conversely, in an *explorative setting*,
if the /epistemic uncertainty/ has been quantified, then it can be used to guide exploration into
regions of the dynamics that have not previously been observed.
This experience can then be used to update the model and reduce its associated /epistemic uncertainty/.

These two settings are the main concepts explored in this dissertation.
If the dynamics are not fully /known a priori/, an agent will not be able to confidently plan a
risk-averse trajectory to the target state.
How can the agent explore its environment, in turn reducing the /epistemic uncertainty/
associated with its dynamics model?
It is assumed that complete knowledge of the transition dynamics and how they
switch between modes is /not known a priori/.
Therefore, it is interested in jointly inferring the mode /constraints/ alongside the underlying dynamics modes,
through repeated interactions with the system.
Once the agent has explored enough, how can the learned model be exploited to plan risk-averse trajectories
that attempt to remain in a desired dynamics mode, and in regions of the learned
dynamics with low /epistemic uncertainty/?

More generally, this dissertation is interested in learning and control in multimodal dynamical systems.
In Chapter ref:chap-dynamics, it starts by formulating learning as approximate Bayesian inference in
a probabilistic representation of the transition dynamics.
Assuming access to a historical data set of state transitions from the entire domain, Chapter ref:chap-traj-opt
develops two risk-averse trajectory optimisation algorithms.
Given the dynamics model from Chapter ref:chap-dynamics, trained on these observations,
these algorithms are capable of finding trajectories that remain in the desired dynamics mode, whilst avoiding
regions of the learned dynamics with high /epistemic uncertainty/.
Finally, Chapter ref:chap-active-learning addresses the question of active learning, i.e. incrementally
learning the dynamics model through repeated interactions with the environment.

# It assumes that complete knowledge of the transition dynamics and how they
# switch between modes is /not known a priori/.
# In essence, the system /constraints/ on the mode are /not known a priori/ and need to be inferred from observations.
# Therefore, it is interested in jointly inferring the /constraints/ alongside the underlying dynamics modes,
# through repeated interactions with the system.

# certain enough in its dynamics, it will be able to plan a risk-averse trajectory
# Once the agent is certain enough in its dynamics, it will be able to plan a risk-averse trajectory
# to the target state.

# Collectively, these two settings are known as the exploration-exploitation trade-off,
# which is well-known in the reinforcement learning and optimal control communities.


# As a result, it is interested in learning dynamics models incrementally, without violating some notion
# of constraint on the dynamics modes.



# This dissertation is interested in learning dynamics models for multimodal dynamical systems with /unknown/, or
# /partially unknown/, transition dynamics, where safety is governed by the underlying dynamics modes.
# It is assumed that complete knowledge of how the transition dynamics switch between modes is also /not known a priori/.
# In essence, the safety constraints are /not known a priori/ and should be learned from observations.
# Therefore, this dissertation is interested in jointly inferring the /constraints/ alongside the underlying dynamics modes,
# through repeated interactions with the system.


# Safe control in systems where the environment is /known a priori/
# has been well studied by the control and formal methods communities.
# However, safe control when the environment is /not known a priori/ has been less well studied.
# Recent work by cite:berkenkampSafe2019 attempts to address safe learning-based control in the
# reinforcement learning setting, where the environment is /not known a priori/.
# In cite:schreiterSafe2015 the authors address active learning in Gaussian process dynamics models, by
# deploying a binary GP classification model that indicates safe and unsafe regions.
# # Although this dissertation is not directly interested in safe control,
# # their work on constrained exploration is particularly relevant.


# It is important to note that the definition of safety varies,
# and should be defined on a system by system basis;
# cite:berkenkampSafe2019 define safety in terms of closed-loop
# stability but it is also common to define safety in terms of constraint satisfaction (on the states and controls).
# They define safety in terms of stability and exploit Lyapanov functions to construct a safe RL framework.
# Other approaches


# This dissertation is motivated by controlling dynamical systems that exhibit multimodalities, which are
# either unknown, or partially unknown.
# Incorrectly specifying a multimodal parameter as unimodal, or incorrectly specifying how a multimodal
# parameter switches between modes will have a detrimental impact on controller performance.
# In some cases, it may even lead to catastrophic failure.

** Contributions
This dissertation explores mode constrained control in multimodal dynamical systems that explicitly reasons
about uncertainties during learning and control.
The risk-averse trajectory optimisation algorithms /know what they do not know/, and only evaluate regions that
they are confident in.
Conversely, the exploratory trajectory optimisation algorithm exploits this information to guide
the system into regions that it has not previously observed.
The primary contributions of this dissertation are as follows:
- Chapter ref:chap-dynamics: details an approach to learning the underlying dynamics modes (and
  how they're separated) in multimodal dynamical systems.
  The approach formulates a probabilistic representation of the transition dynamics resembling a Mixture of
  Gaussian Process Experts model. It then performs approximate Bayesian inference via a novel variational lower
  bound that principally handles uncertainty and provides scalability via stochastic gradient methods.
  The method is tested on a real-world quadcopter data set and two data sets obtained from simulated environments ...
- Chapter ref:chap-traj-opt: introduces two trajectory optimisation techniques that find trajectories that attempt
  to remain in a desired dynamics mode, and in regions of the learned dynamics that have been observed, so can be
  predicted confidently. The first approach (published in cite:scannellTrajectory2021)
  exploits the latent /geometry/ and well-calibrated /epistemic uncertainty/ estimates
  inferred by the probabilistic model from Chapter ref:chap-dynamics.
  The second approach formulates trajectory optimisation as probabilistic inference in a graphical model,
  and achieves the desired behaviour by conditioning on a mode indicator variable.
- Chapter ref:chap-active-learning:

** Associated Publications
# The probabilistic model and variational inference scheme presented in Chapter ref:chap-dynamics is used to learn
# the transition dynamics of a DJI Tello quadcopter in cite:scannellTrajectory2021.
The first trajectory optimisation algorithm presented in Chapter ref:chap-traj-opt, as well as the approach
to learning multimodal dynamical systems in Chapter ref:chap-dynamics are published in:

#+BEGIN_EXPORT latex
{\color{BrickRed}\fullcite{scannellTrajectory2021}}
#+END_EXPORT

** Illustrative Example label:illustrative_example
The methods developed throughout this dissertation are evaluated on an illustrative 2D quadcotper example.
The example considers a quadcopter operating in an environment subject to spatially varying turbulence (induced by a fan).
It is hard to know the exact transition dynamics due to complex and uncertain
interactions between the quadcopter and the fan.
The system's transition dynamics can be represented by two dynamics modes,
- Mode 1 :: a /turbulent/ mode in front of the fan,
- Mode 2 :: a /non-turbulent/ mode everywhere else.
However, the underlying dynamics modes, and how the dynamics switches between them, are \textit{not fully known a priori}.
Fig. ref:fig-problem-statement shows a graphical representation of this environment.
#+BEGIN_EXPORT latex
\begin{figure}[!h]
\centering
  %\includegraphics[width=0.6\columnwidth]{images/quadcopter_bimodal_domain.pdf}
  \includegraphics[width=0.9\columnwidth]{images/quadcopter-domain-ppt.png}
  \caption{
  Diagram showing a top down view of the environment for the 2D quadcopter example.
  The quadcopter is subject to two dynamics modes: 1) a \textit{turbulent}
  dynamics mode induced by a fan (green), and, 2) a \texit{non-turbulent} dynamics mode everywhere else (blue).}
%The goal is learn a factorised representation of the underlying dynamics modes to find trajectories between $\mathbf{x}_0$ and $\mathbf{x}_f$ that either prioritise
%remaining in the non-turbulent mode or prioritise avoiding regions of the learned dynamics
%with high epistemic uncertainty due to lack of training observations.}
\label{fig-problem-statement}
\end{figure}
#+END_EXPORT

This work first considers having prior access to the environment, where it can perform prior flight
trials and observe state transitions over the domain, i.e. collect a data set of state transitions $\mathcal{D}$.
Given such a data set, Chapter ref:chap-dynamics investigates learning a factorised representation of the transition
dynamics.
# In particular, it represents the multimodal transition dynamics as a probabilistic model, and formulates
# the learning problem as performing (approximate) Bayesian inference in the model (given the data set $\mathcal{D}$).

Secondly, this dissertation considers the problem of flying the quadcopter from an initial state $\state_0$,
to a target state $\state_{f}$, whilst attempting to avoid the /turbulent/ dynamics mode.
Given that the underlying dynamics modes, and how the dynamics switches between them, are \textit{not fully known a priori},
these control techniques should /air on the side of caution/.
That is, if the learned dynamics model cannot confidently predict which mode governs the dynamics at a particular region
(because it has not been observed), then the control techniques should find trajectories that avoid entering these regions,
as well as avoiding the /turbulent/ dynamics mode.
Chapter ref:chap-traj-opt investigates trajectory optimisation algorithms that
exploit the probabilistic model from Chapter ref:chap-dynamics (after performing Bayesian inference
with the historical data set $\mathcal{D}$), in order to achieve these goals.

# This work develops model-based control techniques (trajectory optimisation algorithms) that
# exploit the probabilistic model from Chapter ref:chap-dynamics (after performing Bayesian inference
# with the historical data set $\mathcal{D}$).

Finally, this dissertation considers the more realistic scenario of not having prior access to the environment.
As such, the quadcopter does not have access to a historical data set to train the model from Chapter ref:chap-dynamics with.
Instead, it must actively explore its environment to collect data, whilst simultaneously attempting to avoid the /turbulent/
dynamics mode.


# It is motivated by driving a dynamical system from an initial state (in a desired dynamics mode),
# to a target state, whilst remaining in the desired dynamics mode.
# For example, consider controlling a quadrotor in an environment subject to two dynamics modes: 1) a turbulent
# dynamics mode in front of a fan, and 2) a non turbulent dyanmics mode everywhere else.
# The goal in this scenario is to fly the quadrotor to a target location,
# whilst remaining in the operatable (non turbulent) dynamics mode,
# with the added difficulty that the dynamics modes,
# and how the dyanmics switches between the modes, are \textit{not fully known a priori}.


# This dissertation is interested in controlling a
# DJI Tello quadcopter with partially unknown dynamics,
# in an environment
# subject to spatially varying turbulence induced by a fan at the side of the room, shown
# in Fig. ref:fig-problem-statement.
# It is hard to know the exact transition dynamics due to complex and uncertain
# interactions between the quadcopter and the fan.
# The system's transition dynamics can be represented by two dynamics modes,
# 1) a turbulent mode in front of the fan, and,
# 2) a non-turbulent mode everywhere else.
# When planning a trajectory from an intial state $\mathbf{x}_0$ in the desired mode (mode 2),
# to a target state $\mathbf{x}_f$,
# it is preferred to avoid entering the turbulent mode (mode 1), as it
# results in poor performance and sometimes even system failure.

# *Motivation* For example, consider controlling an autonomous vehicle to navigate
# to a desired location in an environment subject to different road surfaces (asphalt, loose gravel, grass)
# operating in an
# environment subject to spatially friction coefficients

# *Motivation* For example, consider controlling an autonomous air vehicle operating in an
# environment subject to regions of high turbulence, i.e. spatially varying  process noise.
# Process noise and observation noise are the constituent sources of aleatoric uncertainty; uncertainties that are inherent in a system and cannot be reduced.
# They pose a significant issue for controlling such systems as we cannot model turbulence and thus account
# for it in control algorithms.

** Introduction traj opt paper :noexport:
Many physical systems operate under switching dynamics modes due to
changing environmental or internal conditions.
Examples include: robotic grasping where objects with different
properties have to be manipulated, robotic locomotion in environments with varying surface types
and the control of aircraft in environments subject to different levels of turbulence.
When controlling these systems, it may be preferred to find trajectories that remain
in a single dynamics mode.
This paper is interested in controlling a DJI Tello quadcopter in an environment
with spatially varying turbulence induced by a fan at the side of the room, shown
in Fig. ref:fig-problem-statement.
It is hard to know the exact transition dynamics due to complex and uncertain
interactions between the quadcopter and the fan.
The system's transition dynamics resemble a mixture of two modes: a turbulent mode in front of
the fan and a non-turbulent mode everywhere else.
When planning a trajectory from start state $\mathbf{x}_0$ to desired state $\mathbf{x}_f$
it is preferred to avoid entering the turbulent mode, as it
results in poor performance and sometimes even system failure.

#+BEGIN_EXPORT latex
\begin{figure}[!t]
\centering
  %\includegraphics[width=0.9\columnwidth]{images/quadcopter_bimodal_domain.pdf}
  \includegraphics[width=0.9\columnwidth]{images/quadcopter-domain-ppt.png}
  \caption{
This work seeks to velocity control a DJI Tello quadcopter in an indoor environment
subject to two modes of operation characterised by process noise (turbulence).
A high turbulence mode is induced by placing a desktop fan at the right side of the room.
Data from four trajectories following a single 2D $\mathbf{x}=(x,y)$ target trajectory captures the variability
(process noise) in the dynamics.
Our goal is to find trajectories between $\mathbf{x}_0$ and $\mathbf{x}_f$ that either prioritise
remaining in the non-turbulent mode or prioritise avoiding regions of the learned dynamics
with high epistemic uncertainty due to lack of training observations.}
\label{fig-problem-statement}
\end{figure}
#+END_EXPORT

Trajectory optimisation comprises a powerful set of techniques for finding open-loop controls of dynamical
systems such that an objective function is minimised whilst satisfying a set of
constraints.
It is commonly used for controlling aircraft, robotic manipulators, and walking
robots cite:VonStryk1992,Betts1998,Garg2010.
One caveat to trajectory optimisation is that it requires a relatively accurate mathematical model of
the system.
Traditionally, these mathematical models are built using first principles based on physics.
However, accurately modelling the underlying transition dynamics can be challenging and
lead to the introduction of model errors.
For example, both observation and process noise
are inherent in many real-world systems and can be hard to model
due to both spatial and temporal variations.
Incorrectly accounting for this uncertainty can have a detrimental impact on controller performance
and is an active area of research in the robust
and stochastic optimal control communities cite:FreemanRandyA.2009,Stengel1988.

The difficulties associated with constructing mathematical models can be overcome by learning from
observations cite:Ljung1997.
However, learning dynamics models for control introduces other difficulties.
For example, it is important to know
where the model cannot predict confidently due to a lack of training observations.
This concept is known as epistemic uncertainty and is reduced in the limit of infinite data.
Probabilistic models have been used to account for epistemic uncertainty and also
provide a principled approach to modelling stochasticity i.e. aleatoric uncertainty
cite:Schneider1996,Deisenroth2011.
For example, cite:Cutler,Deisenroth2011,Pan2014 use Gaussian processes (GPs) to learn
transition dynamics.
GPs lend themselves to data-efficient learning through the selection of informative priors, and
when used in a Bayesian setting offer well calibrated uncertainty estimates.
Methods for learning probabilistic multimodal transition dynamics have also been proposed:
cite:Mckinnon used a mixture of GP experts method,
cite:Moerland studied the used of deep generative models and
cite:Kaiser2020a proposed a Bayesian model that learns independent
dynamics modes whilst maintaining a
probabilistic belief over which mode is responsible for predicting at a given input location.

There has also been work developing control algorithms exploiting learned multimodal transition dynamics
cite:Herzallah2020.
However, our work differs as it seeks to find trajectories that
remain in a single dynamics mode
whilst avoiding regions of the transition dynamics that cannot be predicted confidently.
To the best of our knowledge, there is no previous work addressing such trajectory optimisation
in transition dynamics models.

Probabilistic modelling and Bayesian inference are a promising
avenue for learning dynamics models to be used for controlling real world systems.
\parmarginnote{probabilistic modelling}
The Bayesian framework provides a principled approach to modelling both the
/epistemic uncertainty/ associated with the model,
and the /aleatoric uncertainty/ inherent to the system (e.g. process noise).
For example, cite:deisenrothPILCO2011,cutlerEfficient2015,panProbabilistic2014
use Gaussian processes (GPs) to learn
transition dynamics from observations.
GPs lend themselves to data-efficient learning through the selection of informative priors, and
when used in a Bayesian setting offer well calibrated uncertainty estimates.
Methods for learning probabilistic multimodal transition dynamics have also been proposed:
cite:mckinnonLearning2017 use a Mixture of GP Experts method,
cite:moerlandLearning2017 study the use of deep generative models and
cite:kaiserBayesian2020 propose a Bayesian model that learns independent
dynamics modes whilst maintaining a
probabilistic belief over which mode is responsible for predicting at a given input location.

* Literature Review
# * Background and Related Work
** maths :ignore:
#+begin_export latex
\newcommand{\gpDomain}{\ensuremath{\hat{\stateDomain}}}
\newcommand{\inputDomain}{\ensuremath{\hat{\stateDomain}}}
\newcommand{\outputDomain}{\ensuremath{\stateDomain}}
#+end_export
** intro :ignore:
This chapter provides an overview of learning-based control and details the relevant background information
for the remainder of the dissertation.
** Dynamical Systems :noexport:
*** intro :ignore:
Dynamical systems describe the behaviour of a system over time $t$ and
are a key component of both control theory and reinforcement learning.
At any given time a dynamical system has a state, which is
represented as a vector of real numbers $\mathbf{x}(t) \in \R^D$.
For example, the state of a 2-dimensional robotic system with state comprising of 2D Cartesian coordinates
and an orientation would be represented as $\mathbf{x}(t) = [x(t), y(t), \theta(t)]^{T}$.
The system can be controlled by applying control actions $\mathbf{u}(t) \in \R^F$ at any given time $t$.
This work considers continuous-time, continuous-state, nonlinear stochastic dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-unimodal-dynamics-cont}
\stateDerivative(t) &= f(\state(t), \control(t)) + \epsilon(t) \quad \forall t
\end{align}
#+END_EXPORT
where $f : \R^D \times \R^{F} \rightarrow \R^{D}$ represents the transition dynamics function and $\epsilon(t)$
is an i.i.d process noise term with $\E[\epsilon(t)] = 0$.
The process noise term accounts for unwanted (and, in general, unknown) disturbances of the system.
For example, it is extremely hard to model aerodynamic effects on aircraft so these could be accounted
for in the process noise term.
Throughout this thesis it is assumed that the state $\mathbf{x}$ is observed directly and is not subject to
observation noise.
This is a standard assumption in the \acrfull{mdp} framework which is commonly adopted
in the reinforcement learning literature.
\todo[inline]{Slightly weird referring to MDP literature on continuous time setting}

\todo[inline]{Should I make a bigger point out of the observation noise assumption}

The system is controlled via a policy $\mathbf{u}(t) = \pi(\mathbf{x}(t), t)$, which given the state $\mathbf{x}(t)$
and time step $t$ decides which control action $\mathbf{u}(t)$ to apply to the system.
The policy can be time-dependent and can also depend on all past information up to time step $t$.
In the time-independent case the policy is denoted $\pi(\mathbf{x}(t))$ and
the resulting closed-loop system is denoted $f_{\pi}(\mathbf{x}) = f(\mathbf{x},\pi(\mathbf{x}))$.

** Dynamical Systems and Uncertainty Quantification :ignore:
** Dynamical Systems :noexport:
*** maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{\MakeUppercase{\timeInd}}}

\newcommand{\stateDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\controlDomain}{\ensuremath{\mathcal{U}}}

\newcommand{\dynamicsFunc}{\ensuremath{f}}
#+END_EXPORT

*** intro :ignore:
Dynamical systems describe the behaviour of a system over time, $t$, and
are a key component of both control theory and reinforcement learning.
At any given time, $t$, a dynamical system has a state, which is
represented as a vector of real numbers $\state(t) \in \stateDomain \subseteq \R^D$.
For example, the state of a 2-dimensional robotic system with state comprising of 2D Cartesian coordinates
and an orientation would be represented as $\state(t) = [x(t), y(t), \theta(t)]^{T}$.
The system can be controlled by applying control actions $\mathbf{u}(t) \in \controlDomain \subseteq \R^F$ at any given
time, $t$.
This work considers continuous-time, continuous-state, nonlinear stochastic dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-unimodal-dynamics-cont}
\stateDerivative(t) &= \dynamicsFunc(\state(t), \control(t)) + \epsilon(t) \quad \forall t
\end{align}
#+END_EXPORT
where $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ represents the transition dynamics function and $\epsilon(t)$
is an i.i.d process noise term with $\E[\epsilon(t)] = 0$.
The process noise term accounts for unwanted (and, in general, unknown) disturbances of the system.
For example, it is extremely hard to model aerodynamic effects on aircraft so these could be accounted
for in the process noise term.
Throughout this dissertation it is assumed that the state $\state$ is observed directly and is not subject to
observation noise.
This is a standard assumption in the \acrfull{mdp} framework which is commonly adopted
in the reinforcement learning literature.
\todo[inline]{Slightly weird referring to MDP literature on continuous time setting}

\todo[inline]{Should I make a bigger point out of the observation noise assumption}

The system is controlled via a policy $\control(t) = \pi(\state(t), t)$, which given the state $\state(t)$
and time step $t$ decides which control action $\control(t)$ to apply to the system.
The policy can be time-dependent and can also depend on all past information up to time step $t$.
In the time-independent case the policy is denoted $\pi(\state(t))$ and
the resulting closed-loop system is denoted $f_{\pi}(\state) = \dynamicsFunc(\state,\pi(\state))$.
*** Sources of Uncertainty
\parmarginnote{epistemic uncertainty}
If we have used observations of a system to train a predictive model then we can only be confident in predictions near our observations.
As we extrapolate away from the observations we can no longer be certain and this is known as
epistemic uncertainty.
It can be reduced by collecting more data and retraining a model.
This is shown in Figure [[ref:epistemic]].
# #+NAME: epistemic
# #+ATTR_LATEX: :width 1.\textwidth :placement [h] :center nil
# #+caption: Plot demonstrating the concept of epistemic uncertainty. We can be confident in the learnt function close to our observations but as we exptrapolate away from them we become uncertain what the function should look like. If we have a notion of epistemic uncertainty in a MBRL algorithm we can use it to encourage the agent to visit these areas and collect data, which in turn will reduce the epistemic uncertainty.
# [[file:images/limited_data.pdf]]


\parmarginnote{aleatoric uncertainty}
As mentioned previously, aleatoric uncertainty consists of process noise and observation noise; uncertainties that are inherent in a system and cannot be reduced.
# #+NAME: bimodal-dataset
# #+ATTR_LATEX: :width 1.\textwidth :placement [h] :center nil
# #+caption: An artificial 1D dataset with two levels of process noise.
# [[file:images/dataset.pdf]]
Figure [[ref:bimodal-dataset]] shows an artificial 1D dataset that demonstrates the concept of aleatoric uncertainty.
In our work we generally assume that there is no observation noise and therefore the aleatoric uncertainty only consists of process noise.

** Learning in Dynamical Systems
*** maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{\MakeUppercase{\timeInd}}}

\newcommand{\stateDomain}{\ensuremath{{\mathcal{X}}}}
\newcommand{\controlDomain}{\ensuremath{{\mathcal{U}}}}

\newcommand{\dynamicsFunc}{\ensuremath{f}}

\renewcommand{\u}{\ensuremath{\mathbf{u}}}

\renewcommand{\allInput}{\ensuremath{\hat{\state}_{1:\TimeInd}}}
\renewcommand{\allOutput}{\ensuremath{{\Delta\state}_{1:\TimeInd}}}
#+END_EXPORT

*** intro :ignore:
Dynamical systems describe the behaviour of a system over time, $t$, and
are a key component of both control theory and reinforcement learning.
At any given time, $t$, a dynamical system has a state, which is
represented as a vector of real numbers $\state(t) \in \stateDomain \subseteq \R^D$.
For example, the state of a 2-dimensional robotic system with state comprising of 2D Cartesian coordinates
and an orientation would be represented as $\state(t) = [x(t), y(t), \theta(t)]^{T}$.
The system can be controlled by applying control actions $\mathbf{u}(t) \in \controlDomain \subseteq \R^F$ at any given
time, $t$.
This work considers continuous-time, continuous-state, nonlinear stochastic dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-unimodal-dynamics-cont}
\stateDerivative(t) &= \dynamicsFunc(\state(t), \control(t)) + \epsilon(t) \quad \forall t
\end{align}
#+END_EXPORT
where $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ represents the transition dynamics function and $\epsilon(t)$
is an i.i.d process noise term with $\E[\epsilon(t)] = 0$.
The process noise term accounts for unwanted (and, in general, unknown) disturbances of the system.
For example, it is extremely hard to model aerodynamic effects on aircraft so these could be accounted
for in the process noise term.
Throughout this dissertation it is assumed that the state $\state$ is observed directly and is not subject to
observation noise.
This is a standard assumption in the \acrfull{mdp} framework which is commonly adopted
in the reinforcement learning literature.
\todo[inline]{Slightly weird referring to MDP literature on continuous time setting}

\todo[inline]{Should I make a bigger point out of the observation noise assumption}

# The system is controlled via a policy $\control(t) = \pi(\state(t), t)$, which given the state $\state(t)$
# and time step $t$ decides which control action $\control(t)$ to apply to the system.
# The policy can be time-dependent and can also depend on all past information up to time step $t$.
# In the time-independent case the policy is denoted $\pi(\state(t))$ and
# the resulting closed-loop system is denoted $f_{\pi}(\state) = \dynamicsFunc(\state,\pi(\state))$.

*** Probabilistic Modelling
/Mathematical models/ are compact representations (sets of assumptions) that attempt to capture key features of the
phenomenon of interest, in a precise mathematical form.
Probabilistic modelling provides the capability of constructing /mathematical models/
that can represent and manipulate uncertainty in data, models, decisions and predictions.
As such, linking observed data to underlying phenomena through probabilistic models,
is an interesting direction for modelling, analysing and controlling dynamical systems.

Dynamical systems give rise to temporal observations arriving as a sequence
$\state_{1:\TimeInd} = \{\state_1, \ldots, \state_\TimeInd\}$.
\parmarginnote{measurement noise}
These measurements are often corrupted by (observation) noise due to imperfections in the measurement process.
Even when it is known that there is uncertainty in the measurement process, there still remains uncertainty about its
form.

Given our current understanding of the real-world, many dynamical systems also appear to be
\parmarginnote{process noise}
inherently stochastic.
This is due to our inability to accurately model certain phenomena (e.g. turbulence).
Stochasticity arising from state transitions within a system is known as process noise.
Observation and process noise are the constituent sources of /aleatoric uncertainty/;
uncertainties that are inherent in a system and cannot be reduced.

The structure of models can also be uncertain and there may be unobserved (aka latent) variables present.
The introduction of latent variables in probabilistic models is one of the key components providing
them with interesting and powerful capabilities.
A further form of uncertainty in models arises from unknown model parameters, $\theta$.
\parmarginnote{epistemic uncertainty}
It is worth considering the implications of all these types of uncertainty.
If a predictive dynamics model is learned from observations, then the model
can only be confident predicting near the training observations.
As one extrapolates away from the training observations, one can no longer be certain in the prediction;
this is known as epistemic uncertainty and can be reduced by collecting more data and retraining a model.

Probability theory enables this uncertainty to be represented and manipulated;
It provide a systematic way to combine observations with existing knowledge,
via a /mathematical model/.



# If we have used observations of a system to train a predictive model then we can only be confident in predictions near our observations.
# As we extrapolate away from the observations we can no longer be certain and this is known as
# epistemic uncertainty.
# It can be reduced by collecting more data and retraining a model.
# This is shown in Figure [[ref:epistemic]].
# #+NAME: epistemic
# #+ATTR_LATEX: :width 1.\textwidth :placement [h] :center nil
# #+caption: Plot demonstrating the concept of epistemic uncertainty. We can be confident in the learnt function close to our observations but as we exptrapolate away from them we become uncertain what the function should look like. If we have a notion of epistemic uncertainty in a MBRL algorithm we can use it to encourage the agent to visit these areas and collect data, which in turn will reduce the epistemic uncertainty.
# [[file:images/limited_data.pdf]]


# #+NAME: bimodal-dataset
# #+ATTR_LATEX: :width 1.\textwidth :placement [h] :center nil
# #+caption: An artificial 1D dataset with two levels of process noise.
# [[file:images/dataset.pdf]]
# Figure [[ref:bimodal-dataset]] shows an artificial 1D dataset that demonstrates the concept of aleatoric uncertainty.
# In our work we generally assume that there is no observation noise and therefore the aleatoric uncertainty only consists of process noise.

state-space models
dynamics models

*** Discrete-Time Dynamical Systems
When learning dynamical systems it is common that observations of the system
are sampled from the underlying system at a fixed time step, $\Delta \timeInd=t_*$.
The state and control observations at time, $t$, are denoted
$\state_t \in \stateDomain \subseteq \R^\StateDim$ and
$\control_t \in \controlDomain \subseteq \R^\ControlDim$ respectively.
The concatenation of the state and control domains is
denoted as $\inputDomain \coloneqq \stateDomain \times \controlDomain$ and
a single state-control input is denoted as $\singleInput = (\state_{t-1}, \control_{t-1})$.
A time series of observations from time $a$ to time $b$ (inclusive)
is denoted by $\mathbf{x}_{a:b}$ (analogously for other variables).
Given a data set of state transitions $\mathcal{D} = \{\allInput, \allOutput\}$,
it is natural to consider the discrete-time representation of the dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-unimodal-dynamics-disc}
\singleOutput = \dynamicsFunc (\singleState, &\singleControl ; \Delta t = t_*) + \epsilon_{t-1}
\end{align}
#+END_EXPORT

*** Gaussian Processes
This section introduces Gaussian processes and the sparse approximations that are used throughout this work.
Inference techniques with GPs leverage multivariate Gaussian conditioning operations.
Introducing multivariate Gaussians is a natural place to start.

# In particular, it walks through the lower bounds (used to perform variational inference) that are the building blocks
# of the inference in this thesis.

**** Multivariate Gaussian Identities
Gaussian distributions are very popular in machine learning and control theory. This is not only due to their
natural emergence in statistical scenarios (central limit theorem) but also their intuitiveness and
mathematical properties that render their manipulation tractable and easy.

Consider a multivariate Gaussian whose random variables are partitioned into two vectors $\f$ and $\u$.
The joint distribution takes the following form,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-joint-gaussian}
\left[\begin{array}{c}
      \f \\
      \u
\end{array}\right]
\sim\ &\mathcal{N}\left(
\left[\begin{array}{c}
      \bm\mu_{\f} \\
      \bm\mu_{\u}
 \end{array}\right]
\left[\begin{array}{cc}
      \bm\Sigma_{\f\f} & \bm\Sigma_{\f\u} \\
      \bm\Sigma_{\u\f} & \bm\Sigma_{\u\u}
 \end{array}\right]\right),
\end{align*}
#+END_EXPORT
where $\bm\mu_{\f}$ and $\bm\mu_{\u}$ represent the mean vectors, $\bm\Sigma_{\f\f}$ and $\bm\Sigma_{\u\u}$
represent the covariance matrices,
and $\bm\Sigma_{\u\f}$ and $\bm\Sigma_{\f\u}$ represent the cross-covariance matrices.
The marginalisation property of Gaussian distributions states that for two jointly Gaussian random variables,
the marginals are also Gaussian,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gaussian-marginal}
p(\f) &= \int p(\f, \u) \text{d}\u = \mathcal{N} \left(\f \mid \bm\mu_{\f}, \bm\Sigma_{\f\f} \right), \\
p(\u) &= \int p(\f, \u) \text{d}\f = \mathcal{N} \left(\u \mid \bm\mu_{\u}, \bm\Sigma_{\u\u} \right).
\end{align}
#+END_EXPORT
Conveniently, the conditional densities are also Gaussian,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gaussian-conditional}
\f \mid \u &\sim \mathcal{N} \left(\bm\mu_{\f} + \bm\Sigma_{\f\u} \bm\Sigma_{\u\u}^{-1}(\u - \bm\mu_{\u}), \bm\Sigma_{\f\f} - \bm\Sigma_{\f\u}\bm\Sigma_{\u\u}^{-1}\bm\Sigma_{\u\f} \right), \\
\u \mid \f &\sim \mathcal{N} \left(\bm\mu_{\u} + \bm\Sigma_{\u\f} \bm\Sigma_{\f\f}^{-1}(\f - \bm\mu_{\f}), \bm\Sigma_{\u\u} - \bm\Sigma_{\u\f}\bm\Sigma_{\f\f}^{-1}\bm\Sigma_{\f\u} \right).
\end{align}
#+END_EXPORT
Consider the case where $\u$ represents some observations and $\f$ represents a new test location.
Eq. ref:eq-gaussian-conditional can be used to make
inferences in the location $\f$ given the observations $\u$, i.e. make
sophisticated interpolations on the measurements, based on their closeness.
In real-world scenarios, it is desirable to consider the entire input domain, instead of simply
pre-selecting a discrete set of locations.
Gaussian processes provide this mathematical machinery.

**** Gaussian Processes
Informally, GPs are a generalisation of the multivariate Gaussian distribution, indexed by an
input domain as opposed to an index set.
Similar to how a sample from an $N-\text{dimensional}$ multivariate Gaussian is an $N-\text{dimensional}$ vector,
a sample from a GP is a random function over its domain.
Formally, a GP is defined as follows,
#+BEGIN_EXPORT latex
\begin{definition}[Gaussian process]
A Gaussian process (GP) is a collection of random variables, any finite number of which
have a joint Gaussian distribution \cite{rasmussenGaussian2006}.
\end{definition}
#+END_EXPORT

# More intuitively, a Gaussian process is a distribution over functions $f : \R^{D_f} \rightarrow \R$
More intuitively, a Gaussian process  is a distribution over functions $f(\cdot): \gpDomain \rightarrow \R$
defined over an input domain $\gpDomain \in \R^{D_f}$.
Whilst Gaussian distributions represent distributions over finite-dimensional vectors, GPs represent
distributions over infinite-dimensional functions.
\marginpar{mean and covariance functions}
A GP is fully defined by a mean function $\mu(\cdot) : \gpDomain \rightarrow \R$ and a covariance
function $k(\cdot, \cdot) : \gpDomain \times \gpDomain \rightarrow \R$ (also known as a kernel),
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-mean-cov}
f(\cdot) &\sim \mathcal{GP}\left(\mu(\cdot), k(\cdot,\cdot) \right), \\
\mu(\cdot) &= \E[f(\cdot)], \\
k(\cdot, \cdot') &= \E[(f(\cdot) -\mu(\cdot)) (f(\cdot') - \mu(\cdot'))],
\end{align}
#+END_EXPORT
Importantly, for a given set of training inputs from the
\marginpar{Gaussian process prior}
functions domain $~{\mathbf{X} = \{ \mathbf{x}_1, \ldots, \mathbf{x}_N \}}$, the associated function values
$\mathbf{f} = \{f(\mathbf{x}_1), \ldots, f(\mathbf{x}_N) \}$,
are jointly Gaussian.
This results in an $N-\text{dimensional}$ multivariate Gaussian random variable $\mathbf{f}$, given by,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-marginal}
\mathbf{f} \sim \mathcal{N}(\mu(\mathbf{X}), k(\mathbf{X}, \mathbf{X})).
\end{equation}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{myquote}
A common kernel that is used throughout this dissertation is the Squared Exponential
kernel with Automatic Relevance Determination (ARD), given by,
\begin{equation} \label{eq-se-kernel}
k(x, x') = \sigma_f^2 \exp\left(-\frac{1}{2} \sum_{d=1}^{D_f} \left( \frac{x_{d}- x'_{i}}{l_d} \right) \right),
\end{equation}
where $\sigma_f^2$ represent the signal variance and $l_d$ is a lengthscale parameter associated with
input dimension $d$.
The lengthscale parameter determines the length of the "wiggles" in the function and
the signal variance $\sigma_f^2$ determines the average deviation of the function from its mean.
\end{myquote}
#+END_EXPORT
Given mean and kernel functions with parameters $\bm\theta$, the marginal distribution is given by,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-prior}
p(\mathbf{f} \mid \mathbf{X}) = \mathcal{N}(\mathbf{f} \mid \mu(\mathbf{X}), k(\mathbf{X}, \mathbf{X})).
\end{equation}
#+END_EXPORT
where the dependency on the parameters $\bm\theta$ has been dropped, i.e.
$p(\f \mid \mathbf{X}) = p(\f \mid \mathbf{X}, \bm\theta)$.
This simplification will be used throughout this dissertation for notational conciseness.
By definition, these observations $\mathbf{f}$ are jointly Gaussian with any un-observed function value
\marginpar{jointly Gaussian}
$f_* = f(\mathbf{x}_*)$ at a new test input,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-gp-joint}
\left[\begin{array}{c}
      \mathbf{f} \\
      f^{*}
\end{array}\right]
\sim\ &\mathcal{N}\left(
\left[\begin{array}{c}
      \mu(\mathbf{X}) \\
      \mu(\mathbf{x}_{*})
 \end{array}\right]
\left[\begin{array}{cc}
      k(\mathbf{X}, \mathbf{X}) & k(\mathbf{X},\mathbf{x}_{*}) \\
      k(\mathbf{x}_{*},\mathbf{X}) & k(\mathbf{x}_{*},\mathbf{x}_{*})
 \end{array}\right]\right).
\end{align*}
#+END_EXPORT
Given the multivariate Gaussian conditionals in Eq. ref:eq-gaussian-conditional, it is easy to see how
the distribution over the test function value $f_*$,
\marginpar{noise-free predictions}
can be obtained by conditioning on the observations,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-prediction}
p(f_{*} \mid \mathbf{x}_*, \mathbf{f}, \mathbf{X}) &= \mathcal{N}(f_* \mid \mu, \sigma^2)  \\
\mu &= \mu(\mathbf{x}_*) + k(\mathbf{x}_*, \mathbf{X}) k(\mathbf{X}, \mathbf{X})^{-1} (\mathbf{f} - \mu(\mathbf{X})) \nonumber\\
\sigma^2 &= k(\mathbf{x}_*, \mathbf{x}_*) - k(\mathbf{x}_*, \mathbf{X}) k(\mathbf{X}, \mathbf{X})^{-1} k(\mathbf{X}, \mathbf{x}_*). \nonumber
\end{align}
#+END_EXPORT

It is typical in real-world modelling scenarios that observations of the true function values $\f$
are not directly accessible.
\marginpar{predictions with noise}
Instead, observations are usually corrupted by noise,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-noisy}
\mathbf{y} = f(\mathbf{x}) + \epsilon, \quad \epsilon \sim \mathcal{N}\left( \mathbf{0}, \sigma^2_{n} \mathbf{I} \right).
\end{align}
#+END_EXPORT
where $\sigma^2_n$ is the noise variance.
In this scenario, the function values $\f$ become latent variables and a Gaussian likelihood
is introduced,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-likelihood}
p(\mathbf{y} \mid \f) = \mathcal{N}\left( \mathbf{y} \mid \f, \sigma^2_{n} \mathbf{I} \right),
\end{align}
#+END_EXPORT
to relate the observations to the latent function values $\f$.
The predictive distribution for a test input $\mathbf{x}_*$ follows from Eq. ref:eq-gaussian-conditional,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-prediction-noisy}
p(f_{*} \mid \mathbf{x}_*, \mathbf{y}, \mathbf{X}) &= \mathcal{N}(f_* \mid \mu, \sigma^2)  \\
\mu &= \mu(\mathbf{x}_*) + k(\mathbf{x}_*, \mathbf{X})
\left(k(\mathbf{X}, \mathbf{X}) + \sigma^2_n \mathbf{I} \right)^{-1} (\mathbf{y} - \mu(\mathbf{X})) \nonumber\\
\sigma^2 &= k(\mathbf{x}_*, \mathbf{x}_*) -
k(\mathbf{x}_*, \mathbf{X})
\left( k(\mathbf{X}, \mathbf{X}) + \sigma^2_n \mathbf{I} \right))^{-1}
k(\mathbf{X}, \mathbf{x}_*). \nonumber
\end{align}
#+END_EXPORT
This predictive distribution is the GP posterior.


**** Bayesian Model Selection label:sec-bayesian-model-selection
#+BEGIN_EXPORT latex
\newcommand{\weights}{\ensuremath{\mathbf{w}}}
\newcommand{\hyperparameters}{\ensuremath{\bm\theta}}
\newcommand{\modelStructures}{\ensuremath{\mathcal{H}_i}}
\newcommand{\weightPrior}{\ensuremath{p(\weights \mid \hyperparameters, \modelStructures)}}
\newcommand{\weightLikelihood}{\ensuremath{p(\mathbf{y} \mid \mathbf{X}, \weights, \modelStructures)}}
\newcommand{\weightMarginalLikelihood}{\ensuremath{p(\mathbf{y} \mid \mathbf{X}, \hyperparameters, \modelStructures)}}
\newcommand{\weightPosterior}{\ensuremath{p(\weights \mid \mathbf{y}, \mathbf{X}, \hyperparameters, \modelStructures)}}

\newcommand{\hyperPrior}{\ensuremath{p(\hyperparameters \mid \modelStructures)}}
\newcommand{\hyperLikelihood}{\ensuremath{p(\mathbf{y} \mid \mathbf{X}, \hyperparameters, \modelStructures)}}
\newcommand{\hyperMarginalLikelihood}{\ensuremath{p(\mathbf{y} \mid \mathbf{X}, \modelStructures)}}
\newcommand{\hyperPosterior}{\ensuremath{p(\hyperparameters \mid \mathbf{y}, \mathbf{X}, \modelStructures)}}

\newcommand{\modelPrior}{\ensuremath{p(\modelStructures)}}
\newcommand{\modelLikelihood}{\ensuremath{p(\mathbf{y} \mid \mathbf{X}, \modelStructures)}}
\newcommand{\modelMarginalLikelihood}{\ensuremath{p(\mathbf{y} \mid \mathbf{X})}}
\newcommand{\modelPosterior}{\ensuremath{p(\modelStructures \mid \mathbf{y}, \mathbf{X})}}
#+END_EXPORT
The /posterior/ distribution in Eq. ref:eq-gp-prediction-noisy considers a fixed covariance function.
In most practical applications it is hard to specify all components of the covariance function /a priori/.
Although some properties of the underlying function may be known, such as stationarity,
it may not be easy to specify the hyperparameters, e.g. lengthscales.
The parameters of the /likelihood/ may also be hard to specify /a priori/, e.g. noise variance.
In order to deploy Gaussian process models in practical applications it is important to address
this model selection problem.



Bayesian model selection provides a principled framework for performing learning in probabilistic models.
This section gives a brief overview of the main ideas in Bayesian model selection.
The overview is general but highlights the difficulties that arise when working with Gaussian
process models.

In machine learning it is common to use hierarchical models.
At the lowest level are model parameters $\weights$, such as the weights of a neural network,
or the weights in linear regression.
\marginpar{hierarchical models}
At the next level are hyperparameters $\hyperparameters$, which serve as parameters in the distributions
over the model parameters at the lowest level.
For example, kernel hyperparameters in Gaussian process regression or the weight decay term in
neural networks.
At the highest level is the underlying model structure,
that is, there may be a set of (discrete) model structures $\modelStructures$ to be considered.
For example, the functional form of the covariance function.

*Maximum Likelihood*
Let us quickly introduce a simple (non Bayesian) approach to finding the best
values for the parameters.
This approach finds the best parameter settings by maximising the likelihood $\weightLikelihood$
with respect to the parameters $\weights$.
\marginpar{maximum likelihood}
This approach is known as maximum likelihood and is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-maximum-likelihood}
\weights = \text{arg}\max_{\weights} \weightLikelihood,
\end{align}
#+END_EXPORT
This obtains a point estimate for the "best" parameter values $\weights$.
Unfortunately, as the likelihood function is higher for more complex model structures,
this approach leads to overfitting.

# This can be overcome by noticing that integrating out the parameters (instead of optimising them),
# automatically protects from overfitting.
A more principled approach -- that overcomes overfitting -- is to treat the parameters as random variables
(i.e. place priors over them), and integrate them out (instead of optimising them).
This is advantageous as it considers all possible settings of the parameters,
penalising complex models and preventing overfitting.
This is known as /Bayesian Occam's razor/ citep:mackayProbable1995,rasmussenOccam2001,murrayNote2005
named after the principle of /Occam's razor/ (William of Occam 1285-1349).
The principle says that "one should pick the simplest model that adequately explains the data".
# This provides automatic Occam's razor, penalising complex models and preventing overfitting.
# and returns the
# posterior distribution $p(\mathbf{\Theta} | \mathbf{Y})$ over the unknown variables,
# in contrast to the point estimate in maximum likelihood.

Model selection takes place one step at a time, by repeatedly applying the rules of probability theory,
see cite:mackayBayesian1992 for more details on this framework.
At the lowest level, the /posterior/ distribution over the model parameters $\weightPosterior$
\marginpar{level 1 inference}
is obtained via Bayes' rule,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-bayes-rule-1}
\weightPosterior = \frac{\weightLikelihood \weightPrior}{\weightMarginalLikelihood}
\end{align}
#+END_EXPORT
where $\weightLikelihood$ is the /likelihood/, a statistical model relating the data to the model
parameters and $\weightPrior$ is the /prior/ over the parameters.
The /prior/ distribution encodes our initial belief in the parameters.
It may take a particular form to reflect the underlying structure of the problem, or
it may be broad to reflect having little prior knowledge.
The /posterior/ is the object of interest because it
combines information from the /prior/ distribution, with information from the data,
via the /likelihood/.
The denominator $\weightMarginalLikelihood$ is a normalising constant and is independent of the model
parameters $\weights$. It is known as the /marginal likelihood/  (or evidence) and is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood-1}
\weightMarginalLikelihood = \int \weightLikelihood \weightPrior \text{d} \weights,
\end{align}
#+END_EXPORT
where the variables of interest (the model parameters $\weights$) have been integrated out of the
joint distribution.
So far so good, however, how should the hyperparameters $\hyperparameters$ be set?
The same process can be repeated to obtain the /posterior/ distribution over the
\marginpar{level 2 inference}
hyperparameters,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-bayes-rule-2}
\hyperPosterior = \frac{\hyperLikelihood \hyperPrior}{\hyperMarginalLikelihood}
\end{align}
#+END_EXPORT
where $\hyperPrior$ is the /hyper-prior/, a prior over the hyperparameters.
Notice that the /marginal likelihood/ from the lowest level $\weightMarginalLikelihood$
now takes the role of the /likelihood/. Similarly to before, the normalisation constant
is given by integrating out the object of interest (the hyperparameters $\hyperparameters$),
#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood-2}
\hyperMarginalLikelihood = \int \hyperLikelihood \hyperPrior \text{d} \hyperparameters.
\end{align}
#+END_EXPORT
Finally, at the top level, the /posterior/ distribution over the model structure $\modelPosterior$
\marginpar{level 3 inference}
is computed,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-bayes-rule-3}
\modelPosterior = \frac{\modelLikelihood \modelPrior}{\modelMarginalLikelihood},
\end{align}
#+END_EXPORT
where $\modelMarginalLikelihood = \sum_{i} \modelLikelihood \modelPrior$.
It is important to note how performing Bayesian inference requires the computation of
integrals in the denominator of Bayes' rule.
These integrals are often not analytically tractable, due to details of the model.
As such, approximate inference techniques are often required.
These approximations may be analytical or based on Markov Chain Monte Carlo (MCMC) methods.

*Type II Maximum Likelihood*
In Gaussian process methods, the marginalisation of the hyperparameters in Eq. ref:eq-marginal-likelihood-2
is often difficult.
\marginpar{type II maximum likelihood}
When it is not possible to marginalise all of the variables, the next best thing to a full Bayesian
treatment -- without resorting to Maximum Likelihood --
is to marginalise as many variables as possible, and optimise the remaining variables subject to maximising
the /marginal likelihood/.
This optimisation is known as Type II Maximum Likelihood and is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-type-2-maximum-likelihood}
\weights &= \text{arg}\max_{\weights} \weightMarginalLikelihood \\
&= \text{arg}\max_{\weights} \int \weightLikelihood \weightPrior \text{d} \weights,
\end{align}
#+END_EXPORT


**** old :noexport:
# The integral in Eq. ref:eq:type-2-ml is often always tractable so approximations

# In Bayesian machine learning it is common to use GPs to formulate prior distributions over functions
# $p(\f \mid \mathbf{X})$.
# Different likelihood functions can be used to formulate GP regression and GP classification.
# Selecting a Gaussian likelihood

# It is possible to use GPs to place priors over functions in both the regression and classification settings.

# Bayesian inference with GP priors and different likelihood functions
# Performing Bayesian inference with GP priors and different likelihood functions

#+BEGIN_EXPORT latex
\begin{myquote}
Bayesian machine learning seeks to update a prior belief over latent variables $p(\bm\theta)$
given observations $\mathcal{D}$.
Bayesian inference derives the posterior probability from the prior probability $p(\bm\theta)$
and the likelihood function $p(\mathcal{D} \mid \bm\theta)$ --
a statistical model for the observed data -- using Bayes rule,
\begin{align} \label{eq-bayes-rule}
\underbrace{p(\bm\theta \mid \mathcal{D})}_{\text{posterior}} = \frac{p(\mathcal{D} \mid \bm\theta) p(\bm\theta)}{p(\mathcal{D})}
\propto \underbrace{p(\mathcal{D} \mid \bm\theta)}_{\text{likelihood}} \underbrace{p(\bm\theta)}_{\text{\prior}}
\end{align}
where $p(\mathcal{D})$ is the model evidence (aka marginal likelihood).
Given the posterior distribution $p(\bm\theta \mid \mathcal{D})$, it is possible to make predictions by
marginalising the latent variables -- known as Bayesian model averaging.
\begin{align} \label{eq-bayes-ml-prediction}
p(\mathbf{y}_* \mid \mathbf{x}_*, \mathcal{D} = \int
p(\mathbf{y}_* \mid \mathbf{x}_*, \bm\theta, \mathcal{D})
\underbrace{p(\bm\theta \mid \mathcal{D})}_{\text{posterior}}
\text{d} \bm\theta
\end{align}
\end{myquote}
#+END_EXPORT






**** Model Selection for GP Regression
Bayesian principles are an appealing framework for inference.
However, for most interesting models arising in machine learning, the required computations
(integrals over the latent variables) are analytically intractable.
Gaussian process regression with Gaussian noise is a rare exception, where the posterior
is analytically tractable Eq. ref:eq-gp-prediction-noisy.

The power of Gaussian process regression arises from the tractability of the marginal likelihood of the
observed outputs given the observed inputs.
The marginal likelihood is the product of the Gaussian likelihood and the GP prior, with the latent
variables integrated out,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-marginal-likelihood}
p(\mathbf{y} \mid \mathbf{X}, \bm\theta) &= \int
\underbrace{p(\mathbf{y} \mid \f)}_{\text{likelihood}}
\underbrace{p(\f \mid \mathbf{X}, \bm\theta)}_{\text{GP prior}}
\text{d}\f.
%&= \mathcal{N} \left( \mathbf{y} \mid \mu(\mathbf{X}), k(\mathbf{X}, \mathbf{X}) + \sigma^2_n \mathbf{I} \right).
\end{align}
#+END_EXPORT
This marginalisation implies that a whole family of functions are simultaneously considered.
The covariance function determines the properties of the functions (such as smoothness)
and defines a nonparametric form of $f$.
In this setting, the Gaussian process is used as the /prior/ over the latent function $f$
and the Gaussian likelihood is used to relate the data to the model.
Conditioning the /prior/ on the observed data obtains the /posterior/ distribution, which
fits the data.

Although GPs are nonparametric, the mean and kernel functions may contain hyperparameters $\bm\theta$.
In practical settings, it is desirable to find the best hyperparameter settings given the observations.
Unfortunately, specifying (hyper-)priors over the hyperparameters and marginalising them
(Eq. ref:eq-marginal-likelihood-2), is often difficult for GP models.
As such, it is common to resort to Type II Maximum Likelihood and maximise the log marginal likelihood
with respect to the kernel hyperparameters $\bm\theta$ and noise variance $\sigma^2_n$.
The log marginal likelihood is given by,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-gp-log-marginal-likelihood}
\text{log} p(\mathbf{y} \mid \mathbf{X}, \bm\theta) =
- \underbrace{\frac{1}{2} \mathbf{y}^T (k(\mathbf{X}, \mathbf{X}) + \sigma^2_n \mathbf{I})^{-1} \mathbf{y}}_{\text{data fit}}
- \underbrace{\frac{1}{2} \log \left| k(\mathbf{X}, \mathbf{X}) + \sigma^2_n \mathbf{I} \right|}_{\text{complexity penalty}}
- \underbrace{\frac{n}{2} \log 2 \pi}_{\text{constant}}
\end{align}
\normalsize
#+END_EXPORT
The three terms in the log marginal likelihood have interpretable roles:
1. Data fit term,
   - $\frac{1}{2} \mathbf{y}^T (k(\mathbf{X}, \mathbf{X}) + \sigma^2_n \mathbf{I})^{-1} \mathbf{y}$,
2. Complexity penalty depending only on the covariance function and the inputs,
   - $\frac{1}{2} \log | k(\mathbf{X}, \mathbf{X}) + \sigma^2_n \mathbf{I} |$,
3. Normalisation constant,
   - $\frac{n}{2} \log 2 \pi$.
Importantly, as discussed in Section ref:sec-bayesian-model-selection,
maximising the log marginal likelihood automatically balances the trade-off
between model complexity and data fit, i.e. it provides automatic Occam's razor.
The resulting optimisation is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-type-2-maximum-likelihood}
\bm\theta = \text{arg}\max_{\bm\theta} \log p(\mathbf{y} \mid \mathbf{X}, \bm\theta)
= \text{arg}\max_{\bm\theta} \log \int p(\mathbf{y} \mid \f) p(\f \mid \mathbf{X}, \bm\theta) \text{d} \f.
\end{align}
#+END_EXPORT

# Making predictions in Eq. ref:eq-gp-prediction-noisy is computationally expensive due to conditioning


**** Sparse Gaussian Processes
The nonparametric nature of Gaussian process methods is responsible for their flexibility but also
their shortcomings, namely their memory and computational limitations.
In general, the computational complexity of Gaussian process methods scale with $\mathcal{O}(N^{3})$,
due to the inversion of the $N \times N$ covariance matrix $k(\mathbf{X},\mathbf{X})$.
This makes its application to data sets with more than a few thousand data points prohibitive.
The main direction in the literature attempting to overcome this limitation is sparse approximations
cite:titsiasVariational2009,snelsonSparse2005a,quinonero-candelaUnifying2005,leibfriedTutorial2021.
Many approximation schemes have been proposed to reduce the computational complexity, see
cite:quinonero-candelaUnifying2005 for a review.


# The log marginal likelihood in Eq. ref:eq-gp-log-marginal-likelihood is computationally
# expensive due to the inversion of the covariance matrix evaluated between all of the
# training inputs $\mathbf{X}$.
# In general, the computational complexity of Gaussian process methods scale with $\mathcal{O}(N^{3})$,
# due to the inversion of the covariance matrix $k(\mathbf{X},\mathbf{X})$.


***** Variational Sparse Gaussian Processes

**** Sparse Gaussian Processes for Regression label:sec-sparse-gps

Consider another Gaussian density $q(\u)$ over the vector $\u$, as well as $p(\u)$, given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gaussian-marginal-q}
q(\u) &= \mathcal{N} \left(\u \mid \mathbf{m}_{\u}, \mathbf{S}_{\u\u} \right)
\end{align}
#+END_EXPORT
with mean $\mathbf{m}_{\u}$ and covariance $\mathbf{S}_{\u\u}$.
The marginal distribution over $\f$ can be obtained by conditioning on $\u$ and marginalising w.r.t. this
new distribution $q(\f) = \int p(\f \mid \u) q(\u) \text{d} \u$.
This marginal distribution is also Gaussian,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gaussian-conditional-q}
\f \mid \u &\sim \mathcal{N} \left(\bm\mu_{\f} + \bm\Sigma_{\f\u} \bm\Sigma_{\u\u}^{-1}(\mathbf{m}_{\u} - \bm\mu_{\u}), \bm\Sigma_{\f\f} - \bm\Sigma_{\f\u}
\bm\Sigma_{\u\u}^{-1}
\left(\bm\Sigma_{\u\u} - \mathbf{S}_{\u\u} \right)
\bm\Sigma_{\u\u}^{-1}
\bm\Sigma_{\u\f} \right).
\end{align}
#+END_EXPORT
Replacing $q(\u)$ with $p(\u)$ would recover the original marginal distribution $p(\f)$ with
mean $\bm\mu_{\f}$ and covariance $\bm\Sigma_{\f}$, as expected from Eq. ref:eq-gaussian-marginal.




This work focuses on inducing point methods citep:snelsonSparse2005a, where the latent variables
are augmented with inducing input-output pairs known as inducing "inputs" $\Z \in \inputDomain$, and inducing
"variables" $\u = f(\Z)$.
Introducing a set of $M$ inducing points from the same GP prior
$p(\u \mid \Z) &= \mathcal{N}(\hat{\mathbf{f}} \mid \mu(\Z), k(\Z, \Z))$
can reduce the computational cost
if $M < N$.
Note that these inducing variables $\u$ are jointly gaussian with the latent function values $\mathbf{f}$,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-spare-gp-joint}
p(\mathbf{f}, \u \mid \mathbf{X}, \mathbf{Z}) =\ &\mathcal{N}\left(
\left[\begin{array}{c}
      \mathbf{f} \\
      \u
\end{array}\right]
      \mid \bm{0} ,
\left[\begin{array}{cc}
      \mathbf{K}_{nn} & \mathbf{K}_{nm} \\
      \mathbf{K}_{nm}^{T} & \mathbf{K}_{mm}
 \end{array}\right]\right).
\end{align*}
#+END_EXPORT
where $\K_{mm}$ is the covariance function evaluated between all inducing inputs $\Z$, and
$\K_{nm}$ is the covariance function evaluated between the data inputs $\mathbf{X}$ and the inducing inputs $\Z$.
This joint distribution can be re-written using the properties of multivariate normal distributions as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-joint-cond}
p(\mathbf{f}, \u \mid \mathbf{X}, \mathbf{Z}) &= p(\mathbf{f} \mid \u) p(\u \mid \mathbf{Z}) \\
&= \mathcal{N}(\mathbf{f} \mid \mathbf{K}_{nm} \mathbf{K}_{mm}^{-1} \u, \mathbf{K}_{nn} - \mathbf{Q}_{nn})
\mathcal{N}(\u \mid \mathbf{0}, \mathbf{K}_{mm})
\end{align}
#+END_EXPORT
where $\mathbf{Q}_{nn} = \mathbf{K}_{nm}\mathbf{K}_{mm}^{-1}\mathbf{K}_{nm}^{T}$.
The full joint distribution is then given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-full-joint}
p(\mathbf{y}, \mathbf{f}, \u \mid \mathbf{X}, \mathbf{Z}) &=  p(\mathbf{y} \mid \mathbf{f}) p(\mathbf{f} \mid \u, \mathbf{X}) p(\u \mid \mathbf{Z})
\end{align}
#+END_EXPORT
Computationally efficient inference is obtained by approximating the integration over $\mathbf{f}$.
The popular Fully Independent Training Conditional (FITC) method (in the case of a Gaussian likelihood)
is obtained via the factorisation: $p(\mathbf{y} \mid \u) \approx \prod^{N}_{n=1} p(y_{n} \mid \u)$.
To obtain a variational approximation Jensen's inequality is first used to bound this conditional,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-bound-cond}
\text{log}p(\mathbf{y}\mid\u) \geq \E_{p(\F \mid \u)} \left[ \text{log} p(\mathbf{y} \mid \F) \right] := \mathcal{L}_{1}.
\end{align}
#+END_EXPORT
This is the standard bound shown in cite:hensmanGaussian2013 ($\mathcal{L}_{1}$ defined in Eq. 1).
In cite:titsiasVariational2009 this bound is then substituted into the marginal likelihood
to obtain a tractable lower bound,
\marginpar{Titsias' bound}
#+BEGIN_EXPORT latex
\begin{align}
\text{log} p(\mathbf{y} \mid \mathbf{X})
&= \text{log} \E_{p(\u \mid \mathbf{Z})} \left[ p(\mathbf{y} \mid \u) \right] \\
&\geq \text{log} \E_{p(\u \mid \mathbf{Z})} \left[ \text{exp}(\mathcal{L}_{1}) \right] := \mathcal{L}_{\text{titsias}} \label{eq-titsias-bound-2} \\
&= \text{log}\mathcal{N}(\mathbf{y} \mid \mathbf{0}, \mathbf{Q}_{nn} + \sigma^{2} \mathbf{I})
- \frac{1}{2\sigma^2}\text{tr}(\K_{nn} - \mathbf{Q}_{nn}) \label{eq-titsias-bound},
\end{align}
#+END_EXPORT
where $\sigma^2$ is the noise variance associated with the Gaussian likelihood.
The bound in Eq. ref:eq-titsias-bound becomes exact (i.e. recovers the true log marginal likelihood)
when the inducing inputs $\Z$ equal the
data inputs $\mathbf{X}$ as $\K_{nm}=\K_{mm}=\K_{nn}$ so $\mathbf{Q}_{nn}=\K_{nn}$.

cite:hensmanGaussian2013 noted that this bound has complexity $O(NM^{2})$ so
introduced additional variational parameters to achieve a more computationally scalable bound
with complexity $O(M^{3})$.
Instead of collapsing the inducing points in Eq. ref:eq-titsias-bound
they explicitly represent them as a variational distribution,
#+BEGIN_EXPORT latex
\begin{align} \label{eq--inducing-dist}
q(\u) = \mathcal{N}\left(\u \mid \mathbf{m}, \mathbf{S} \right),
\end{align}
#+END_EXPORT
which they use to lower bound the marginal likelihood $p(\mathbf{y} \mid \mathbf{X})$ by
\marginpar{Hensman's bound}
further bounding $\mathcal{L}_{\text{titsias}}$,
#+BEGIN_EXPORT latex
\begin{align}
\mathcal{L}_{\text{titsias}} &= \text{log} \E_{p(\u \mid \mathbf{Z})} \left[ \text{exp}(\mathcal{L}_{1}) \right] \\
&\geq \E_{q(\u)} \left[ \mathcal{L}_{1} \right]
- \text{KL}\left[q(\u) \mid\mid p(\u \mid \mathbf{Z})\right] \\
&\geq \E_{q(\F)} \left[ p(\mathbf{y} \mid \mathbf{f}) \right]
- \text{KL}\left[q(\u) \mid\mid p(\u \mid \mathbf{Z})\right] := \mathcal{L}_{\text{hensman}}, \label{eq-hensman-bound}
\end{align}
#+END_EXPORT
where $q(\mathbf{f}) = \int p(\mathbf{f} \mid \u) q(\u) \text{d} \u$.
This is equivalent to $\mathcal{L}_{3}$ defined in Eq. 4 cite:hensmanGaussian2013.
If the likelihood factorizes across data $p(\mathbf{y} \mid \mathbf{f}) = \prod_{n=1}^{N} p(y_{n} \mid f_{n})$
only the marginals of $q(\mathbf{f})$ are needed to calculate Eq. ref:eq-hensman-bound,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-bound-3-fact}
\mathcal{L}_{\text{hensman}}
= \sum_{n=1}^{N}\E_{q(f_{n})} \left[ \text{log} p(y_{n} \mid f_{n}) \right]
- \text{KL}\left[q(\u) \mid\mid p(\u \mid \mathbf{Z}) \right].
\end{align}
#+END_EXPORT
Importantly, this bound is written as a sum over input-output pairs which has induced the necessary conditions
to preform stochastic gradient methods on $q(\u)$.



**** Variational Inference :noexport:
\todo{seek to convert inference into an optimisation}

Variational approaches seek to lower bound the marginal likelihood (or evidence) $p(\mathbf{Y})$ using a functional that depends on variational parameters $q(\mathbf{\Theta})$.

#+BEGIN_EXPORT latex
\begin{align}
	\text{log}\ p(\mathbf{Y}) &= \text{log} \int p(\mathbf{Y}, \mathbf{\Theta}) \text{d}\mathbf{\Theta} = \text{log} \int p(\mathbf{Y} | \mathbf{\Theta}) p(\mathbf{\Theta}) \text{d}\mathbf{\Theta}
	\\
	&= \text{log} \int \frac{q(\mathbf{\Theta})}{q(\mathbf{\Theta})} p(\mathbf{Y}, \mathbf{\Theta}) \text{d}\mathbf{\Theta}
	\\
	&= \text{log} \int q(\mathbf{\Theta}) \frac{p(\mathbf{Y}, \mathbf{\Theta})}{q(\mathbf{\Theta})} \text{d}\mathbf{\Theta}
	\\
	&= \text{log} \E_{q(\mathbf{\Theta})} \bigg[ \frac{p(\mathbf{Y}, \mathbf{\Theta})}{q(\mathbf{\Theta})} \bigg]
	\\
	&\geq \E_{q(\mathbf{\Theta})} \bigg[ \text{log} \frac{p(\mathbf{Y}, \mathbf{\Theta})}{q(\mathbf{\Theta})} \bigg]
\end{align}
#+END_EXPORT

The bound becomes exact if $q(\mathbf{\Theta}) = p(\mathbf{\Theta} | \mathbf{Y})$. The true posterior is intractable but the variational posterior $q(\mathbf{\Theta})$ must be constrained (so that it is tractable). It is common to factorise $q(\mathbf{\Theta})$ with respect to groups of elements belonging to parameter set $\mathbf{\Theta}$, this is known as mean field approximation.



**** Sparse Gaussian Processes :noexport:
A GP cite:edwardrasmussenGaussian2010 is a distribution over functions $f : \R^{D_f} \rightarrow \R$
fully defined by a mean function $\mu(\cdot)$ and a covariance function $k(\cdot, \cdot)$.
For a given set of inputs  from the
functions domain $\mathbf{X} = \{ \mathbf{x}_1, \ldots, \mathbf{x}_N \}$ the associated function values
$\mathbf{f} = \{f(\mathbf{x}_1), \ldots, f(\mathbf{x}_N) \}$
are jointly Gaussian,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-prior}
p(\mathbf{f} \mid \mathbf{X}) = \mathcal{N}(\mathbf{f} \mid \bm\mu_{\mathbf{X}}, \mathbf{K}_{\mathbf{X}, \mathbf{X}})
\end{equation}
#+END_EXPORT
where $\bm\mu_{\mathbf{X}}= \mu(\mathbf{X})$ is the mean vector, and
$\mathbf{K}_{\mathbf{X},\mathbf{X}} = k(\mathbf{X}, \mathbf{X})$ is the covariance function evaluated
between the inputs $\mathbf{X}$.
In this work, the squared exponential covariance function with Automatic Relevance Determination
is used for all GPs.
The distribution over the function value $f_*$ at a new input $\mathbf{x}_*$ (i.e. to make a prediction)
is given by the conditional,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-gp-prediction}
p(f_{*} \mid \mathbf{x}_*, \mathbf{f}, \mathbf{X}) &= \mathcal{N}(f_* \mid \mu, \sigma^2) \numberthis \\
\mu &= \mu_{\mathbf{x}_*} + \mathbf{k}_{\mathbf{x}_*, \mathbf{X}} \mathbf{K}_{\mathbf{X}, \mathbf{X}}^{-1} (\mathbf{f} - \bm\mu_{\mathbf{X}}) \\
\sigma^2 &= k_{\mathbf{x}_*, \mathbf{x}_*} - \mathbf{k}_{\mathbf{x}_*, \mathbf{X}} \mathbf{K}_{\mathbf{X}, \mathbf{X}}^{-1} \mathbf{k}_{\mathbf{X}, \mathbf{x}_*}.
\end{align*}
#+END_EXPORT
The conditional in Eq. ref:eq-gp-prediction is computationally expensive due to conditioning
on all of the training data $\mathbf{X}, \mathbf{f}$.
Introducing a set of $M$ inducing points from the same GP prior can reduce the computational cost
if $M < N$.
The inducing inputs are denoted $\bm\xi$ and outputs as
$\hat{\mathbf{f}} = f(\bm\xi)$.
The inducing variables
$\hat{\mathbf{f}}$ are jointly Gaussian with the latent function values $\mathbf{f}$.
The GP predictive distribution can be approximated by conditioning on this
smaller set of inducing variables,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-prediction}
p({f}_{*} \mid \mathbf{x}_*, \mathbf{f}, \mathbf{X}) &\approx
p({f}_* \mid \mathbf{x}_*, \hat{\mathbf{f}}, \bm\xi) \\
p(\hat{\mathbf{f}}) &= \mathcal{N}(\hat{\mathbf{f}} \mid \mathbf{m}, \mathbf{S}).
\end{align}
#+END_EXPORT
The approximation becomes exact when the
inducing variables $\hat{\mathbf{f}}$ are a sufficient statistic
for the latent function values $\mathbf{f}$ cite:Titsias2009.
# The predicted latent function values are then mutually independent given the inducing variables.
# A central assumption is that given enough well
# placed inducing variables $\hat{\mathbf{f}}$ they are a
# sufficient statistic for the latent function values $\mathbf{f}$.

*** Gaussian Processes :noexport:
This section introduces Gaussian processes and the sparse approximations that are used throughout this work.
In particular, it walks through the lower bounds (used to perform variational inference) that are the building blocks
of the inference in this thesis.


#+BEGIN_EXPORT latex
\begin{definition}[Gaussian process]
A Gaussian process (GP) is a collection of random variables, any finite number of which
have a joint Gaussian distribution \cite{rasmussenGaussian2006}.
\end{definition}
#+END_EXPORT

# More intuitively, a Gaussian process is a distribution over functions $f : \R^{D_f} \rightarrow \R$
More intuitively, a Gaussian process  is a distribution over functions $f(\cdot): \inputDomain \rightarrow \R$
defined over an input domain $\inputDomain \in \R^{D_f}$.
Whilst Gaussian distributions represent distributions over finite-dimensional vectors, GPs represent
distributions over infinite-dimensional functions. Vector indexes in multivariate Gaussian random variables
correspond to evaluation points $\mathbf{X} \in \inputDomain$ in GP random functions.
\marginpar{mean and covariance functions}
A GP is fully defined by a mean function $\mu(\cdot) : \inputDomain \rightarrow \R$ and a covariance
function $k(\cdot, \cdot) : \inputDomain \times \inputDomain \rightarrow \R$ (also known as a kernel),
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-mean-cov}
f(\cdot) &\sim \mathcal{GP}\left(\mu(\cdot), k(\cdot,\cdot) \right), \\
\mu(\cdot) &= \E[f(\cdot)], \\
k(\cdot, \cdot') &= \E[(f(\cdot) -\mu(\cdot)) (f(\cdot') - \mu(\cdot'))],
\end{align}
#+END_EXPORT
Importantly, for a given set of training inputs from the
\marginpar{Gaussian process prior}
functions domain $~{\mathbf{X} = \{ \mathbf{x}_1, \ldots, \mathbf{x}_N \}}$, the associated function values
$\mathbf{f} = \{f(\mathbf{x}_1), \ldots, f(\mathbf{x}_N) \}$,
are jointly Gaussian.
This results in an $N-\text{dimensional}$ multivariate Gaussian random variable $\mathbf{f}$.
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-prior}
\mathbf{f} \sim \mathcal{N}(\mu(\mathbf{X}), k(\mathbf{X}, \mathbf{X})).
\end{equation}
#+END_EXPORT
By definition, the training observations $\mathbf{f}$ are jointly Gaussian with any un-observed function value
\marginpar{jointly Gaussian}
$f_*$ at any new test input $\mathbf{x}_*$,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-spare-gp-joint}
\left[\begin{array}{c}
      \mathbf{f} \\
      f^{*}
\end{array}\right]
\sim\ &\mathcal{N}\left(
\left[\begin{array}{c}
      \mu(\mathbf{X}) \\
      \mu(\mathbf{x}_{*})
 \end{array}\right]
\left[\begin{array}{cc}
      k(\mathbf{X}, \mathbf{X}) & k(\mathbf{X},\mathbf{x}_{*}) \\
      k(\mathbf{x}_{*},\mathbf{X}) & k(\mathbf{x}_{*},\mathbf{x}_{*})
 \end{array}\right]\right).
\end{align*}
#+END_EXPORT

# The prior distribution over $\mathbf{f}$ is then given by,
# #+BEGIN_EXPORT latex
# \begin{equation} \label{eq-gp-prior}
# p(\mathbf{f} \mid \mathbf{X}) = \mathcal{N}(\mathbf{f} \mid \mu(\mathbf{X}), k(\mathbf{X}, \mathbf{X})).
# \end{equation}
# #+END_EXPORT
# By definition, the training observations $\mathbf{f}$ are jointly Gaussian with any un-observed function value
# \marginpar{jointly Gaussian}
# $f_*$ at a new test input $\mathbf{x}_*$,
# #+BEGIN_EXPORT latex
# \begin{align*} \label{eq-spare-gp-joint}
# \left[\begin{array}{c}
#       \mathbf{f} \\
#       f^{*}
# \end{array}\right]
# \sim\ &\mathcal{N}\left(
#       \bm{0} ,
# \left[\begin{array}{cc}
#       k(\mathbf{X}, \mathbf{X}) & k(\mathbf{X},\mathbf{x}_{*}) \\
#       k(\mathbf{x}_{*},\mathbf{X}) & k(\mathbf{x}_{*},\mathbf{x}_{*})
#  \end{array}\right]\right).
# \end{align*}
# #+END_EXPORT
The distribution over the test function value $f_*$ (i.e. to make a prediction)
\marginpar{noise-free predictions}
can therefore easily be obtained by conditioning on the prior observations.
This is easily obtained using the properties of multivariate Normal distributions to give
the predictive conditional distribution,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-prediction}
p(f_{*} \mid \mathbf{x}_*, \mathbf{f}, \mathbf{X}) &= \mathcal{N}(f_* \mid \mu, \sigma^2)  \\
\mu &= \mu(\mathbf{x}_*) + k(\mathbf{x}_*, \mathbf{X}) k(\mathbf{X}, \mathbf{X})^{-1} (\mathbf{f} - \mu(\mathbf{X})) \nonumber\\
\sigma^2 &= k(\mathbf{x}_*, \mathbf{x}_*) - k(\mathbf{x}_*, \mathbf{X}) k(\mathbf{X}, \mathbf{X})^{-1} k(\mathbf{X}, \mathbf{x}_*). \nonumber
\end{align}
#+END_EXPORT
The conditional in Eq. ref:eq-gp-prediction is computationally expensive due to conditioning
on all of the training data $\mathbf{X}, \mathbf{f}$.
The computational complexity of Gaussian process methods scale with $\mathcal{O}(N^{3})$
due to the inversion of the covariance matrix $k(\mathbf{X},\mathbf{X})$.
Many approximation schemes have been proposed to reduce the computational complexity, see
cite:quinonero-candelaUnifying2005 for a review.

**** Bayesian Inference with Gaussian Processes


\todo{introduce noisy outputs and Gaussian likelihood}

Suppose we have some observations $\mathbf{Y}$ generated from a family of models $p(\mathbf{Y}, \mathbf{\Theta})$, where $\mathbf{\Theta}$ represents the unknown random variables that the model depends on.

*Maximum Likelihood*
In maximum likelihood we seek to find the best model by maximising the likelihood $p(\mathbf{Y} | \mathbf{\Theta})$. We obtain a point estimate for the "best" variables $\mathbf{\Theta}$. The likelihood function is higher for more complex model structures, leading to overfitting.

*Type II MLE/MAP*
Bayesian methods overcome overfitting by treating the model parameters as random variables and averaging over the likelihood for different settings of the parameters. They achieve this by maximising the logarithm of the marginal likelihood (or evidence) $p(\mathbf{Y})$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq:type-2-ml}
	\text{log}\ p(\mathbf{Y}) &= \text{log} \int p(\mathbf{Y}, \mathbf{\Theta}) \text{d}\mathbf{\Theta} = \text{log} \int p(\mathbf{Y} | \mathbf{\Theta}) p(\mathbf{\Theta}) \text{d}\mathbf{\Theta}.
\end{align}
#+END_EXPORT
This is advantageous as we now take a weighted average over all possible settings of $\mathbf{\Theta}$ and we obtain the posterior $p(\mathbf{\Theta} | \mathbf{Y})$ for the unknown variables, as opposed to just a point estimate as in maximum likelihood. This provides automatic Occam's razor, penalising complex models and preventing overfitting.

The integral in Eq. ref:eq:type-2-ml is often always tractable so approximations

**** Variational Inference
\todo{seek to convert inference into an optimisation}

Variational approaches seek to lower bound the marginal likelihood (or evidence) $p(\mathbf{Y})$ using a functional that depends on variational parameters $q(\mathbf{\Theta})$.

#+BEGIN_EXPORT latex
\begin{align}
	\text{log}\ p(\mathbf{Y}) &= \text{log} \int p(\mathbf{Y}, \mathbf{\Theta}) \text{d}\mathbf{\Theta} = \text{log} \int p(\mathbf{Y} | \mathbf{\Theta}) p(\mathbf{\Theta}) \text{d}\mathbf{\Theta}
	\\
	&= \text{log} \int \frac{q(\mathbf{\Theta})}{q(\mathbf{\Theta})} p(\mathbf{Y}, \mathbf{\Theta}) \text{d}\mathbf{\Theta}
	\\
	&= \text{log} \int q(\mathbf{\Theta}) \frac{p(\mathbf{Y}, \mathbf{\Theta})}{q(\mathbf{\Theta})} \text{d}\mathbf{\Theta}
	\\
	&= \text{log} \E_{q(\mathbf{\Theta})} \bigg[ \frac{p(\mathbf{Y}, \mathbf{\Theta})}{q(\mathbf{\Theta})} \bigg]
	\\
	&\geq \E_{q(\mathbf{\Theta})} \bigg[ \text{log} \frac{p(\mathbf{Y}, \mathbf{\Theta})}{q(\mathbf{\Theta})} \bigg]
\end{align}
#+END_EXPORT

The bound becomes exact if $q(\mathbf{\Theta}) = p(\mathbf{\Theta} | \mathbf{Y})$. The true posterior is intractable but the variational posterior $q(\mathbf{\Theta})$ must be constrained (so that it is tractable). It is common to factorise $q(\mathbf{\Theta})$ with respect to groups of elements belonging to parameter set $\mathbf{\Theta}$, this is known as mean field approximation.




**** Sparse Gaussian Processes for Regression label:sec-sparse-gps

Consider another Gaussian density $q(\u)$ over the vector $\u$, as well as $p(\u)$, given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gaussian-marginal-q}
q(\u) &= \mathcal{N} \left(\u \mid \mathbf{m}_{\u}, \mathbf{S}_{\u\u} \right)
\end{align}
#+END_EXPORT
with mean $\mathbf{m}_{\u}$ and covariance $\mathbf{S}_{\u\u}$.
The marginal distribution over $\f$ can be obtained by conditioning on $\u$ and marginalising w.r.t. this
new distribution $q(\f) = \int p(\f \mid \u) q(\u) \text{d} \u$.
This marginal distribution is also Gaussian,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gaussian-conditional-q}
\f \mid \u &\sim \mathcal{N} \left(\bm\mu_{\f} + \bm\Sigma_{\f\u} \bm\Sigma_{\u\u}^{-1}(\mathbf{m}_{\u} - \bm\mu_{\u}), \bm\Sigma_{\f\f} - \bm\Sigma_{\f\u}
\bm\Sigma_{\u\u}^{-1}
\left(\bm\Sigma_{\u\u} - \mathbf{S}_{\u\u} \right)
\bm\Sigma_{\u\u}^{-1}
\bm\Sigma_{\u\f} \right).
\end{align}
#+END_EXPORT
Replacing $q(\u)$ with $p(\u)$ would recover the original marginal distribution $p(\f)$ with
mean $\bm\mu_{\f}$ and covariance $\bm\Sigma_{\f}$, as expected from Eq. ref:eq-gaussian-marginal.


Consider a set of $N$ random variable with a joint Gaussian distribution,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-prior}
\f \sim \mathcal{N}(\mathbf{f} \mid \bm\mu, \bm\Sigma_{\f\f})
= (2\pi)^{-\frac{n}{2}} -\K_{ff}^{-\frac{1}{2}}\exp\left( - \frac{1}{2} \mathbf{f}^T \K_{ff}^{-1} \mathbf{F} \right)
\end{equation}
#+END_EXPORT




The prior distribution over $\mathbf{f}$ is then given by,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-prior}
p(\mathbf{f} \mid \mathbf{X}) = \mathcal{N}(\mathbf{f} \mid \mu(\mathbf{X}), k(\mathbf{X}, \mathbf{X}))
= (2\pi)^{-\frac{n}{2}} -\K_{ff}^{-\frac{1}{2}}\exp\left( - \frac{1}{2} \mathbf{f}^T \K_{ff}^{-1} \mathbf{F} \right)
\end{equation}
#+END_EXPORT


This work focuses on inducing point methods citep:snelsonSparse2005a, where the latent variables
are augmented with inducing input-output pairs known as inducing "inputs" $\Z \in \inputDomain$, and inducing
"variables" $\uF = f(\Z)$.
Introducing a set of $M$ inducing points from the same GP prior
$p(\uF \mid \Z) &= \mathcal{N}(\hat{\mathbf{f}} \mid \mu(\Z), k(\Z, \Z))$
can reduce the computational cost
if $M < N$.
Note that these inducing variables $\uF$ are jointly gaussian with the latent function values $\F$,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-spare-gp-joint}
p(\mathbf{f}, \uF \mid \mathbf{X}) =\ &\mathcal{N}\left(
\left[\begin{array}{c}
      \mathbf{f} \\
      \uF
\end{array}\right]
      \mid \bm{0} ,
\left[\begin{array}{cc}
      \mathbf{K}_{nn} & \mathbf{K}_{nm} \\
      \mathbf{K}_{nm}^{T} & \mathbf{K}_{mm}
 \end{array}\right]\right).
\end{align*}
#+END_EXPORT
where $\K_{mm}$ is the covariance function evaluated between all inducing inputs $\Z$, and
$\K_{nm}$ is the covariance function evaluated between the data inputs $\mathbf{X}$ and the inducing inputs $\Z$.
This joint distribution can be re-written using the properties of multivariate normal distributions as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-joint-cond}
p(\mathbf{f}, \uF \mid \mathbf{X}) &= p(\mathbf{f} \mid \uF) p(\uF \mid \mathbf{X}) \\
&= \mathcal{N}(\mathbf{f} \mid \mathbf{K}_{nm} \mathbf{K}_{mm}^{-1} \uF, \mathbf{K}_{nn} - \mathbf{Q}_{nn})
\mathcal{N}(\uF \mid \mathbf{0}, \mathbf{K}_{mm})
\end{align}
#+END_EXPORT
where $\mathbf{Q}_{nn} = \mathbf{K}_{nm}\mathbf{K}_{mm}^{-1}\mathbf{K}_{nm}^{T}$.
The full joint distribution is then given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-full-joint}
p(\mathbf{y}, \mathbf{f}, \uF \mid \mathbf{X}) &=  p(\mathbf{y} \mid \mathbf{f}) p(\mathbf{f} \mid \uF) p(\uF \mid \mathbf{X})
\end{align}
#+END_EXPORT
Computationally efficient inference is obtained by approximating the integration over $\mathbf{f}$.
\marginpar{FITC}
The popular Fully Independent Training Conditional (FITC) method (in the case of a Gaussian likelihood)
is obtained via the factorisation: $p(\mathbf{y} \mid \uF) \approx \prod^{N}_{n=1} p(y_{n} \mid \uF)$.
To obtain a variational approximation Jensen's inequality is first used to bound this conditional,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-bound-cond}
\text{log}p(\mathbf{y}\mid\uF) \geq \E_{p(\F \mid \uF)} \left[ \text{log} p(\mathbf{y} \mid \F) \right] := \mathcal{L}_{1}.
\end{align}
#+END_EXPORT
This is the standard bound shown in cite:hensmanGaussian2013 ($\mathcal{L}_{1}$ defined in Eq. 1).
In cite:titsiasVariational2009 this bound is then substituted into the marginal likelihood
to obtain a tractable lower bound,
\marginpar{Titsias' bound}
#+BEGIN_EXPORT latex
\begin{align}
\text{log} p(\mathbf{y} \mid \mathbf{X})
&= \text{log} \E_{p(\uF \mid \mathbf{X})} \left[ p(\mathbf{y} \mid \uF) \right] \\
&\geq \text{log} \E_{p(\uF \mid \mathbf{X})} \left[ \text{exp}(\mathcal{L}_{1}) \right] := \mathcal{L}_{\text{titsias}} \label{eq-titsias-bound-2} \\
&= \text{log}\mathcal{N}(\mathbf{y} \mid \mathbf{0}, \mathbf{Q}_{nn} + \sigma^{2} \mathbf{I})
- \frac{1}{2\sigma^2}\text{tr}(\K_{nn} - \mathbf{Q}_{nn}) \label{eq-titsias-bound},
\end{align}
#+END_EXPORT
where $\sigma^2$ is the noise variance associated with the Gaussian likelihood.
The bound in Eq. ref:eq-titsias-bound becomes exact (i.e. recovers the true log marginal likelihood)
when the inducing inputs $\Z$ equal the
data inputs $\mathbf{X}$ as $\K_{nm}=\K_{mm}=\K_{nn}$ so $\mathbf{Q}_{nn}=\K_{nn}$.

cite:hensmanGaussian2013 noted that this bound has complexity $O(NM^{2})$ so
introduced additional variational parameters to achieve a more computationally scalable bound
with complexity $O(M^{3})$.
Instead of collapsing the inducing points in Eq. ref:eq-titsias-bound
they explicitly represent them as a variational distribution,
#+BEGIN_EXPORT latex
\begin{align} \label{eq--inducing-dist}
q(\uF) = \mathcal{N}\left(\uF \mid \mathbf{m}, \mathbf{S} \right),
\end{align}
#+END_EXPORT
which they use to lower bound the marginal likelihood $p(\mathbf{y} \mid \mathbf{X})$ by
\marginpar{Hensman's bound}
further bounding $\mathcal{L}_{\text{titsias}}$,
#+BEGIN_EXPORT latex
\begin{align}
\mathcal{L}_{\text{titsias}} &= \text{log} \E_{p(\uF \mid \mathbf{X})} \left[ \text{exp}(\mathcal{L}_{1}) \right] \\
&\geq \E_{q(\uF)} \left[ \mathcal{L}_{1} \right]
- \text{KL}\left[q(\uF) \mid\mid p(\uF \mid \mathbf{X})\right] \\
&\geq \E_{q(\F)} \left[ p(\mathbf{y} \mid \F) \right]
- \text{KL}\left[q(\uF) \mid\mid p(\uF \mid \mathbf{X})\right] := \mathcal{L}_{\text{hensman}}, \label{eq-hensman-bound}
\end{align}
#+END_EXPORT
where $q(\F) = \int p(\F \mid \uF) q(\uF) \text{d} \uF$.
This is equivalent to $\mathcal{L}_{3}$ defined in Eq. 4 cite:hensmanGaussian2013.
If the likelihood factorizes across data $p(\mathbf{y} \mid \F) = \prod_{n=1}^{N} p(y_{n} \mid f_{n})$
only the marginals of $q(\F)$ are needed to calculate Eq. ref:eq-hensman-bound,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-bound-3-fact}
\mathcal{L}_{\text{hensman}}
= \sum_{n=1}^{N}\E_{q(f_{n})} \left[ \text{log} p(y_{n} \mid f_{n}) \right]
- \text{KL}\left[q(\uF) \mid\mid p(\uF \mid \mathbf{X}) \right].
\end{align}
#+END_EXPORT
Importantly, this bound is written as a sum over input-output pairs which has induced the necessary conditions
to preform stochastic gradient methods on $q(\uF)$.


**** Sparse Gaussian Processes :noexport:
A GP cite:edwardrasmussenGaussian2010 is a distribution over functions $f : \R^{D_f} \rightarrow \R$
fully defined by a mean function $\mu(\cdot)$ and a covariance function $k(\cdot, \cdot)$.
For a given set of inputs  from the
functions domain $\mathbf{X} = \{ \mathbf{x}_1, \ldots, \mathbf{x}_N \}$ the associated function values
$\mathbf{f} = \{f(\mathbf{x}_1), \ldots, f(\mathbf{x}_N) \}$
are jointly Gaussian,
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-gp-prior}
p(\mathbf{f} \mid \mathbf{X}) = \mathcal{N}(\mathbf{f} \mid \bm\mu_{\mathbf{X}}, \mathbf{K}_{\mathbf{X}, \mathbf{X}})
\end{equation}
#+END_EXPORT
where $\bm\mu_{\mathbf{X}}= \mu(\mathbf{X})$ is the mean vector, and
$\mathbf{K}_{\mathbf{X},\mathbf{X}} = k(\mathbf{X}, \mathbf{X})$ is the covariance function evaluated
between the inputs $\mathbf{X}$.
In this work, the squared exponential covariance function with Automatic Relevance Determination
is used for all GPs.
The distribution over the function value $f_*$ at a new input $\mathbf{x}_*$ (i.e. to make a prediction)
is given by the conditional,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-gp-prediction}
p(f_{*} \mid \mathbf{x}_*, \mathbf{f}, \mathbf{X}) &= \mathcal{N}(f_* \mid \mu, \sigma^2) \numberthis \\
\mu &= \mu_{\mathbf{x}_*} + \mathbf{k}_{\mathbf{x}_*, \mathbf{X}} \mathbf{K}_{\mathbf{X}, \mathbf{X}}^{-1} (\mathbf{f} - \bm\mu_{\mathbf{X}}) \\
\sigma^2 &= k_{\mathbf{x}_*, \mathbf{x}_*} - \mathbf{k}_{\mathbf{x}_*, \mathbf{X}} \mathbf{K}_{\mathbf{X}, \mathbf{X}}^{-1} \mathbf{k}_{\mathbf{X}, \mathbf{x}_*}.
\end{align*}
#+END_EXPORT
The conditional in Eq. ref:eq-gp-prediction is computationally expensive due to conditioning
on all of the training data $\mathbf{X}, \mathbf{f}$.
Introducing a set of $M$ inducing points from the same GP prior can reduce the computational cost
if $M < N$.
The inducing inputs are denoted $\bm\xi$ and outputs as
$\hat{\mathbf{f}} = f(\bm\xi)$.
The inducing variables
$\hat{\mathbf{f}}$ are jointly Gaussian with the latent function values $\mathbf{f}$.
The GP predictive distribution can be approximated by conditioning on this
smaller set of inducing variables,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sparse-gp-prediction}
p({f}_{*} \mid \mathbf{x}_*, \mathbf{f}, \mathbf{X}) &\approx
p({f}_* \mid \mathbf{x}_*, \hat{\mathbf{f}}, \bm\xi) \\
p(\hat{\mathbf{f}}) &= \mathcal{N}(\hat{\mathbf{f}} \mid \mathbf{m}, \mathbf{S}).
\end{align}
#+END_EXPORT
The approximation becomes exact when the
inducing variables $\hat{\mathbf{f}}$ are a sufficient statistic
for the latent function values $\mathbf{f}$ cite:Titsias2009.
# The predicted latent function values are then mutually independent given the inducing variables.
# A central assumption is that given enough well
# placed inducing variables $\hat{\mathbf{f}}$ they are a
# sufficient statistic for the latent function values $\mathbf{f}$.

*** Learning in Multimodal Dynamical Systems
From deep structured mixture of expets

products of experts

Naive-Local-Experts (NLE) cite:trespBayesian2000a,vasudevanGaussian2009a

Products-of-Experts (PoE) cite:cohenHealing2020s

Generalised  cite:gaddEnriched2020


mixture of experts

Gaussian process state-space models
state-space models
cite:doerrProbabilistic2018,schonSystem2011
cite:eleftheriadisIdentification2017
**** Mixtures of Gaussian Process Experts
Tresp

infinite mixtures

DAGP

pros and cons of different gating networks

**** Our Motivations
Motivated by obtaining well-calibrated variance's throughout the model
placing priors on the gating network
**** Mixtures of Gaussian Process Experts
Gaussian processes (GPs) are the state-of-the art approach for Bayesian nonparametric regression.
However, they suffer from two important limitations.
Firstly, the covariance function is commonly assumed to be stationary
due to the challenge of parameterising them to be non-stationary.
This limits their modelling flexibility.
For example, if the function has a discontinuity due to different underlying lengthscales
in different parts of the input space then a stationary covariance function will not be adequate.
Similarly, if the observations are subject to different noise variances in different regions
of the input space then conventional homoscedastic regression will not suffice.
Secondly, GPs cannot model multimodal predictive distributions,
i.e. where there are multiple
regions of high probability mass with regions of smaller probability mass in between.
GP regression with a Gaussian likelihood models a Gaussian predictive distribution
that is not capable of modelling such multimodal distributions.
Using any likelihood but a Gaussian requires approximate inference techniques which
are usually accompanied by increased computational cost.
# A motivating factor for adopting GP methods is their associated uncertainty quantification.
# #+begin_export latex
# \hl{Standard GP regression (with a Gaussian likelihood) models a Gaussian predictive distribution
# that is not capable of modelling such multimodal distributions.
# Using any likelihood but a Gaussian also results in inference no longer being
# analytically tractable. These likelihoods require approximate inference techniques which
# are usually accompanied by computational issues.}
# #+end_export
# We can think of the first limitation as fitting the predictive mean to the observations
# and the second with correctly quantifying uncertainty in the predictive posterior.

Mixture models have been proposed that
for a set of input $\mathbf{x}$ and output $\mathbf{y}$ observations
can model a multimodal distribution over the output
$p(\mathbf{y} | \mathbf{x}) = \sum_{k=1}^K \pi_k\ p(\mathbf{y} | \alpha=k, \mathbf{x})$.
The predictive distribution $p(\mathbf{y} | \mathbf{x})$ consists of
$k$ mixture components $p(\mathbf{y} | \alpha=k, \mathbf{x})$ that are weighted according to
the mixing coefficients $\pi_k$.
The mixture components may take the form of any distribution, for
example, Bernoulli or Gaussian.
Mixture models assume that each observation belongs to one of the components and then try to infer the
distribution of each component separately.
The capability of these models can be further extended by allowing the mixing coefficients themselves
to be a function of the input variable $x$.
This was introduced by cite:Jacobs1991 in the mixture of experts (ME) model where the mixing coefficients
are known as gating functions $\pi_k(\mathbf{x}) = p(\alpha=k | \mathbf{x})$
(and collectively the gating network).
The individual component densities $p(\mathbf{y} | \alpha=k, \mathbf{x})$ are then referred to as experts
because at different regions in the input space different components are responsible for predicting.
The gating network is governed by an expert indicator variable $\alpha_n \in \{1, ..., K\}$
that assigns each observation to one of the $K$ experts.
# are given by $\pi_k(\mathbf{x}) = p(\alpha=k | \mathbf{x})$ and
# referred to as gating functions (or collectively the gating network).
# This results in the predictive distribution
# $p(\mathbf{y} | \mathbf{x}) = \sum_{k=1}^K \pi_k(\mathbf{x}) p(\mathbf{y} | \alpha=k, \mathbf{x})$.
# The role of the gating network is to indicate which expert is most likely responsible for generating
# the data in a given region of the input space.

Modelling the experts as GPs gives rise to a class of powerful models known as
mixture of GP experts (MoGPE).
These models address both of the limitations that we discussed.
They can model multimodal predictive distributions as they model a mixture of distributions
over the outputs, usually a Gaussian mixture in the regression setting (cite:Tresp,Rasmussen2002).
They are able to model non-stationary functions as
each expert can learn different lengthscales, noise variances etc and the gating
network can turn each one "on" and "off" in different regions of the input space.
Many approaches to MoGPE have been proposed and in general the key differences between them are
the formulation of the gating network
and their inference algorithms (and associated approximations) cite:Yuksel2012.
The original MoGPE work by cite:Tresp proposed a gating network as a softmax of $K$ GPs
(bottom left plot in Figure [[ref:gating_network_comparison]]).
An EM inference scheme is proposed, which assuming there are $N$ observations requires
$\mathcal{O}(3KN^3)$ computations per iteration.
# The original MoGPE work by cite:Tresp proposed a gating network resembling a
# GP classification model (bottom left plot in Figure [[ref:gating_network_comparison]]).
# An EM inference scheme is proposed, requiring $\mathcal{O}(3KN^3)$ computations per iteration,
# assuming that there are $N$ observations.

Although this gating function divides up the input space cite:Rasmussen2002 argue that
data not assigned to a GP expert will lead to bias near the boundaries.
Instead cite:Rasmussen2002 formulate the gating network in terms of conditional
distributions on the expert indicator variable, on which they place an input-dependent
Dirichlet process prior.
cite:Meeds2005 proposed an alternative infinite MoGPE similar to cite:Rasmussen2002 except
that they specify a full generative model over the input and output space $p(\mathbf{y}, \mathbf{x})$
as opposed to just a
conditional model $p(\mathbf{y} | \mathbf{x})$.
Each experts inputs are now Gaussian distributed
(middle top plot of Figure [[ref:gating_network_comparison]]) giving rise to the gating
network in the middle bottom plot of Figure [[ref:gating_network_comparison]].
This gating network is not capable of turning an expert "on" in multiple regions
of the input space. It will instead generate a new cluster and assign it a new expert.
These approaches reduce the computational burden by associating each expert with a
subset of the observations but rely on Markov Chain Monte Carlo (MCMC) inference schemes.
# #+NAME: gating_network_comparison
# #+ATTR_LATEX: :width 0.7\textwidth :placement [h] :center t
# #+caption: Comparison of different gating networks (one per column).
# #+caption: The bottom plots show the expert mixing probabilities $p(\alpha=k|x)$
# #+caption: and the top plots show any associated distributions over the inputs.
# #+caption: The left and right regions (shaded green) are subsets of the domain where expert
# #+caption: one has generated the observations (observations are not shown) and the middle region (shaded blue) where expert two has.
# #+caption: The bottom left plot shows the mixing probabilities of a gating network based on GPs.
# #+caption: The middle top plot shows three separate Gaussian pdfs representing clustering of the inputs
# #+caption: (we have introduced a cluster indicator variable $z$).
# #+caption: The middle bottom plot shows the associated expert mixing probabilities (assuming $z=\alpha$).
# #+caption: In the right top plot we have assumed that there are only two experts (with $z\neq \alpha$) and
# #+caption: marginalised the cluster indicator variable from the plot to the left - leading to
# #+caption: a Gaussian mixture over the inputs.
# #+caption: The plot below shows the resulting mixing probabilities.
# [[file:images/gating-network-comparison-2by3-cropped.pdf]]
cite:Yuan proposed a MoGPE with a variational inference scheme which is much
faster than using MCMC.
They assume that the expert indicators $\alpha$ are generated by a Dirichlet distribution and
that given the expert indicator the inputs follow a Gaussian mixture model (GMM), i.e. they
have introduced a cluster indicator variable $z$.
This contrasts earlier approaches that let the expert indicator act as the input cluster
indicator.
This is illustrated in the right hand plots of Figure [[ref:gating_network_comparison]] which
show two Gaussian mixtures over the inputs (one for each expert) and
the resulting expert mixing probabilities.
Introducing the separate cluster indicator variable has given rise to a
gating network that can turn a single expert "on" in multiple regions of the input space.
However, it does not provide a handle for encoding prior information like the GP based gating network.
# This resembles two separate mixture models, one over the inputs and one over the outputs.

We note that mixture models are inherently unidentifiable as
different combinations of mixture components
and mixing coefficients can generate the same predictive distributions.
The gating network can be considered a handle for encoding prior knowledge that can be used to constrain
the set of admissible functions.
This can improve identifiability and lead to learned representations that reflect
our understanding of the system. The simplest case being reordering the experts.
Figure ref:gating_network_comparison provides a visual comparison of
different gating network formulations,
providing intuition for how they can differ in restricting the set of admissible functions.

# Modelling the relationship between the  input space and the expert indicator variable (gating network)

# with GPs enables
# us to efficiently capture the dependencies through the choice of GP prior.
# Modelling the gating network with GPs enables
# us to efficiently capture the dependencies through the choice of mean and covariance functions.
Modelling the gating network with GPs enables us
to encode informative prior knowledge through the choice of mean and covariance functions.
For example, adopting a squared exponential covariance function would encode prior belief that
the mixing probabilities should vary smoothly across the input space.
If modelling a dynamical system with oscillatory behaviour (e.g. an engine)
a periodic kernel could be adopted.
Prior knowledge of the mixing probability values can be encoded through
the choice of mean function.
We may also be interested in exploiting techniques from differential geometry
e.g. finding probabilistic geodesics (cite:Tosi2014) on the gating network.
Again, our choice of covariance function can be used to encode how differentiable
the gating network should be.
Learning better representations will also improve the models
ability to extrapolate as it will be more true to the underlying system.
# Again, our choice of covariance function can encode how differentiable
# our gating network should be.

# We may also be interested in harnessing the structure learnt by the gating network,
# for example, finding probabilistic geodesics (cite:Tosi2014).
# This would require a differential covariance function to be used.
# Learning better representations will also improve the models
# ability to extrapolate as it will be more true to the underlying system.
# For example, if one wanted to exploit techniques from differential geometry (e.g. finding geodesics)
# they could utilise a differentiable covariance function.
# For example, if one wanted to exploit techniques from differential geometry (e.g. finding geodesics)
# they could utilise a differentiable covariance function.

# The extension from the middle plot of Figure ref:gating_network_comparison
# to modelling a GMM over the inputs (bottom plot) leads to a
# more powerful gating network from a modelling perspective - as it can turn a single expert "on"
# Introducing the separate cluster indicator variable (middle plot of
# Figure ref:gating_network_comparison to bottom plot) gives rise to a
# gating network that can turn a single expert "on" in multiple regions of the input space.
# However, it does not provide a handle for encoding prior information like a GP based gating network.
# The formulation of the gating network is commonly motivated by improving
# the computational complexity of the inference scheme.
# As such, we consider our method (GP based gating network) as trading in the computational
# benefits of gating networks based on GMMs
# (bottom plots of Figure ref:gating_network_comparison)
# for the ability to improve identifiability through informative gating network priors.
# \todo{Insert reference to periodic gating function in results once it's added.}


We can also consider the implications of the mentioned gating networks from an inference perspective.
It is known that developing inference algorithms for propagating uncertain (Gaussian) inputs
through GPs is itself a challenging problem (see cite:Ustyuzhaninov2019).
Therefore, developing a scalable inference algorithm to propagate inputs that are distributed according to
a Gaussian mixture will likely lead to valuable information loss.
Theoretically the approaches by cite:Rasmussen2002,Meeds2005 are able to achieve very
accurate results but their inference relies on MCMC sampling methods,
which can be slow to converge.
# Previous work that formulated a gating network based on GPs cite:Tresp used an EM inference scheme
# which decouples the learning of the gating network and the experts.
# The main contributions of this paper are twofold,
# \todo{What is a true MoE model???}
# 1. We re-formulate a gating network based on GPs to a) improve indentifiability and b) achieve a true mixture of experts model, i.e. not split the observations between experts,
# 2. We derive a variational lower bound which improves the complexity issues associated with inference when adopting a GP based gating network.
#    - Motivated by learning representations of dynamical systems with two regimes we instantiate
#      the model with two experts as it is  a special case where the gating network can be calculated
#      in closed form.
#      We seek to learn an operatable mode with one expert and explain away the un-operatable mode
#      with the other. With this representation the gating network indicates which regions of the
#      input space correspond to the operatable mode and provides a convenient space for planning.
# The GMM formulation of the gating network is motivated by improving
# the computational complexity of the inference scheme.
# We consider our approach as trading in the computational benefits of a GMM gating network
# for the ability to improve identifiability by placing informative GP priors on the gating network.
We consider our approach as trading in the computational benefits that can be obtained through
the formulation of the gating network
for the ability to improve identifiability with informative GP priors.
The main contributions of this paper are twofold. Firstly, we re-formulate a gating network
based on GPs to improve indentifiability.
Secondly, we derive a variational lower bound
which improves on the complexity issues associated with inference when adopting such a gating network.
Motivated by learning representations of dynamical systems with two regimes we instantiate
the model with two experts as it is  a special case where the gating network can be calculated
in closed form.
This is because we seek to learn an operatable mode with one expert and explain away the un-operatable mode
with the other. This results
in the gating network indicating which regions of the
input space are operatable, providing a convenient space for planning.
# This is because we seek to learn an operatable mode with one expert and explain away the un-operatable mode
# with the other.
# The gating network then indicates which regions of the
# input space are operatable, providing a convenient space for planning.
# with the other. With this representation the gating network indicates which regions of the
# input space correspond to the operatable mode which provides a convenient space for planning.

The remainder of the paper is organised as follows. We first introduce our model
in Section ref:sec-modelling
and then derive our variational lower
bound in Section ref:sec-variational-approximation.
In Section ref:sec-model-validation we test our method on an artificial data set where we show
the benefits of adopting informative covariance functions in the gating network.
We then test our model on the motorcycle data set and compare it to a sparse variational GP.
# The remainder of the paper is organised as follows. First we introduce our generative model
# in Section ref:sec-modelling where we compare our marginal likelihood to the literature.
# We then derive our variational lower
# bound in Section ref:sec-variational-approximation and detail how we optimise it and make
# predictions.
# In Section ref:sec-model-validation we test our model on an artificial data set where we show
# the benefits of adopting informative covariance functions in the gating network GP.
# We then test our model on the motorcycle data set and compare its performance
# to a sparse variational GP.

** Optimal Control
*** maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\costFunc}{\ensuremath{c}}
\newcommand{\constraintsFunc}{\ensuremath{g}}

\newcommand{\terminalCostFunc}{\ensuremath{C_T}}
\newcommand{\integralCostFunc}{\ensuremath{C}}

\newcommand{\stateTraj}{\ensuremath{\bar{\state}}}
\newcommand{\controlTraj}{\ensuremath{\bar{\control}}}

\newcommand{\policySpace}{\ensuremath{\Pi}}
\newcommand{\policy}{\ensuremath{\pi}}
#+END_EXPORT

*** intro :ignore:
The problem of (deterministic) optimal control can be summarised as follows:
#+BEGIN_EXPORT latex
\begin{myquote}
Given a dynamical system (Eq. \ref{eq-unimodal-dynamics-cont}) with
states, $\state (t) \in \stateDomain \subseteq \R^{\StateDim}$, and
controls, $\control(t) \in \controlDomain \subseteq \R^{\ControlDim}$,
find a set of controls, $\controlTraj$,
over a time period, $t \in [t_0, t_f]$, that optimises an objective function $J$.
The objective function might be formulated to solve a particular task, or to make the dynamical
system behave in a certain way.
Typically this objective function is given by,
\begin{align} \label{eq-objective}
J \defeq \terminalCostFunc(\state(t_{f}), t_{f})
+ \int^{t_{f}}_{t_{0}}
\integralCostFunc(\state(t), \control(t), t) \text{d}t,
\end{align}
which consists of two terms:
\begin{itemize}
\item a terminal cost $\terminalCostFunc : \stateDomain \times \R \rightarrow \R$,
\item an integral cost (or Lagrangian) $\integralCostFunc : \stateDomain \times \controlDomain \times \R \rightarrow \R$.
\end{itemize}
\end{myquote}
#+END_EXPORT
In practice, the system is often corrupted by process noise, or is not completely known, so the resulting
dynamical system is actually stochastic.
As such, the definition above must be adapted for stochastic systems.
In stochastic systems, a control sequence is not associated with a specific
trajectory, but rather a distribution over trajectories.
The resulting problem is known as stochastic optimal control (SOC).

This section reviews the relevant background on SOC,
in particular, trajectory optimisation algorithms for controlling
stochastic systems that have been learned from observations.

*** Stochastic Optimal Control
Let us formalise the stochastic optimal control problem here.
Given a stochastic, fully-observed, nonlinear system,
with transition dynamics,
${\dynamicsFunc: \stateDomain \times \controlDomain \rightarrow \stateDomain}$,
SOC seeks to find the control sequence (trajectory), $\controlTraj$, over a horizon,
$\TimeInd$, that minimises a cost
function, ${\integralCostFunc: \stateDomain \times \controlDomain \rightarrow \R}$,
in expectation over the stochastic dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-soc-problem-statement}
\min_{\controlTraj} &\E_{\state(\cdot), \control(\cdot) \mid \state(0), \policy}\left[ \terminalCostFunc(\state(T))
+ \int_{t_0}^{T} \integralCostFunc(\state(t), \control(t), t) \right] \\
\text{s.t.} \quad &\state(t) = \dynamicsFunc(\state(t), \control(t)) \text{d}t + \epsilon_\modeInd,
\quad \epsilon \sim \mathcal{N}(\mathbf{0}, \Sigma_{\epsilon})
\end{align}
#+END_EXPORT
*Policy Space* The policy space, $\policySpace$, defines the set of policies which optimisation is performed over,
i.e. it is the optimisation domain.
In this dissertation, a policy, $\policy$, shall be a conditional distribution over $\controlDomain$.
In (deterministic) optimal control the policy space is usually given by open loop policies, i.e.
only conditioned on time $\policy(\control \mid t)$.
In contrast, SOC (and RL) are mainly interested in feedback policies, where the controls are conditioned on both time and
state, i.e. $\policy(\control(t) \mid t, \state(t))$
\footnote{for notational conciseness dependency on time is assumed from here on, i.e.
$\policy(\control(t) \mid t, \state(t) = \policy(\control(t) \mid \state(t))$}.
A special case are deterministic policies, given by $\policy(\control \mid \state) = \delta_{\control=f(\state)}$,
where $\delta$ is the dirac delta distribution and $f$ is a function.

*** Overview
#+BEGIN_EXPORT latex
\begin{figure}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}
  %[scale=.7,auto=center,every node/.style={fill=blue!20}] % here, node/.style is the style pre-defined, that will be the default layout of all the nodes. You can also create different forms for different nodes.
  %\tikzstyle{highlight}  = [fill=red!20, color=red!60]
  [
    every node/.style={rectangle, fill=blue!20, very thick, minimum size=7mm},
    highlight/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},
    discretegeneration/.style={rectangle, draw=red!60, fill=blude!20, very thick, minimum size=7mm},
  ]

  \node (oc) at (1,2) {Optimal Control};
  \node[highlight, below=of oc, xshift=-4.8cm] (ol) {Open Loop};
  \node[highlight, below=of oc, xshift=4.8cm] (cl) {Closed Loop};

  \node[highlight, below=of oc] (mpc) {MPC};

  \node[highlight, below=of ol, xshift=-2.6cm] (i) {Indirect Methods};
  \node[highlight, below=of ol, xshift=2.6cm] (d) {Direct Methods};

  \node[highlight, below=of d] (gd) {GD};

  \node[highlight, below=of cl, xshift=-2.6cm] (dp) {Dynamic Programming};
  \node[below=of cl, xshift=2.6cm] (hjb) {HJB/HJI};

  \node[discretegeneration, below=of dp] (ilqr) {iLQR};
  \node[left=of ilqr] (lqr) {LQR};
  \node[right=of ilqr] (ddp) {DDP};

  \draw[->] (oc) -- (cl);
  \draw[->] (oc) -- (ol);
  \draw[->] (ol) -- (i);
  \draw[->] (ol) -- (d);
  \draw[->] (cl) -- (dp);
  \draw[->] (cl) -- (hjb);
  \draw[->] (ol) -- (mpc);
  \draw[->] (mpc) -- (cl);
  \draw[->] (dp) -- (ilqr);
  \draw[->] (dp) -- (lqr);
  \draw[->] (dp) -- (ddp);
\end{tikzpicture}
}
\caption{Roadmap of optimal control where red indicates applicability in discrete-time setting.}
\label{fig-optimal-control-roadmap}
\end{figure}
#+END_EXPORT

*** Problem Statement :noexport:
Let us formalise the stochastic optimal control problem here.
Given a stochastic, discrete-time, fully-observed, nonlinear system,
with transition dynamics,
${\dynamicsFunc: \stateDomain \times \controlDomain \rightarrow \stateDomain}$,
SOC seeks to find the control sequence (trajectory), $\controlTraj$, over a horizon,
$\TimeInd$, that minimises a cost
function, ${\integralCostFunc: \stateDomain \times \controlDomain \rightarrow \R}$,
in expectation over the stochastic dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-soc-problem-statement}
\min_{\controlTraj} &\E_{\state(\cdot), \control(\cdot) \mid \state(0), \policy}\left[ \terminalCostFunc(\state_\TimeInd)
+ \sum_{\timeInd=1}^{\TimeInd-1} \integralCostFunc(\state_\timeInd, \control_\timeInd) \right] \\
\text{s.t.} \quad &\state_{\timeInd+1} = \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \epsilon_\modeInd,
\quad \epsilon \sim \mathcal{N}(\mathbf{0}, \Sigma_{\epsilon})
\end{align}
#+END_EXPORT

*** Dynamic Programming
*** Model-based Control
- robust optimal control
- stochastic optimal control
*** Trajectory Optimisation
**** maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{\MakeUppercase{\timeInd}}}

\newcommand{\stateDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\controlDomain}{\ensuremath{\mathcal{U}}}

\newcommand{\dynamicsFunc}{\ensuremath{f}}
\newcommand{\costFunc}{\ensuremath{c}}
\newcommand{\constraintsFunc}{\ensuremath{g}}

\newcommand{\stateTraj}{\ensuremath{\bar{\state}}}
\newcommand{\controlTraj}{\ensuremath{\bar{\control}}}
#+END_EXPORT

**** intro :ignore:
Formally, trajectory optimisation seeks to find the state and
control trajectories,
$\stateTraj = \{\state(\timeInd) \in \stateDomain \mid \forall \timeInd \in [\timeInd_0, \timeInd_f] \}$,
and
$\controlTraj = \{\control(\timeInd) \in \controlDomain \mid \forall \timeInd \in [\timeInd_0, \timeInd_f] \}$,
respectively,
that minimise some cost function $\costFunc$, whilst satisfying constraints $\constraintsFunc$,
and boundary conditions.
The trajectory optimisation problem is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-traj-opt}
\min_{\state(t), \control(t)} &\int_{t_0}^{t_f} \costFunc(\state(t), \control(t)) \text{d}t \quad \forall t \\
\text{s.t. }&\text{Eq. \ref{eq-unimodal-dynamics-cont}}  \\
&\constraintsFunc(\mathbf{x}(t)) \leq 0 \quad \forall t \\
&\state(t_0) = \state_0  \quad \state(t_f) = \state_f
\end{align}
#+END_EXPORT
Solving this problem is often computationally expensive and
is an active area of research.
Many (approximate) solutions have been introduced in the literature,
that seek to balance the computational complexity
(for real-time control) and accuracy trade-off differently
cite:bettsSurvey1998.
This section briefly reviews several approaches to trajectory
optimisation that can be used with learned dynamics models.
It then provides an in-depth review of the control-as-inference
framework, which is used in Chapter ref:chap-traj-opt.

*Iterative Linear Quadratic Regulator (iLQR)*
can be used to generate trajectories for non-linear systems by
iteratively approximating
the dynamics to be linear around a nominal trajectory and optimising for the controls.
iLQR works well for quadratic cost functions but can be used with any cost function by approximating the cost
function with a second order Taylor expansion. However, in this case iLQR is susceptible to converging
to terrible (local) optima if the true cost function is highly non-convex.
cite:boedeckerApproximate2014 present a real-time iLQR controller
based on sparse GPs.
cite:rohrProbabilistic2021 propose a novel LQR
controller synthesis for
linearised GP dynamics that yields robust controllers with
respect to a probabilistic stability margin.
In cite:marcoAutomatic2016 they parameterise the weight matrices
$\stateMatrix$ and $\controlMatrix$ with parameters $\theta$
and perform BO (entropy search) on $J(\theta)$. Their method handles model mis-specification by adapting the parameters.

*Iterative Linear Quadratic Gaussian (iLQG)* is an extension of
iLQR to a stochastic setting, where the process
noise is assumed to be Guassian.
cite:mitrovicAdaptive2010 combine learned dynamics models with iLQG to develop an adaptive optimal feedback controller.

*Differential Dynamic Programming (DDP)* iLQR and iLQG  are variants of the classic Differential Dynamic Programming (DDP) algorithm cite:rosenbrockDifferential1972,
the main difference being that only a /first-order/ approximation of the dynamics is used, instead of a /second-order/.
cite:liaoAdvantages1992 show that DDP is comparable to a full Gauss-Newton step (for nonlinear least squares)
over the entire control sequence.
Therefore, care must be taken when either the Hessian isn't positive-definite or the minimum isn't close
and the quadratic model is inaccurate.
\todo{remember improvements to standard iLQR/DDP from cite:tassaSynthesis2012}
DDP solutions provides greater accuracy than iLQG/iLQR but
at the cost
of increased computational time cite:tassaSynthesis2012.
The extension of DDP to stochastic state and controls is addressed in cite:theodorouStochastic2010.
DDP under parametric uncertainty using generalised polynomial chaos cite:aoyamaReceding2021.

cite:panProbabilistic2014 present a probabilistic trajectory optimisation
framework for systems with unknown dynamics called probabilistic
differential dynamic programming (PDDP).
It extends DDP to explicitly account
for uncertainty in dynamics modelled using gaussian processes.
In contrast to typical gradient based policy search methods, PDDP does not
require a policy parameterisation as it learns a locally optimal,
time-varying control policy.
Based on the second-order local approximation of the value function,
PDDP performs Dynamic Programming around a nominal trajectory in Gaussian belief spaces.

*Path Integral*  control is a method for developing optimal controls based on stochastic
sampling of trajectories cite:theodorouGeneralized2010.
This work was originally limited to control affine systems cite:williamsModel2017
but has since been extended to general nonlinear
dynamical systems cite:williamsInformation2017,williamsInformationTheoretic2018.

**** Collocation
**** Trajectory Optimisation as Probabilistic Inference
***** maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\startStateDist}{\ensuremath{p(\state_{1})}}
\newcommand{\transitionDist}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)}}
\newcommand{\trajectoryDist}{\ensuremath{p(\stateControlTraj)}}
\newcommand{\controlDist}{\ensuremath{p(\control_\timeInd \mid \state_\timeInd)}}

\newcommand{\trajectoryVarDist}{\ensuremath{q(\state_{1:\TimeInd}, \control_{1:\TimeInd})}}
\newcommand{\controlTrajVarDist}{\ensuremath{q(\control_{1:\TimeInd} \mid \state_{1:\TimeInd})}}
\newcommand{\controlVarDist}{\ensuremath{q(\control_{\timeInd})}}

\newcommand{\optimalVar}{\ensuremath{\mathcal{O}}}
\newcommand{\monotonicFunc}{\ensuremath{f_{\text{mon}}}}

\newcommand{\optimalProb}{\ensuremath{\Pr(\optimalVar_\timeInd = 1 \mid \state_\timeInd, \control_\timeInd)}}
\newcommand{\optimalDist}{\ensuremath{P(\optimalVar_\timeInd \mid \state_\timeInd, \control_\timeInd)}}

\newcommand{\terminalCostDist}{\ensuremath{\Pr(\optimalVar_\TimeInd=1 \mid \state_\TimeInd)}}

\newcommand{\marginalLikelihood}{\ensuremath{p(\optimalVar_{1:\TimeInd} = 1)}}
\newcommand{\jointDist}{\ensuremath{p(\optimalVar_{1:\TimeInd}=1, \state_{1:\TimeInd}, \control_{1:\TimeInd})}}

\newcommand{\temperature}{\ensuremath{\gamma}}
#+END_EXPORT
***** intro :ignore:
The second trajectory algorihtm optimisation in Chapter ref:chap-traj-opt
builds on top of the control-as-inference framework
citep:toussaintProbabilistic2006,kappenOptimal2013,
which frames optimal control as probabilistic inference.
Translating optimal control to inference can be viewed as a subset of probabilistic numerics
citep:hennigProbabilistic2015, where statistical methods are used to solve numerical problems,
providing uncertainty quantification, regularisation and faster convergence.

Control-as-inference has been considered since the beginning of modern optimal control due to
the concurrent development of LQR, Kalman filtering and LQG citep:kalmanNew1960,hoBayesian1964.
Recent work on control-as-inference stems from path integral cite:kappenOptimal2013,williamsModel2017

cite:rawlikStochastic2013

cite:toussaintRobot2009 formulate trajectory optimisation as
approximate inference and derive a local approximation
where the maximum likelihood solution returns the deterministic optimal trajectory, and the local Gaussian belief
approximation is equivalent to the LQG perturbation model around the optimal trajectory.
cite:levineReinforcement2018 followed a similar approach formulating RL as probabilistic inference
and showing how different approximate inference algorithms instantiated different popular RL algorithms.


Recent work by cite:watsonStochastic2021 has formulated
stochastic optimal control as input estimation

***** Cost Functions as Likelihoods
\newline
To formulate optimal control as probabilistic inference
it is first embedded into a graphical model (see Figure ref:fig-basic-control-graphical-model).
The joint probability model (over a trajectory) is augmented with an
additional variable to encode the notion of cost (or reward)
over the trajectory (see Figure ref:fig-augmented-control-graphical-model).
The new variable is a Bernoulli random variable $\optimalVar_\timeInd \in \{0, 1\}$, that indicates if
time step $\timeInd$ is /optimal/ $\optimalVar_\timeInd=1$, or not /optimal/ $\optimalVar_\timeInd=0$.
The likelihood distribution can then be formulated
by mapping the negative
cost through a monotonically increasing function $\monotonicFunc$,
giving the likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-monotonicOptimalityLikelihood}
\optimalProb &\coloneqq \monotonicFunc( -\costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
A common (and convenient) approach is to formulate
the likelihood using an exponential transform of
the cost. This results in a Boltzmann distribution where the
inverse temperature, $\temperature$, is used to scale the cost,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-exponentialOptimalityLikelihood}
\optimalProb &\propto \exp ( -\temperature\costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
The resulting negative log-likelihood is an affine transformation of the cost, which preserves convexity.
#+BEGIN_EXPORT latex
\todo[inline]{add detials on Cross Entropy Method (CEM) cite:chuaDeep2018 and other methods}
\begin{myquote}
As shown in \cite{okadaVariational2020}, the choice of monotonic function,
$\monotonicFunc$,
leads to the inference algorithm resembling different well-known algorithms.
For example, selecting $\monotonicFunc$ to be the exponential function,
\begin{align} \label{eq-}
\optimalProb &\propto \text{exp}( -\costFunc(\state_\timeInd, \control_\timeInd))
\end{align}
leads to the algorithm resembling Model Predictive Path Integral (MPPI)
\cite{williamsModel2017,williamsInformation2017}, whilst
selecting $\monotonicFunc$ to as,
\begin{align} \label{eq-}
\optimalProb &\propto \mathbb{I}\left[( \costFunc(\state_\timeInd, \control_\timeInd)) \leq \costFunc_{\text{thd}} \right]
\end{align}
recovers an algorihtm resembling the Cross Entropy Method (CEM).
\end{myquote}
#+END_EXPORT

***** Joint Probability Model
\newline
Given the "cost" likelihood
(Eq. ref:eq-monotonicOptimalityLikelihood)
the joint probability for an optimal trajectory (i.e.
for $\optimalVar_\timeInd=1$ for all $\timeInd \in \{1,\ldots,\TimeInd\}$), is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-traj-opt-joint-dist}
\jointDist =
& \startStateDist \underbrace{\terminalCostDist}_{\text{Terminal Cost}}  \\
&\prod_{\timeInd=1}^{\TimeInd-1}
\underbrace{\optimalProb}_{\text{Cost}}
\underbrace{\transitionDist}_{\text{Dynamics}}
\underbrace{\controlDist}_{\text{Controller}} \nonumber
\end{align}
#+END_EXPORT
where $\controlDist$ denotes the controller or policy.
In the case of RL the policy is parameterised with parameters, $\theta$,
whereas the control setting is usually interested in
optimising the states and controls directly.
The object of interest in this model is the distribution over the control, $\control_\timeInd$,
conditioned on the state, $\state_\timeInd$, and all future time steps being optimal,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\control_{\timeInd} \mid \state_{\timeInd}, \optimalVar_{\timeInd:\TimeInd}=1) =
\frac{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)
\controlDist p(\state_\timeInd)}{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd)
p(\state_\timeInd)}.
\end{align}
#+END_EXPORT
The distributions $p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)$
and
$p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd)$,
can be obtained via recursive message passing,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd)
&= p(\state_\timeInd)
\prod_{\timeInd}^{\TimeInd-1}
\int \int
\optimalProb p(\state_{\timeInd+1} \mid \state_{\timeInd}, \control_{\timeInd}) \controlDist
\text{d}\state_{\timeInd+1} \text{d}\control_{\timeInd+1} \nonumber \\
p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd)
&=
\int p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd)
p(\control_\timeInd \mid \state_\timeInd)
\text{d}\control_{\timeInd}.
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo[inline]{Considering process noise in the dynamics separately from the epistemic (parametric) uncertainty,
\begin{align} \label{eq-}
p(\optimalVar_{1:\TimeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})
= \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \optimalDist
p(\state_{\timeInd+1} \mid \dynamicsFunc(\state_\timeInd)) p(\dynamicsFunc(\state_\timeInd) \mid \state_\timeInd, \control_\timeInd)
\controlDist
\end{align}
\begin{align} \label{eq-}
\optimalProb &= \monotonicFunc( -\costFunc(\state_\timeInd, \dynamicsFunc(\state_{\timeInd-1}), \control_\timeInd)) \\
\end{align}
}
#+END_EXPORT

***** graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \begin{subfigure}{.45\textwidth}
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);
    \end{tikzpicture}
    %}
  \caption{Graphical model with states and controls.}
\label{fig-basic-control-graphical-model}
\end{subfigure}
\begin{subfigure}{.45\textwidth}
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[obs, above=of u1] (o1) {$\optimalVar_1$};
      \node[obs, right=of o1] (o2) {$\optimalVar_2$};
      \node[obs, right=of o2] (o3) {$\optimalVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);
    \end{tikzpicture}
    %}
  \caption{Graphical model with optimality variable.}
\label{fig-augmented-control-graphical-model}
\end{subfigure}
\caption{Graphical models of control formulated as inference.}
\label{fig-control-graphical-model}
\end{figure}
#+END_EXPORT

***** Variational Inference
In variational inference the distribution over the latent variables
$p(\state_{1:\TimeInd}, \control_{1:\TimeInd})$
is approximated with another, potentially simpler
(e.g. factorised) distribution $q(\state_{1:\TimeInd}, \control_{1:\TimeInd})$.
In this case, the aim is to approximate the distribution over the optimal policy,
$p(\control_{1:\TimeInd} \mid \state_{1:\TimeInd}, \optimalVar_{1:\TimeInd}=1)$.
Introducing the variational distribution,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\trajectoryVarDist = \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \transitionDist \controlVarDist
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\text{log} \marginalLikelihood
&= \text{log} \E_\trajectoryVarDist \left[ \frac{\jointDist}{\trajectoryVarDist} \right] \\
&= \text{log} \E_\trajectoryVarDist \left[
\frac{\cancel{\startStateDist} \prod_{\timeInd=1}^{\TimeInd} \optimalProb \cancel{\transitionDist} \controlDist}{\cancel{\startStateDist} \prod_{\timeInd=1}^\TimeInd \cancel{\transitionDist} \controlVarDist}
\right] \\
&\geq \E_\trajectoryVarDist \left[
\sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb - \sum_{\timeInd=1}^\TimeInd \text{log}
\frac{\controlVarDist}{\controlDist} \right] \\
&= \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \costFunc(\state_\timeInd, \control_\timeInd)
- \text{log}\controlVarDist\right]
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\text{log} \marginalLikelihood &\geq
 \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb
- \text{log}\controlVarDist\right] \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \text{log} \optimalProb \right]
- \E_{\controlVarDist q(\state_{\timdInd})} \left[ \text{log}\controlVarDist \right] \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
+ \sum_{\timeInd=1}^{\TimeInd} \underbrace{- \E_{\controlVarDist} \left[ \text{log} \controlVarDist \right]}_{\text{Entropy}} \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
+ \sum_{\timeInd=1}^{\TimeInd} \mathcal{H}(\control_{\timeInd})
\end{align}
#+END_EXPORT

** Model-Based Control :noexport:
*** Dynamical Systems :noexport:
**** intro :ignore:
Dynamical systems describe the behaviour of a system over time, $t$, and
are a key component of both control theory and reinforcement learning.
At any given time a dynamical system has a state, which is
represented as a vector of real numbers $\state(t) \in \stateDomain \subseteq \R^D$.
For example, the state of a 2-dimensional robotic system with state comprising of 2D Cartesian coordinates
and an orientation would be represented as $\state(t) = [x(t), y(t), \theta(t)]^{T}$.
The system can be controlled by applying control actions $\mathbf{u}(t) \in \controlDomain \subseteq \R^F$ at any given time $t$.
This work considers continuous-time, continuous-state, nonlinear stochastic dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-unimodal-dynamics-cont}
\stateDerivative(t) &= \dynamicsFunc(\state(t), \control(t)) + \epsilon(t) \quad \forall t
\end{align}
#+END_EXPORT
where $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ represents the transition dynamics function and $\epsilon(t)$
is an i.i.d process noise term with $\E[\epsilon(t)] = 0$.
The process noise term accounts for unwanted (and, in general, unknown) disturbances of the system.
For example, it is extremely hard to model aerodynamic effects on aircraft so these could be accounted
for in the process noise term.
Throughout this dissertation it is assumed that the state $\state$ is observed directly and is not subject to
observation noise.
This is a standard assumption in the \acrfull{mdp} framework which is commonly adopted
in the reinforcement learning literature.
\todo[inline]{Slightly weird referring to MDP literature on continuous time setting}

\todo[inline]{Should I make a bigger point out of the observation noise assumption}

The system is controlled via a policy $\control(t) = \pi(\state(t), t)$, which given the state $\state(t)$
and time step $t$ decides which control action $\control(t)$ to apply to the system.
The policy can be time-dependent and can also depend on all past information up to time step $t$.
In the time-independent case the policy is denoted $\pi(\state(t))$ and
the resulting closed-loop system is denoted $f_{\pi}(\state) = \dynamicsFunc(\state,\pi(\state))$.

*** Optimal Control :ignore:
Optimal control is a branch of mathematical optimisation that seeks to find a set of controls
over a time period $t \in [t_0, t_f]$ that optimises an objective function $\mathbf{J}_{\pi}(\x)$.
The objective function might be formulated to solve a particular task or to make the dynamical
system behave in a certain way.

Typically this objective function is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-objective}
\mathbf{J} \defeq \phi(\x(t_{f}), t_{f}) + \int^{t_{f}}_{t_{0}} L(\mathbf{x}(t), \mathbf{u}(t), t) \text{d}t,
\end{align}
#+END_EXPORT
which consists of two terms:
1. a terminal cost term $\phi : \R^{D} \times \R \rightarrow \R$,
2. an integral cost term (or Lagrangian) $L : \R^{D} \times \R^{F} \times \R \rightarrow \R$.

Model-based control ...

*** Trajectory Optimisation
**** intro :ignore:
Formally, trajectory optimisation seeks to find the state and
control trajectories,
$\stateTraj = \{\state(\timeInd) \in \stateDomain \mid \forall \timeInd \in [\timeInd_0, \timeInd_f] \}$,
and
$\controlTraj = \{\control(\timeInd) \in \controlDomain \mid \forall \timeInd \in [\timeInd_0, \timeInd_f] \}$,
respectively,
that minimise some cost function $\costFunc$, whilst satisfying constraints $\constraintsFunc$,
and boundary conditions.
The trajectory optimisation problem is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-traj-opt}
\min_{\state(t), \control(t)} &\int_{t_0}^{t_f} \costFunc(\state(t), \control(t)) \text{d}t \quad \forall t \\
\text{s.t. }&\text{Eq. \ref{eq-unimodal-dynamics-cont}}  \\
&\constraintsFunc(\mathbf{x}(t)) \leq 0 \quad \forall t \\
&\state(t_0) = \state_0  \quad \state(t_f) = \state_f
\end{align}
#+END_EXPORT
**** Collocation
**** Trajectory Optimisation as Probabilistic Inference
***** maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\startStateDist}{\ensuremath{p(\state_{1})}}
\newcommand{\transitionDist}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)}}
\newcommand{\trajectoryDist}{\ensuremath{p(\stateControlTraj)}}
\newcommand{\controlDist}{\ensuremath{p(\control_\timeInd \mid \state_\timeInd)}}

\newcommand{\trajectoryVarDist}{\ensuremath{q(\state_{1:\TimeInd}, \control_{1:\TimeInd})}}
\newcommand{\controlTrajVarDist}{\ensuremath{q(\control_{1:\TimeInd} \mid \state_{1:\TimeInd})}}
\newcommand{\controlVarDist}{\ensuremath{q(\control_{\timeInd})}}

\newcommand{\optimalVar}{\ensuremath{\mathcal{O}}}
\newcommand{\monotonicFunc}{\ensuremath{f_{\text{mon}}}}

\newcommand{\optimalProb}{\ensuremath{\Pr(\optimalVar_\timeInd = 1 \mid \state_\timeInd, \control_\timeInd)}}
\newcommand{\optimalDist}{\ensuremath{P(\optimalVar_\timeInd \mid \state_\timeInd, \control_\timeInd)}}

\newcommand{\terminalCostDist}{\ensuremath{\Pr(\optimalVar_\TimeInd=1 \mid \state_\TimeInd)}}

\newcommand{\marginalLikelihood}{\ensuremath{p(\optimalVar_{1:\TimeInd} = 1)}}
\newcommand{\jointDist}{\ensuremath{p(\optimalVar_{1:\TimeInd}=1, \state_{1:\TimeInd}, \control_{1:\TimeInd})}}

\newcommand{\temperature}{\ensuremath{\gamma}}
#+END_EXPORT
***** intro :ignore:
The second trajectory algorihtm optimisation in Chapter ref:chap-traj-opt
builds on top of the control-as-inference framework
citep:toussaintProbabilistic2006,kappenOptimal2013,
which frames optimal control as probabilistic inference.
Translating optimal control to inference can be viewed as a subset of probabilistic numerics
citep:hennigProbabilistic2015, where statistical methods are used to solve numerical problems,
providing uncertainty quantification, regularisation and faster convergence.

Control-as-inference has been considered since the beginning of modern optimal control due to
the concurrent development of LQR, Kalman filtering and LQG cite:kalmanNew1960,hoBayesian1964.
Recent work on control-as-inference stems from path integral cite:kappenOptimal2013,williamsModel2017

cite:rawlikStochastic2013

cite:toussaintRobot2009 formulate trajectory optimisation as
approximate inference and derive a local approximation
where the maximum likelihood solution returns the deterministic optimal trajectory, and the local Gaussian belief
approximation is equivalent to the LQG perturbation model around the optimal trajectory.
cite:levineReinforcement2018 followed a similar approach formulating RL as probabilistic inference
and showing how different approximate inference algorithms instantiated different popular RL algorithms.


Recent work by cite:watsonStochastic2021 has formulated
stochastic optimal control as input estimation

***** Cost Functions as Likelihoods
To formulate optimal control as probabilistic inference
it is first embedded into a graphical model (see Figure ref:fig-basic-control-graphical-model).
The joint probability model (over a trajectory) is augmented with an
additional variable to encode the notion of cost (or reward)
over the trajectory (see Figure ref:fig-augmented-control-graphical-model).
The new variable is a Bernoulli random variable $\optimalVar_\timeInd \in \{0, 1\}$, that indicates if
time step $\timeInd$ is /optimal/ $\optimalVar_\timeInd=1$, or not /optimal/ $\optimalVar_\timeInd=0$.
The likelihood distribution can then be formulated
by mapping the negative
cost through a monotonically increasing function $\monotonicFunc$,
giving the likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-monotonicOptimalityLikelihood}
\optimalProb &\coloneqq \monotonicFunc( -\costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
A common (and convenient) approach is to formulate
the likelihood using an exponential transform of
the cost. This results in a Boltzmann distribution where the
inverse temperature, $\temperature$, is used to scale the cost,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-exponentialOptimalityLikelihood}
\optimalProb &\propto \exp ( -\temperature\costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
The resulting negative log-likelihood is an affine transformation of the cost, which preserves convexity.
#+BEGIN_EXPORT latex
\todo[inline]{add detials on Cross Entropy Method (CEM) cite:chuaDeep2018 and other methods}
\begin{myquote}
As shown in \cite{okadaVariational2020}, the choice of monotonic function,
$\monotonicFunc$,
leads to the inference algorithm resembling different well-known algorithms.
For example, selecting $\monotonicFunc$ to be the exponential function,
\begin{align} \label{eq-}
\optimalProb &\propto \text{exp}( -\costFunc(\state_\timeInd, \control_\timeInd))
\end{align}
leads to the algorithm resembling Model Predictive Path Integral (MPPI)
\cite{williamsModel2017,williamsInformation2017}, whilst
selecting $\monotonicFunc$ to as,
\begin{align} \label{eq-}
\optimalProb &\propto \mathbb{I}\left[( \costFunc(\state_\timeInd, \control_\timeInd)) \leq \costFunc_{\text{thd}} \right]
\end{align}
recovers an algorihtm resembling the Cross Entropy Method (CEM).
\end{myquote}
#+END_EXPORT

***** Joint Probability Model
Given the "cost" likelihood
(Eq. ref:eq-monotonicOptimalityLikelihood)
the joint probability for an optimal trajectory (i.e.
for $\optimalVar_\timeInd=1$ for all $\timeInd \in \{1,\ldots,\TimeInd\}$), is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-traj-opt-joint-dist}
\jointDist =
& \startStateDist \underbrace{\terminalCostDist}_{\text{Terminal Cost}} \\
&\prod_{\timeInd=1}^{\TimeInd-1}
\underbrace{\optimalProb}_{\text{Cost}}
\underbrace{\transitionDist}_{\text{Dynamics}}
\underbrace{\controlDist}_{\text{Controller}}
\end{align}
#+END_EXPORT
where $\controlDist$ denotes the policy.
In the case of RL the policy is parameterised with parameters, $\theta$, but in control we are usually interested in
optimising the states and controls directly.
The object of interest in this model is the distribution over the control, $\control_\timeInd$,
conditioned on the state, $\state_\timeInd$, and all future time steps being optimal,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\control_{\timeInd} \mid \state_{\timeInd}, \optimalVar_{\timeInd:\TimeInd}=1) =
\frac{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)
\controlDist p(\state_\timeInd)}{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd)
p(\state_\timeInd)}.
\end{align}
#+END_EXPORT
The distributions $p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd)$,
$p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd)$,
$p(\modeVar_{\timeInd:\TimeInd}=\modeInd \mid \state_\timeInd, \control_\timeInd)$ and
$p(\modeVar_{\timeInd:\TimeInd}=\modeInd \mid \state_\timeInd)$
can be obtained via recursive message passing,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd) =
\prod_{\timeInd+1}^{\TimeInd-1}
\int \int \optimalProb p(\state_\timeInd \mid \state_{\timeInd-1}, \control_{\timeInd-1}) \controlDist \text{d}\state_{\timeInd} \text{d}\control_{\timeInd}.
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\todo[inline]{Considering process noise in the dynamics separately from the epistemic (parametric) uncertainty,
\begin{align} \label{eq-}
p(\optimalVar_{1:\TimeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})
= \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \optimalDist
p(\state_{\timeInd+1} \mid \dynamicsFunc(\state_\timeInd)) p(\dynamicsFunc(\state_\timeInd) \mid \state_\timeInd, \control_\timeInd)
\controlDist
\end{align}
\begin{align} \label{eq-}
\optimalProb &= \monotonicFunc( -\costFunc(\state_\timeInd, \dynamicsFunc(\state_{\timeInd-1}), \control_\timeInd)) \\
\end{align}
}
#+END_EXPORT

***** graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \begin{subfigure}{.45\textwidth}
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);
    \end{tikzpicture}
    %}
  \caption{Graphical model with states and controls.}
\label{fig-basic-control-graphical-model}
\end{subfigure}
\begin{subfigure}{.45\textwidth}
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[obs, above=of u1] (o1) {$\optimalVar_1$};
      \node[obs, right=of o1] (o2) {$\optimalVar_2$};
      \node[obs, right=of o2] (o3) {$\optimalVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);
    \end{tikzpicture}
    %}
  \caption{Graphical model with optimality variable.}
\label{fig-augmented-control-graphical-model}
\end{subfigure}
\caption{Graphical models of control formulated as inference.}
\label{fig-control-graphical-model}
\end{figure}
#+END_EXPORT

***** Variational Inference
In variational inference the distribution over the latent variables
$p(\state_{1:\TimeInd}, \control_{1:\TimeInd})$
is approximated with another, potentially simpler
(e.g. factorised) distribution $q(\state_{1:\TimeInd}, \control_{1:\TimeInd})$.
In this case, the aim is to approximate the distribution over the optimal policy,
$p(\control_{1:\TimeInd} \mid \state_{1:\TimeInd}, \optimalVar_{1:\TimeInd}=1)$.
Introducing the variational distribution,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\trajectoryVarDist = \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \transitionDist \controlVarDist
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\text{log} \marginalLikelihood
&= \text{log} \E_\trajectoryVarDist \left[ \frac{\jointDist}{\trajectoryVarDist} \right] \\
&= \text{log} \E_\trajectoryVarDist \left[
\frac{\cancel{\startStateDist} \prod_{\timeInd=1}^{\TimeInd} \optimalProb \cancel{\transitionDist} \controlDist}{\cancel{\startStateDist} \prod_{\timeInd=1}^\TimeInd \cancel{\transitionDist} \controlVarDist}
\right] \\
&\geq \E_\trajectoryVarDist \left[
\sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb - \sum_{\timeInd=1}^\TimeInd \text{log}
\frac{\controlVarDist}{\controlDist} \right] \\
&= \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \costFunc(\state_\timeInd, \control_\timeInd)
- \text{log}\controlVarDist\right]
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\text{log} \marginalLikelihood &\geq
 \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb
- \text{log}\controlVarDist\right] \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \text{log} \optimalProb \right]
- \E_{\controlVarDist q(\state_{\timdInd})} \left[ \text{log}\controlVarDist \right] \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
+ \sum_{\timeInd=1}^{\TimeInd} \underbrace{- \E_{\controlVarDist} \left[ \text{log} \controlVarDist \right]}_{\text{Entropy}} \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
+ \sum_{\timeInd=1}^{\TimeInd} \mathcal{H}(\control_{\timeInd})
\end{align}
#+END_EXPORT

*** Model-Based Control in Uncertain Systems
- robust optimal control
- stochastic optimal control
*** Literature Review (iLQR/iLQG/DDP/MPPI for Trajectory Generation)
*Iterative LQR/LQG* can be used to generate trajectories for non-linear systems by iteratively approximating
the dynamics to be linear around a nominal trajectory and optimising for the controls.
iLQR works well for quadratic cost functions and can be used with any cost function by approximating the cost
function with a second order Taylor expansion. However, in this case iLQR is susceptible to converging
to terrible (local) optima if the true cost function is highly non-convex.

cite:mitrovicAdaptive2010 combine learned dynamics models with iLQG to develop an adaptive optimal feedback controller.

cite:boedeckerApproximate2014 present a real-time iLQR based on sparse GPs.

cite:rohrProbabilistic2021 propose a novel controller synthesis for
linearised GP dynamics that yields robust controllers with
respect to a probabilistic stability margin.

In cite:marcoAutomatic2016 they parameterise the weight matrices
$\stateMatrix$ and $\controlMatrix$ with parameters $\theta$
and perform BO (entropy search) on $J(\theta)$. Their method handles model mis-specification
by adapting the parameters.

*Differential Dynamic Programming* iLQR and iLQG  are variants of the classic Differential Dynamic Programming (DDP) algorithm cite:rosenbrockDifferential1972,
the main difference being that only a /first-order/ approximation of the dynamics is used, instead of /second-order/.
cite:liaoAdvantages1992 show that DDP is comparable to a full Newton step for the entire control
sequence. Therefore, care must be taken when either the Hessian isn't positive-definite or the minimum isn't close
and the quadratic model is inaccurate.
\todo{remember improvements to standard iLQR/DDP from cite:tassaSynthesis2012}

The extension of DDP to stochastic state and controls is addressed in cite:theodorouStochastic2010.

DDP under parametric uncertainty using generalised polynomial chaos cite:aoyamaReceding2021.


cite:panProbabilistic2014 present a probabilistic trajectory optimisation
framework for systems with unknown dynamics called probabilistic
differential dynamic programming (PDDP).
It extends differential dynamic programming to explicitly account
for uncertainty in the learned dynamics modelled using gaussian processes.
In contrast to typical gradient based policy search methods, PDDP does not
require a policy parameterisation as it learns a locally optimal,
time-varying control policy.
Based on the second-order local approximation of the value function,
PDDP performs Dynamic Programming around a nominal trajectory in Gaussian belief spaces.

*Path Integral*  control is a method for developing optimal controls based on stochastic
sampling of trajectories cite:theodorouGeneralized2010.
This work was originally limited to control affine systems cite:williamsModel2017
but has since been extended to general nonlinear
dynamical systems cite:williamsInformation2017,williamsInformationTheoretic2018.

** Exploration in Dynamical Systems
*** Bandits and Bayesian Optimisation
*** Information Theoretic Active Learning with Gaussian Processe
cite:krauseNonmyopic2007,houlsbyBayesian2011
*** Active Learning in Dynamical Systems
cite:yuActive2021,schreiterSafe2015,caponeLocalized2020,buisson-fenetActively2020

** Old :noexport:
In cite:NhatAnhNguyen2018 they highlight that the previously mentioned approaches are limited as
each expert is trained using only the local data
assigned to it. This means that they do not take into account the global information, i.e. the
correlations between clusters.
The experts are therefore likely to overfit to their local training data.
cite:NhatAnhNguyen2018 propose a model based on both global and local sparse GPs to overcome this
and also employ stochastic optimisation providing greater scalability.
They reduce the computational complexity in their bound through the variational
approximation on the indicator variable as well as the sparse GP framework cite:Hensman2013.

Our approach does not lead to over fitting as each GP is trained using all of the observations.
However, each of our experts has a set of inducing inputs that are unique to the expert which
greatly improves computational complexity (at training and prediction).
These inducing inputs enable stochastic variational inference in our model which also significantly speeds
up inference.

In contrast, our main motivation is learning good representations of the underlying functions
(both the experts and the gating network).
Mixtures of GP experts models are inherently unidentifiable as
different combinations of experts
and gating networks can generate the same distributions over the observations.
As such, we trade in the computational benefits of the gating networks used by much
of the literature for the modelling advantages offered by a gating network based on GPs.
This provides a handle for encoding prior knowledge that can be used to constrain
the set of admissible functions that we wish to learn within.
This in turn improves identifiability and results in learned representations that are more true
to our understanding of the system.
It is worth noting that we do derive a variational lower bound with competitive complexity.
To the best of our knowledge the complexity of our variational lower bound is
state of the art for \gls{mgp} models with GP based gating networks.




Before comparing our work to the literature we first provide some intuition for the modelling
assumptions behind different gating networks in Figure [[ref:gating_network_comparison]].
We denote the inputs $x$, expert indicator variable $\alpha$ and the cluster indicator variable $z$.
The expert mixing probabilities are given by $P(\alpha=k|x)$ and the distributions over the inputs
are given by $p(x | \alpha=k, z=c)$.
\todo{dist or density?}
The top plot is a gating
network based on GPs and importantly we can see that it has been able to turn expert 1 "on" in
multiple regions of the domain and is not limited to a single local subset of the domain.
Alternatively, many approaches utilise some notion of clustering the inputs locally and the middle right plot
shows three Gaussian pdfs representing this.
It is common to assign each cluster to a single expert,
i.e. let the cluster indicator equal the expert indicator ($k=c$).
This leads to improved computational complexity by dividing up the domain and decomposing the
covariance matrix into a set of smaller matrices.
However, this leads to fitting separate experts to the left and right regions (shaded green) which may be
contrary to our knowledge of the generation of observations.
We apply Bayes' rule to recover the expert mixing probabilities $P(\alpha=k |x)$ shown in the
middle left plot (so that we can compare it to the GP based gating network above).

One can maintain separate expert and cluster indicator variables (as seen in cite:Yuan) which
leads to each experts inputs being modelled as a mixture of Gaussians. This is shown in the bottom
right plot where all we have done is take the middle right plot and simply marginalising the
cluster indicator variable.
The distribution over the inputs associated with expert
one $p(x | \alpha=1)$ (dashed green) has high *cluster* mixing
probabilities for the clusters in the left and right (green) regions and a low *cluster* mixing
probability in the middle (blue) region.
This leads to a gating network that is also able to turn expert 1 "on" in multiple regions of the domain
like the GP based gating network.

This is good but we now consider the added modelling benefit of a GP based gating network.
\todo[inline]{insert text/example about encoding prior info and making hyperparameters not trainable etc}

We can also consider the implications from an inference perspective.
It is known that developing inference algorithms for propagating uncertain (Gaussian) inputs
through GPs is itself an unsolved problem (see cite:Ustyuzhaninov).
Therefore developing an inference algorithm to propagate inputs that are distributed according to
a Gaussian mixture model will likely lead to valuable information loss.
This is a purely intuitive statement and we have not performed any experiments.

# Stochastic variational inference enables GP models to scale to larger data sets so we would like
# to develop such an algorithm.

Hopefully this provides some intuition about the different assumptions that can be encoded (or not)
by different gating network formulations.
We now highlight XXX of the factors that might make a GP based gating network desirable.

1. Handle for encoding prior information,
   - Constrain the set of admissible functions.
2. Information loss,
   - Propagating Gaussian inputs through GPs is an active area of research cite:Ustyuzhaninov and we
     argue that any approximations required to propagate inputs that are distributed according to
     a Gaussian mixture model will likely lose valuable information.

\todo[inline]{We can encode more info into a GP prior that into a GMM?}
\todo[inline]{Can GMMs extrapolate away from data??}
\todo[inline]{With GP gating function we can ensure it is a Riemannian manifold i.e. differentiable. Is this the case with GMMs?}


\todo{Yuan uses VI and changed gating network structure}
cite:Yuan proposed a mixture of GP experts with a variational inference scheme which is much
faster than using MCMC.
They assume that the expert indicators are generated by a Dirichlet distribution and
that given the expert indicator the inputs follow a Gaussian mixture model, i.e. they
have introduced a cluster indicator variable.
This is in contrast to earlier approaches that let the expert indicator act as the input cluster
indicator.
Intuitively we think that this gives rise to a gating network similar
to the bottom plot of Figure [[ref:gating_network_comparison]].
However, the expert indicator does not depend on the inputs, rather the inputs depend on the cluster
indicator which depends on the expert indicator.
\todo{However, the "gating network" is no longer input dependent, unless a similar modification to cite:Rasmussen2002 is made to the DP.}

In cite:NhatAnhNguyen2018 they highlight that these approaches are limited as
each expert is trained using only the local data
assigned to it. This means that they do not take into account the global information, i.e. the
correlations between clusters.
The experts are therefore likely to overfit to their local training data.
cite:NhatAnhNguyen2018 propose a model based on both global and local sparse GPs to overcome this
and also employ stochastic optimisation providing greater scalability.
They reduce the computational complexity in their bound through the variational
approximation on the indicator variable as well as the sparse GP framework cite:Hensman2013.
Our approach does not lead to over fitting as each GP is trained using all of the observations.
However, each of our experts has a set of inducing inputs that are unique to the expert which
greatly improves computational complexity (at training and prediction).
These inducing inputs enable stochastic variational inference in our model which also significantly speeds
up inference.

Our gating function only requires a single GP as we instantiate our model with only two experts.
We exploit the probit link function (as opposed to the softmax function used by cite:Tresp)
because it leads to analytic mixing probabilities
and factorisation across the data given $M$ global inducing points.
We do not introduce a variational distribution over the indicator variable as we are able
to analytically marginalise it in our evidence lower bound without a significant computational burden.


Our model is similar to cite:Tresp but overcomes the limitations highlighted
by cite:Rasmussen2002.
That is, we simultaneously improve the models computational complexity and
the data assignment at prediction time.
Each of our experts predicts on the basis of a set of inducing inputs that are unique to the expert.
These inducing inputs enable stochastic variational inference in our model which significantly speeds
up inference.

** Ignore :noexport:

The \glspl{mgp} literature commonly highlights two main issues that they
seek to address.
Firstly, \glspl{gp} cannot handle multi-modality, i.e. they assume that the observations
were generated by a single underlying function.
Secondly, they suffer from both time and memory complexity issues
arising from the calculation and storage of the inverse covariance matrix.
Given a data set with $N$ observations, training has time and memory complexity of $\mathcal{O}(N^3)$
and $\mathcal{O}(N^2)$ respectively.
Many approaches to a \gls{mgp} method have been proposed which attempt
to address these issues cite:Tresp,Rasmussen2002,Meeds2005,Yuan,NhatAnhNguyen2018.
However, most previous work appears to be motivated by addressing the complexity issues
associated with \glspl{gp}.
That is, they are interested in decomposing the $N \times N$ covariance matrix into a set of smaller
matrices.
In contrast, our main motivation is learning good representations of the underlying functions
(both the experts and the gating network).
Mixtures of GP experts models are inherently unidentifiable as
different combinations of experts
and gating networks can generate the same distributions over the observations.
As such, we trade in the computational benefits of the gating networks used by much
of the literature for the modelling advantages offered by a gating network based on GPs.
This provides a handle for encoding prior knowledge that can be used to constrain
the set of admissible functions that we wish to learn within.
This in turn improves identifiability and results in learned representations that are more true
to our understanding of the system.
It is worth noting that we do derive a variational lower bound with competitive complexity.
To the best of our knowledge the complexity of our variational lower bound is
state of the art for \gls{mgp} models with GP based gating networks.

In cite:NhatAnhNguyen2018 they exploit a global GP to coarsely model the
entire data set as well as local GP experts to overcome this limitation.
Our approach does not lead to over fitting as each GP is trained using all of the observations.
We exploit the factorization achieved via the sparse variational GP framework to
couple the learning of the gating network and the experts.
Our bound achieves this with little extra computational burden.

A significant amount of recent work builds on cite:Rasmussen2002 with the
computational benefits as a key driving factor.
However, we highlight that there is also a modelling assumption that distinguishes these approaches.
The gating network of cite:Tresp is able to model spatial structure.
That is, it is capable of turning each expert "on" and "off" in different regions of the input space
i.e. expert $k$ can be turned on in multiple regions.
This contrasts placing a GMM on the inputs and associating each expert with the observations
associated with specific components.
This approach improves the computational complexity but is not capable of capturing spatial structure.
For example, if expert $k$ is responsible for generating the data in multiple areas of the
input space then a GP based gating network will be capable of capturing this.

\todo[inline]{what do we mean by spatial structure??? Essentially we have GP compared to GMM. Do Gaussian mixture models capture spatial structure??}


We do not directly assign each observation to a single expert but instead assume that
it is generated as a mixture of the experts.

cite:Nguyen2014 also provide fast variational inference based on the sparse GP framework.
Their model also "splits" the dataset such that each expert acts only on the
observations assigned to it.
Each observation is assigned to an expert based on its proximity to the experts centroids.


A limitation of these approaches is that each expert is trained using only the local data
assigned to it. This means that they do not take into account the global information, i.e. the
correlations between clusters.
The experts are therefore likely to overfit to their local training data.

Our approach is similar to cite:Yuan in the sense that we break the dependency among the training
outputs enabling variational inference.
However, we achieve this through the sparse variational GP framework seen in cite:Hensman2013.
We actually induce the necessary conditions for stochastic variational inference as
we obtain factorisation over data points given a set of $M$ global inducing points.

cite:NhatAnhNguyen2018 propose a model based on both global and local sparse GPs
and also employ stochastic optimisation providing greater scalability.
They reduce the computational complexity in their bound through the variational
approximation on the indicator variable as well as the sparse GP framework cite:Hensman2013.
We do not introduce a variational distribution over the indicator variable as we are able
to analytically marginalise it in our evidence lower bound without a significant computational burden.
\todo[inline]{Variational dists allow us to condider the unc in a variable and perform full Bayesian inference. What is the significance of what we have done? I think that ideally we would analytically marginalise all our latent variables so what we have done is very nice.}
cite:NhatAnhNguyen2018 use a GMM gating network which and achieve complexity $\mathcal{O}(NM^2K)$.
We want spatial structure so use GP based gating network which leads to $\mathcal{O}(NM^3K)$.


The gating network enforces that each component of the GP mixture is localized.
In cite:Lazaro-Gredilla2011 they formulate a mixture without the use of a gating function
which enables a given
location in the input space to be associated with multiple outputs. This could be used for modelling
multiple objects in a tracking system and is known as the data association problem.
We are not interested in the data association problem but instead wish to encode spatial structure
through the use of a gating function.
We introduce our model from a generative model perspective which provides insight
into how we can easily switch between the data association setting and a mixture of GPs by
simply marginalising the indicator variable.

Our gating function only requires a single GP as we instantiate our model with only two experts.
We exploit the probit link function (as opposed to the softmax function used by cite:Tresp)
because it leads to analytic mixing probabilities
and factorisation across the data given $M$ global inducing points.


Our model is similar to cite:Tresp but overcomes the limitations highlighted
by cite:Rasmussen2002.
That is, we simultaneously improve the models computational complexity and
the data assignment at prediction time.
Each of our experts predicts on the basis of a set of inducing inputs that are unique to the expert.
These inducing inputs enable stochastic variational inference in our model which significantly speeds
up inference.

* Probabilistic Inference for Learning Multimodal Dynamical Systems label:chap-dynamics
#+begin_export latex
\epigraph{All models are wrong, but some are useful.}{\textit{George Box}}
#+end_export
** Maths Symbols :ignore:
*** Domains :ignore:
#+BEGIN_EXPORT latex
\newcommand{\stateDomain}{\ensuremath{\mathcal{X}}}
%\renewcommand{\stateDomain}{\ensuremath{\mathcal{S}}}
\renewcommand{\controlDomain}{\ensuremath{\mathcal{U}}}
\renewcommand{\modeDomain}{\ensuremath{\mathcal{A}}}
%\renewcommand{\inputDomain}{\ensuremath{\mathcal{X}}}
\renewcommand{\inputDomain}{\ensuremath{\mathcal{Z}}}

%\renewcommand{\state}{\ensuremath{\mathbf{s}}}
\renewcommand{\state}{\ensuremath{\mathbf{x}}}

\renewcommand{\nominalDynamics}{\ensuremath{\mathbf{n}}}
\renewcommand{\unknownDynamics}{\ensuremath{\mathbf{f}}}
\renewcommand{\nominalDynamicsK}{\ensuremath{\mode{\mathbf{n}}}}
\renewcommand{\unknownDynamicsK}{\ensuremath{\mode{\mathbf{f}}}}

\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{T}}
\newcommand{\inputDim}{\ensuremath{d}}
\newcommand{\InputDim}{\ensuremath{D}}
#+END_EXPORT

*** Bounds :ignore:
#+BEGIN_EXPORT latex
\newcommand{\tightBound}{\ensuremath{\mathcal{L}_{\text{tight}}}}
\newcommand{\furtherBound}{\ensuremath{\mathcal{L}_{\text{further}}}}
\newcommand{\furtherBoundTwo}{\ensuremath{\mathcal{L}_{\text{further}^2}}}
#+END_EXPORT
*** maths :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\mode}[1]{\ensuremath{#1_{\modeInd}}}
\renewcommand{\modei}[2]{\ensuremath{#1_{#2}}}
%\renewcommand{\singleOutput}{\ensuremath{\Delta x_{\numData}}}
%\renewcommand{\allOutput}{\ensuremath{\Delta\mathbf{\state}}}
\newcommand{\singleModeVar}{\ensuremath{\singleData{\modeVar}}}
\newcommand{\allModeVar}{\ensuremath{\bm{\modeVar}}}
\newcommand{\singleModeVarK}{\ensuremath{\singleModeVar = \modeInd}}
\newcommand{\allModeVarK}{\ensuremath{\bm{\modeVar}_{\modeInd}}}
%\newcommand{\allModeVarK}{\ensuremath{\{\singleModeVarK\}_{\numData=1}^\NumData}}
\newcommand{\modeVarnk}{\ensuremath{\modeVar_{\numData,\modeInd}}}

% new
\renewcommand{\numData}{\ensuremath{n}}
\renewcommand{\NumData}{\ensuremath{N}}
\renewcommand{\singleOutput}{\ensuremath{y_{\numData}}}
\renewcommand{\singleInput}{\ensuremath{\mathbf{x}_{\numData}}}
\renewcommand{\allInput}{\ensuremath{\mathbf{X}}}
\renewcommand{\allOutput}{\ensuremath{\mathbf{y}}}
\renewcommand{\allOutput}{\ensuremath{\mathbf{y}}}
%\renewcommand{\allInputK}{\ensuremath{\{\singleInput : \singleModeVarK \}}}
%\renewcommand{\allOutputK}{\ensuremath{\{\singleOutput : \singleModeVarK\}}}
%\renewcommand{\allInputK}{\ensuremath{\allInput^{\modeInd}}}
%\renewcommand{\allOutputK}{\ensuremath{\allOutput^{\modeInd}}}
\renewcommand{\singleInputK}{\ensuremath{\mathbf{x}_{\numData, \modeInd}}}
\renewcommand{\allInputK}{\ensuremath{\mode{\allInput}}}
\renewcommand{\allOutputK}{\ensuremath{\mode{\allOutput}}}

%\renewcommand{\x}{\ensuremath{\mathbf{z}}}
%\renewcommand{\y}{\ensuremath{y}}
%\renewcommand{\singleInput}{\ensuremath{\mathbf{z}_{\numData}}}
%\renewcommand{\allInput}{\ensuremath{\mathbf{Z}}}
%\renewcommand{\singleInputK}{\ensuremath{\mathbf{z}_{\numData, \modeInd}}}
%\renewcommand{\allInputK}{\ensuremath{\mode{\allInput}}}

%\newcommand{\expertPrior}{\ensuremath{p\left(\mode{f}(\allInput) \right)}}
\newcommand{\expertPrior}{\ensuremath{p\left(\mode{f}(\allInputK) \right)}}
\newcommand{\expertsPrior}{\ensuremath{p\left(\LatentFunc(\allInput) \right)}}
\newcommand{\expertMeanFunc}{\ensuremath{\mode{\mu}}}
\newcommand{\expertCovFunc}{\ensuremath{\mode{k}}}
\newcommand{\expertLikelihood}{\ensuremath{p\left(\allOutput \mid \mode{f}(\allInput)\right)}}
\newcommand{\singleExpertLikelihood}{\ensuremath{p(\singleOutput \mid \mode{f}(\singleInput))}}
%\newcommand{\allExpertLikelihood}{\ensuremath{p(\allOutput \mid \mode{f}(\allInput))}}
%\newcommand{\allExpertLikelihood}{\ensuremath{p(\allOutputK \mid \mode{f}(\allInputK))}}
\newcommand{\allExpertLikelihood}{\ensuremath{p(\allOutputK \mid \mode{f}(\allInputK))}}
\newcommand{\expertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVarK, \allInput \right)}}
\newcommand{\singleExpertPosterior}{\ensuremath{p\left(\singleOutput \mid \singleModeVarK, \allInput \right)}}
% \newcommand{\expertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVarK \right)}}

\newcommand{\gatingPrior}{\ensuremath{p\left(\GatingFunc(\allInput ) \right)}}
\newcommand{\gatingMeanFunc}{\ensuremath{\mode{\hat{\mu}}}}
\newcommand{\gatingCovFunc}{\ensuremath{\mode{\hat{k}}}}
\newcommand{\singleGatingLikelihood}{\ensuremath{\Pr\left(\singleModeVarK \mid \GatingFunc(\singleInput) \right)}}
%\newcommand{\allGatingLikelihood}{\ensuremath{\Pr\left(\allModeVarK \mid \GatingFunc(\allInput) \right)}}
\newcommand{\allGatingLikelihood}{\ensuremath{p\left(\allModeVar \mid \GatingFunc(\allInput) \right)}}
\newcommand{\gatingLikelihood}{\ensuremath{P\left(\singleModeVar \mid \GatingFunc(\singleInput) \right)}}
%\newcommand{\gatingPosterior}{\ensuremath{\Pr\left( \singleModeVar \mid \singleInput \right)}}
\newcommand{\gatingPosterior}{\ensuremath{\Pr\left( \allModeVarK \mid \allInput \right)}}
\newcommand{\singleGatingPosterior}{\ensuremath{\Pr\left( \singleModeVarK \mid \singleInput, \gatingParams \right)}}
\newcommand{\evidence}{\ensuremath{p\left(\allOutput \mid \allInput \right)}}

\newcommand{\moeExpertPosterior}{\ensuremath{p\left(\singleOutput \mid \singleModeVarK, \singleInput, \expertParamsK \right)}}
\newcommand{\moeGatingPosterior}{\ensuremath{\Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right)}}
\newcommand{\moeEvidence}{\ensuremath{p\left(\allOutput \mid \allInput, \expertParams, \gatingParams \right)}}
\newcommand{\singleMoeEvidence}{\ensuremath{p\left(\singleOutput \mid \singleInput, \expertParams, \gatingParams \right)}}

\newcommand{\npmoeExpertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVar, \allInput, \expertParams \right)}}
\newcommand{\npmoeGatingPosterior}{\ensuremath{p\left(\allModeVar \mid \allInput, \gatingParams \right)}}

\newcommand{\moeLikelihood}{\ensuremath{p\left(\allOutput \mid \LatentFunc(\allInput), \GatingFunc (\allInput) \right)}}
\newcommand{\singleMoeLikelihood}{\ensuremath{p\left(\singleOutput \mid \mode{\latentFunc}(\allInput), \GatingFunc (\allInput) \right)}}

#+END_EXPORT
*** kernels :ignore:
#+BEGIN_EXPORT latex
%\renewcommand{\expertKernelnn}{\ensuremath{k_{\singleInput\singleInput}}}
%\renewcommand{\expertKernelnM}{\ensuremath{\mathbf{k}_{\singleInput \expertInducingInput}}}
%\renewcommand{\expertKernelMM}{\ensuremath{\mathbf{K}_{\expertInducingInput\expertInducingInput}}}
%\renewcommand{\expertKernelMn}{\ensuremath{\mathbf{k}_{\expertInducingInput \singleInput}}}
\renewcommand{\expertKernelnn}{\ensuremath{k_{\modeInd \numData \numData}}}
\renewcommand{\expertKernelNN}{\ensuremath{\mathbf{K}_{\modeInd \NumData \NumData}}}
\renewcommand{\expertKernelnM}{\ensuremath{\mathbf{k}_{\modeInd \numData \NumInducing}}}}
\renewcommand{\expertKernelNM}{\ensuremath{\mathbf{K}_{\modeInd \NumData \NumInducing}}}}
\renewcommand{\expertKernelMM}{\ensuremath{\mathbf{K}_{\modeInd \NumInducing \NumInducing}}}
\renewcommand{\expertKernelMn}{\ensuremath{\mathbf{k}_{\modeInd \NumInducing \numData}}}
\renewcommand{\expertKernelMN}{\ensuremath{\mathbf{K}_{\modeInd \NumInducing \NumData}}}
\renewcommand{\expertKernelsM}{\ensuremath{\mathbf{k}_{\modeInd * \NumInducing}}}}
\renewcommand{\expertKernelss}{\ensuremath{k_{\modeInd **}}}
\renewcommand{\expertKernelSM}{\ensuremath{\mathbf{K}_{\modeInd * \NumInducing}}}}
\renewcommand{\expertKernelSS}{\ensuremath{\mathbf{K}_{\modeInd **}}}

%\renewcommand{\gatingKernelnn}{\ensuremath{\hat{k}_{\singleInput\singleInput}}}
%\renewcommand{\gatingKernelnM}{\ensuremath{\hat{\mathbf{k}}_{\singleInput \gatingInducingInput}}}
%\renewcommand{\gatingKernelMM}{\ensuremath{\hat{\mathbf{K}}_{\gatingInducingInput\gatingInducingInput}}}
%\renewcommand{\gatingKernelMn}{\ensuremath{\hat{\mathbf{k}}_{\gatingInducingInput \singleInput}}}
\renewcommand{\gatingKernelnn}{\ensuremath{\hat{k}_{\modeInd \numData \numData}}}
\renewcommand{\gatingKernelNN}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumData \NumData}}}
\renewcommand{\gatingKernelnM}{\ensuremath{\hat{\mathbf{k}}_{\modeInd \numData \NumInducing}}}
\renewcommand{\gatingKernelNM}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumData \NumInducing}}}
\renewcommand{\gatingKernelMM}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumInducing \NumInducing}}}
\renewcommand{\gatingKernelMn}{\ensuremath{\hat{\mathbf{k}}_{\modeInd \NumInducing \numData}}}
\renewcommand{\gatingKernelss}{\ensuremath{\hat{k}_{\modeInd **}}}
\renewcommand{\gatingKernelsM}{\ensuremath{\hat{\mathbf{k}}_{\modeInd * \NumInducing}}}
\renewcommand{\gatingKernelMs}{\ensuremath{\hat{\mathbf{k}}_{\modeInd \NumInducing *}}}
\renewcommand{\gatingKernelSM}{\ensuremath{\hat{\mathbf{K}}_{\modeInd * \NumInducing}}}
\renewcommand{\gatingKernelMS}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumInducing *}}}
\renewcommand{\gatingKernelSS}{\ensuremath{\hat{\mathbf{K}}_{\modeInd **}}}

\renewcommand{\expertA}{\ensuremath{\mode{\mathbf{A}}}}
\renewcommand{\gatingA}{\ensuremath{\mode{\hat{\mathbf{A}}}}}
#+END_EXPORT

*** inference :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\input}{\ensuremath{\hat{\state}}}
\renewcommand{\output}{\ensuremath{\Delta \state}}

\newcommand{\kernel}{\ensuremath{k}}
\newcommand{\expertKernel}{\ensuremath{\mode{\kernel}}}
\newcommand{\gatingKernel}{\ensuremath{\mode{\hat{\kernel}}}}

\newcommand{\numInducing}{\ensuremath{m}}
\newcommand{\NumInducing}{\ensuremath{\MakeUppercase{\numInducing}}}
%\newcommand{\inducingInput}{\ensuremath{\mathbf{Z}}}
\newcommand{\inducingInput}{\ensuremath{\bm{\zeta}}}
\newcommand{\inducingOutput}{\ensuremath{\mathbf{u}}}

%\newcommand{\expertInducingInput}{\ensuremath{\mode{\inducingInput}}}
%\newcommand{\expertsInducingInput}{\ensuremath{\inducingInput}}}
\newcommand{\expertInducingInput}{\ensuremath{\mode{\bm{\zeta}}}}
\newcommand{\expertsInducingInput}{\ensuremath{\bm{\zeta}}}
\newcommand{\expertInducingOutput}{\ensuremath{\mode{\inducingOutput}}}
\newcommand{\expertsInducingOutput}{\ensuremath{\MakeUppercase{\inducingOutput}}}

%\newcommand{\gatingInducingInput}{\ensuremath{\hat{\inducingInput}}}
\newcommand{\gatingInducingInput}{\ensuremath{\bm{\xi}}}
\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\hat{\inducingOutput}}}}
\newcommand{\gatingsInducingOutput}{\ensuremath{\hat{\MakeUppercase{\inducingOutput}}}}

%\newcommand{\expertInducingPrior}{\ensuremath{p(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput \mid \expertInducingInput)}}
\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput \mid \expertsInducingInput)}}
%\newcommand{\expertInducingVariational}{\ensuremath{q(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\expertInducingVariational}{\ensuremath{q(\expertInducingOutput)}}
\newcommand{\expertsInducingVariational}{\ensuremath{q(\expertsInducingOutput)}}
\newcommand{\expertVariational}{\ensuremath{q(\mode{\latentFunc}(\singleInput))}}
\newcommand{\expertsVariational}{\ensuremath{q(\LatentFunc_\numData)}}
%\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \expertInducingOutput)}}
\newcommand{\allExpertGivenInducing}{\ensuremath{p(\allOutput \mid \expertInducingOutput)}}
%\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \expertInducingOutput)}}
\newcommand{\allLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\allInput) \mid \expertInducingOutput)}}


%\newcommand{\gatingInducingPrior}{\ensuremath{p(\GatingFunc(\gatingInducingInput))}}
\newcommand{\gatingInducingPrior}{\ensuremath{p(\gatingInducingOutput \mid \gatingInducingInput)}}
\newcommand{\gatingsInducingPrior}{\ensuremath{p(\gatingsInducingOutput \mid \gatingInducingInput)}}
%\newcommand{\gatingInducingVariational}{\ensuremath{q(\GatingFunc(\gatingInducingInput))}}
\newcommand{\gatingInducingVariational}{\ensuremath{q(\gatingInducingOutput)}}
\newcommand{\gatingsInducingVariational}{\ensuremath{q(\gatingsInducingOutput)}}
\newcommand{\gatingsVariational}{\ensuremath{q(\GatingFunc_\numData)}}
\newcommand{\singleGatingGivenInducing}{\ensuremath{\Pr(\singleModeVarK \mid \gatingsInducingOutput)}}
\newcommand{\allGatingGivenInducing}{\ensuremath{\Pr(\allModeVarK \mid \gatingInducingOutput)}}
\newcommand{\allGatingsGivenInducing}{\ensuremath{\Pr(\allModeVarK \mid \gatingsInducingOutput)}}
\newcommand{\singleLatentGatingsGivenInducing}{\ensuremath{p(\GatingFunc(\singleInput) \mid \gatingsInducingOutput)}}
\newcommand{\allLatentGatingsGivenInducing}{\ensuremath{p(\GatingFunc(\allInput) \mid \gatingsInducingOutput)}}
\newcommand{\singleLatentGatingGivenInducing}{\ensuremath{p(\mode{\gatingFunc}(\singleInput) \mid \gatingInducingOutput)}}

\newcommand{\expertKL}{\ensuremath{\text{KL}\left( \expertInducingVariational \mid\mid \expertInducingPrior \right)}}
\newcommand{\expertsKL}{\ensuremath{\sum_{\modeInd=1}^\ModeInd\text{KL}\left( \expertInducingVariational \mid\mid \expertInducingPrior \right)}}
\newcommand{\gatingKL}{\ensuremath{\text{KL}\left( \gatingInducingVariational \mid\mid \gatingInducingPrior \right)}}
\newcommand{\gatingsKL}{\ensuremath{\sum_{\modeInd=1}^\ModeInd \text{KL}\left( \gatingInducingVariational \mid\mid \gatingInducingPrior \right)}}
#+END_EXPORT
** Intro :ignore:
This chapter is interested in /learning/ representations of multimodal dynamical systems,
that can be leveraged to control robotic systems in uncertain environments, where both the underlying
dynamics modes, and how the system switches between them, are /not fully known a priori/.
This chapter assumes access to a data set of state transitions $\dataset$, that
have previously been sampled from the system at a constant frequency, i.e. with a fixed time-step.

# However, it does assume that the dynamics modes vary over the
# input (state, control) domain, such that the mode switching can be modelled by input-dependent functions.
# This set of functions governing the mode switching is commonly referred to as a gating network.

# This work is motivated by learning synergising model learning for controlling multimodal dynamical systems.

# This work is motivated by learning /latent/ structure that can be exploited for model-based control.

# In particular

# As such, correctly identifying the underlying dynamics modes is extremely important

# Identifiability

# Given such a data set, this chapter first constructs a discrete-time representation of
# multimodal dynamical systems.
# It then formulates a probabilistic representation of this model and details an approach to performing Bayesian
# inference in the model.

Motivated by synergising model learning and control, the probabilistic model constructed in this chapter
resembles a \acrfull{mogpe} with a \acrshort{gp}-based gating network.
The \acrshort{gp}-based gating network infers latent /geometric/ structure that is exploited by the control
algorithms in Section ref:sec-traj-opt-geometric.
This work derives a novel variational lower bound based on sparse GP approximations, that provides
well-calibrated uncertainty estimates and scalability via stochastic gradient methods.

Following other \acrshort{mogpe} methods, the model and inference are first evaluated on the
motorcyle data set cite:Silverman1985.
They are then tested on the real-world quadcopter data set from the illustrative example detailed in cref:illustrative_example.
The work in this chapter is implemented in GPflow/TensorFlow and is available on GitHub
cite:matthewsGPflow2017,GPflow2017,tensorflow2015-whitepaper[fn:1:Code accompanying this Chapter can be found @ [[https://github.com/aidanscannell/mogpe][\icon{\faGithub}/aidanscannell/mogpe]].].


# and a data set obtained from a
# velocity controlled point mass simulation.

# The model and inference are tested on the real-world quadcopter data set and a data set obtained from a
# velocity controlled point mass simulation.
# \todo{Add that this method is tested on mcycle?}
# The work in this chapter is implemented in GPflow/TensorFlow and is available on GitHub
# cite:matthewsGPflow2017,GPflow2017,tensorflow2015-whitepaper[fn:1:Code accompanying this Chapter can be found @ [[https://github.com/aidanscannell/mogpe][\icon{\faGithub}/aidanscannell/mogpe]].].

# This chapter first introduces the set of continuous-time multimodal dynamical systems



# Based on this assumption, this chapter constructs a discrete-time representation of the system's transition dynamics
# and then formulates it as a probabilistic model based on GPs.

# This thesis is interested in multimodal systems where the dynamics modes vary over the
# input (state, control) domain, such that the mode switching can by modelled
# by input-dependent functions.
# This set of functions governing the mode switching is commonly referred to as a gating network.

# Motivated by data-efficient learning this work formulates multimodal on probabilistic models


# This work is motivated by data-efficient learning and specifically focuses on gating networks
# where prior knowledge of the system can be encoded into the model via informative priors.
# As a result, the probabilistic model constructed in this chapter resembles a \acrfull{mogpe}
# with a \acrshort{gp}-based gating network.
# This work derives a novel variational lower bound based on sparse GPs that
# enables the model to be trained with stochastic gradient methods.


# that this work considers and then details an approach to performing Bayesian inference in such models.

# Motivated by control,
# Given this data set $\dataset$, this chapter constructs a discrete-time, probabilistic
# representation of the multimodal transition dynamics
# and then formulates it as a probabilistic model based on GPs.

# the system's transition dynamics
# and then formulates it as a probabilistic model based on GPs.

# that pairs of input $\x$ and output $\y$ observations
# have previously been sampled from the system at a constant frequency,
# (i.e. with a fixed time-step) to give a data set $\dataset$.

# introduces the class of continuous-time multimodal dynamical systems

# This chapter introduces the class of continuous-time multimodal dynamical systems
# that this work considers and then details an approach to performing Bayesian inference in such models.

# Throughout this chapter it is assumed that pairs of input $\x$ and output $\y$ observations
# have previously been sampled from the system at a constant frequency,
# (i.e. with a fixed time-step) to give a data set $\dataset$.
# Based on this assumption, this chapter constructs a discrete-time representation of the system's transition dynamics
# and then formulates it as a probabilistic model based on GPs.

# This thesis is interested in multimodal systems where the dynamics modes vary over the
# input (state, control) domain, such that the mode switching can by modelled
# by input-dependent functions.
# This set of functions governing the mode switching is commonly referred to as a gating network.

# This work is motivated by data-efficient learning and specifically focuses on gating networks
# where prior knowledge of the system can be encoded into the model via informative priors.
# As a result, the probabilistic model constructed in this chapter resembles a \acrfull{mogpe}
# with a \acrshort{gp}-based gating network.
# This work derives a novel variational lower bound based on sparse GPs that
# enables the model to be trained with stochastic gradient methods.
# The work in this chapter is implemented in GPflow/TensorFlow and is available on GitHub
# cite:matthewsGPflow2017,GPflow2017,tensorflow2015-whitepaper[fn:1:Code accompanying this Chapter can be found @ [[https://github.com/aidanscannell/mogpe][\icon{\faGithub}/aidanscannell/mogpe]].].

** Problem Statement
This work is interested in learning /unknown/, or /partially unknown/  multimodal,
stochastic, nonlinear dynamical systems,
with continuous states $\state \in \stateDomain \subseteq \R^\StateDim$ and controls
$\control \in \controlDomain \subseteq \R^\ControlDim$, given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-multimodal-dynamics-cont}
\dot{\state}(t) &=
\mathbf{g}(\state(t), \control(t)) + \bm\epsilon(t) & \nonumber \\
&= \mode{\mathbf{g}}(\state(t), \control(t)) + \mode{\bm\epsilon}(t)
\quad &\text{if} \quad \alpha(t)=\modeInd, \nonumber \\
%&= \underbrace{\nominalDynamics(\state(t), \control(t))}_{\text{nominal}}
%+ \underbrace{\unknownDynamics(\state(t), \control(t))}_{\text{unknown}}
%+ \underbrace{\bm\epsilon(t)}_{\text{process noise}} \\
% &= \fk(\state(t), \control(t)) + \mode{\bm\epsilon}(t)
&= \underbrace{\nominalDynamicsK(\state(t), \control(t))}_{\text{nominal mode \modeInd}}
+ \underbrace{\unknownDynamicsK(\state(t), \control(t))}_{\text{unknown error mode \modeInd}}
+ \mode{\bm\epsilon}(t)
\quad &\text{if} \quad \alpha(t)=\modeInd,
%\bm\mode{\epsilon}(t) &\sim \mathcal{N}\left(\mathbf{0}, \bm\Sigma_{\mode{\epsilon}} \right),
\end{align}
#+END_EXPORT
where $\dot{\state}$ denotes the state derivative w.r.t. time, i.e. Newton's dot notation.
One of the $\ModeInd$ dynamics modes,
$\{\mode{\mathbf{g}} : \stateDomain \times \controlDomain \rightarrow \stateDomain \}_{\modeInd=1}^\ModeInd$, and associated noise models,
$\mode{\bm\epsilon}(t) &\sim \mathcal{N}\left(\mathbf{0}, \bm\Sigma_{\mode{\bm\epsilon}} \right)$, where
$\bm\Sigma_{\mode{\epsilon}} = \text{diag}\left[ \sigma_{\modeInd,1}^{2}, \ldots, \sigma_{\modeInd,\StateDim}^2 \right]$,
are selected by a discrete mode indicator variable,
$\alpha(t) \in \modeDomain = \mathbb{Z} \cap [1,\ModeInd]$, at any given time $t$.
The nominal dynamics
$\{\nominalDynamicsK : \stateDomain \times \controlDomain \rightarrow \stateDomain \}_{\modeInd=1}^\ModeInd$
represents the known dynamics and the additive dynamics
$\{\unknownDynamicsK : \stateDomain \times \controlDomain \rightarrow \stateDomain \}_{\modeInd=1}^\ModeInd$
represents the initially unknown dynamics, which are to be learned from observations.
This work considers systems that are stochastic (i.e. subject to process noise), but
are not subject to observation noise.
Thus, the $\mode{\bm\epsilon}$ term solely represents the process noise.

This chapter assumes access to historical data comprising state transitions from $\NumEpisodes$ trajectories
of length $T$, sampled  with a  fixed  time step $\Delta t=t_{*}$.
The data set has ${N=ET}$ elements
and we abuse notation by appending the independent trajectories along
time to get the data set $\mathcal{D}=\{(\state_{\timeInd-1},\control_{\timeInd-1}),\Delta \state_t\}^\NumData_{\timeInd=1}$.
The discrete-time representation of Eq. ref:eq-multimodal-dynamics-cont, corresponding to this data set,
is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-multimodal-dynamics-disc}
\Delta \state_{\timeInd} &= \mode{\mathbf{g}} (\state_{\timeInd-1}, \control_{\timeInd-1} ; \Delta t = t_*) + \mode{\bm\epsilon}
&\text{if} \quad \modeVar_{\timeInd} = \modeInd \nonumber \\
&= \underbrace{\nominalDynamicsK(\state_{t-1}, \control_{t-1} ; \Delta t = t_*)}_{\text{nominal mode \modeInd}}
+ \underbrace{\unknownDynamicsK(\state_{t-1}, \control_{t-1} ; \Delta t = t_*)}_{\text{unknown error mode \modeInd}}
+ \mode{\bm\epsilon}
&\text{if} \quad \alpha_{t} = \modeInd,
\end{align}
#+END_EXPORT
where
$\Delta \state_{\timeInd} = \state_{\timeInd} - \state_{\timeInd-1}$ is the change in state between time $\timeInd-1$
and $\timeInd$ and
$\state_t \in \stateDomain$, $\control_t \in \controlDomain$ and $\alpha_t \in \modeDomain$
are the state, control and mode indicator variable at time, $t$, respectively.

To help with understanding and ease of notation, our modelling only considers a single output dimension,
i.e. $\fk : \stateDomain \times \controlDomain \rightarrow \stateDomain$ is assumed to
be $\mode{\latentFunc} : \R \times \controlDomain \rightarrow \R$.
The extension to multiple output dimensions follows from standard GP methodologies and is detailed where necessary.
To further ease notation, the state-control input domain is denoted
$\inputDomain = \stateDomain \times \controlDomain \subseteq \R^{\InputDim}$,
and a single state-control input is dentoed
$\singleInput = (\state_{\timeInd-1},\control_{\timeInd-1})$.
Given this formulation, our task is to learn the mapping $\unknownDynamics$,
where the mapping switches between $\ModeInd$ different functions $\mode{\latentFunc}$.
This is a regression problem,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-multimodal-dynamics-disc-regression}
\underbrace{\Delta \state_t - \nominalDynamicsK(\state_{t-1}, \control_{t-1} ; \Delta t = t_*)}_{\text{\singleOutput}}
= \underbrace{\unknownDynamicsK(\state_{t-1}, \control_{t-1} ; \Delta t = t_*)}_{\text{\mode{\latentFunc} (\singleInput)}}
+ \mode{\epsilon}
\quad \text{if} \quad \modeVarK,
\end{align}
#+END_EXPORT
where both the latent dynamics functions $\unknownDynamicsK$ and how the system switches between them
$\modeVar$, must be inferred from observations.
A single output is denoted as
$\singleOutput = \Delta \state_t - \nominalDynamicsK(\state_{t-1}, \control_{t-1} ; \Delta t = t_*)$.
The set of all inputs is denoted as $\allInput \in \R^{\NumData \times \InputDim$
and the set of all outputs as $\allOutput \in \R^{\NumData \times 1}$.
With this notation, the regression problem can be written as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-multimodal-dynamics-disc-regression-simple}
\singleOutput
= \mode{\latentFunc}(\singleInput) + \mode{\epsilon}
\quad \text{if} \quad \modeVarK.
\end{align}
#+END_EXPORT


# from inputs $\singleInput$ to
# outputs $\singleOutput$,


# Given this notation, our task is to learn the mapping $\f$, from inputs $\singleInput$ to
# outputs $\singleOutput$,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-multimodal-dynamics-disc}
# \singleOutput = \mode{\latentFunc} (\singleInput) + \mode{\epsilon}
# \quad \text{if} \quad \modeVarK,
# \end{align}
# #+END_EXPORT
# where the mapping switches between $\ModeInd$ different functions $\mode{\latentFunc}$.

# The $\stateDim^{\text{th}}$ dimension of the $\modeInd^{\text{th}}$ mode's latent transition dynamics
# function $\fk$, evaluated at $\singleInput$,
# is denoted $\fknd = \fkd (\singleInput)$,
# for all dimensions as $\fkn$ and at all data points as $\Fk$.

# A time series of observations from time $a$ to time $b$ (inclusive)
# is denoted by $\mathbf{x}_{a:b}$ (analogously for other variables).
# The concatenation of the state and control domains is
# denoted $\inputDomain \coloneqq \stateDomain \times \controlDomain$.
# A single state-control input is denoted as
# $\singleInput = (\state_{t-1}, \control_{t-1})$, all inputs are denoted as
# $\allInput$, and the set of all outputs as $\allOutput$.
# The $\stateDim^{\text{th}}$ dimension of the $\modeInd^{\text{th}}$ mode's latent transition dynamics
# function $\fk$, evaluated at $\singleInput$,
# is denoted $\fknd = \fkd (\singleInput)$,
# for all dimensions as $\fkn$ and at all data points as $\Fk$.

# This chapter assumes access to a historical data set, $\dataset$, comprising state transitions
# from $\NumEpisodes$ trajectories (episodes), sampled with a fixed time step, $\Delta t=t_*$, resulting in
# $\NumTimesteps$ steps per episode.
# This gives a data set with $N = \NumEpisodes \NumTimesteps$ elements.


# giving the data set,
# $~{\mathcal{D} = \left\{ \left\{ (\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t} \right\}_{\numTimesteps=1}^{\NumTimesteps} \right\}_{\numEpisodes=1}^{\NumEpisodes}}$.

# This chapter assumes access to a historical data set, $\dataset$, comprising state transitions
# from $\NumEpisodes$ trajectories (episodes), sampled with a fixed time step, $\Delta t=t_*$, resulting in
# $\NumTimesteps$ steps per episode.
# This gives a data set with $N = \NumEpisodes \NumTimesteps$ elements.
# We combine the independent episodes to get the data set
# $\mathcal{D} = \left\{ \left\{ (\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t} \right\}_{\numTimesteps=1}^{\NumTimesteps} \right\}_{\numEpisodes=1}^{\NumEpisodes}$.
# We abuse notation (considering time indexing) and denote the data set
# $\mathcal{D} = \left\{ (\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t} \right\}_{\numTimesteps=1}^{\NumTimesteps}$.

# We combine the independent episodes to get the data set
# $\mathcal{D} = \left\{(\mathbf{x}_{t-1}, \mathbf{u}_{t-1}), \Delta\mathbf{x}_{t}\right\}_{t=1}^{T}$
# where we have abused the time indexing notation.

# This work learns a discrete-time representation of Eq. ref:eq-multimodal-dynamics-cont,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-multimodal-dynamics-disc}
# \singleOutput = \fk (\singleState, &\singleControl ; \Delta t = t_*) + \mode{\epsilon_{t-1}}
# \quad \text{if} \quad \modeVarK,
# \end{align}
# #+END_EXPORT
# where $\state_t \in \R^D$ and $\control_t \in \R^F$ are the states and controls
# at time $t$ respectively, and $\alpha_t \in \modeDomain = \{1, \dotsc, \ModeInd\}$ is the mode indicator variable (that
# indicates one of $\ModeInd$ dynamics modes) at time $t$.

# A time series of observations from time $a$ to time $b$ (inclusive)
# is denoted by $\mathbf{x}_{a:b}$ (analogously for other variables).
# The concatenation of the state and control domains is
# denoted $\inputDomain \coloneqq \stateDomain \times \controlDomain$.
# A single state-control input is denoted as
# $\singleInput = (\state_{t-1}, \control_{t-1})$, all inputs are denoted as
# $\allInput$, and the set of all outputs as $\allOutput$.
# The $\stateDim^{\text{th}}$ dimension of the $\modeInd^{\text{th}}$ mode's latent transition dynamics
# function $\fk$, evaluated at $\singleInput$,
# is denoted $\fknd = \fkd (\singleInput)$,
# for all dimensions as $\fkn$ and at all data points as $\Fk$.

** Motivation and Related Work
*** intro :ignore:
# In particular, trajectory optimisation algorithms that find trajectories that remain in a desired dynamics mode.
# This chapter is motivated by learning representations of multimodal dynamical systems
# that can be exploited by model-based control algorithms.
# One caveat to model-based control is that it requires a relatively accurate mathematical model of
# the system.
# Traditionally, these mathematical models are built using first principles based on physics.
# However, accurately modelling the underlying transition dynamics can be challenging and
# lead to the introduction of model errors.
# For example, both observation and process noise
# are inherent in many real-world systems and can be hard to model
# due to both spatial and temporal variations.
# Incorrectly accounting for this uncertainty can have a detrimental impact on controller performance
# and is an active area of research in the robust
# and stochastic optimal control communities citep:freemanRobust1996,stengelStochastic1986.

# The difficulties associated with constructing mathematical models can be overcome by learning from
# observations cite:ljungSystem1999.
# However, learning dynamics models for control introduces other difficulties.
# For example, it is important to know
# where the model cannot predict confidently due to a lack of training observations.
# This concept is known as epistemic uncertainty and is reduced in the limit of infinite data.

This chapter is motivated by learning representations of multimodal dynamical systems
that can be exploited by model-based control algorithms.
Probabilistic models provide the capability of constructing mathematical
models that can represent and manipulate uncertainty in data, models, decisions and
predictions.
\marginpar{probabilistic models}
Learning representations of dynamical systems for control using probabilistic models has shown much promise.
For example, see
cite:deisenrothPILCO2011,schneiderExploiting1996,chuaDeep2018,hafnerLearning2019,cutlerEfficient2015,deisenrothPILCO2011,panProbabilistic2014.
# In particular, use Gaussian processes (GPs) to learn
# transition dynamics models.

Our work is interested in learning and control in multimodal dynamical systems.
As such, a primary interest is correctly /identifying/ the underlying dynamics modes.
That is, ensuring that the learned dynamics modes accurately model the true underlying dynamics modes.
\marginpar{identifiability}
Incorrectly identifying the underlying dynamics modes will have a detrimental impact on performance
and may even lead to system failure.
# Learning
# and inferring latent spaces that are convenient for control.

The introduction of /latent variables/ into probabilistic models is a key component providing them with interesting
and powerful capabilities for synergising model learning and control.
For example, cite:hafnerLearning2019,rybkinModelBased2021
\todo{add more latent space dynamics refs}
learn /latent spaces/ which provide convenient spaces for planning.
\marginpar{latent spaces for control}
Of particular interest in this work, is how the system switches between the underlying dynamics modes.
As such, this work is interested in learning /latent spaces/
that are rich with information regarding how a system switches between its underlying dynamics modes.

With this in mind, our main goals are to construct a model which,
1. can /identify/ the true underlying dynamics modes accurately,
   - this could be achieved by providing handles for encoding informative domain knowledge, in turn, constraining the set of admissible functions,
2. has a convenient /latent space/ for planning,
   - this space should be rich with information regarding mode switching.

Methods for learning probabilistic multimodal transition dynamics have been proposed.
cite:moerlandLearning2017 use deep generative models, namely a conditional Variational Auto-Encoder,
to learn multimodal transition dynamics for Model-Based Reinforcement Learning.
However, encoding expert domain knowledge into Variational Auto-Encoders is difficult, which makes
it hard to ensure that the correct underlying dynamics modes are learned.

Early work combining hidden Markov models (HMM) and linear dynamical systems, known as
switching state-space models cite:ghahramaniVariational2000,ghahramaniSwitching1996,
offer a dynamical generalisation of Mixture of Experts.
These models have been extended to nonlinear dynamics modes but are limited in
the switching behaviour that they can model.
In particular, they do not provide mechanisms for placing informative priors on the switching behaviour.
\todo{is this right about HMM? Can they incorporate GP priors}

# cite:kaiserBayesian2020 use a Bayesian model that learns independent
# dynamics modes whilst maintaining a
# probabilistic belief over which mode is responsible for predicting at a given input location.
# The methods differs as it does not assign observations to modes.
cite:mckinnonLearning2017 use a Mixture of Gaussian Process Experts (MoGPE) method to learn switching
robot dynamics online.
Their model uses a gating network based on a Dirichlet process to model how the system switches between the underlying
modes. The Dirichlet process models the switching behaviour via clustering.
As such, it cannot model complex nonlinear dependencies between the switching behaviour
and the state-control inputs.
For example, if a dynamical system oscillates between two dynamics modes over its state-control input space,
their method would assign new dynamics modes at each oscillation.
In contrast, we seek to learn only the two true dynamics modes and turn them
"on" and "off" in different regions of the input space.

Gaussian processes are the state-of-the art approach for Bayesian nonparametric regression
and they provide a powerful mechanism for encoding expert domain knowledge.
As such, MoGPE methods are a promising direction for modelling multimodal systems.
Motivated by improving identifiability and learning latent spaces for control, we now
review the MoGPE literature, starting from their origin, the mixture model.

# Let us now introduce mixture models and detail how they are extended to
# Let us now introduce mixture models and detail how they are extended to

# Looking at some of the MoGPE methods from the literature, it is clear that many are motivated
# by the computational issues GP methods.

# Mixture models are a natural choice for modelling multimodal systems.
# After highlighting the inherent identifiability issues associated with mixture models, it introduces the Mixture of
# Experts (MoE) model, an extension to mixture models where
# the mixture weight (aka mixing probability) depends on the input variable.
# It then details how it breaks down for nonparametric experts
# and introduces the infinite Mixture of Gaussian Process Experts (MoGPE) model from the literature,
# which is motivated by the computational issues associated with inference and prediction.
# Although our model suffers from the complexity issues highlighted by cite:rasmussenInfinite2001, our variational
# inference scheme significantly improves scalability of inference and prediction in our model, whilst providing
# attractive mechanisms for improving identifiablity.



# In contrast to this approach, our work seeks mechanisms for encoding expert domain


# This work follows a similar approach but is interested in learning /latent spaces/ for multimodal dynamical systems
# that are rich with information regarding how the systems switches between the underlying dynamics modes.


# Gaussian processes are the state-of-the art approach for Bayesian nonparametric regression.
# However, they suffer from two important limitations.
# Firstly, the covariance function is commonly assumed to be stationary
# due to the challenge of parameterising them to be non-stationary.
# This limits their modelling flexibility.
# For example, if the function has a discontinuity due to different underlying lengthscales
# in different parts of the input space then a stationary covariance function will not be adequate.
# Similarly, if the observations are subject to different noise variances in different regions
# of the input space then conventional homoscedastic regression will not suffice.
# Secondly, GPs cannot model multimodal distributions, i.e. where there are multiple
# regions of high probability mass with regions of low probability mass in between.
# # GP regression with a Gaussian likelihood models a Gaussian predictive distribution
# # that is not capable of modelling such multimodal distributions.


# Let us now introduce mixture models as they are a natural choice for modelling
# multimodal systems.
# After highlighting the inherent identifiability issues associated with mixture models, it introduces the Mixture of
# Experts (MoE) model, an extension to mixture models where
# the mixture weight (aka mixing probability) depends on the input variable.
# It then details how it breaks down for nonparametric experts
# and introduces the infinite Mixture of Gaussian Process Experts (MoGPE) model from the literature,
# which is motivated by the computational issues associated with inference and prediction.
# Although our model suffers from the complexity issues highlighted by cite:rasmussenInfinite2001, our variational
# inference scheme significantly improves scalability of inference and prediction in our model, whilst providing
# attractive mechanisms for improving identifiablity.

*Mixture Models*
Mixture models are a natural choice for modelling multimodal systems.
Given an input $\singleInput$ and an output $\singleOutput$, mixture models
model a mixture of distributions over the output,
\marginpar{mixture models}
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-mixture-model}
\singleMoeEvidence = \sum_{\modeInd=1}^\ModeInd
\underbrace{\Pr(\singleModeVarK \mid \gatingParams)}_{\text{mixing probability}}
\underbrace{p(\singleOutput \mid \modeVarK, \singleInput, \expertParamsK)}_{\text{component } k}.
\end{equation}
#+END_EXPORT
where $\gatingParams$ and $\expertParams$ represent model parameters
and $\singleModeVar$ is a discrete indicator variable assigning observations to components.
The predictive distribution $\singleMoeEvidence$ consists of
$K$ mixture components ${p(\singleOutput \mid \modeVarK, \singleInput, \expertParamsK)}$ that are weighted according to
the mixing probabilities $\Pr(\modeVarK \mid \gatingParams)$.

*Mixture of Experts (MoE)* The Mixture of Experts model citep:jacobsAdaptive1991 is
an extension where the mixing probabilities
depend on the input variable $\moeGatingPosterior$, and are
collectively referred to as the *gating network*.
The individual component densities $\moeExpertPosterior$ are then referred to as *experts*,
as at different regions in the input space, different components are responsible for predicting.
Given $\ModeInd$ experts $\moeExpertPosterior$, each with parameters $\expertParamsK$,
the MoE marginal likelihood is given by,
\marginpar{mixture of experts}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixture-marginal-likelihood}
\moeEvidence = \prod_{\numData=1}^\NumData \sum_{\modeInd=1}^{\ModeInd}
\underbrace{\moeGatingPosterior}_{\text{gating network}}
\underbrace{\moeExpertPosterior}_{\text{expert } k},
\end{align}
#+END_EXPORT
where $\singleModeVar$ is the expert indicator variable assigning observations to experts.
The probability mass function over the expert indicator variable is referred to as the gating network and
indicates which expert governs the model at a given input location.
See cite:yukselTwenty2012 for a survey of MoE methods.

*** Nonparametric Mixtures of Experts :ignore:

\newline
*Nonparametric Mixtures of Experts*
Modelling the experts as GPs gives rise to a class of powerful models known as
Mixture of Gaussian Process Experts (MoGPE).
They can model multimodal distributions as they model a mixture of distributions
over the outputs, usually a Gaussian mixture in the regression setting citep:trespMixtures2000a,rasmussenInfinite2001.
They are able to model non-stationary functions as
each expert learns separate hyperparameters (e.g. lengthscales, noise variances).
Many MoGPE methods have been proposed and in general they differ via
the formulation of their gating network and their approximate inference algorithms.
# and the gating network can turn each expert "on" and "off" in different regions of the input space.

As highlighted by cite:rasmussenInfinite2001,
the traditional MoE marginal likelihood does not apply when the
experts are nonparametric.
This is because the model assumes that the observations are i.i.d. given the model parameters,
which is contrary to GP models, which model the dependencies in the joint distribution, given the
hyperparameters.
\marginpar{mixture of nonparametric experts}
cite:rasmussenInfinite2001 point out that there is a joint distribution corresponding to every possible
combination of assignments (of observations to experts).
The marginal likelihood is then a sum over exponentially many ($\ModeInd^{\NumData}$) sets of assignments,
#+BEGIN_EXPORT latex
\small
\begin{align}  \label{eq-np-moe-marginal-likelihood-assign}
\moeEvidence &= \sum_{\allModeVar} \npmoeGatingPosterior \npmoeExpertPosterior  \nonumber \\
&= \sum_{\allModeVar} \npmoeGatingPosterior
\left[ \prod_{\modeInd=1}^\ModeInd p\left(\allOutputK \mid \allInputK, \expertParamsK \right) \right],
\end{align}
\normalsize
#+END_EXPORT
where $\allModeVar = \{\modeVar_1, \ldots, \modeVar_\NumData \}$ represents a set of assignments for all observations.
Note that $\allInputK = \{\singleInput : \singleModeVarK \}_{\numData=1}^{\NumData_{\modeInd}}$ and
$\allOutputK = \{\singleOutput : \singleModeVarK \}_{\numData=1}^{\NumData_{\modeInd}}$
indicate the sets of $\NumData_{\modeInd}$ inputs and outputs assigned to the $\modeInd^{\text{th}}$
expert respectively.
This distribution factors into the product over experts, where each expert models the joint Gaussian distribution
over the observations assigned to it.
Assuming a mixture of Gaussian process regression models,
the marginal likelihood in cref:eq-np-moe-marginal-likelihood-assign
can be expanded to show each of the experts' latent variables,
#+BEGIN_EXPORT latex
\marginpar{marginal likelihood}
\begin{align} \label{eq-np-moe-marginal-likelihood}
%\moeEvidence
\evidence
%&= \sum_{\allModeVar} \npmoeGatingPosterior \npmoeExpertPosterior  \\
%&= \sum_{\allModeVar} \npmoeGatingPosterior
%\left[ \prod_{\modeInd=1}^\ModeInd p\left(\allOutputK \mid \allInputK, \expertParamsK \right) \right]   \\
&= \sum_{\allModeVar} \npmoeGatingPosterior
\left[ \prod_{\modeInd=1}^\ModeInd
\E_{\expertPrior \right} \left[
\prod_{\numData=1}^{\NumData_{\modeInd}}
p\left(\singleOutput \mid \mode{\latentFunc}(\singleInput) \right)
\right] \right],
\end{align}
#+END_EXPORT
where each expert follows the standard Gaussian likelihood model,
\marginpar{expert's Gaussian likelihood}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-likelihood}
\singleOutput &= \mode{f}(\singleInput) + \epsilon_{k}, \quad \epsilon_{k} \sim \mathcal{N}(0, \mode{\noiseVar}^2), \\
\singleExpertLikelihood &= \mathcal{N}\left( \singleOutput \mid \mode{f}(\singleInput), \mode{\noiseVar}^2 \right),
%\singleOutput \mid \modeVar, \mode{\latentFunc}(\singleInput) &\sim \mathcal{N}\left( \mode{f}(\singleInput), \mode{\noiseVar}^2 \right),
\end{align}
#+END_EXPORT
with $\mode{f}$ and $\mode{\noiseVar}^2$ representing the latent function and the noise variance associated
with the $\modeInd$^{\text{th}} expert.
Note that for notational conciseness the dependency on $\modeVarK$ is dropped from
$\singleExpertLikelihood$ as it is implied by the mode indexing $\mode{\latentFunc}$.
The dependence on $\gatingParams$ and $\expertParams$ is also dropped from here on in.
Independent GP priors are placed on each of the expert's latent functions,
\marginpar{expert's GP prior}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior}
%\expertPrior &= \mathcal{N}\left( \mode{f}(\allInput) \mid \expertMeanFunc(\allInput), \expertCovFunc(\allInput, \allInput) \right) \\
%\mode{\latentFunc}(\allInput) &\sim \mathcal{N}\left( \expertMeanFunc(\allInput), \expertCovFunc(\allInput, \allInput) \right) \\
\mode{\latentFunc}(\allInputK) &\sim \mathcal{N}\left( \expertMeanFunc(\allInputK), \expertCovFunc(\allInputK, \allInputK) \right)
\end{align}
#+END_EXPORT
where $\expertMeanFunc(\cdot)$ and $\expertCovFunc(\cdot, \cdot)$ represent the mean and
covariance functions associated with the $\modeInd^{\text{th}}$ expert respectively.
This leads to each expert resembling a standard GP regression model with a Gaussian likelihood.
For notational conciseness the dependence on the inputs $\allInputK$ and hyperparameters $\expertParamsK$
is dropped for each GP prior,
i.e. $\expertPrior = p\left( \mode{f}(\allInputK) \mid \allInputK, \expertParamsK \right)$.
See cref:fig-graphical-model-npmoe for a graphical model representation.
# Note that the expectation over each experts' latent function in cref:eq-np-moe-marginal-likelihood
# is inside the marginalisation of the expert indicator variable $\modeVar$.

*** npmoe graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
    \begin{minipage}[r]{0.49\textwidth}
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInputK$};

      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\allInputK)$};

      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[latent, below=of x, xshift=0.0cm] (a) {$\modeVarnk$};

      \node[const, right=of a, xshift=-0.4cm] (phik) {$\gatingParams$};

      \node[obs, below=of f, yshift=0.4cm] (y) {$\allOutputK$};

      \node[const, left=of y] (sigmak) {$\noiseVarK$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      %\draw[post] (f)--(yk);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)--(a);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(a);
      %\draw[post] (sigmak)|-(yk);
      \draw[post] (sigmak)|-(y);
      \draw[post] (f)--(y);

      \plate {} {(x) (yk) (a)} {$\NumData_{\modeInd}$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      %\plate {} {(f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(x) (y) (f) (sigmak) (thetak) (a)} {$\ModeInd$};
    \end{tikzpicture}
    %}
    \subcaption{}
\label{fig-graphical-model-npmoe}
\end{minipage}
    \begin{minipage}[r]{0.49\textwidth}
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInputK$};

      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\allInputK)$};
      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[latent, below=of x, xshift=0.0cm] (a) {$\modeVarnk$};
      \node[latent, right=of a, yshift=0.0cm] (h) {$\mode{\gatingFunc}(\allInput)$};
      \node[const, right=of h, xshift=-0.4cm] (phik) {$\gatingParamsK$};

      \node[obs, below=of f, yshift=0.4cm] (y) {$\allOutputK$};
      \node[const, left=of y] (sigmak) {$\noiseVarK$};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      \draw[post] (x)-|(h);
      \draw[post] (h)--(a);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      %\draw[post] (sigmak)|-(yk);
      \draw[post] (sigmak)|-(y);
      \draw[post] (f)--(y);

      \plate {} {(x) (yk) (a)} {$\NumData_{\modeInd}$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      %\plate {} {(f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(h) (phik)} {$\ModeInd$};
      \plate {} {(x) (y) (f) (sigmak) (thetak)  (a)} {$\ModeInd$};
    \end{tikzpicture}
    %}
    \subcaption{}
\label{fig-graphical-model-gp-gating-network}
\end{minipage}
  %\caption{Graphical model where the output $\singleOutput$}
  \caption{
  Graphical models where the outputs $\allOutput = \{\allOutputK\}_{\modeInd=1}^{\ModeInd}$
are generated by mapping the inputs $\allInput = \{\allInputK\}_{\modeInd=1}^{\ModeInd}$ through the latent process.
  An input assigned to expert $\modeInd$ is denoted $\singleInputK$
  and the sets of all $\NumData_{\modeInd}$ inputs and outputs assigned to expert $\modeInd$ are denoted
  $\allInputK$ and $\allOutputK$ respectively.
The experts are shown on the left of each model and the gating network on the right.
The generative process involves evaluating the gating network
and sampling an expert mode indicator variable $\singleModeVar$.
The indicated mode's latent function $\mode{f}$ and noise model $\mathcal{N}(0, \noiseVarK)$ are
then evaluated to generate the output $\singleOutput$.
  (\subref{fig-graphical-model-npmoe}) shows
   the Mixture of Gaussian Process Experts model first presented in
  \cite{rasmussenInfinite2001} but without the Dirichlet process prior on the gating network.
  This represents the basic conditional model, not the full generative model over both the inputs and outputs as
  presented in \cite{NIPS2005_f499d34b}.
  (\subref{fig-graphical-model-gp-gating-network}) shows our model with a GP-based gating network
which involves evaluating $\ModeInd$ latent gating functions $\GatingFunc$
and normalising their output to obtain the mixing probabilities $\singleGatingLikelihood$.
The mode indicator variable $\singleModeVar$ is then sampled from the Categorical distribution
governed by these probabilities.}
\label{fig-graphical-model-comparison}
\end{figure}
#+END_EXPORT

*** Gating Networks and Identifiability
# *** Gating Networks :ignore:
# \newline
# *Gating Networks*
Mixture models are inherently unidentifiable as different combinations of mixture components
and mixing probabilities can generate the same distributions over the output.
The gating network can be considered a handle for encoding prior knowledge that can be used to constrain
the set of admissible functions.
This can improve identifiability and lead to learned representations that better reflect
our understanding of the system. The simplest case being reordering the experts.
cref:gating_network_comparison provides a visual comparison of
different gating network formulations,
providing intuition for how they can differ in restricting the set of admissible functions.

# An EM inference scheme is proposed, which assuming there are $N$ observations, requires
# $\mathcal{O}(3KN^3)$ computations per iteration.
# Importantly, this gating network is capable of turning a single expert "on" in multiple regions of the input space.
The original MoGPE work by cite:trespMixtures2000a, proposed a gating network resembling a GP classification
model, i.e. a softmax likelihood with independent GPs placed over each of the $K$ latent functions.
Importantly, this gating network provides a mechanism for encoding informative prior knowledge,
that can improve identifiability by restricting the set of admissible functions.
This is achieved by placing informative GP priors over the gating functions.
As such, this gating network is capable of modelling complex nonlinear dependencies between the
input-space and the expert indicator variable.
For example, it is capable of turning a single expert "on" in multiple regions of the input space.
This is visualised in the bottom left plot in cref:gating_network_comparison, where
the left and right regions (shaded green) are subsets of the domain where expert
one has generated the observations (observations are not shown) and the middle region (shaded blue) where expert two has.
#+NAME: gating_network_comparison
#+ATTR_LATEX: :width 0.95\textwidth :placement [h] :center t
#+caption: Comparison of different gating networks (one per column).
#+caption: The bottom plots show the expert mixing probabilities $p(\alpha=k|x)$
#+caption: and the top plots show any associated distributions over the inputs.
#+caption: The left and right regions (shaded green) are subsets of the domain where expert
#+caption: one has generated the observations (observations are not shown) and the middle region (shaded blue) where expert two has.
#+caption: The bottom left plot shows the mixing probabilities of a gating network based on GPs.
#+caption: The middle top plot shows three separate Gaussian pdfs representing clustering of the inputs
#+caption: (we have introduced a cluster indicator variable $z$).
#+caption: The middle bottom plot shows the associated expert mixing probabilities (assuming $z=\alpha$).
#+caption: In the right top plot we have assumed that there are only two experts (with $z\neq \alpha$) and
#+caption: marginalised the cluster indicator variable from the plot to the left - leading to
#+caption: a Gaussian mixture over the inputs.
#+caption: The plot below shows the resulting mixing probabilities.
[[file:images/model/gating-network-comparison-2by3.pdf]]
# [[file:images/model/gating-network-comparison-2by3-cropped.pdf]]

# Although this gating network divides up the input space, cite:rasmussenInfinite2001 argue that
# data not assigned to a GP expert will lead to bias near the boundaries.
# Instead they formulate the gating network in terms of conditional
cite:rasmussenInfinite2001 introduced the infinite Mixtures of Gaussian Process Experts method
which automatically infers the number of experts from observations via an input-dependent
Dirichlet process prior.
cite:NIPS2005_f499d34b proposed an alternative infinite MoGPE that models the joint distribution
over the input and output space $p(\allOutput, \allInput)$,
as opposed to just a conditional model $p(\allOutput | \allInput)$.
These methods are not capable of turning a single expert "on" in multiple regions
of the input space. Instead they will generate a new expert.
This is illustrated in the middle column of cref:gating_network_comparison.
The top plot shows clustering of the input space
where each Gaussian cluster $z=\{1,\ldots, \ModeInd\}$
corresponds to the inputs for a particular expert $\alpha = \{1, \ldots, \ModeInd\}$.
These models assume that the cluster indicator variable $z$ equals the expert indicator variable $z=\alpha$.
This gives rise to the gating network in the middle bottom plot of cref:gating_network_comparison, where
a third expert $k=3$ has been introduced to model the right hand region associated with expert one.

# cite:rasmussenInfinite2001 formulate the gating network in terms of conditional
# distributions on the expert indicator variable, on which they place an input-dependent
# Dirichlet process prior.
# cite:Meeds2005 proposed an alternative infinite MoGPE similar to cite:rasmussenInfinite2001 except
# that they specify a full generative model over the input and output space $p(\mathbf{y}, \mathbf{x})$,
# as opposed to just a conditional model $p(\mathbf{y} | \mathbf{x})$.
# The input space is divided into Gaussian clusters where each cluster $z=\{1,\ldots, K\}$
# corresponds to the inputs for a particular expert $\alpha = \{1, \ldots, K\}$.
# This is shown in the middle top plot of Figure [[ref:gating_network_comparison]]
# where the cluster indicator variable equals the expert indicator variable $z=\alpha$.
# This gives rise to the gating network in the middle bottom plot of Figure [[ref:gating_network_comparison]], where
# a third expert $k=3$ has been introduced to model the right hand region associated with expert one.
# This gating network is not capable of turning an expert "on" in multiple regions
# of the input space. Instead it generates a new cluster and assign it a new expert.
# Although these approaches reduce the computational burden by associating each expert with a
# subset of the observations, they rely on Markov Chain Monte Carlo (MCMC) inference schemes.

cite:NIPS2008_f4b9ec30 proposed a MoGPE method with a variational inference scheme, which is much faster
than the previous approaches that rely on MCMC.
Their method introduces a separate cluster indicator variable $z$, resulting in each experts' inputs
following a Gaussian mixture model (GMM).
Introducing the separate cluster indicator variable enables the
gating network to turn a single expert "on" in multiple regions of the input space.
This is illustrated in the right hand plots of Figure [[ref:gating_network_comparison]], which
show Gaussian mixtures over each experts' inputs (top) and the resulting expert mixing probabilities (bottom).
This method enables a single expert to be turned "on" in multiple regions of the input space.
However, it still does not provide a handle for encoding prior information like the GP based gating network.
# prevents an explosion in the number of experts

# cite:Yuan proposed a MoGPE with a variational inference scheme which is much faster than using MCMC.
# They assume that the expert indicators $\alpha$ are generated by a Dirichlet distribution and
# that given the expert indicator the inputs follow a Gaussian mixture model (GMM), i.e. they
# introduce a separate cluster indicator variable $z$.
# This contrasts earlier approaches that let the expert indicator act as the input cluster
# indicator.
# This is illustrated in the right hand plots of Figure [[ref:gating_network_comparison]] which
# show two Gaussian mixtures over the inputs (one for each expert) and
# the resulting expert mixing probabilities.
# Introducing the separate cluster indicator variable has given rise to a
# gating network that can turn a single expert "on" in multiple regions of the input space.
# However, it does not provide a handle for encoding prior information like the GP based gating network.

*GP-based Gating Network* Modelling the gating network with a set of input-dependent gating functions
and a softmax likelihood, enables complex nonlinear dependencies between the expert indicator
variable and the input-space to be modelled.
If knowledge regarding how the model switches between the experts over the input-space is /known a priori/,
then this can be encoded via the choice of mean and covariance functions.
For example, adopting a squared exponential covariance function would encode prior belief that
the mixing probabilities should vary smoothly across the input space.
If modelling a dynamical system which oscillates between different dynamics modes over the input-space
(with a constant frequency), then a periodic kernel could be adopted.
More accurately modelling this dependency will improve identifiability and result in superior generalisation,
i.e. the model will be able to interpolate and extrapolate more accurately.

With regards to constructing convenient /latent spaces/ for control, formulating the gating network GPs
with differentiable mean and covariance functions, enables techniques from Riemannian geometry
to be deployed on the gating functions citep:carmoRiemannian1992.
In particular, our work is interested in finding length minimising trajectories on the
gating functions' GP posteriors, aka geodesic trajectories citep:tosiMetrics2014.


# Modelling the gating network with GPs (resembling a GP classification model) enables
# informative prior knowledge to be encoded through the choice of mean and covariance functions.
# For example, adopting a squared exponential covariance function would encode prior belief that
# the mixing probabilities should vary smoothly across the input space.
# If modelling a dynamical system with oscillatory behaviour (e.g. an engine)
# a periodic kernel could be adopted.
# Prior knowledge of the mixing probability values can be encoded through
# the choice of mean function.
# Our work is interested in exploiting the gating network for techniques from differential geometry,
# in particular, finding geodesics citep:tosiMetrics2014.
# Selecting mean and covariance functions which are differentiable with respect to their inputs, enables
# techniques from Riemannian geometry citep:carmoRiemannian1992 to be deployed on the gating functions.
# Learning better representations will also improve the models
# ability to extrapolate as it will be more true to the underlying system.

More recent MoGPE methods also exist citep:trappDeep2020,nguyenStochastic2018,gaddEnriched2020.
The method by cite:trappDeep2020 presents a Deep Structured Mixtures of Gaussian Processes
based on sum product networks.
It provides exact inference and attractive computational and memory costs but results in worse predictive densities
than a Sparse Variational Gaussian Process (SVGP).
This is indicated by worse Negative Log Predictive Density (NLPD) scores than a SVGP on 6 out of the 7
benchmark data sets it is tested on.
In contrast, our work seeks improved NLPD scores -- relative to a SVGP -- as we seek learned representations that
more accurately model the data distribution.
# cite:gaddEnriched2020 present a method that models the joint distribution of the inputs and targets explicitly.

\todo{talk more about nguyenStochastic2018,gaddEnriched2020}

# *Inference* We can also consider the implications of the mentioned gating networks from an inference perspective.
# It is known that developing inference algorithms for propagating uncertain (Gaussian) inputs
# through GPs is itself a challenging problem (see cite:ustyuzhaninovCompositional2020).
# Therefore, developing a scalable inference algorithm to propagate inputs that are distributed according to
# a Gaussian mixture will likely lead to valuable information loss.
# We consider our approach as trading in the computational benefits that can be obtained through
# the formulation of the gating network for the ability to improve identifiability with informative GP priors.

# Theoretically the approaches by cite:rasmussenInfinite2001,NIPS2005_f499d34b are able to achieve very
# accurate results but their inference relies on MCMC sampling methods, which can be slow to converge.

# The gating network models how the dynamics switch between modes over the input space and is of particular importance
# in this work.
# The gating network is of particular importance in this work as it models how the dynamics switch
# between modes over the input space.

** Related Work :noexport:
Modelling the experts as GPs gives rise to a class of powerful models known as
Mixture of Gaussian Process Experts (MoGPE).
These models address both of the limitations that we discussed.
They can model multimodal predictive distributions as they model a mixture of distributions
over the outputs, usually a Gaussian mixture in the regression setting (cite:Tresp,Rasmussen2002).
They are able to model non-stationary functions as
each expert can learn different lengthscales, noise variances etc and the gating
network can turn each one "on" and "off" in different regions of the input space.
Many approaches to MoGPE have been proposed and in general the key differences between them are
the formulation of the gating network
and their inference algorithms (and associated approximations) cite:Yuksel2012.
The original MoGPE work by cite:Tresp proposed a gating network as a softmax of $K$ GPs
(bottom left plot in Figure [[ref:gating_network_comparison]]).
An EM inference scheme is proposed, which assuming there are $N$ observations requires
$\mathcal{O}(3KN^3)$ computations per iteration.


Although this gating function divides up the input space cite:Rasmussen2002 argue that
data not assigned to a GP expert will lead to bias near the boundaries.
Instead cite:Rasmussen2002 formulate the gating network in terms of conditional
distributions on the expert indicator variable, on which they place an input-dependent
Dirichlet process prior.
cite:Meeds2005 proposed an alternative infinite MoGPE similar to cite:Rasmussen2002 except
that they specify a full generative model over the input and output space $p(\mathbf{y}, \mathbf{x})$
as opposed to just a
conditional model $p(\mathbf{y} | \mathbf{x})$.
Each experts inputs are now Gaussian distributed
(middle top plot of Figure [[ref:gating_network_comparison]]) giving rise to the gating
network in the middle bottom plot of Figure [[ref:gating_network_comparison]].
This gating network is not capable of turning an expert "on" in multiple regions
of the input space. It will instead generate a new cluster and assign it a new expert.
These approaches reduce the computational burden by associating each expert with a
subset of the observations but rely on Markov Chain Monte Carlo (MCMC) inference schemes.

cite:Yuan proposed a MoGPE with a variational inference scheme which is much
faster than using MCMC.
They assume that the expert indicators $\alpha$ are generated by a Dirichlet distribution and
that given the expert indicator the inputs follow a Gaussian mixture model (GMM), i.e. they
have introduced a cluster indicator variable $z$.
This contrasts earlier approaches that let the expert indicator act as the input cluster
indicator.
This is illustrated in the right hand plots of Figure [[ref:gating_network_comparison]] which
show two Gaussian mixtures over the inputs (one for each expert) and
the resulting expert mixing probabilities.
Introducing the separate cluster indicator variable has given rise to a
gating network that can turn a single expert "on" in multiple regions of the input space.
However, it does not provide a handle for encoding prior information like the GP based gating network.

We note that mixture models are inherently unidentifiable as
different combinations of mixture components
and mixing coefficients can generate the same predictive distributions.
The gating network can be considered a handle for encoding prior knowledge that can be used to constrain
the set of admissible functions.
This can improve identifiability and lead to learned representations that better reflect
our understanding of the system. The simplest case being reordering the experts.
Figure ref:gating_network_comparison provides a visual comparison of
different gating network formulations,
providing intuition for how they can differ in restricting the set of admissible functions.


Modelling the gating network with GPs enables us
to encode informative prior knowledge through the choice of mean and covariance functions.
For example, adopting a squared exponential covariance function would encode prior belief that
the mixing probabilities should vary smoothly across the input space.
If modelling a dynamical system with oscillatory behaviour (e.g. an engine)
a periodic kernel could be adopted.
Prior knowledge of the mixing probability values can be encoded through
the choice of mean function.
We may also be interested in exploiting techniques from differential geometry
e.g. finding probabilistic geodesics (cite:Tosi2014) on the gating network.
Again, our choice of covariance function can be used to encode how differentiable
the gating network should be.
Learning better representations will also improve the models
ability to extrapolate as it will be more true to the underlying system.

We can also consider the implications of the mentioned gating networks from an inference perspective.
It is known that developing inference algorithms for propagating uncertain (Gaussian) inputs
through GPs is itself a challenging problem (see cite:Ustyuzhaninov2019).
Therefore, developing a scalable inference algorithm to propagate inputs that are distributed according to
a Gaussian mixture will likely lead to valuable information loss.
Theoretically the approaches by cite:Rasmussen2002,Meeds2005 are able to achieve very
accurate results but their inference relies on MCMC sampling methods,
which can be slow to converge.

We consider our approach as trading in the computational benefits that can be obtained through
the formulation of the gating network
for the ability to improve identifiability with informative GP priors.
The main contributions of this paper are twofold. Firstly, we re-formulate a gating network
based on GPs to improve indentifiability.
Secondly, we derive a variational lower bound
which improves on the complexity issues associated with inference when adopting such a gating network.
Motivated by learning representations of dynamical systems with two regimes we instantiate
the model with two experts as it is  a special case where the gating network can be calculated
in closed form.
This is because we seek to learn an operatable mode with one expert and explain away the un-operatable mode
with the other. This results
in the gating network indicating which regions of the
input space are operatable, providing a convenient space for planning.

# The remainder of the paper is organised as follows. We first introduce our model
# in Section ref:sec-modelling
# and then derive our variational lower
# bound in Section ref:sec-variational-approximation.
# In Section ref:sec-model-validation we test our method on an artificial data set where we show
# the benefits of adopting informative covariance functions in the gating network.
# We then test our model on the motorcycle data set and compare it to a sparse variational GP.

** Probabilistic Modelling
*** intro :ignore:
Motivated by synergising model learning and control, the probabilistic model constructed in this chapter
resembles a \acrfull{mogpe} with a \acrshort{gp}-based gating network.
The \acrshort{gp}-based gating network infers latent /geometric/ structure that is exploited by the control
algorithm in Section ref:sec-traj-opt-geometric.
*** intro :ignore:noexport:
Motivated by synergising model learning and control, the probabilistic model constructed in this chapter
resembles a \acrfull{mogpe} with a \acrshort{gp}-based gating network.
The \acrshort{gp}-based gating network infers latent /geometric/ structure that is exploited by the control
algorithm in Section ref:sec-traj-opt-geometric.
# This work derives a novel variational lower bound based on sparse GP approximations, that provides
# well-calibrated uncertainty estimates and scalability via stochastic gradient methods.

# Given a factorised likelihood for each expert (e.g. Gaussian),
# an alternative approach to cref:eq-np-moe-marginal-likelihood-assign is to let each expert
# predict on the basis of all observations and leave the gating network to soft assign the observations.
# An alternative approach to cref:eq-np-moe-marginal-likelihood-assign is to let each expert
# predict on the basis of all observations (i.e. $\ModeVarK = \{\modeVarK\}_{\numData=1}^{\NumData}$)
# and leave the gating network to soft assign the observations.
# The marginal likelihood of this model is then given by,

The marginal likelihood in cref:eq-np-moe-marginal-likelihood-assign
can be expanded to show each of the experts' latent variables,
#+BEGIN_EXPORT latex
\marginpar{marginal likelihood}
\begin{align} \label{eq-np-moe-marginal-likelihood}
\moeEvidence &= \sum_{\allModeVar} \npmoeGatingPosterior \npmoeExpertPosterior  \\
&= \sum_{\allModeVar} \npmoeGatingPosterior
\left[ \prod_{\modeInd=1}^\ModeInd p\left(\allOutputK \mid \allInputK, \expertParamsK \right) \right]   \\
&= \sum_{\allModeVar} \npmoeGatingPosterior
\left[ \prod_{\modeInd=1}^\ModeInd
\prod_{\numData=1}^{\NumData}
p\left(\singleOutput \mid \mode{\latentFunc}(\singleInput) \right)
p\left(\mode{\latentFunc}(\allInputK) \mid \allInputK \right)
\right],
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \marginpar{marginal likelihood}
# \begin{align} \label{eq-np-moe-marginal-likelihood}
# \moeEvidence &=
# \sum_{\modeInd=1}^\ModeInd
# \underbrace{ \Pr\left(\allModeVarK \mid \allInput, \gatingParams \right)}_{\text{gating network}}
# \underbrace{ p(\allOutput \mid \allModeVarK, \allInput, \expertParamsK) }_{\text{expert } \modeInd} \\
# &=
# \sum_{\modeInd=1}^\ModeInd
# \underbrace{ \left[ \prod_{\numData=1}^\NumData
# \Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right) \right]}_{\text{gating network}}
# \underbrace{\E_{\expertPrior} \left[ \prod_{\numData=1}^\NumData \singleExpertLikelihood \right]}_{\text{expert } \modeInd},
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \marginpar{marginal likelihood}
# \begin{align} \label{eq-np-moe-marginal-likelihood}
# \moeEvidence &=
# \prod_{\numData=1}^\NumData \sum_{\modeInd=1}^\ModeInd
# \underbrace{\Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right)}_{\text{gating network}}
# \underbrace{\E_{\expertPrior} \left[ \singleExpertLikelihood \right]}_{\text{expert } \modeInd},
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \marginpar{marginal likelihood}
# \begin{align} \label{eq-np-moe-marginal-likelihood}
# \moeEvidence &=
# \underbrace{\E_{\expertPrior}}_{\text{expert } k \text{ prior}} \left[
# \prod_{\numData=1}^\NumData \sum_{\modeInd=1}^\ModeInd
# \underbrace{\Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right)}_{\text{gating network}}
# \underbrace{ \singleExpertLikelihood}_{\text{expert } k \text{ likelihood}}
# \right],
# \end{align}
# #+END_EXPORT
where each expert follows the standard Gaussian likelihood model,
\marginpar{expert's Gaussian likelihood}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-likelihood}
\singleOutput &= \mode{f}(\singleInput) + \epsilon_{k}, \quad \epsilon_{k} \sim \mathcal{N}(0, \mode{\noiseVar}^2), \\
\singleExpertLikelihood &= \mathcal{N}\left( \singleOutput \mid \mode{f}(\singleInput), \mode{\noiseVar}^2 \right),
%\singleOutput \mid \modeVar, \mode{\latentFunc}(\singleInput) &\sim \mathcal{N}\left( \mode{f}(\singleInput), \mode{\noiseVar}^2 \right),
\end{align}
#+END_EXPORT
with $\mode{f}$ and $\mode{\noiseVar}^2$ representing the latent function and the noise variance associated
with the $\modeInd$^{\text{th}} expert.
Note that the dependency on $\modeVarK$ is dropped from $\singleExpertLikelihood$ for notational conciseness.
Independent GP priors are placed on each of the expert's latent functions,
\marginpar{expert's GP prior}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior}
%\expertPrior &= \mathcal{N}\left( \mode{f}(\allInput) \mid \expertMeanFunc(\allInput), \expertCovFunc(\allInput, \allInput) \right) \\
%\mode{\latentFunc}(\allInput) &\sim \mathcal{N}\left( \expertMeanFunc(\allInput), \expertCovFunc(\allInput, \allInput) \right) \\
\mode{\latentFunc}(\allInputK) &\sim \mathcal{N}\left( \expertMeanFunc(\allInputK), \expertCovFunc(\allInputK, \allInputK) \right)
\end{align}
#+END_EXPORT
where $\expertMeanFunc(\cdot)$ and $\expertCovFunc(\cdot, \cdot)$ represent the mean and
covariance functions associated with the $\modeInd^{\text{th}}$ expert respectively.
This leads to each expert resembling a standard GP regression model with a Gaussian likelihood.
For notational conciseness the dependence on the inputs $\allInputK$ and hyperparameters $\expertParamsK$
is dropped for each GP prior,
i.e. $\expertPrior = p\left( \mode{f}(\allInputK) \mid \allInputK, \expertParamsK \right)$.
\todo{add reference to section number}
Note that the expectation over each experts' latent function in cref:eq-np-moe-marginal-likelihood
is inside the marginalisation of the expert indicator variable $\modeVar$.
# Importantly, this model retains the uncertainty in the assignment of observations to experts
# and will lead to each expert not overfitting to the observations assigned to it.
# This approach can be see as trading in the computational benefits of assigning observations to experts
# (Equation ref:eq-np-moe-marginal-likelihood-assign) in favour
# of directly capturing the correlations between all observations.

For ease of notation and understanding, only a single output dimension has been considered,
although in most scenarios the output dimension will be greater than $1$.
It is trivial to extend this work to multiple output dimensions following multioutput GP
methodologies cite:vanderwilkFramework2020.
# The extension to multiple output dimensions is fairly straight forward.
\todo{Can I just say it's left out because trivial?}

*** Identifiable Mixtures of Gaussian Process Experts for Control
The inductive bias of a model encodes what solutions we think are /a-priori/ likely.
Just as the kernel function controls the support and inductive biases in GP models,
the complex interaction between the gating network and experts takes on this role in MoGPE models.
The gating network models how the system switches between dynamics modes over the input space.
It can be seen as a handle for encoding prior knowledge that can be used to constrain the set of admissible functions.
It is of particular interest in this work as it
can improve identifiability and lead to learned representations that better reflect our understanding of the system.
This work formulates a gating network aimed at,
# The simplest case being reordering the experts.
# This work derives a variational approximation that can,
# As highlighted previously, the gating network can be formulated to hard assign observations to experts with the
# goal of improving the computational problems associated with inference and prediction in MoGPE models.
# This work derives a variational approximation that can,
# This work chooses to sacrifice these computational benefits in favour of constructing a gating network that can,
1) Improving *identifiability* in MoGPE models,
   - Selecting informative mean and covariance functions for each gating function GP prior can ensure that each expert corresponds to the underlying dynamics mode that it was intended to model. Learning representations that are more true to the underlying system will also enhance the models ability to generalise away from training observations.
2) Inferring *informative latent structure* that can be exploited for *control*,
   - The goal of Chapter ref:chap-traj-opt is to construct a control technique that attempts to find trajectories that remain in a single dynamics mode and avoid regions of the dynamics that cannot be predicted confidently e.g. because they have not been observed.
     The GP-based gating network presented here, infers informative geometric structure regarding how the dynamics switches between modes over the input space, whilst providing a principled approach to modelling the epistemic uncertainty associated with the gating functions. This makes the gating network a convenient latent space to project the control problem onto.

This work adopts a GP-based gating network resembling that used in the original
MoGPE model cite:trespMixtures2000a. The marginal likelihood of this model is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood-assign}
\evidence &=
\sum_{\allModeVar}
\underbrace{\E_{\gatingPrior} \left[
%\frac{1}{Z}
\prod_{\numData=1}^{\NumData}
\gatingLikelihood
% p(\modeVarn \mid \GatingFunc(\singleInput))
 \right]}_{\text{GP gating network}}
  \underbrace{\prod_{\modeInd=1}^\ModeInd p\left(\{\singleOutput : \singleModeVarK \} \mid \{\singleInput : \singleModeVarK \}, \right)}_{\text{experts}}
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-augmented-marginal-likelihood}
# \evidence &\approx
# \E_{\expertsInducingPrior \gatingPrior} \left[
# \prod_{\numData=1}^{\NumData}
# \sum_{\modeInd=1}^{\ModeInd}
#  \singleGatingLikelihood \singleExpertGivenInducing \right],
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-marginal-likelihood}
# \evidence &=
# \sum_{\modeInd=1}^\ModeInd
# \underbrace{\E_{\gatingPrior} \left[ \prod_{\numData=1}^\NumData \singleGatingLikelihood \right]}_{\text{gating network}}
# \underbrace{\E_{\expertPrior} \left[ \prod_{\numData=1}^\NumData \singleExpertLikelihood \right]}_{\text{expert } k}.
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-marginal-likelihood}
# \evidence &=
# \E_{\gatingPrior \expertPrior} \left[
#  \sum_{\modeInd=1}^\ModeInd
#  \prod_{\numData=1}^\NumData
# \underbrace{\singleGatingLikelihood}_{\text{gating network}}
# \underbrace{\singleExpertLikelihood}_{\text{expert } k}
# \right]
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-marginal-likelihood}
# \evidence &=
# \prod_{\numData=1}^\NumData \sum_{\modeInd=1}^\ModeInd
# \underbrace{\E_{\gatingPrior} \left[ \singleGatingLikelihood \right]}_{\text{gating network}}
# \underbrace{\E_{\expertPrior} \left[ \singleExpertLikelihood \right]}_{\text{expert } k}.
# \end{align}
# #+END_EXPORT
where the gating network resembles a Gaussian process
classification model, with a factorised classification likelihood $\gatingLikelihood$ dependent on
input dependent functions
$\GatingFunc = \{\mode{\gatingFunc} : \inputDomain \rightarrow \R \}_{\modeInd=1}^\ModeInd$,
known as gating functions.
The probability mass function over the expert indicator variable is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-prob-mass}
\gatingLikelihood &= \prod_{\modeInd=1}^\ModeInd \left(\singleGatingLikelihood\right)^{[\singleModeVarK]},
%\singleModeVar \mid \GatingFunc(\singleInput) &\sim \text{Categorical}\left(\ModeInd, \text{softmax}(\GatingFunc(\singleInput)) \right)
%= \prod_{\modeInd=1}^\ModeInd \left(\singleGatingLikelihood\right)^{[\singleModeVarK]}
\end{align}
#+END_EXPORT
where $[\singleModeVarK]$ denotes the Iverson bracket.
The probabilities of this Categorical distribution $\singleGatingLikelihood$
are governed by a classification likelihood (e.g. Bernoulli, Softmax).
Fig. [[ref:fig-graphical-model-gp-gating-network]] shows the graphical model where the $\ModeInd$ latent gating functions $\GatingFunc$
are evaluated and normalised to obtain the mixing probabilities
$\singleGatingLikelihood$.
The mode indicator variable $\singleModeVar$ is then sampled from the Categorical distribution
governed by these probabilities.
The indicated mode's latent function $\mode{f}$ and process noise $\noiseVarK$ are
then evaluated to generate the output $\singleOutput$.


*Softmax ($\ModeInd>2$)* In the general case, that is, when there are more than two experts,
$\ModeInd > 2$, the gating network's likelihood is defined as the Softmax function,
\marginpar{softmax}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-softmax}
\singleGatingLikelihood = \text{softmax}_{\modeInd}\left(\GatingFunc(\singleInput)\right) = \frac{\text{exp}\left(\mode{\gatingFunc}(\singleInput)\right)}{\sum_{j=1}^{\ModeInd} \text{exp}\left(\gatingFunc_j(\singleInput) \right)}.
\end{align}
#+END_EXPORT
Each gating function $\mode{\gatingFunc}$ describes how its corresponding mode's mixing
probability varies over the input space.
Modelling the gating network with input-dependent functions enables
informative prior knowledge to be encoded through the placement of GP priors on each gating function.
Further to this, if the modes are believed to only vary over a subset of the state-control input space,
then the gating functions can depend only on this subset.
Independent GP priors are placed on each gating function, giving the gating network prior,
\marginpar{GP priors}
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-prior}
%\gatingPrior &= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \mode{\gatingFunc}(\allInput) \mid \gatingMeanFunc(\allInput), \gatingCovFunc(\allInput, \allInput) \right) \\
\GatingFunc(\allInput) &\sim \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \gatingMeanFunc(\allInput), \gatingCovFunc(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
where $\gatingMeanFunc(\cdot)$ and $\gatingCovFunc(\cdot,\cdot)$ are the mean and covariance functions
associated with the $\modeInd^\text{th}$ gating function.
Similarly to the experts, dependence on the inputs and hyperparameters is dropped from the gating network's GP prior,
i.e. $\gatingPrior = p(\GatingFunc(\allInput) \mid \allInput, \gatingParams)$.
In contrast to the experts, partitioning of the data set is not desirable for the gating network GPs,
as each gating function should depend on all of training observations.

# Each mode's mixing probability $\singleGatingPosterior$ is then obtained by marginalising
# *all* of the gating functions,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-gating-posterior}
# \singleGatingPosterior
# &= \int \gatingPrior \singleGatingLikelihood \text{d} \GatingFunc(\allInput).
# %&= \E_{\gatingPrior}\left[ \singleGatingLikelihood \right].
# \end{align}
# #+END_EXPORT
# In the general case where $\singleGatingLikelihood$ uses the softmax function
# (Eq. ref:eq-softmax) this integral is intractable, so it is approximated with Monte carlo quadrature.
Each mode's mixing probability $\singleGatingPosterior$ is then obtained by marginalising
*all* of the gating functions.
In the general case where $\singleGatingLikelihood$ uses the softmax function
(Eq. ref:eq-softmax) this integral is intractable, so it is approximated with Monte carlo quadrature.

*Bernoulli ($\ModeInd=2$)* Instantiating the model with two experts, $\singleModeVar \in \{1, 2\}$, is a special case
where only a single gating function is needed.
\marginpar{two experts}
This is because the output of a function $\gatingFunc(\singleInput)$ can be mapped through a sigmoid
function $\text{sig} : \R \rightarrow [0, 1]$ and interpreted as a probability,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sigmoid}
\Pr(\singleModeVar=1 \mid \modei{\gatingFunc}{1}(\singleInput)) = \text{sig}(\modei{\gatingFunc}{1}(\singleInput)).
\end{align}
#+END_EXPORT
If this sigmoid function satisfies the point symmetry condition then
the following holds,
$\Pr(\singleModeVar=2 \mid \modei{\gatingFunc}{1}(\singleInput)) = 1 - \Pr(\singleModeVar=1 \mid \modei{\gatingFunc}{1}(\singleInput))$.
This only requires a single gating function and no normalisation term needs to be calculated.
If the sigmoid function in Eq. ref:eq-sigmoid is selected
to be the Gaussian cumulative distribution function
$\Phi(\gatingFunc(\cdot)) = \int^{\gatingFunc(\cdot)}_{-\infty} \mathcal{N}(\tau | 0, 1) \text{d} \tau$,
then the mixing probability can be calculated in closed-form,
# then the integral in Eq. ref:eq-gating-posterior can be calculated in closed-form,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixing-probabilities-analytic}
\Pr(\modeVarn=1 \mid \singleInput, \gatingParams) &=
 \int \Phi\left(\gatingFunc(\singleInput)\right) \mathcal{N}\left(\gatingFunc(\singleInput) \mid \mu_h, \sigma^2_h \right) \text{d} \gatingFunc(\singleInput) \nonumber \\
&= \Phi \left(\frac{\mu_{h}}{\sqrt{1 + \sigma^2_{h} }}\right),
\end{align}
#+END_EXPORT
where $\mu_h$ and $\sigma^2_{h}$ represent the mean and variance of the gating GP at $\singleInput$ respectively.


# This model makes single-step probabilistic predictions,
# where the predictive distribution over the output $\singleOutput$ is
# given by a mixture of Gaussians.
# This provides the model flexibility to model multimodal transition dynamics $f$ as mixtures of $\modeInd$ experts.
# In the two expert case the marginal likelihood is a mixture of two Gaussians and can be calculated in
# closed-form.
# \todo{can it be calculated in closed form?}
# It has complexity $\mathcal{O}(\ModeInd\NumData^4)$ to evaluate
# and the general case (with more than two experts) has complexity $\mathcal{O}(\ModeInd^2\NumData^4)$.

*** graphical model :ignore:noexport:

#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};

      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\singleInput)$};

      \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\expertInducingOutput$};
      \node[const, left=of uk, xshift=0.4cm] (zk) {$\expertInducingInput$};


      \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\expertParamsK$};
%      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[const, below=of f] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      \node[latent, right=of a, yshift=1.4cm, xshift=-0.4cm] (h) {$\mode{\gatingFunc}(\allInput)$};
      \node[const, right=of h, xshift=-0.4cm] (phik) {$\gatingParamsK$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \draw[post] (uk)--(f);
      \draw[post] (zk)--(uk);
      \draw[post] (thetak)--(f);

      \plate {} {(x) (y) (a) (f)} {$\NumData$};
      \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    %}
  \caption{Graphical model where the output $\singleOutput$
is generated by mapping the input $\singleInput$ through the latent process. The experts are shown on the
left and the gating network is shown on the right.
The geneartive process involves evaluating the $\ModeInd$ latent gating functions $\GatingFunc$
and normalising their output to obtain the mixing probabilities $\singleGatingLikelihood$.
The mode indicator variable $\singleModeVar$ is then sampled from the Categorical distribution
governed by these probabilities.
The indicated mode's latent function $\mode{f}$ and noise model $\mathcal{N}(\mathbf{0}, \noiseVarK)$ are
then evaluated to generate the output $\singleOutput$.}
\label{fig-graphical-model}
\end{figure}
#+END_EXPORT

*** graphical model non sparse experts :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\allInput)$};
     % \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\hk$};


      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[const, below=of f] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      \node[latent, right=of a, yshift=1.4cm, xshift=-0.4cm] (h) {$\mode{\gatingFunc}(\allInput)$};
      \node[const, right=of h, xshift=-0.4cm] (phik) {$\gatingParamsK$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    %}
  \caption{Graphical model of the transition dynamics where the state difference $\singleOutput$
is generated by pushing the state and control $\singleInput$ through the latent process.}
\label{fig-graphical-model}
\end{figure}
#+END_EXPORT


# #+BEGIN_EXPORT latex
# \begin{figure}[t]
#   \centering
#   \resizebox{0.8\columnwidth}{!}{
#     \begin{tikzpicture}[
#       pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
#       post/.style={->,shorten >=0.4pt,>=stealth',semithick}
#       ]
#       \node[const] (x) {$\hat{\mathbf{x}}_{t-1}$};
#       \node[latent, left=of x, yshift=-1.4cm] (f) {$\mathbf{f}^{(k)}_t$};
#       %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
#       \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {${h}^{(k)}_t$};

#       \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\hat{\mathbf{f}}^{(k)}$};
#       \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\hat{\mathbf{h}}^{(k)}$};
#       \node[const, left=of uk, xshift=0.4cm] (zk) {$\bm\xi^{(k)}_f$};
#       \node[const, right=of uh, xshift=-0.4cm] (zh) {$\bm\xi^{(k)}_h$};

#       \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\bm\theta^{(k)}$};
#       \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\bm\phi^{(k)}$};

#       \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\sigma^{(k)}$};

#       \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\Delta\mathbf{x}_t$};
#       %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
#       \node[latent, right=of y, xshift=-0.4cm] (a) {${\alpha}_t$};

#       %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

#       \factor[above=of a] {h-a} {left:Cat} {h} {a};

#       \draw[post] (a)--(y);
#       \draw[post] (x)-|(f);
#       %\draw[post] (f)--(yk);
#       \draw[post] (f)--(y);
#       %\draw[post] (yk)--(y);
#       %\draw[post] (h)--(a);
#       \draw[post] (x)-|(h);
#       \draw[post] (uk)--(f);
#       \draw[post] (uh)--(h);
#       \draw[post] (zk)--(uk);
#       \draw[post] (zh)--(uh);
#       \draw[post] (thetak)--(f);
#       \draw[post] (phik)--(h);
#       \draw[post] (sigmak)|-(y);

#       \plate {} {(x) (y) (a) (f) (h)} {$T$};
#       %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
#       \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$K$};
#       \plate {} {(zh) (uh) (h) (phik)} {$K$};
#     \end{tikzpicture}
#     }
#   \caption{Graphical model of the transition dynamics
# where the state difference $\Delta\mathbf{x}_{t}$
# is generated by pushing the state and control $\hat{\mathbf{x}}_{t-1}$
# through the latent process.}
# %\label{fig:graphical_model}
# \end{figure}
# #+END_EXPORT

*** Generative Model :noexport:
# The marginal likelihood of this model can be written to clearly show the factorised likelihood (mixture of Gaussians)
# and the expectation over the latent variables,
With this formulation, the marginal likelihood of this model can be written to clearly show the
expectations over the latent variables,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood}
\evidence &=
\sum_{\modeInd=1}^\ModeInd
\underbrace{\E_{\gatingPrior} \left[ \prod_{\numData=1}^\NumData \singleGatingLikelihood \right]}_{\text{gating network}}
\underbrace{\E_{\expertPrior} \left[ \prod_{\numData=1}^\NumData \singleExpertLikelihood \right]}_{\text{expert } k}.
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-marginal-likelihood-factorised}
\evidence &=
\sum_{\modeInd=1}^\ModeInd
\E_{\gatingPrior \expertsPrior} \left[
\prod_{\numData=1}^\NumData
\singleGatingLikelihood \singleExpertLikelihood \right],
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-marginal-likelihood-factorised}
# \evidence &=
# \E_{\gatingPrior \expertsPrior} \left[
# \sum_{\modeInd=1}^\ModeInd
# \prod_{\numData=1}^\NumData
# \singleGatingLikelihood \singleExpertLikelihood \right],
# \end{align}
# #+END_EXPORT
# where $\expertsPrior = \prod_{\modeInd=1}^\ModeInd \expertPrior$ is the experts' prior and
where $\expertPrior$ is the $\modeInd^{\text{th}}$ expert's prior and
$\gatingPrior$ is the gating network's prior.
Observe that  the  GP  priors  have  removed  the  factorisation  over  data
which  is  present  in  the  ME  marginal  likelihood  cref:eq-mixture-marginal-likelihood.
Fig. [[ref:fig-graphical-model]]
shows the graphical model where the $\ModeInd$ latent gating functions $\GatingFunc$
are evaluated and normalised to obtain the mixing probabilities
$\singleGatingLikelihood$.
The mode indicator variable $\singleModeVar$ is then sampled from the Categorical distribution
governed by these probabilities.
The indicated mode's latent function $\mode{f}$ and process noise $\noiseVarK$ are
then evaluated to generate the state difference $\singleOutput$.

This model makes single-step probabilistic predictions,
where the predictive distribution over the output $\singleOutput$ is
given by a mixture of Gaussians.
This provides the model flexibility to model multimodal transition dynamics $f$ as mixtures of $\modeInd$ experts.
In the two expert case the marginal likelihood is a mixture of two Gaussians and can be calculated in
closed-form.
\todo{can it be calculated in closed form?}
It has complexity $\mathcal{O}(\ModeInd\NumData^4)$ to evaluate
and the general case (with more than two experts) has complexity $\mathcal{O}(\ModeInd^2\NumData^4)$.

For ease of notation and understanding, only a single output dimension has been considred,
although in most scenarios the state dimension will be greater than $1$.
This work considers independent output dimensions which follow trivially from multioutput GP
methodologies.
# The extension to multiple output dimensions is fairly straight forward.
\todo{Can I just say it's left out because trivial?}
*** Gating Network :noexport:
The gating network governs how the dynamics switch between modes.
This work is interested in spatially varying modes so
formulates an input dependent Categorical distribution over $\alpha_t$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-prob-mass}
P\left(\alpha_t \mid \mathbf{h}_t\right) = \prod_{k=1}^K \left(\Pr(\alpha_t=k \mid \mathbf{h}_t)\right)^{[\alpha_t = k]},
\end{align}
#+END_EXPORT
where $[\alpha_t=k]$ denotes the Iverson bracket.
It should be noted that other gating networks have been proposed that provide computational benefits
for inference. However, these gating networks often lack a handle for encoding domain knowledge
that can be used to restrict the set of possible

\todo[inline]{insert gating network references}
The probabilities of this Categorical distribution $\Pr(\alpha_t=k \mid \mathbf{h}_t)$
are obtained by evaluating $K$ latent
gating functions $\{h^{(k)}\}_{k=1}^K$ and normalising their output.
Each gating function evaluated at $\hat{\mathbf{x}}_{t-1}$ is denoted
as $h^{(k)}_t = h^{(k)}(\hat{\mathbf{x}}_{t-1})$
and at all observations ${h}^{(k)}_{1:T}$.
The set of all gating functions evaluated at $\hat{\mathbf{x}}_{t-1}$ is denoted
as $\mathbf{h}_t$ and at all observations as $\mathbf{h}_{1:T}$.
Each gating function $h^{(k)}$ describes how its corresponding mode's mixing
probability varies over the input space.


This work is interested in finding trajectories that can avoid areas of the transition dynamics model
that cannot be predicted confidently.
Placing independent sparse GP priors on each gating function
provides a principled approach to
modelling the epistemic uncertainty associated with each gating function.
The gating function's posterior covariance is a quantitative value that can be exploited by the
trajectory optimisation.
# GPs also provide data-efficient and interpretable learning through the selection of informative mean
# and covariance functions.
# For example, if the transition dynamics modes are subject to oscillatory mixing then a periodic
# covariance function could be selected.

Each gating function's inducing inputs are denoted
$\bm\xi_h^{(k)}$ and outputs as $\hat{\mathbf{h}}^{(k)}$.
For all gating functions, they are collected as $\bm\xi_h$ and $\hat{\mathbf{h}}$ respectively.
The probability that the $t^{\text{th}}$ observation is generated by mode $k$
given the inducing inputs is obtained by marginalising $\mathbf{h}_t$,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha=\modeInd \mid \mathbf{h}(\cdot) )
&= \E_{p(\mathbf{h}(\cdot))} \left[ \Pr(\alpha_t=k \mid \mathbf{h}(\cdot)) \right],
\end{align*}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha=\modeInd \mid \mathbf{h}(\cdot) )
&= \E_{p(\mathbf{h}(\cdot))} \left[ \text{softmax}_k(\mathbf{h}(\cdot)) \right],
\end{align*}
#+END_EXPORT
#+BEGIN_EXPORT latex
% \small
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha_t=k \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}} )
&= \int \text{softmax}_k(\mathbf{h}_t) p(\mathbf{h}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}) \text{d} \mathbf{h}_t,
\end{align*}
#+END_EXPORT
where $p(\mathbf{h}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}) = \prod_{k=1}^K p\left({h}^{(k)}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}^{(k)}\right)$
is the $K$ independent sparse GP priors on the gating functions.

**** Two Experts
Instantiating the model with two experts is a special case where only a single gating function is needed.
The output of a function can be mapped through a sigmoid
function $\text{sig} : \R \rightarrow [0, 1]$ and interpreted as
a probability $\Pr(\alpha=1 \mid \gatingFunc(\cdot))$.
If this sigmoid function satisfies the point symmetry condition then
we know that $\Pr(\alpha=2 \mid \gatingFunc(\cdot)) = 1 - \Pr(\alpha =1 \mid \gatingFunc(\cdot))$.

The distribution over $\gatingFunc(\cdot)$

We note that $p({h}_n^{(k)} \mid \mathbf{h}_{\neg n}, \mathbf{X})$
is a GP conditional and denote its mean $\mu_h$ and variance $\sigma^2_h$.
Choosing the sigmoid as the Gaussian cdf
$\Phi(h_n) = \int^{h_n}_{-\infty} \mathcal{N}(\tau | 0, 1) \text{d} \tau$
leads to,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixing-probabilities-analytic}
\Pr(\alpha_n=1 | \mathbf{X}) &=
 \int \Phi({h}_n) \mathcal{N}(h_n \mid \mu_h, \sigma^2_h) \text{d} {h}_n
= \Phi \left(\frac{\mu_{h}}{\sqrt{1 + \sigma^2_{h} }}\right).
\end{align}
#+END_EXPORT
Given this gating network our marginal likelihood is now an analytic mixture of two Gaussians.

can analytically marginalise.

** Bayesian Model Old 2 :noexport:
*** Intro :ignore:
The model is built upon GP priors on each of the transition dynamics functions $\fk$
with independent GPs placed on each output (state) dimension $d$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior-single-dim}
p\left(\Fkd \mid \allInput \right) &= \mathcal{N}\left( \Fkd \mid \singleDimMode{\mu}(\allInput), \singleDimMode{k}(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
where $\singleDimMode{\mu}(\cdot)$ and $\singleDimMode{k}(\cdot, \cdot)$ represent the mean and
covariance functions associated with the $d^{\text{th}}$ dimension of the $k^{\text{th}}$ dynamics mode respectively.
Each mode's output dimensions are assumed independent and are given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-prior}
\pFk &= \prod_{\stateDim=1}^{\StateDim} \pFkd.
\end{align}
#+END_EXPORT
The process noise in each mode is assumed to be factorised across both observations and output (state) dimensions.
For each mode it is modelled as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert-likelihood}
\pYkGivenFk
&= \prod_{\numData=1}^{\NumData} \prod_{\stateDim=1}^{\StateDim} \pykGivenfkd \\
&= \prod_{\numData=1}^{\NumData} \prod_{\stateDim=1}^{\StateDim} \mathcal{N}\left( \singleOutput \mid \fknd, \text{diag}\left[ \left(\noiseVarOneK\right)^{2}, \ldots, \left( \noiseVarDK \right)^{2} \right]  \right)
\end{align}
#+END_EXPORT
where $\noiseVardK$ represents the noise variance associated with the $d$^{\text{th}} dimension of the $k$^{\text{th}} mode.
Each mode is then obtained by marginalising its associated latent function values,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-single-expert}
\pYkGivenX = \int  \pYkGivenFk \pFk \text{d} \Fk,
\end{align}
#+END_EXPORT
akin to GP regression.
Each dynamics mode $k$ is assumed to be independent,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-prior}
\pF &= \prod_{k=1}^{K} \pFk.
\end{align}
#+END_EXPORT
At a given input location the mode governing the dynamics is indicated by the discrete mode indicator
variable $\modeVar$.
Following a mixture model formulation, we construct a discrete probability distribution over the mode
indicator variable $\PrA$.
The resulting marginal likelihood is then given by marginalising the mode indicator variable,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixture-marginal-likelihood}
\pYGivenX = \sum_{\modeInd=1}^{\ModeInd} \PrA \pYkGivenX.
\end{align}
#+END_EXPORT
The distribution over the discrete mode indicator variable $\PrA$ is referred to as the gating network.
It governs how the dynamics switch between modes.
# This work is interested in spatially varying modes and formulates a
# which will now be detailed.
# formulates an input dependent Categorical distribution over $\alpha_t$,

*** Gating Network
Mixture models are inherently unidentifiable as different combinations of mixture components
and mixing coefficients can generate the same predictive distributions.
The gating network can be seen as a handle for encoding prior knowledge that can be used to constrain
\marginpar{identifiability}
the set of admissible functions.
This can improve identifiability and lead to learned representations that better reflect
our understanding of the system. The simplest case being reordering the experts.
This motivates Mixtures of Experts models citep:jacobsAdaptive1991, where the distribution over the mode
indicator variable is formulated to be input-dependent. It's probability mass function is then given by,
#+BEGIN_EXPORT latex
%\begin{align} \label{eq-prob-mass}
%P\left(\modeVar \mid \x \right) = \prod_{k=1}^K \left(\Pr(\modeVar= \modeInd \mid \x) \right)^{[\modeVar=\modeInd]}
%\end{align}
\begin{align} \label{eq-prob-mass}
P\left(\alpha \mid \mathbf{h}(\cdot) \right) = \prod_{k=1}^K \left(\Pr(\alpha=k \mid \mathbf{h}(\cdot))\right)^{[\alpha = \modeInd]}
\end{align}
#+END_EXPORT
where $[\modeVar=\modeInd]$ denotes the Iverson bracket.
The probabilities of this Categorical distribution $\Pr(\modeVar=\modeInd \mid \GatingFunc(\cdot))$
are obtained by evaluating $K$ latent
gating functions $\GatingFunc(\cdot)=\{\mode{\gatingFunc}(\cdot)\}_{\modeInd=1}^\ModeInd$ and normalising their output.
Each gating function $\mode{\gatingFunc}(\cdot)$ describes how its corresponding mode's mixing
probability varies over the input space.
\marginpar{softmax}
In the general case, the gating network is defined by the softmax function,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-softmax}
\Pr\left(\modeVar=\modeInd \mid \GatingFunc(\cdot)\right) = \text{softmax}_{\modeInd}\left(\GatingFunc(\cdot)\right) = \frac{\text{exp}\left(\hk(\cdot)\right)}{\sum_{\modeInd=1}^{\ModeInd} \hk(\cdot)}.
\end{align}
#+END_EXPORT
However, instantiating the model with two experts is a special case where only a single gating function is needed.
\marginpar{two experts}
The output of a function $\gatingFunc(\cdot)$ can be mapped through a sigmoid
function $\text{sig} : \R \rightarrow [0, 1]$ and interpreted as
a probability,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sigmoid}
\Pr(\alpha=1 \mid \gatingFunc(\cdot)) = \text{sig}(\gatingFunc(\cdot)).
\end{align}
#+END_EXPORT
If this sigmoid function satisfies the point symmetry condition then
we know that $\Pr(\alpha=2 \mid \gatingFunc(\cdot)) = 1 - \Pr(\alpha =1 \mid \gatingFunc(\cdot))$.
This is neat because it only requires a single gating function and no normalisation term needs to be calculated.

Modelling the gating network with input-dependent functions enables
informative prior knowledge to be encoded through the placement of GP priors.
\marginpar{GP priors}
For example, if modelling a dynamical system with oscillatory behaviour (e.g. an engine)
a periodic kernel could be adopted.
GPs also lend themselves to data-efficient learning whilst providing a principled
approach to modelling the epistemic uncertainty associated with the gating functions.
Learning better representations improves the models
ability to extrapolate as it will be more true to the underlying system.

We place independent GP priors on each gating function,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-prior}
\Hk \sim \mathcal{N}\left( \mu_{h,k}(\allInput), k_{h,k}(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
We place independent GP priors on each gating function, giving the distribution over all gating functions as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-prior}
p\left(\GatingFunc(\allInput)\right) &= \prod_{\modeInd=1}^{\ModeInd} \pHkGivenX = \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mu_{h,k}(\allInput), k_{h,k}(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-prior}
\pHGivenX &= \prod_{\modeInd=1}^{\ModeInd} \pHkGivenX = \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mu_{h,k}(\allInput), k_{h,k}(\allInput, \allInput) \right)
\end{align}
#+END_EXPORT
where $\mu_{h,k}(\cdot)$ and $k_{h,k}(\cdot,\cdot)$ are the mean and covariance functions
associated with the $k^\text{th}$ gating function.

Each mode's mixing probability $\Pr(\modeVarK \mid \singleInput)$ is then obtained by marginalising
*all* of the gating functions,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-indicator-mult}
\Pr(\modeVarK \mid \singleInput )
&= \E_{p(\mathbf{h}(\singleInput))} \left[ \Pr(\modeVarK \mid \mathbf{h}(\singleInput)) \right].
\end{align*}
#+END_EXPORT
In the general case where $\Pr(\modeVar=\modeInd \mid \mathbf{h}(\cdot))$ uses the softmax function
(Eq. ref:eq-softmax) this integral is intractable, so we approximate it using monte carlo
quadrature.
However, in the two expert case (Eq. ref:eq-sigmoid), selecting the sigmoid function to be the Gaussian cumulative distribution
function
$\Phi(\gatingFunc(\cdot)) = \int^{\gatingFunc(\cdot)}_{-\infty} \mathcal{N}(\tau | 0, 1) \text{d} \tau$
leads to analytic integration,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixing-probabilities-analytic}
\Pr(\modeVarn=1 \mid \singleInput) &=
 \int \Phi\left(\gatingFunc(\singleInput)\right) \mathcal{N}\left(\gatingFunc(\singleInput) \mid \mu_h, \sigma^2_h \right) \text{d} \gatingFunc(\singleInput) \\
&= \Phi \left(\frac{\mu_{h}}{\sqrt{1 + \sigma^2_{h} }}\right).
\end{align}
#+END_EXPORT
In the two expert case the marginal likelihood is given as an analytic mixture of two Gaussians.
\marginpar{marginal likelihood}

*** Generative Model
This model makes single-step probabilistic predictions,
where the predictive distribution over the state difference $\Delta \mathbf{x}_{t}$ is
given by a mixture of Gaussians.
This provides the model flexibility to model multimodal transition dynamics $f$ as mixtures of $K$ modes.
With this formulation, the marginal likelihood can be rewritten as,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-marginal-likelihood}
\pYGivenX &= \sum_{\modeInd=1}^\ModeInd \PrAGivenX \pYkGivenX \\
&= \sum_{\modeInd=1}^\ModeInd \PraGivenx \pykGivenx \\
\end{align*}
#+END_EXPORT
#+BEGIN_EXPORT latex
\small
\begin{align*} \label{eq-marginal-likelihood}
p(\Delta\mathbf{x}_{1:T} | \hat{\mathbf{x}}_{1:T}) =
&\prod_{t=1}^T  \sum_{k=1}^K \Bigg(
\underbrace{\left\langle \Pr\left(\alpha_t = k | \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}\right) \right\rangle_{p\left(\hat{\mathbf{h}} \mid \bm\xi_h\right)}}_{\text{Mixing Probability}}  \\
&\underbrace{\left\langle p\left(\Delta\mathbf{x}_t | \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{f}}^{(k)}\right) \right\rangle_{p\left(\hat{\mathbf{f}}^{(k)} \mid \bm\xi^{(k)}_f\right)}}_{\text{Dynamics mode } k} \Bigg), \numberthis
\end{align*}
\normalsize
#+END_EXPORT
where $\left\langle \cdot \right\rangle_{p(x)}$ denotes an expectation under $p(x)$.
Fig. [[ref:fig-graphical-model]]
shows the graphical model where the $K$ latent gating functions $h^{(k)}$
are evaluated and normalised to obtain the mixing probabilities
$\Pr(\alpha_t=k \mid \hat{\mathbf{x}}_{t-1})$.
The mode indicator variable $\alpha_t$ is then sampled from a Categorical distribution
governed by these probabilities.
The indicated mode's latent function $f^{(k)}$ and process noise $\sigma^{(k)}$ are
then evaluated to generate the state difference $\Delta\mathbf{x}_{t}$.

*** graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\Fk$};
     % \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\hk$};


      \node[const, left=of f, xshift=0.4cm] (thetak) {$\expertParamsK$};

      \node[const, below=of f] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      \node[latent, right=of a, yshift=1.4cm, xshift=-0.4cm] (h) {$\Hk$};
      \node[const, right=of h, xshift=-0.4cm] (phik) {$\gatingParamsK$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    %}
  \caption{Graphical model of the transition dynamics where the state difference $\singleOutput$
is generated by pushing the state and control $\singleInput$ through the latent process.}
\label{fig-graphical-model}
\end{figure}
#+END_EXPORT


# #+BEGIN_EXPORT latex
# \begin{figure}[t]
#   \centering
#   \resizebox{0.8\columnwidth}{!}{
#     \begin{tikzpicture}[
#       pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
#       post/.style={->,shorten >=0.4pt,>=stealth',semithick}
#       ]
#       \node[const] (x) {$\hat{\mathbf{x}}_{t-1}$};
#       \node[latent, left=of x, yshift=-1.4cm] (f) {$\mathbf{f}^{(k)}_t$};
#       %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
#       \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {${h}^{(k)}_t$};

#       \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\hat{\mathbf{f}}^{(k)}$};
#       \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\hat{\mathbf{h}}^{(k)}$};
#       \node[const, left=of uk, xshift=0.4cm] (zk) {$\bm\xi^{(k)}_f$};
#       \node[const, right=of uh, xshift=-0.4cm] (zh) {$\bm\xi^{(k)}_h$};

#       \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\bm\theta^{(k)}$};
#       \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\bm\phi^{(k)}$};

#       \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\sigma^{(k)}$};

#       \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\Delta\mathbf{x}_t$};
#       %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
#       \node[latent, right=of y, xshift=-0.4cm] (a) {${\alpha}_t$};

#       %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

#       \factor[above=of a] {h-a} {left:Cat} {h} {a};

#       \draw[post] (a)--(y);
#       \draw[post] (x)-|(f);
#       %\draw[post] (f)--(yk);
#       \draw[post] (f)--(y);
#       %\draw[post] (yk)--(y);
#       %\draw[post] (h)--(a);
#       \draw[post] (x)-|(h);
#       \draw[post] (uk)--(f);
#       \draw[post] (uh)--(h);
#       \draw[post] (zk)--(uk);
#       \draw[post] (zh)--(uh);
#       \draw[post] (thetak)--(f);
#       \draw[post] (phik)--(h);
#       \draw[post] (sigmak)|-(y);

#       \plate {} {(x) (y) (a) (f) (h)} {$T$};
#       %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
#       \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$K$};
#       \plate {} {(zh) (uh) (h) (phik)} {$K$};
#     \end{tikzpicture}
#     }
#   \caption{Graphical model of the transition dynamics
# where the state difference $\Delta\mathbf{x}_{t}$
# is generated by pushing the state and control $\hat{\mathbf{x}}_{t-1}$
# through the latent process.}
# %\label{fig:graphical_model}
# \end{figure}
# #+END_EXPORT



*** Gating Network :noexport:
The gating network governs how the dynamics switch between modes.
This work is interested in spatially varying modes so
formulates an input dependent Categorical distribution over $\alpha_t$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-prob-mass}
P\left(\alpha_t \mid \mathbf{h}_t\right) = \prod_{k=1}^K \left(\Pr(\alpha_t=k \mid \mathbf{h}_t)\right)^{[\alpha_t = k]},
\end{align}
#+END_EXPORT
where $[\alpha_t=k]$ denotes the Iverson bracket.
It should be noted that other gating networks have been proposed that provide computational benefits
for inference. However, these gating networks often lack a handle for encoding domain knowledge
that can be used to restrict the set of possible

\todo[inline]{insert gating network references}
The probabilities of this Categorical distribution $\Pr(\alpha_t=k \mid \mathbf{h}_t)$
are obtained by evaluating $K$ latent
gating functions $\{h^{(k)}\}_{k=1}^K$ and normalising their output.
Each gating function evaluated at $\hat{\mathbf{x}}_{t-1}$ is denoted
as $h^{(k)}_t = h^{(k)}(\hat{\mathbf{x}}_{t-1})$
and at all observations ${h}^{(k)}_{1:T}$.
The set of all gating functions evaluated at $\hat{\mathbf{x}}_{t-1}$ is denoted
as $\mathbf{h}_t$ and at all observations as $\mathbf{h}_{1:T}$.
Each gating function $h^{(k)}$ describes how its corresponding mode's mixing
probability varies over the input space.


This work is interested in finding trajectories that can avoid areas of the transition dynamics model
that cannot be predicted confidently.
Placing independent sparse GP priors on each gating function
provides a principled approach to
modelling the epistemic uncertainty associated with each gating function.
The gating function's posterior covariance is a quantitative value that can be exploited by the
trajectory optimisation.
# GPs also provide data-efficient and interpretable learning through the selection of informative mean
# and covariance functions.
# For example, if the transition dynamics modes are subject to oscillatory mixing then a periodic
# covariance function could be selected.

Each gating function's inducing inputs are denoted
$\bm\xi_h^{(k)}$ and outputs as $\hat{\mathbf{h}}^{(k)}$.
For all gating functions, they are collected as $\bm\xi_h$ and $\hat{\mathbf{h}}$ respectively.
The probability that the $t^{\text{th}}$ observation is generated by mode $k$
given the inducing inputs is obtained by marginalising $\mathbf{h}_t$,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha=\modeInd \mid \mathbf{h}(\cdot) )
&= \E_{p(\mathbf{h}(\cdot))} \left[ \Pr(\alpha_t=k \mid \mathbf{h}(\cdot)) \right],
\end{align*}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha=\modeInd \mid \mathbf{h}(\cdot) )
&= \E_{p(\mathbf{h}(\cdot))} \left[ \text{softmax}_k(\mathbf{h}(\cdot)) \right],
\end{align*}
#+END_EXPORT
#+BEGIN_EXPORT latex
% \small
\begin{align*} \label{eq-indicator-mult}
\Pr(\alpha_t=k \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}} )
&= \int \text{softmax}_k(\mathbf{h}_t) p(\mathbf{h}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}) \text{d} \mathbf{h}_t,
\end{align*}
#+END_EXPORT
where $p(\mathbf{h}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}) = \prod_{k=1}^K p\left({h}^{(k)}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{h}}^{(k)}\right)$
is the $K$ independent sparse GP priors on the gating functions.

**** Two Experts
Instantiating the model with two experts is a special case where only a single gating function is needed.
The output of a function can be mapped through a sigmoid
function $\text{sig} : \R \rightarrow [0, 1]$ and interpreted as
a probability $\Pr(\alpha=1 \mid \gatingFunc(\cdot))$.
If this sigmoid function satisfies the point symmetry condition then
we know that $\Pr(\alpha=2 \mid \gatingFunc(\cdot)) = 1 - \Pr(\alpha =1 \mid \gatingFunc(\cdot))$.

The distribution over $\gatingFunc(\cdot)$

We note that $p({h}_n^{(k)} \mid \mathbf{h}_{\neg n}, \mathbf{X})$
is a GP conditional and denote its mean $\mu_h$ and variance $\sigma^2_h$.
Choosing the sigmoid as the Gaussian cdf
$\Phi(h_n) = \int^{h_n}_{-\infty} \mathcal{N}(\tau | 0, 1) \text{d} \tau$
leads to,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixing-probabilities-analytic}
\Pr(\alpha_n=1 | \mathbf{X}) &=
 \int \Phi({h}_n) \mathcal{N}(h_n \mid \mu_h, \sigma^2_h) \text{d} {h}_n
= \Phi \left(\frac{\mu_{h}}{\sqrt{1 + \sigma^2_{h} }}\right).
\end{align}
#+END_EXPORT
Given this gating network our marginal likelihood is now an analytic mixture of two Gaussians.

can analytically marginalise.

** Bayesian Model Old :noexport:
The model is built upon sparse GP priors on each of the transition dynamics functions $f^{(k)}$
with independent GPs placed on each state dimension $d$,

#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-prior}
p(\Fk \mid \allInput, \uFk) = \prod_{\numData = 1}^{\dataInd} p(\Fkn \mid \singleInput, \uFk)
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-dynamics-prior}
p\left(\mathbf{f}^{(k)}_{1:T} \mid \hat{\mathbf{x}}_{1:T}, \hat{\mathbf{f}}^{(k)} \right) =
&\prod_{t=1}^T
p\left(\mathbf{f}^{(k)}_{t} \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{f}}^{(k)} \right) \numberthis \\
&\prod_{t=1}^T \prod_{d=1}^D
p\left(f^{(k)}_{t,d} \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{f}}^{(k)}_{d} \right)
\end{align*}
#+END_EXPORT
where $p\left(f^{(k)}_{t,d} \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{f}}^{(k)}_{d} \right)$
is a sparse GP conditional (Eq. ref:eq-sparse-gp-prediction).
For notational conciseness, the dependency on the inducing inputs $\bm\xi^{(k)}_f$ is dropped
throughout.
The $M$ inducing inputs and outputs associated with the $d^{\text{th}}$ dimension
of the $k^{\text{th}}$ mode's latent function $f^{(k)}$ are denoted as
$\bm\xi_{f,d}^{(k)}$ and
$\hat{\mathbf{f}}^{(k)}_d$ respectively.
They are collected as
$\bm\xi^{(k)}_f$ and $\hat{\mathbf{f}}^{(k)}$ for all output dimensions
and as $\bm\xi_f$ and $\hat{\mathbf{f}}$ for all modes.
The process noise in each mode is modelled as,
#+BEGIN_EXPORT latex
\small
\begin{align*} \label{eq-likelihood}
p\left(\Delta\mathbf{x}_{t} \mid \mathbf{f}^{(k)}_{t}\right)
= \mathcal{N}\left(\Delta\mathbf{x}_{t} \mid \mathbf{f}_t^{(k)}, \text{diag}\left(\left(\sigma^{(k)}_{1}\right)^2, \ldots, \left(\sigma^{(k)}_D\right)^2\right) \right),
\end{align*}
\normalsize
#+END_EXPORT
where $\left(\sigma^{(k)}_{d}\right)^2$ represents the noise variance associated
with the $d^{\text{th}}$ dimension of the $k^{\text{th}}$ mode.
# #+BEGIN_EXPORT latex
# \small
# \begin{gathered} \label{eq-likelihood}
# p\left(\Delta\mathbf{x}_{t} \mid \mathbf{f}^{(k)}_{t}\right)
# = \mathcal{N}\left(\Delta\mathbf{x}_{t} \mid \mathbf{f}_t^{(k)}, \text{diag}\left(\left(\sigma^{(k)}_{1}\right)^2, \ldots, \left(\sigma^{(k)}_D\right)^2\right) \right),
# \end{gathered}
# \normalsize
# #+END_EXPORT

# The dynamics modes given the inducing variables,
# #+BEGIN_EXPORT latex
# \small
# \begin{align*} \label{eq-dynamics-mode}
# p(\Delta\mathbf{x}_{1:T} \mid \hat{\mathbf{x}}_{1:T}, \hat{\mathbf{f}}^{(k)}) =
# \prod_{t=1}^T \int
# &p\left(\Delta\mathbf{x}_t \mid \mathbf{f}^{(k)}_t\right)
# p\left(\mathbf{f}^{(k)}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{f}}^{(k)} \right)
# \text{d} \mathbf{f}^{(k)}_t
# \end{align*}
# \normalsize
# #+END_EXPORT

# The dynamics modes are combined by the gating network to obtain,
# #+BEGIN_EXPORT latex
# \small
# \begin{align*} \label{eq-marginal-likelihood}
# p(\Delta\mathbf{x}_{1:T} \mid \hat{\mathbf{x}}_{1:T}, \hat{\mathbf{f}}) =
# \prod_{t=1}^T \sum_{k=1}^K
# &\Pr(\alpha_t = k \mid \hat{\mathbf{x}}_{t-1})
# p(\Delta\mathbf{x}_t \mid \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{f}}^{(k)})
# %&\int p\left(\Delta\mathbf{x}_t \mid \hat{\mathbf{f}}^{(k)}\right)
# %p\left(\hat{\mathbf{f}}^{(k)} \mid \hat{\mathbf{x}}_{t-1}\right) \text{d} \hat{\mathbf{f}}^{(k)}
# %&p(\Delta\mathbf{x}_t \mid \alpha_t=k, \hat{\mathbf{x}}_{t-1}, \hat{\mathbf{f}}^{(k)}). \numberthis \\
# \end{align*}
# \normalsize
# #+END_EXPORT

** Approximate Inference [[label:sec-inference]]
#+begin_export latex
\epigraph{Nature laughs at the difficulties of integration.}{\textit{Pierre-Simon Laplace}}
#+end_export
*** intro :ignore:
Performing Bayesian inference involves finding the posterior over the latent variables,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\GatingFunc(\allInput), \LatentFunc(\allInput) \mid \allInput, \allOutput, \gatingParams, \expertParams)
&= \frac{\sum_{\allModeVar} \allGatingLikelihood \gatingPrior
\prod_{\modeInd=1}^{\ModeInd} \allExpertLikelihood \expertPrior}{\evidence}
\end{align}
#+END_EXPORT
where the denominator is the marginal likelihood from Eq. ref:eq-marginal-likelihood-assign.
Exact inference in our model is intractable, so we resort to a variational approximation.
The rich structure of our model makes it hard to construct a variational lower bound that can
be evaluated in closed-form, whilst accurately modelling the complex dependencies.
Further to this, the marginal likelihood is extremely expensive to evaluate,
as there are $\ModeInd^{\NumData}$ sets of assignments $\allModeVar$ that need to be marginalised.
For each set of assignments, there are then $\ModeInd$ GP experts that need to be evaluated, each with
complexity $\mathcal{O}(\NumData^{3})$.
\todo{add marginal likelihood's correct complexity}
For these reasons, this work derives a variational approximation based on inducing variables, that provides scalability
by utilising stochastic gradient-based optimisation.
# The resulting complexity is  $\mathcal{O}(\ModeInd^{\NumData} \NumData_{\modeInd}^3)$,
# where $\NumData_{\modeInd}$ represents the largest number of data points assigned to an expert for particular
# set of assignments.
# and is also intractable due to the marginalisation of $\mathbf{h}$ in the gating network.

Stochastic variational inference (SVI) citep:hoffmanStochastic2013 relies upon having a set of local variables
factorised across observations and a set of global variables.
Annoyingly, the marginalisation over the set of expert indicator variables $\allModeVar$
in cref:eq-np-moe-marginal-likelihood is prohibitive to SVI.
Following the approach by cite:titsiasVariational2009, the probability space is augmented
with a set of $\NumInducing$ inducing variables for each GP.
However, instead of collapsing these inducing variables they are explicitly
represented as variational distributions and used to lower bound
(and then further bound) the marginal likelihood, similar to
cite:hensmanGaussian2013,hensmanScalable2015.

# The marginal likelihood in Eq. ref:eq-marginal-likelihood-factorised is extremely expensive
# to evaluate $\mathcal{O}(\ModeInd^2 \NumData^{4})$
# \todo{add marginal likelihood's correct complexity}
# and is also intractable due to the marginalisation of $\mathbf{h}$ in the gating network
# (except for the two expert case).
# For these reasons, this work derives a variational approximation based on inducing variables that provides scalability
# by utilising stochastic gradient-based optimisation.


# *Inducing Variables* As this approach essentially parameterises a nonparametric model,
# it is interesting to pause here and consider the implications of defining inducing inputs in different ways.
# For example, what are the implications of having shared or separate inducing inputs
# for the gating network GPs, for the expert GPs and for combinations of the experts and gating functions?

# 1. *Separate inducing inputs* for each *expert* GP, i.e. $\expertInducingOutput=\mode{\latentFunc}(\expertInducingInput)$,
#    - Introducing separate inducing inputs for each expert's GP can loosely be seen as "partitioning"
#       the observations between experts. This requires less inducing points for each expert and achieves data partitioning behaviour like other MoGPE methods.
# - Introducing separate inducing inputs for each expert's GP can loosely be seen as "partitioning"
#   the observations between experts,
# - Less inducing inputs needed for each expert,
# - Achieves data partitioning behaviour like other MoGPE methods.
# 3. *Shared inducing inputs* for the *gating network* GPs, i.e. $\gatingInducingOutput=\mode{\gatingFunc}(\gatingInducingInput)$,
#    - Partitioning of the data set is not desirable for the gating function GPs as each gating function
#       should depend on all of training observations. For this reason the inducing inputs should be shared between each gating function GP.
# - Partitioning of the data set is not desirable for the gating function GPs as each gating function
#   should depend on all of training observations,
# - For this reason the inducing inputs should be shared between each gating function GP.

*** sparse graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\singleInput)$};
      %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
      \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\mode{\gatingFunc}(\singleInput)$};

      \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\expertInducingOutput$};
      \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\gatingInducingOutput$};
      \node[const, left=of uk, xshift=0.4cm] (zk) {$\expertInducingInput$};
      \node[const, right=of uh, xshift=-0.4cm] (zh) {$\gatingInducingInput$};

      \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\expertParamsK$};
      \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\gatingParamsK$};

      \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (uk)--(f);
      \draw[post] (uh)--(h);
      \draw[post] (zk)--(uk);
      \draw[post] (zh)--(uh);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a) (f) (h)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(uh) (h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    }
  \caption{Graphical model of the augmented probability space where the joint distribution over the data is captured by the inducing variables $\expertInducingOutput=\mode{\latentFunc}(\expertInducingInput)$ and $\gatingInducingOutput=\mode{\gatingFunc}(\gatingInducingInput)$.
  The observations assigned to expert $\modeInd$ are modelled by the
  inducing points $\{\expertInducingInput, \expertInducingOutput\}_{\modeInd=1}^{\ModeInd}$.
  This model avoids the hard assignment of observations to experts by letting the gating network
  softly assign them in the variational lower bound.}
\label{fig-graphical-model-sparse}
\end{figure}
#+END_EXPORT

*** Augmented Probability Space :ignore:

*Augmented Experts*
This work sidesteps the hard assignment of observations to experts by augmenting each expert with a set
of separate independent inducing points,
$\expertInducingOutput = \mode{\latentFunc}(\expertInducingInput)$.
Each expert's inducing points are assumed to be from its GP prior,
# Each set of inducing points are assumed to be from the GP prior associated with the expert,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-inducing-prior}
\expertsInducingPrior
&= \prod_{\modeInd=1}^\ModeInd \expertInducingPrior
= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \expertInducingOutput) \mid
\expertMeanFunc(\expertInducingInput),
\expertCovFunc(\expertInducingInput, \expertInducingInput) \right),
\end{align}
#+END_EXPORT
where the set of all inducing inputs associated with the experts has been denoted $\expertsInducingInput$
and the  set of all inducing variables as $\expertsInducingOutput$.
Introducing separate inducing points from each expert's GP can loosely be seen as "partitioning"
the observations between experts, i.e. the inducing points can be seen as approximating the data partition if
$\expertInducingOutput \approx \allOutputK$ and $\expertInducingInput \approx \allInputK$.
However, as the assignment of observations to experts is /not known a priori/, the inducing inputs
$\expertInducingInput$ and variables $\expertInducingOutput$, must be inferred from observations.

*Augmented Gating Network* Following a similar approach for the gating network, each gating function is augmented with a
set of $\NumInducing$ inducing points from its corresponding GP prior,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-inducing-prior}
\gatingsInducingPrior
&= \prod_{\modeInd=1}^\ModeInd \gatingInducingPrior
= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \gatingInducingOutput) \mid
\gatingMeanFunc(\gatingInducingInput),
\gatingCovFunc(\gatingInducingInput, \gatingInducingInput) \right),
\end{align}
#+END_EXPORT
where  $\gatingInducingOutput = \mode{\gatingFunc}(\gatingInducingInput)$
are the inducing points associated with the $\modeInd^{\text{th}}$ gating function.
The set of all inducing variables associated with the gating network has been
denoted $\gatingsInducingOutput$.
Partitioning of the data set is not desirable for the gating function GPs as each gating function
should depend on all of training observations.
For this reason the gating functions share the same inducing inputs $\gatingInducingInput$.

*Marginal Likelihood* These inducing points are used to approximate the marginal likelihood with a factorisation over observations
that is favourable for constructing a GP-based gating network.
Our approximate marginal likelihood is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-augmented-marginal-likelihood}
\evidence &\approx
\E_{\gatingsInducingPrior \expertsInducingPrior} \left[
\prod_{\numData=1}^{\NumData}
\sum_{\modeInd=1}^{\ModeInd}
 \singleGatingGivenInducing \singleExpertGivenInducing \right],
\end{align}
#+END_EXPORT
where the conditional distributions $\singleExpertGivenInducing$ and $\singleGatingGivenInducing$
follow from standard sparse GP methodologies,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\singleExpertGivenInducing &= \E_{\singleLatentExpertGivenInducing} \left[ \singleExpertLikelihood \right] \\
\singleGatingGivenInducing &= \E_{\singleLatentGatingsGivenInducing} \left[ \singleGatingLikelihood \right] \\
\singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
\expertKernelnM \expertKernelMM^{-1} \expertInducingOutput,
\expertKernelnn - \expertKernelnM \expertKernelMM^{-1} \expertKernelMn \right), \\
\singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mode{\gatingFunc}(\singleInput) \mid
\gatingKernelnM \gatingKernelMM^{-1} \gatingInducingOutput,
\gatingKernelnn - \gatingKernelnM \gatingKernelMM^{-1} \gatingKernelMn \right),
\end{align}
#+END_EXPORT
where
$\expertKernelMM = k_{\modeInd} (\expertInducingInput,\expertInducingInput)$
represents the $\modeInd^{\text{th}}$ expert's kernel evaluated between its inducing inputs,
$\expertKernelnn = k_k (\singleInput, \singleInput)$
represents it evaluated between the $\numData^{\text{th}}$ training input and
$\expertKernelnM = k_k (\singleInput, \expertInducingInput)$
between the $\numData^{\text{th}}$ training input and its inducing inputs.
Similarly for the gating network.
Figure ref:fig-graphical-model-sparse shows the graphical model of this augmented joint probability model.

A central assumption of our work follows from sparse GP methodologies, that assume,
given the inducing variables, the latent function values factorise over observations.
Our approximation assumes that given the inducing points,
the marginalisation over every possible assignment of data points to experts, can be factorised over data.
In a similar spirit to the FITC approximation citep:naish-guzmanGeneralized2008,quinonero-candelaUnifying2005,
this can be viewed as a likelihood approximation,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-likelihood-approximation}
p(\allOutput \mid \expertsInducingOutput)
&\approx \prod_{\numData=1}^{\NumData} p(\singleOutput \mid \expertsInducingOutput)
= \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \moeGatingPosterior
\prod_{\modeInd=1}^{\ModeInd} \singleExpertGivenInducing.
\end{align}
#+END_EXPORT
Importantly, the factorisation over observations has been moved
outside of the marginalisation over the expert indicator variable, i.e.
the expert indicator variable can be marginalised for each data point separately.
This approximation assumes that the inducing variables,
$\{\expertInducingOutput\}_{\modeInd=1}^\ModeInd$, are
a sufficient statistic for their associated latent function values,
$\{\mode{\latentFunc}(\allInputK) \}_{\modeInd=1}^\ModeInd$
and the set of assignments $\allModeVar$.
This approximation becomes exact in the limit $\ModeInd\NumInducing=\NumData$,
if each expert's inducing points $(\expertInducingInput, \expertInducingOutput)$
represent the true data partition $(\expertInducingInput, \expertInducingOutput) = (\allInputK, \allOutputK)$.
It is also worth noting that cref:eq-likelihood-approximation captures a rich approximation of each
expert's covariance but as $\ModeInd\NumInducing \ll \NumData$ the computational complexity is
much lower.
This approximation efficiently couples the gating network and the experts by marginalising the expert
indicator variable for each data point separately.

Our approximate marginal likelihood captures
the joint distribution over the data and assignments through the inducing variables
$\expertsInducingOutput$.
As such, information regarding the assignment of observations to experts must pass through the bottleneck of the
inducing variables.
This approximation induces a local factorisation over observations and a set of global variables
-- the necessary conditions for SVI.
cref:fig-graphical-model-sparse shows the graphical model of this augmented joint probability model.
This approach can loosely be viewed as parameterising the full nonparametric model
in cref:eq-np-moe-marginal-likelihood-assign to obtain a desirable
factorisation for 1) constructing a GP-based gating network and 2) deriving a variational lower bound that can
be optimised with stochastic gradient methods,
whilst still capturing the complex dependencies between the gating network and experts.

*** Augmented probability space :ignore:noexport:

Stochastic variational inference (SVI) citep:hoffmanStochastic2013 relies upon having a set of local variables
factorised across observations and a set of global variables.
Annoyingly, the order of the marginalisation over the expert indicator variable
and the product over observations $\NumData$
in cref:eq-marginal-likelihood is prohibitive to SVI.


# which is detailed in Section ref:sec-sparse-gps Eqs. ref:eq-titsias-bound - ref:eq-hensman-bound,
*Augmented Probability Space* Following the approach by cite:titsiasVariational2009 and cite:hensmanGaussian2013,
the probability space is first augmented with a set of $\NumInducing$ inducing variables from each GP prior,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-inducing-prior}
\expertsInducingPrior
&= \prod_{\modeInd=1}^\ModeInd \expertInducingPrior
= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \expertInducingOutput) \mid
\expertMeanFunc(\expertInducingInput),
\expertCovFunc(\expertInducingInput, \expertInducingInput) \right) \\
\gatingsInducingPrior
&= \prod_{\modeInd=1}^\ModeInd \gatingInducingPrior
= \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \gatingInducingOutput) \mid
\gatingMeanFunc(\gatingInducingInput),
\gatingCovFunc(\gatingInducingInput, \gatingInducingInput) \right).
\end{align}
#+END_EXPORT
where  $\gatingInducingOutput = \mode{\gatingFunc}(\gatingInducingInput)$
are the inducing points associated with the $\modeInd^{\text{th}}$ gating function.
Partitioning of the data set is not desirable for the gating function GPs as each gating function
should depend on all of training observations.
For this reason the gating functions share the same inducing inputs $gatingInducingInput$.
The set of all inducing variables associated with the gating network has been
denoted $\gatingsInducingOutput$.

Our approximate marginal likelihood is then given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-augmented-marginal-likelihood}
\evidence &\approx
\E_{\gatingsInducingPrior \expertsInducingPrior} \left[
\prod_{\numData=1}^{\NumData}
\sum_{\modeInd=1}^{\ModeInd}
 \singleGatingGivenInducing \singleExpertGivenInducing \right],
\end{align}
#+END_EXPORT
where the joint distribution over the data is captured by the inducing variables
$(\gatingsInducingOutput, \expertsInducingOutput)$.
As such, information regarding the assignment of observations to experts must pass through the bottleneck of the
inducing variables.
This approximation induces a local factorisation over observations and a set of global variables -- the inducing
variables -- the necessary conditions for SVI.
Figure ref:fig-graphical-model-sparse shows the graphical model of this augmented joint probability model.


The augmented marginal likelihood is then given by,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-augmented-marginal-likelihood}
# \evidence &=
# \E_{\gatingsInducingPrior \expertsInducingPrior} \left[
# \sum_{\modeInd=1}^{\ModeInd} \allGatingGivenInducing \allExpertGivenInducing
# \right],
# \end{align}
# #+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-augmented-marginal-likelihood-not-factorised}
\evidence &=
\E_{\gatingsInducingPrior} \left[
\sum_{\modeInd=1}^{\ModeInd}
 \allGatingsGivenInducing
\E_{\expertInducingPrior} \left[ \allExpertGivenInducing \right]
\right],
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-augmented-marginal-likelihood}
# \evidence &\approx
# \sum_{\modeInd=1}^{\ModeInd}
# \E_{\gatingsInducingPrior} \left[ \prod_{\numData=1}^\NumData \singleGatingGivenInducing \right]
# \E_{\expertsInducingPrior} \left[ \prod_{\numData=1}^\NumData \singleExpertGivenInducing \right]
# \right],
# \end{align}
# #+END_EXPORT
where the conditional distributions $\allExpertGivenInducing$ and $\allGatingGivenInducing$
follow from standard sparse GP methodologies,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\allExpertGivenInducing &= \E_{\allLatentExpertGivenInducing} \left[ \allExpertLikelihood \right] \\
\allGatingGivenInducing &= \E_{\allLatentGatingsGivenInducing} \left[ \allGatingLikelihood \right] \\
%\allLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\allInput) \mid
%\mode{\mathbf{A}} \expertInducingOutput,
%\expertKernel(\allInput, \allInput) -
%\mode{\mathbf{A}}
%\expertKernel(\expertInducingInput, \expertInducingInput)
%\mode{\mathbf{A}}^T \right), \\
%\allLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mathbf{\gatingFunc}(\allInput) \mid
%\mode{\hat{\mathbf{A}}} \gatingInducingOutput,
%\gatingKernel(\allInput, \allInput) -
%\mode{\hat{\mathbf{A}}}
%\gatingKernel(\gatingInducingInput, \gatingInducingInput)
%\mode{\hat{\mathbf{A}}}^T \right), \\
\allLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\allInput) \mid
\expertKernelNM \expertKernelMM^{-1} \expertInducingOutput,
\expertKernelNN - \expertKernelNM \expertKernelMM^{-1} \expertKernelMN \right), \\
\allLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd}
\mathcal{N}\left( \mode{\gatingFunc}(\allInput) \mid
\gatingKernelNM \gatingKernelMM^{-1} \gatingInducingOutput,
\gatingKernelNN - \gatingKernelNM \gatingKernelMM^{-1} \gatingKernelMN \right),
\end{align}
#+END_EXPORT
where
$\expertKernelMM = k_{\modeInd} (\expertInducingInput,\expertInducingInput)$
represents the $\modeInd^{\text{th}}$ experts kernel evaluated between its inducing inputs,
$\expertKernelNN = k_k (\allInput, \allInput)$
represents it evaluated between the training inputs and
$\expertKernelNM = k_k (\allInput, \expertInducingInput)$
between the training inputs and its inducing inputs.
Similarly for the gating network.

A central assumption of our work follows from sparse GP methodologies, that assume,
given the inducing variables, the latent function values factorise over observations.
In a similar spirit to the FITC approximation cite:naish-guzmanGeneralized2008,quinonero-candelaUnifying2005,
we propose the following likelihood approximation,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-likelihood-approximation}
p(\allOutput \mid \gatingsInducingOutput, \expertsInducingOutput)
\approx \prod_{\numData=1}^{\NumData} p(\singleOutput \mid \gatingsInducingOutput, \expertsInducingOutput)
= \prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \singleGatingGivenInducing \singleExpertGivenInducing.
\end{align}
#+END_EXPORT
# This approximation factorises over the observed outputs given the inducing points.
Importantly, this approximation moves the factorisation over observations
outside of the marginalisation over the expert indicator variable.
It captures a rich approximation of each expert's covariance whilst marginalising the expert
indicator variable.
This approximation assumes that the inducing variables,
$\{\gatingInducingOutput, \expertInducingOutput\}_{\modeInd=1}^\ModeInd$, are
a sufficient statistic for their associated latent function values,
$\{\mode{\latentFunc}(\allInput), \mode{\gatingFunc}(\allInput) \}_{\modeInd=1}^\ModeInd$.
When all of the expert's inducing inputs $\expertInducingInput$
and the gating network's inducing inputs
$\gatingInducingInput$ are equal to the training inputs
$\expertInducingInput = \gatingInducingInput=\allInput$, this approximation is exact.
This approximation becomes exact in the limit $M=N$.
Our approximate marginal likelihood is then given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-augmented-marginal-likelihood}
\evidence &\approx
\E_{\gatingsInducingPrior \expertsInducingPrior} \left[
\prod_{\numData=1}^{\NumData}
\sum_{\modeInd=1}^{\ModeInd}
 \singleGatingGivenInducing \singleExpertGivenInducing \right],
\end{align}
#+END_EXPORT
where the joint distribution over the data is captured by the inducing variables
$(\gatingsInducingOutput, \expertsInducingOutput)$.
As such, information regarding the assignment of observations to experts must pass through the bottleneck of the
inducing variables.
This approximation induces a local factorisation over observations and a set of global variables -- the inducing
variables -- the necessary conditions for SVI.
Figure ref:fig-graphical-model-sparse shows the graphical model of this augmented joint probability model.


# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \allExpertGivenInducing &= \prod_{\numData=1}^{\NumData} \singleExpertGivenInducing
# = \prod_{\numData=1}^{\NumData} \E_{\singleLatentExpertGivenInducing} \left[ \singleExpertLikelihood \right] \\
# \allGatingsGivenInducing &= \prod_{\numData=1}^{\NumData} \singleGatingGivenInducing
# = \prod_{\numData=1}^{\NumData} \E_{\singleLatentGatingsGivenInducing} \left[ \singleGatingLikelihood \right] \\
# %\singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
# %\mode{\mathbf{A}} \expertInducingOutput,
# %\expertKernel(\singleInput, \singleInput) -
# %\mode{\mathbf{A}} \expertKernel(\expertInducingInput, \expertInducingInput) \mode{\mathbf{A}}^T \right), \\
# %\singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mathbf{\gatingFunc}(\singleInput) \mid
# %\mode{\hat{\mathbf{A}}} \gatingInducingOutput,
# %\gatingKernel(\singleInput, \singleInput) -
# %\mode{\hat{\mathbf{A}}} \gatingKernel(\gatingInducingInput, \gatingInducingInput) \mode{\hat{\mathbf{A}}}^T \right), \\
# %\singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
# %\expertA \expertInducingOutput,
# %\expertKernelnn - \expertA \expertKernelMM \expertA^T \right), \\
# %\singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mathbf{\gatingFunc}(\singleInput) \mid
# %\gatingA \gatingInducingOutput,
# %\gatingKernelnn - \gatingA \gatingKernelMM \gatingA^T \right), \\
# \singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
# \expertKernelnM \expertKernelMM^{-1} \expertInducingOutput,
# \expertKernelnn - \expertKernelnM \expertKernelMM^{-1} \expertKernelMn \right), \\
# \singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mode{\gatingFunc}(\singleInput) \mid
# \gatingKernelnM \gatingKernelMM^{-1} \gatingInducingOutput,
# \gatingKernelnn - \gatingKernelnM \gatingKernelMM^{-1} \gatingKernelMn \right),
# \end{align}
# #+END_EXPORT
# where,
# $\mathbf{K}_{\modeInd \numInducing \numInducing} = k_{\modeInd} (
# ${k}_{\modeInd \numData \numData} = k_k (\singleInput, \singleInput)
# $\mathbf{k}_{\modeInd \numData \numInducing} = k_k (\singleInput, \eInput)
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \mode{\mathbf{A}} &= \expertKernelnM \expertKernelMM^{-1} \\
# \mode{\mathbf{A}} &= \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1} \\
# \mode{\hat{\mathbf{A}}} &= \gatingKernel(\singleInput, \gatingInducingInput)\left(\gatingKernel(\gatingInducingInput, \gatingInducingInput)\right)^{-1}.
# \end{align}
# #+END_EXPORT


# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \singleExpertGivenInducing &= \E_{\singleLatentExpertGivenInducing} \left[ \singleExpertLikelihood \right] \\
# \singleGatingGivenInducing &= \E_{\singleLatentGatingsGivenInducing} \left[ \singleGatingLikelihood \right] \\
# \singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
# \mode{\mathbf{A}} \expertInducingOutput,
# \expertKernel(\singleInput, \singleInput) -
# \mode{\mathbf{A}}
# \expertKernel(\expertInducingInput, \expertInducingInput)
# \mode{\mathbf{A}}^T
# \right), \\
# \singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mathbf{\gatingFunc}(\singleInput) \mid
# \mode{\hat{\mathbf{A}}} \gatingInducingOutput,
# \gatingKernel(\singleInput, \singleInput) -
# \mode{\hat{\mathbf{A}}}
# \gatingKernel(\gatingInducingInput, \gatingInducingInput)
# \mode{\hat{\mathbf{A}}}^T
# \right),
# \end{align}
# %\mode{\K}
# %\mode{\tilde{\K}}
# #+END_EXPORT
# where,
# # #+BEGIN_EXPORT latex
# # \begin{align} \label{eq-}
# # \mode{\mathbf{A}} &= \expertKernel(\allInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1} \\
# # \mode{\hat{\mathbf{A}}} &= \gatingKernel(\allInput, \gatingInducingInput)\left(\gatingKernel(\gatingInducingInput, \gatingInducingInput)\right)^{-1}.
# # \end{align}
# # #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \mode{\mathbf{A}} &= \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1} \\
# \mode{\hat{\mathbf{A}}} &= \gatingKernel(\singleInput, \gatingInducingInput)\left(\gatingKernel(\gatingInducingInput, \gatingInducingInput)\right)^{-1}.
# \end{align}
# #+END_EXPORT
# $\mode{\mathbf{A}} = \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}$ and
# $\mode{\hat{\mathbf{A}}} = \gatingKernel(\singleInput, \gatingInducingInput)\left(\expertKernel(\gatingInducingInput, \gatingInducingInput)\right)^{-1}$.

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \singleExpertGivenInducing &= \E_{\singleLatentExpertGivenInducing} \left[ \singleExpertLikelihood \right] \\
# \singleGatingGivenInducing &= \E_{\singleLatentGatingsGivenInducing} \left[ \singleGatingLikelihood \right] \\
# \singleLatentExpertGivenInducing &= \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
# \expertKernel(\singleInput, \expertInducingInput) \expertKernel^{-1}(\expertInducingInput, \expertInducingInput) \expertInducingOutput,
# \mode{\K}
# \right), \\
# \singleLatentGatingsGivenInducing &= \mathcal{N}\left( \mathbf{\gatingFunc}(\singleInput) \mid
# \gatingKernel(\singleInput, \gatingInducingInput) \gatingKernel^{-1}(\gatingInducingInput, \gatingInducingInput) \gatingInducingOutput,
# \mode{\tilde{\K}}
# \right),
# \end{align}
# #+END_EXPORT
# where,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \mode{\K} = \expertKernel(\singleInput, \singleInput) - \expertKernel(\singleInput, \gatingInducingInput) (\expertKernel(\gatingInducingInput,\gatingInducingInput))^{-1} \expertKernel(\gatingInducingInput, \singleInput) \\
# \mode{\tilde{\K}} = \gatingKernel(\singleInput, \singleInput) - \gatingKernel(\singleInput, \gatingInducingInput) (\gatingKernel(\gatingInducingInput,\gatingInducingInput))^{-1} \gatingKernel(\gatingInducingInput, \singleInput).
# \end{align}
# #+END_EXPORT


# $\expertInducingInput=\allInput$, and for the gating network's inducing inputs,
# $\gatingInducingInput=\allInput$, this approximation becomes exact in the limit $M=N$.

# For each expert's inducing inputs,
# $\expertInducingInput=\allInput$, and for the gating network's inducing inputs,
# $\gatingInducingInput=\allInput$, this approximation becomes exact in the limit $M=N$.

# This minimises the KL divergence and ensures that $\expertsInducingInput$ are distributed amongst the training
# inputs $\allInput$ such that ....

*** Variational Lower Bound
Instead of collapsing the inducing variables as seen in cite:titsiasVariational2009,
they can be explicitly represented as variational distributions,
$(\expertsInducingVariational, \gatingsInducingVariational)$
and used to obtain a variational lower bound.
# \newline
*Tight Lower Bound*
Following a similar approach to cite:hensmanGaussian2013,hensmanScalable2015,
a variational lower bound on Eq. ref:eq-augmented-marginal-likelihood can be obtained,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-lower-bound-tight}
\text{log} \evidence &\geq  \sum_{\numData=1}^\NumData \E_{\gatingsInducingVariational \expertsInducingVariational}
\left[ \text{log} \left( \sum_{\modeInd=1}^\ModeInd
\singleGatingGivenInducing \singleExpertGivenInducing \right) \right] \nonumber \\
&- \gatingsKL \nonumber \\
&- \expertsKL := \tightBound,
\end{align}
#+END_EXPORT
# From Eq. ref:eq-lower-bound-tight it is clear that the optimal distribution for gating network's variational
# distribution consists of independent Guassians.
# The experts' variational posterior is not tractable so we assume a Gaussian approximate posterior.
where we parameterise the variational posteriors to be independent Gaussians,
# Our variational posteriors are then given by,
# From Eq. ref:eq-lower-bound-tight it is clear that the optimal distribution for each of the variational distributions
# is Guassian, so we parameterise them as such,
# The posterior is not tractable so we assume a Gaussian approximate posterior.
#+BEGIN_EXPORT latex
\begin{align} \label{eq-variational-inducing-dist}
\expertsInducingVariational &= \prod_{\modeInd=1}^{\ModeInd} \expertInducingVariational
= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left(\expertInducingOutput \mid \mode{\mathbf{m}}, \mode{\mathbf{S}} \right) \\
\gatingsInducingVariational &= \prod_{\modeInd=1}^{\ModeInd} \gatingInducingVariational
= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left(\gatingInducingOutput \mid \mode{\hat{\mathbf{m}}}, \mode{\hat{\mathbf{S}}} \right).
\end{align}
#+END_EXPORT
The bound in cref:eq-lower-bound-tight meets the necessary conditions to perform stochastic gradient methods on
$\expertsInducingVariational$ and $\gatingsInducingVariational$, as the variational expectation (first term)
is written as a sum over input-output pairs.
However, this expectation cannot be calculated in closed-form and must be approximated.
The joint distributions over the inducing variables for each expert GP $\expertInducingVariational$
and each gating function GP $\gatingInducingVariational$,
are $\NumInducing$ dimensional multivariate normal distributions.
Therefore, each expectation requires an $\NumInducing$ dimensional integral to be approximated.
# integral being approximated is $\NumInducing$ dimensional.
\todo{add more on why we don't want M dimensional integral}

*Further Lower Bound* Following cite:hensmanScalable2015, the bound in
Eq. ref:eq-lower-bound-tight ($\tightBound$) can be further bounded to remove the $\NumInducing$ dimensional integrals
associated with each of the gating functions.
Jensen's inequality can be applied to the conditional probability $\singleLatentGatingGivenInducing$,
obtaining the further bound,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-lower-bound-further}
\tightBound \geq \sum_{\numData=1}^\NumData &\E_{\gatingsVariational \expertsInducingVariational}
\left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertGivenInducing \right] \nonumber \\
&- \gatingsKL \nonumber \\
&- \expertsKL := \furtherBound,
\end{align}
#+END_EXPORT
where $\gatingsVariational$ represents the variational posterior given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-variational-posteriors}
%\expertsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{ \expertInducingVariational} \left[ \singleLatentExpertGivenInducing \right] \\
\gatingsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{\gatingInducingVariational} \left[ \singleLatentGatingGivenInducing \right].
\end{align}
#+END_EXPORT
Moving the marginalisation over the latent gating functions $\GatingFunc(\singleInput)$
outside of the marginalisation over the expert indicator
variable is possible because the mixing probabilities are dependent on *all* of the gating functions and
not just their associated gating function.
Moving the marginalisation over each expert's latent function $\mode{\latentFunc}(\singleInput)$
outside of the marginalisation over the expert
indicator variable, corresponds to changing the underlying model, in particular, the likelihood
approximation in cref:eq-likelihood-approximation.
# This is not the case for the experts' as they only depend on their corresponding latent function.

*Further^2 Lower Bound*
Nevertheless, we proceed and further bound the experts for comparison.
Jensen's inequality is applied to the conditional probability $\singleLatentExpertGivenInducing$,
obtaining the further^2 bound,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-lower-bound-further-2}
\furtherBound \geq \sum_{\numData=1}^\NumData &\E_{\gatingsVariational \expertsVariational}
\left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \nonumber \\
&- \gatingsKL \nonumber \\
&- \expertsKL := \furtherBoundTwo,
\end{align}
#+END_EXPORT
where $\expertsVariational$ represents the variational posterior given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-variational-posteriors}
\expertsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{ \expertInducingVariational} \left[ \singleLatentExpertGivenInducing \right].
\end{align}
#+END_EXPORT
Intuitively, this bound can be seen as modifying the likelihood approximation in cref:eq-likelihood-approximation.
Instead of mixing the GPs associated with each expert, this approximation simply mixes their associated
noise models.

# *Further Lower Bound* Following cite:hensmanScalable2015, the bound in
# Eq. ref:eq-lower-bound-tight ($\tightBound$) can be further bounded to remove the $\NumInducing$ dimensional integrals.
# Applying Jensen's inequality to the conditional probability $\singleGatingGivenInducing \singleExpertGivenInducing$,
# obtains a further bound,
# # #+BEGIN_EXPORT latex
# # \begin{align} \label{eq-further-bound}
# # \singleGatingGivenInducing \singleExpertGivenInducing \geq
# # \E_{\singleLatentExpertGivenInducing \singleLatentGatingsGivenInducing}
# # \left[ \text{log} \left( \singleGatingLikelihood \singleExpertLikelihood \right ) \right]
# # \end{align}
# # #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-lower-bound-further}
# \tightBound \geq \sum_{\numData=1}^\NumData &\E_{\gatingsVariational \expertsVariational}
# \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \nonumber \\
# &- \gatingsKL \nonumber \\
# &- \expertsKL := \furtherBound,
# \end{align}
# #+END_EXPORT
# # #+BEGIN_EXPORT latex
# # \begin{align} \label{eq-lower-bound-further}
# # \mathcal{L}_{1}
# # \geq \sum_{\numData=1}^\NumData &\E_{\expertsInducingVariational \gatingsInducingVariational}
# # \left[ \E_{\singleLatentGatingsGivenInducing \singleLatentExpertGivenInducing} \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \right] \nonumber \\
# # &- \gatingsKL - \expertsKL \\
# # = \sum_{\numData=1}^\NumData &\E_{\gatingsVariational \expertsVariational}
# # \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \nonumber \\
# # &- \gatingsKL - \expertsKL := \mathcal{L}_{2},
# # \end{align}
# # #+END_EXPORT
# where $\gatingsVariational$ and $\expertsVariational$ represents the variational posteriors given by,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-variational-posteriors}
# \expertsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{ \expertInducingVariational} \left[ \singleLatentExpertGivenInducing \right] \\
# \gatingsVariational &= \prod_{\modeInd=1}^\ModeInd \E_{\gatingInducingVariational} \left[ \singleLatentGatingGivenInducing \right].
# \end{align}
# #+END_EXPORT
As each GP's inducing variables are Normally distributed the functional form of the
variational posteriors are given by,
#+BEGIN_EXPORT latex
\begin{align}
\label{eq--variational-posteriors-functional-experts}
\expertsVariational &= \prod_{\modeInd=1}^\ModeInd \mathcal{N} \left( \mode{\latentFunc}(\singleInput) \mid
\mode{\mathbf{A}} \mode{\mathbf{m}},
\expertKernelnn
+ \mode{\mathbf{A}}
(\mode{\mathbf{S}} - \expertKernelMM)
\mode{\mathbf{A}}^T
\right) \\
\gatingsVariational &= \prod_{\modeInd=1}^\ModeInd \mathcal{N} \left( \mode{\gatingFunc}(\singleInput) \mid
\mode{\hat{\mathbf{A}}} \mode{\hat{\mathbf{m}}},
\gatingKernelnn
+ \mode{\hat{\mathbf{A}}}
(\mode{\hat{\mathbf{S}}} - \gatingKernelMM)
\mode{\hat{\mathbf{A}}}^T
\right), \label{eq--variational-posteriors-functional-gating}
\end{align}
#+END_EXPORT
# $\mode{\mathbf{A}} = \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}$ and
where
$\mode{\mathbf{A}} = \expertKernelnM \expertKernelMM^{-1}$ and
$\mode{\hat{\mathbf{A}}} = \gatingKernelnM \gatingKernelMM^{-1}$.
Importantly, these variational posteriors marginalise the inducing variables in closed-form,
with Gaussian convolutions.
$\furtherBound$ removes $\ModeInd$ of the undesirable approximate $M$ dimensional integrals from
Eq. ref:eq-lower-bound-tight and $\furtherBoundTwo$ removes $\ModeInd^2$.
The variational expectation in Eq. ref:eq-lower-bound-further still requires approximation,
however, the integrals are now only one dimensional.
These integrals are approximated with Gibbs sampling and
in practice only single samples are used because the added stochasticity helps the optimisation.

*** Optimisation
#+BEGIN_EXPORT latex
\renewcommand{\expertSampleInd}{\ensuremath{s}}
\renewcommand{\ExpertSampleInd}{\ensuremath{S}}
\renewcommand{\gatingSampleInd}{\ensuremath{\hat{s}}}
\renewcommand{\GatingSampleInd}{\ensuremath{\hat{S}}}
\renewcommand{\batchSampleInd}{\ensuremath{i}}

\newcommand{\singleExpertLikelihoodSample}{\ensuremath{p\left(\y_{\batchSampleInd} \mid \expertInducingOutput^{(\expertSampleInd)}\right)}}
%\newcommand{\singleGatingLikelihoodSample}{\ensuremath{\Pr\left(\alpha_\batchSampleInd = \modeInd \mid \gatingsInducingOutput^{(\gatingSampleInd)} \right)}}
\renewcommand{\singleGatingLikelihoodSample}{\ensuremath{\Pr\left(\alpha_\batchSampleInd = \modeInd \mid \GatingFunc(\x_{\batchSampleInd})^{(\gatingSampleInd)} \right)}}

%\newcommand{\singleExpertLikelihoodSample}{\ensuremath{p\left(\y_{\batchSampleInd} \mid \mode{f}(\x_{\batchSampleInd})^{(\expertSampleInd)}\right)}}
\newcommand{\singleGatingLikelihoodSample}{\ensuremath{\Pr\left(\alpha_\batchSampleInd = \modeInd \mid \GatingFunc_{\batchSampleInd}^{(\gatingSampleInd)} \right)}}
%\newcommand{\gatingsVariationalSample}{\ensuremath{q(\GatingFunc(\x_{\batchSampleInd-1}))}}
%\newcommand{\expertVariationalSample}{\ensuremath{q(\mode{\latentFunc}(\x_{\batchSampleInd-1}))}}

\newcommand{\gatingsVariationalSample}{\ensuremath{q(\GatingFunc(\x_{\batchSampleInd}))}}
#+END_EXPORT
\todo{add unbiased estimate of ELBO}
The bounds in  cref:eq-lower-bound-further,eq-lower-bound-tight,eq-lower-bound-further-2
meet the necessary conditions to perform stochastic
gradient methods on $\expertsInducingVariational$ and $\gatingsInducingVariational$.
Firstly, they contain a sum of $\NumData$ terms corresponding to input-output pairs, enabling optimisation
with mini-batches.
Secondly, the expectations over the log-likelihood are calculated using Monte Carlo samples.

*Stochastic Optimisation*
At each iteration $j$, a random subset of $\NumData_b$ data points are sampled from the data set $\mathcal{D}$,
to get a minibatch $\mathcal{D}_j = \{\x_i, \y_i\}_{i=1}^{\NumData_b}$.
The further lower bound $\furtherLowerBound$ is then approximated by,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-approx-lower-bound-further}
# \hat{\mathcal{L}}_{2} = \frac{\NumData}{\NumData_b} \sum_{\singleInput, \singleOutput \in \mathcal{D}_i}
# &\E_{\gatingsVariational \expertsVariational}
# \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertLikelihood \right] \\
# &- \gatingsKL - \expertsKL.
# \end{align}
# #+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-approx-lower-bound-further}
\hat{\mathcal{L}}_{\text{further}} = \frac{\NumData}{\NumData_b}
&\sum_{\x_{\batchSampleInd}, \y_{\batchSampleInd} \in \mathcal{D}_j}
\left(
\frac{1}{\ExpertSampleInd} \sum_{\expertSampleInd=1}^{\ExpertSampleInd}
\frac{1}{\GatingSampleInd} \sum_{\gatingSampleInd=1}^{\GatingSampleInd} \nonumber
\text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihoodSample \singleExpertLikelihoodSample \right)  \\
- &\gatingsKL \nonumber \\
- &\expertsKL,
\end{align}
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \renewcommand{\singleExpertLikelihoodSample}{\ensuremath{p\left(\y_{\batchSampleInd} \mid \mode{f}(\x_{\batchSampleInd-1})^{(\expertSampleInd)}\right)}}
# \renewcommand{\singleGatingLikelihoodSample}{\ensuremath{\Pr\left(\alpha_\batchSampleInd = \modeInd \mid \GatingFunc(\x_{\batchSampleInd})^{(\gatingSampleInd)} \right)}}
# \newcommand{\expertVariationalSample}{\ensuremath{q(\mode{\latentFunc}(\x_{\batchSampleInd-1}))}}
# \small
# %\sum_{\singleInput, \singleOutput \in \mathcal{D}_i}
# \begin{align} \label{eq-approx-lower-bound-further}
# \hat{\mathcal{L}}_{2} &= \frac{\NumData}{\NumData_b}
# \sum_{d_n \in \mathcal{D}_i}
# \left(
# \frac{1}{\ExpertSampleInd} \sum_{\expertSampleInd=1}^{\ExpertSampleInd}
# \frac{1}{\GatingSampleInd} \sum_{\gatingSampleInd=1}^{\GatingSampleInd} \nonumber
# \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihoodSample \singleExpertLikelihoodSample \right)  \\
# &- \gatingsKL - \expertsKL, \\
# \text{where} \quad &\mode{\latentFunc}(\x_{\batchSampleInd-1})^{(\expertSampleInd)} \sim \expertVariationalSample, \\
# &\mathbf{\gatingFunc}(\x_{\batchSampleInd-1})^{(\gatingSampleInd)} \sim \gatingsVariationalSample
# \end{align}
# \normalsize
# #+END_EXPORT
where $\expertInducingOutput^{(\expertSampleInd)} &\sim \expertInducingVariational$
and
$\mathbf{\gatingFunc}(\x_{\batchSampleInd})^{(\gatingSampleInd)} &\sim \gatingsVariationalSample$
denote samples from the variational posteriors.
The variational distributions over the inducing variables are represented using the mean vector $\mode{\mathbf{m}}$
and the lower triangular $\mode{\mathbf{L}}$ of the covariance matrix
$\mode{\mathbf{S}} = \mode{\mathbf{L}} \mode{\mathbf{L}}^T$.
A downside to this formulation is that $(\ModeInd^2)(M-1)M/2 + \ModeInd^2 M$ extra parameters need to be optimised.
In the two expert case this reduces to $(\ModeInd+1)(M-1)M/2 + (\ModeInd+1) M$ extra parameters.
Optimising the inducing inputs ($\gatingInducingInput$ and $\expertsInducingInput$) introduces a further
$\ModeInd^2\NumInducing\InputDim$ optimisation parameters.
The inducing inputs $\gatingInducingInput,\{\expertInducingInput\}_{\modeInd=1}^{\ModeInd}$,
kernel hyperparameters and noise variances, are treated as
variational hyperparameters and optimised alongside the variational parameters, using stochastic gradient descent
e.g. Adam citep:kingmaAdam2017.

# Note that the augmented model captures the dependencies in the joint distribution of the data through the
# inducing variables, but as $M \ll \NumData$ these have a much lower computational burden.

# ${\E_{\gatingsVariational \expertsInducingVariational} \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood \singleExpertGivenInducing \right]}$,

*Computational Complexity* Assuming that each expert has the same number of inducing points $\NumInducing$,
the cost of computing the KL divergences and their derivatives is $\mathcal{O}\left( \ModeInd \NumInducing^{3} \right)$.
The cost of computing the expected likelihood term is dependent on the batch size $\NumData_b$.
For each data point in the minibatch, each of the $\ModeInd$ gating function variational posteriors has complexity
$\mathcal{O}\left( \NumInducing^{2} \right)$ to evaluate.
For each data point, only a single sample is drawn from each of these distributions.
Sampling each expert's inducing variational distribution $\expertInducingVariational$
has complexity $\mathcal{O}(\NumInducing^2)$ because
the covariance is represented as the lower triangular (via the cholesky decomposition).
In addition to this sampling, calculating each experts conditional $\singleExpertGivenInducing$ given these samples
has complexity $\mathcal{O}(\NumInducing^2)$.
\todo{How to combine all of these complexities?}


# For each data point in the minibatch, each of the $\ModeInd$ gating function GPs and $\ModeInd$ expert GPs has complexity
# $\mathcal{O}\left( \NumInducing^{2} \right)$ to evaluate.


# The gating network's variational posterior has complexity
# $\mathcal{O}\left( \ModeInd \NumInducing^{2} \right)$ to evaluate.
# Drawing a single sample from this

# Each expert's variational posterior $\expertInducingVariational$ is represented using a cholesky decomposition
# so sampling from all of them has complexity $\mathcal{O}\left( \ModeInd \NumInducing^{2} \right)$.

# # The cost of computing the expected likelihood term
# # is dependent on the batch size $\NumData_b$ and has complexity
# # $\mathcal{O}\left( \NumData_{b} \ModeInd^2 \NumInducing^{2} \right)$ to evaluate.


# When the number of inducing points $\NumInducing$ is smaller than the batch size, most of the cost will arise from
# computing the expected likelihood term.
# The bound has complexity
# $\max\left(\mathcal{O}\left( \ModeInd \NumInducing^{3} \right),\mathcal{O}\left( \NumData_{b} \ModeInd \NumInducing^{2} \right)\right)$.
# \todo{check complexities. What is complexity of approximating M dimensional integral with gibbs sampling?}

*** Predictions
#+BEGIN_EXPORT latex
\renewcommand{\testInput}{\ensuremath{\mathbf{X}^*}}
\renewcommand{\testOutput}{\ensuremath{\mathbf{y}^*}}
\renewcommand{\NumTest}{\ensuremath{\NumData^*}}
\renewcommand{\singleTestInput}{\ensuremath{\mathbf{x}_n^*}}
\renewcommand{\singleTestOutput}{\ensuremath{y_n^*}}
\newcommand{\testModeVarK}{\ensuremath{\bm\modeVar^* = \modeInd}}
\newcommand{\singleTestModeVar}{\ensuremath{\modeVar_n^*}}
\newcommand{\singleTestModeVarK}{\ensuremath{\singleTestModeVar = \modeInd}}

%\newcommand{\predictivePosterior}{\ensuremath{p(\testOutput \mid \testInput, \allOutput, \allInput)}}
%\newcommand{\predictiveProb}{\ensuremath{\Pr(\singleTestModeVarK \mid \singleTestInput, \allOutput, \allInput)}}
%\newcommand{\predictiveExpert}{\ensuremath{p(\singleTestOutput \mid \singleTestModeVarK, \singleTestInput, \allOutput, \allInput)}}
\newcommand{\predictivePosterior}{\ensuremath{p(\testOutput \mid \allOutput)}}
\newcommand{\predictiveProb}{\ensuremath{\Pr(\singleTestModeVarK \mid \allOutput)}}
\newcommand{\predictiveProbBernoulli}{\ensuremath{\Pr(\singleTestModeVar=1 \mid \allOutput)}}
\newcommand{\predictiveExpert}{\ensuremath{p(\singleTestOutput \mid \singleTestModeVarK, \allOutput)}}

\newcommand{\approxPredictiveProb}{\ensuremath{q(\singleTestModeVarK)}}
\newcommand{\approxPredictiveExpert}{\ensuremath{q(\singleTestOutput \mid \singleTestModeVarK)}}

\newcommand{\predictiveExpertLikelihood}{\ensuremath{p(\singleTestOutput \mid \mode{f}(\singleTestInput))}}
\newcommand{\predictiveGatingLikelihood}{\ensuremath{\Pr(\singleTestModeVarK \mid \GatingFunc(\singleTestInput))}}
\newcommand{\predictiveGatingLikelihoodBernoulli}{\ensuremath{\Pr(\modeVar_n^*=1 \mid \modei{\gatingFunc}{1}(\singleTestInput))}}

%\renewcommand{\expertPosterior}{\ensuremath{p(\mode{\latentFunc}(\singleTestInput) \mid \singleTestInput, \allOutput, \allInput)}}
%\renewcommand{\gatingPosterior}{\ensuremath{p(\GatingFunc(\singleTestInput) \mid \singleTestInput, \allOutput, \allInput)}}
\renewcommand{\expertPosterior}{\ensuremath{p(\mode{\latentFunc}(\singleTestInput) \mid \allOutput)}}
\renewcommand{\gatingPosterior}{\ensuremath{p(\GatingFunc(\singleTestInput) \mid \allOutput)}}

\renewcommand{\expertVariationalPosterior}{\ensuremath{q(\mode{\latentFunc}(\singleTestInput)}}
\renewcommand{\gatingVariationalPosterior}{\ensuremath{q(\GatingFunc(\singleTestInput)}}
\renewcommand{\gatingVariationalPosteriorBernoulli}{\ensuremath{q(\modei{\gatingFunc}{1}(\singleTestInput))}}
#+END_EXPORT
For a given set of test inputs $\testInput \in \R^{\NumTest \times \InputDim$,
this model makes probabilistic predictions following
a mixture of $\ModeInd$ Gaussians.
Making predictions with this model involves calculating a density over the output for each expert and combining
them using the probabilities obtained from the gating network, i.e. marginalising the expert indicator variable,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-predictive-posterior}
\predictivePosterior &= \prod_{\numData=1}^{\NumTest} \sum_{\modeInd=1}^\ModeInd \underbrace{\predictiveProb}_{\text{gating network posterior}} \underbrace{\predictiveExpert}_{\text{expert } \ModeInd \text{ posterior}}} \\
&\approx \prod_{\numData=1}^{\NumTest} \sum_{\modeInd=1}^\ModeInd \approxPredictiveProb \approxPredictiveExpert
\end{align}
#+END_EXPORT

*Experts*
The experts make predictions at new test locations by integrating over their latent function posteriors,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-prediction}
\predictiveExpert
&= \int \underbrace{\predictiveExpertLikelihood}_{\text{likelihood}}
\underbrace{\expertPosterior}_{\text{posterior}} \text{d} \mode{\latentFunc}(\singleTestInput) \nonumber \\
&\approx \int \underbrace{\predictiveExpertLikelihood}_{\text{likelihood}}
\underbrace{\expertVariationalPosterior}_{\text{approx posterior}} \text{d} \mode{\latentFunc}(\singleTestInput)
\coloneqq \approxPredictiveExpert.
\end{align}
#+END_EXPORT
However, the experts' true posteriors $\expertPosterior$ are not known and have been approximated.
Each expert's approximate posterior is given by
$q(\mode{\latentFunc}(\allInputK), \expertInducingOutput) = p(\mode{\latentFunc}(\allInputK) \mid \expertInducingOutput) q(\expertInducingOutput)$.
To make a prediction at a set of test locations $\testInput$, we substitute our approximate posterior
into the standard probabilistic rule,
# *Experts* Each expert's predictive distribution over the output $\predictiveExpert$,
# is obtained by marginalising its predictive posterior,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-predictive-expert}
%\predictiveExpert &=
\underbrace{\expertPosterior}_{\text{posterior}} &=
\int p(\mode{\latentFunc}(\singleTestInput) \mid \mode{\latentFunc}(\allInputK), \expertInducingOutput)
p(\mode{\latentFunc}(\allInputK), \expertInducingOutput \mid \allOutput)
\text{d} \mode{\latentFunc}(\allInputK) \text{d} \expertInducingOutput \nonumber \\
&\approx \int p(\mode{\latentFunc}(\singleTestInput) \mid \mode{\latentFunc}(\allInputK), \expertInducingOutput)
p(\mode{\latentFunc}(\allInputK) \mid \expertInducingOutput) q(\expertInducingOutput)
\text{d} \mode{\latentFunc}(\allInputK) \text{d} \expertInducingOutput \nonumber \\
&= \int p(\mode{\latentFunc}(\singleTestInput) \mid \expertInducingOutput)
\expertInducingVariational
\text{d} \expertInducingOutput \nonumber \\
&= \mathcal{N} \left( \mode{\latentFunc}(\singleTestInput) \mid
\mode{\mathbf{A}} \mode{\mathbf{m}},
\expertKernelss
+ \mode{\mathbf{A}}
(\mode{\mathbf{S}} - \expertKernelMM)
\mode{\mathbf{A}}^T \right) \coloneqq \underbrace{\expertVariationalPosterior}_{\text{approx posterior}},
%\E_{\predictiveExpertPrior} \left[ \predictiveExpertLikelihood \right],
\end{align}
#+END_EXPORT
where $\mode{\mathbf{A}} = \expertKernelsM \expertKernelMM^{-1}$.
This integral has complexity $\mathcal{O}(\NumInducing^2)$.


*Gating Network* The mixing probabilities associated with the gating network are obtained
by integrating the gating network's posterior through the gating likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-prediction}
\predictiveProb
&= \int \underbrace{\predictiveGatingLikelihood}_{\text{likelihood}}
\underbrace{\gatingPosterior}_{\text{posterior}} \text{d} \GatingFunc(\singleTestInput) \nonumber \\
&\approx \int \underbrace{\predictiveGatingLikelihood}_{\text{likelihood}}
\underbrace{\gatingVariationalPosterior}_{\text{approx posterior}} \text{d} \GatingFunc(\singleTestInput)
\coloneqq \approxPredictiveProb.
\end{align}
#+END_EXPORT
Again, the gating network's true posterior $\gatingPosterior$ has been approximated,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-predictive-gating}
\underbrace{\gatingPosterior}_{\text{posterior}}
&\approx \int p(\GatingFunc(\singleTestInput) \mid \gatingInducingOutput)
\gatingInducingVariational \text{d} \gatingInducingOutput \nonumber \\
&= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N} \left( \GatingFunc(\singleTestInput) \mid
\mode{\hat{\mathbf{A}}} \mode{\hat{\mathbf{m}}},
\gatingKernelss
+ \mode{\hat{\mathbf{A}}}
(\mode{\hat{\mathbf{S}}} - \gatingKernelMM)
\mode{\hat{\mathbf{A}}}^T \right) \coloneqq \underbrace{\gatingVariationalPosterior}_{\text{approx posterior}},
\end{align}
#+END_EXPORT
where $\mode{\hat{\mathbf{A}}} = \gatingKernelsM \gatingKernelMM^{-1}$.
In the general case, when $\ModeInd > 2$, the gating likelihood is the softmax,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-likelihood-softmax}
\underbrace{\predictiveGatingLikelihood}_{\text{likelihood}} = \text{softmax}(\GatingFunc(\singleTestInput)),
\end{align}
#+END_EXPORT
so Eq. ref:eq-gating-prediction is approximated with Monte Carlo quadrature.
In the two expert case there is only a single gating function, $\gatingFunc_1$, as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\Pr(\singleTestModeVar=2 \mid \modei{\gatingFunc}{1}(\singleTestInput)) = 1 - \Pr(\singleTestModeVar=1 \mid \modei{\gatingFunc}{1}(\singleTestInput)).
\end{align}
#+END_EXPORT
In this case, the gating likelihood is the Gaussian cdf,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gating-likelihood-bernoulli}
\underbrace{\predictiveGatingLikelihoodBernoulli}_{\text{likelihood}} = \Phi(\gatingFunc_1(\singleTestInput)),
\end{align}
#+END_EXPORT
so Eq. ref:eq-gating-prediction can be calculated in closed-form with,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-predictive-gating-bernoulli}
\predictiveProbBernoulli &=
\int \gatingVariationalPosteriorBernoulli \Phi\left(\modei{\gatingFunc}{1}(\singleTestInput)\right) \text{d} \modei{\gatingFunc}{1}(\singleTestInput) \nonumber \\
&= \Phi \left(\frac{\mu_{h*}}{\sqrt{1 + \sigma^2_{h_*} }}\right).
\end{align}
%\mathcal{N}\left(\gatingFunc(\x_*) \mid \mu_h, \sigma^2_h \right) \text{d} \gatingFunc(\x_*) \\
#+END_EXPORT
where $\mu_{h_*}$ and $\sigma^2_{h_*}$ are the mean and variance of the variational posterior
$\gatingVariationalPosteriorBernoulli$ at $\singleTestInput$.



# # $\predictiveGatingLikelihood$,
# # *Gating Network* The mixing probabilities associated with the gating network are obtained
# # by taking the expectation of the gating likelihood $\predictiveGatingLikelihood$,
# # under the predictive posterior of the gating network,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-predictive-gating}
# \predictiveProb &= \int \predictiveGatingPrior \predictiveGatingLikelihood \text{d} \GatingFunc(\x_*),
# %\predictiveProb &= \E_{\predictiveGatingPrior} \left[ \predictiveGatingLikelihood \right],
# \end{align}
# #+END_EXPORT
# where the predictive posterior $\predictiveGatingPrior$ is given by
# Eq. ref:eq--variational-posteriors-functional-gating.

# In the general case, when $\ModeInd > 2$, the gating likelihood is the softmax,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-gating-likelihood-softmax}
# \predictiveGatingLikelihood = \text{softmax}(\GatingFunc(\x_*)),
# \end{align}
# #+END_EXPORT
# so Eq. ref:eq-predictive-gating is approximated with Monte Carlo quadrature.
# In the two expert case there is only a single gating function, $\gatingFunc_1$, as,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# ${\Pr(\singleModeVar=2 \mid \modei{\gatingFunc}{1}(\singleInput)) = 1 - \Pr(\singleModeVar=1 \mid \modei{\gatingFunc}{1}(\singleInput))}$.
# \end{align}
# #+END_EXPORT
# In this case, the gating likelihood is the Gaussian cdf,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-gating-likelihood-bernoulli}
# \predictiveGatingLikelihoodBernoulli = \Phi(\gatingFunc_1(\x_*)),
# \end{align}
# #+END_EXPORT
# so Eq. ref:eq-predictive-gating can be calculated in closed-form with,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-predictive-gating-bernoulli}
# \Pr(\modeVar_*=1 \mid \x_*) &=
# \E_{\predictiveGatingPriorBernoulli} \left[ \Phi\left(\gatingFunc(\x_*)\right) \right] \\
# &= \Phi \left(\frac{\mu_{h*}}{\sqrt{1 + \sigma^2_{h_*} }}\right).
# \end{align}
# %\mathcal{N}\left(\gatingFunc(\x_*) \mid \mu_h, \sigma^2_h \right) \text{d} \gatingFunc(\x_*) \\
# #+END_EXPORT
# where $\mu_{h_*}$ and $\sigma^2_{h_*}$ are the mean and variance of the predictive posterior
# $\predictiveGatingPriorBernoulli$ at $\x_*$.

*** sparse graphical model old :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\fkn$};
      %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
      \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\hkn$};

      \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\uFk$};
      \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\Hku$};
      \node[const, left=of uk, xshift=0.4cm] (zk) {$\zFk$};
      \node[const, right=of uh, xshift=-0.4cm] (zh) {$\zHk$};

      \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\expertParamsK$};
      \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\gatingParamsK$};

      \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (uk)--(f);
      \draw[post] (uh)--(h);
      \draw[post] (zk)--(uk);
      \draw[post] (zh)--(uh);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a) (f) (h)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(zh) (uh) (h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    }
  \caption{Graphical model of the transition dynamics where the state difference $\singleOutput$
is generated by pushing the state and control $\singleInput$ through the latent process.}
\label{fig-graphical-model-sparse}
\end{figure}
#+END_EXPORT



#+BEGIN_EXPORT latex
\todo[inline]{
\begin{itemize}
\item We want $\pFGivenFu$ outside the log (and not just $\qFu$) so we can analytically integrate $\qFu$ (like in Eq. \ref{eq-experts-var-dist})??
\item In the ICRA paper I had only $\qFu$ outside the log so we had to approximate the M-dimensional integral over $\qFu$ with samples.
\end{itemize}
}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo[inline]{
This is the bound I had coded up previously,
\begin{align} \label{eq-expert-bound-old}
\mathcal{L}_{\text{old}} =
&\sum_{\numData}^{\NumData} E_{\prod_{\modeInd=1}^{\ModeInd} \qFku} \text{log} \sum_{k=1}^{K} \PrA \left[ \pykGivenFku \right] \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right]
\end{align}
This bound is valid but it is nicer to move the expectations over $\pFk$ outside the log, like in Eq. \ref{eq-experts-bound-4-fact}.
}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo[inline]{
This is another bound I implemented,
\begin{align} \label{eq-expert-bound-old}
\mathcal{L}_{\text{old}} =
&\sum_{\numData}^{\NumData} \text{log} \sum_{k=1}^{K} \PrA \E_{\prod_{\modeInd=1}^{\ModeInd} \qFku} \left[ \pykGivenFku \right] \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right]
\end{align}
\begin{itemize}
\item THIS IS WRONG as the expectation over $\qFku$ should be outside the log.
\item which is why I used to approximate it with single samples....
\end{itemize}
}
#+END_EXPORT

\todo[inline]{Need to extend bound for the gating network}

** Inference_old [[label:sec-inference]] :noexport:
#+begin_export latex
\epigraph{Nature laughs at the difficulties of integration.}{\textit{Pierre-Simon Laplace}}
#+end_export
#+BEGIN_EXPORT latex
\renewcommand{\modei}[2]{\ensuremath{#1_{#2}}}
\renewcommand{\singleOutput}{\ensuremath{\Delta x_{\numData}}}
\renewcommand{\allOutput}{\ensuremath{\Delta\mathbf{\state}}}
\newcommand{\expertInducingInput}{\ensuremath{\singleData{\modeVar}}}
\newcommand{\expertInducingOutput}{\ensuremath{\singleData{\modeVar}}}
#+END_EXPORT
*** intro :ignore:
The marginal likelihood in Eq. ref:eq-marginal-likelihood-factorised is extremely expensive
to evaluate $\mathcal{O}(\ModeInd^2 \NumData^{4})$
\todo{add marginal likelihood's correct complexity}
and is also intractable due to the marginalisation of $\mathbf{h}$ in the gating network
(except for the two expert case).
For these reasons, this work derives a variational approximation based on inducing variables that provides scalability
by utilising stochastic gradient-based optimisation.

Following the approach by cite:titsiasVariational2009, the probability space is augmented
with a set of $M$ inducing variables for each GP.
However, instead of collapsing these inducing variables they are explicitly
represented as variational distributions and used to lower bound citep:hensmanGaussian2013
and then further bound citep:hensmanScalable2015 the marginal likelihood.



Let us first consider lower bounding the MoGPE model for the general gating network case
i.e. without specifying a particular gating network. The marginal likelihood of this model is given by,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-general-moe-marginal-likelihood}
\text{log} \pYGivenX &= \text{log} \sum_{k=1}^{K} \PrA \E_{\pFk} \left[ \pYkGivenFk \right],
\end{align*}
#+END_EXPORT
Following the approach by cite:titsiasVariational2009 and cite:hensmanGaussian2013, which is
detailed in Section ref:sec-sparse-gps Eqs. ref:eq-titsias-bound - ref:eq-hensman-bound,
the probability space is first augmented with a set of $M$ inducing variables from each GP prior,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-experts-inducing-dist}
\pFuGivenX = \prod_{\modeInd=1}^{\ModeInd} \pFkuGivenX = \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \uFk \mid \mode{\mu}(\zFk), \mode{k}(\zFk, \zFk) \right).
\end{align*}
#+END_EXPORT
The augmented log marginal likelihood is then given by,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-general-moe-marginal-likelihood}
\text{log} \pYGivenX &= \text{log} \sum_{k=1}^{K} \PrA \E_{\prod_{\modeInd=1}^{\ModeInd} \pFkuGivenX} \left[ \E_{\pFkGivenFku} \left[ \pYkGivenFk \right] \right] .
%\text{log} \pYGivenX &= \text{log} \sum_{k=1}^{K} \PrA \prod_{\modeInd=1}^{\ModeInd} \pYkGivenFk \pFkGivenFku \pFkuGivenX.
\end{align*}
#+END_EXPORT
For notational conciseness the dependency on the inputs has been dropped.
Note that we have assumed each expert's inducing variables are independent and augmented each expert with
the product over all experts inducing variables.
This will enable us to move the expectation over the inducing variables outside the sum over the indicator variable.
\todo[inline]{no need to say this...}

We start by considering the augmented log marginal likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-marginal-likelihood}
\text{log} \pYGivenX &= \text{log} \sum_{k=1}^{K} \PrA \E_{\prod_{\modeInd=1}^{\ModeInd} \pFku} \left[ \pYkGivenFku \right].
\end{align}
#+END_EXPORT
and moving the integrals over the inducing variables outside of the sum over the indicator variable,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-marginal-likelihood-fact}
\text{log} \pYGivenX = \text{log} \E_{\prod_{\modeInd=1}^{\ModeInd} \pFku} \left[ \sum_{k=1}^{K} \PrA  \pYkGivenFku \right],
\end{align}
\todo{Don't think I need to mention Fubini-Tonelli theorem here}
#+END_EXPORT
Instead of collapsing the inducing variables here citep:titsiasVariational2009
they can be explicitly represented as variational distributions citep:hensmanGaussian2013,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-var-experts-inducing-dist}
\qFu = \prod_{k=1}^{K} \qFku = \prod_{k=1}^{K} \mathcal{N}\left(\uFk \mid \mode{\mathbf{m}}, \mode{\mathbf{S}} \right)
\end{align}
#+END_EXPORT
These variational distributions can then be used to obtain a lower
bound on Eq. ref:eq-expert-marginal-likelihood-fact,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-bound-2}
\text{log} \pYGivenX
\geq &\E_{\prod_{\modeInd=1}^{\ModeInd}\qFku} \left[ \text{log} \sum_{k=1}^{K} \PrA  \pYkGivenFku \right] \nonumber \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right] := \mathcal{L}_{1}.
\end{align}
#+END_EXPORT
This bound meets the necessary conditions to perform stochastic gradient methods on $\qFu$ as the first term
can be written as a sum over input-output pairs,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-bound-2-fact}
\mathcal{L}_{1} =
&\sum_{\numData=1}^{\NumData} \E_{\prod_{\modeInd=1}^{\ModeInd}\qFku} \left[ \text{log} \sum_{k=1}^{K} \Pra  \pykGivenFku \right] \nonumber \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right].
\end{align}
#+END_EXPORT
However, the expectation over the inducing variables cannot be calculated in closed form and
must be approximated. The joint distribution over the inducing variables for each GP $\qFku$ is $M$ dimensional and as
a result the integral is $M$ dimensional.
Following cite:hensmanScalable2015 this bound (Eq. ref:eq-expert-bound-2-fact) can be further bounded by
moving the expectations over each expert's conditional of its latent function values given its inducing variables
$\pFkGivenFku$
outside the log, i.e. applying Jensen's inequality to the conditional probability $\pYkGivenFku$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-bound-3}
\mathcal{L}_{1}
\geq &\E_{\prod_{k=1}^{K}\qFku} \left[ \E_{\pFkGivenFku} \left[ \text{log} \sum_{k=1}^{K} \PrA \pYkGivenFk \right] \right] \nonumber \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right] := \mathcal{L}_{\text{experts}},
\end{align}
#+END_EXPORT
Denoting the variational posterior
$\qF := \prod_{k=1}^{K} \qFk = \prod_{k=1}^{K} \int \pFkGivenFku \qFku \text{d} \uFk$ enables us
to rewrite Eq. ref:eq-expert-bound-3 as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expert-bound-4}
\mathcal{L}_{\text{experts}}
&= \E_{\qF} \left[ \text{log} \sum_{k=1}^{K} \PrA \pYkGivenFk \right] - \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right]
\end{align}
#+END_EXPORT
As each experts inducing variables distribution $\qFku$ is Gaussian the functional form of the variational posterior is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-var-dist}
\qF = \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \Fk \mid \mode{\mathbf{A}} \mode{\mathbf{m}}, \mode{\K}_{\numData \numData } + (\mode{\mathbf{S}} - \mode{\K}_{mm}) \left(\mode{\mathbf{A}}\right)^{T} \right),
\end{align}
#+END_EXPORT
where $\mode{\mathbf{A}} = \mode{\K}_{\numData m}\left(\mode{\K}_{mm}\right)^{-1}$.
The model's likelihood is a mixture of Gaussians and factorizes
across data as $\prod_{\numData=1}^{\NumData} \Pra \pykGivenfk$. $\mathcal{L}_{\text{experts}}$ can thus be
written as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-bound-4-fact}
\mathcal{L}_{\text{experts}} =
&\sum_{\numData=1}^{\NumData}\E_{\qfn} \left[ \text{log} \sum_{k=1}^{K} \Pra \pykGivenfk \right] \nonumber \\
&- \sum_{\modeInd=1}^{\ModeInd} \text{KL}\left[\qFku \mid\mid \pFku\right].
\end{align}
#+END_EXPORT
Importantly, the variational posterior $\qfn$ analytically marginalises each $\qFku$
(with Gaussian convolutions) removing the undesirable approximate $M$ dimensional integrals from
Eq. ref:eq-expert-bound-2-fact. The expectation in Eq. ref:eq-experts-bound-4-fact still requires approximation
but now only requires one dimensional integrals of the log-likelihood to be approximated.
We approximate the integrals with Gibbs sampling and
in practice only use single samples because the added stochasticity helps the optimisation.

The bound in Eq. ref:eq-experts-bound-4-fact meets the necessary conditions to perform stochastic
gradient methods on $\qFu$ as the sum of $\NumData$ terms corresponds to input-output pairs.
The inducing inputs $\{\Z\}_{\modeInd=1}^{\ModeInd}$, kernel hyperparameters and noise variances are treated as
variational hyperparameters and are optimised using stochastic gradient descent alongside the variational
parameters.
Note that our augmented model captures the dependencies in the joint distribution of the data through the
inducing variables, but as $M \ll \NumData$ these have a much lower computational burden.
Given a batch of $\NumData_b$ observations this bound has complexity
$\mathcal{O}\left( \NumData_{b} \ModeInd M^{3} \right)$ to evaluate.
It is worth noting that in contrast to other MoGPE models, our model does not partition the data set,
i.e. each of our expert depends on **all** of the training inputs.
However, after augmenting each expert with inducing points,
the augmented model has the flexibility to /partition/ our inducing points.
As such, when selecting the number of inducing points $\NumInducing$ for each GP, one could partition $\NumData$
and select the number of inducing points based off of the number of data points believed to be
associated with each expert.
\todo{citation for selecting number of inducing points}

*** sparse graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[const] (x) {$\singleInput$};
      \node[latent, left=of x, yshift=-1.4cm] (f) {$\fkn$};
      %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
      \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\hkn$};

      \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\uFk$};
      \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\Hku$};
      \node[const, left=of uk, xshift=0.4cm] (zk) {$\zFk$};
      \node[const, right=of uh, xshift=-0.4cm] (zh) {$\zHk$};

      \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\expertParamsK$};
      \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\gatingParamsK$};

      \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\noiseVarK$};

      \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\singleOutput$};
      %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
      \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVarn$};

      %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

      \factor[above=of a] {h-a} {left:Cat} {h} {a};

      \draw[post] (a)--(y);
      \draw[post] (x)-|(f);
      %\draw[post] (f)--(yk);
      \draw[post] (f)--(y);
      %\draw[post] (yk)--(y);
      %\draw[post] (h)--(a);
      \draw[post] (x)-|(h);
      \draw[post] (uk)--(f);
      \draw[post] (uh)--(h);
      \draw[post] (zk)--(uk);
      \draw[post] (zh)--(uh);
      \draw[post] (thetak)--(f);
      \draw[post] (phik)--(h);
      \draw[post] (sigmak)|-(y);

      \plate {} {(x) (y) (a) (f) (h)} {$\NumData$};
      %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
      \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$\ModeInd$};
      \plate {} {(zh) (uh) (h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    }
  \caption{Graphical model of the transition dynamics where the state difference $\singleOutput$
is generated by pushing the state and control $\singleInput$ through the latent process.}
\label{fig-graphical-model-sparse}
\end{figure}
#+END_EXPORT



#+BEGIN_EXPORT latex
\todo[inline]{
\begin{itemize}
\item We want $\pFGivenFu$ outside the log (and not just $\qFu$) so we can analytically integrate $\qFu$ (like in Eq. \ref{eq-experts-var-dist})??
\item In the ICRA paper I had only $\qFu$ outside the log so we had to approximate the M-dimensional integral over $\qFu$ with samples.
\end{itemize}
}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo[inline]{
This is the bound I had coded up previously,
\begin{align} \label{eq-expert-bound-old}
\mathcal{L}_{\text{old}} =
&\sum_{\numData}^{\NumData} E_{\prod_{\modeInd=1}^{\ModeInd} \qFku} \text{log} \sum_{k=1}^{K} \PrA \left[ \pykGivenFku \right] \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right]
\end{align}
This bound is valid but it is nicer to move the expectations over $\pFk$ outside the log, like in Eq. \ref{eq-experts-bound-4-fact}.
}
#+END_EXPORT

#+BEGIN_EXPORT latex
\todo[inline]{
This is another bound I implemented,
\begin{align} \label{eq-expert-bound-old}
\mathcal{L}_{\text{old}} =
&\sum_{\numData}^{\NumData} \text{log} \sum_{k=1}^{K} \PrA \E_{\prod_{\modeInd=1}^{\ModeInd} \qFku} \left[ \pykGivenFku \right] \\
&- \sum_{k=1}^{K} \text{KL}\left[\qFku \mid\mid \pFku\right]
\end{align}
\begin{itemize}
\item THIS IS WRONG as the expectation over $\qFku$ should be outside the log.
\item which is why I used to approximate it with single samples....
\end{itemize}
}
#+END_EXPORT

\todo[inline]{Need to extend bound for the gating network}

** Evaluation of Model and Approximate Inference
*** Intro :ignore:
# As a mixture of experts method we aim to improve on standard GP regression
# with the ability to model non-stationary functions and multimodal distributions.
# This work also proposes to address their inherent identifiability issues, by adopting a gating network
# that we can place informative priors on.
# As such, the method is tested on two data sets,
# 1) *Artificial data set* to demonstrate how the method improves identifiability,
# 2) *Motorcycle data set* cite:Silverman1985 to provide a comparison to other MoGPE methods and to,
#    - thoroughly compare the different ELBO's in Section ref:sec-inference,
#    - evaluate the impact of the number of inducing points $M$ and the batch size $\NumData_b$.

# This work also proposes to address their inherent identifiability issues, by adopting a gating network
# that can encode domain knowledge through informative priors.
# The method is then tested on an artificial data set to demonstrate its power at improving identifiability.

As a mixture of experts method, our model aims to improve on standard GP regression
with the ability to model non-stationary functions and multimodal distributions over the output variable.
With this in mind, the model and approximate inference scheme are evaluated on two data sets.
Following other MoGPE work, they are first tested on the motorcycle data set citep:Silverman1985.
Although this data set does not represent state transitions from a dynamical system,
it does contain non-stationary points and heterogeneous noise,
making it interesting to study from the MoGPE perspective.
Secondly, they are tested on a data set collected onboard a DJI Tello quadcopter flying in the Bristol Robotics
Laboratory.
*** Experiments
#+BEGIN_EXPORT latex
\newcommand{\numTest}{\ensuremath{n}}
\newcommand{\NumTest}{\ensuremath{N}}
%\newcommand{\testSingleInput}{\ensuremath{\x_{\numTest}}}
%\newcommand{\testSingleOutput}{\ensuremath{\y_{\numTest}}}
%\newcommand{\allTestInput}{\ensuremath{\allInput_*}}
%\newcommand{\allTestOutput}{\ensuremath{\allOutput_*}}
\newcommand{\predictedSingleInput}{\ensuremath{\testSingleInput^*}}
\newcommand{\predictedSingleOutput}{\ensuremath{\testSingleOutput^*}}
\renewcommand{\predictedSingleInput}{\ensuremath{\testSingleInput^*}}
\renewcommand{\predictedSingleInput}{\ensuremath{\hat{\x}^*_{\numTest}}}
\renewcommand{\predictedSingleOutput}{\ensuremath{\hat{\y}^*_{\numTest}}}
#+END_EXPORT

Each experiment was carried out on a system with an Intel Core i9 CPU at 2.4GHz with 16GB DDR4 RAM.
All data sets were split into test and training sets with $70\%$ for training and $30\%$ for testing.
In order to evaluate and compare the full predictive posteriors the Negative Log Predictive Probability (NLPP)
is computed on the test set.
The models are also compared using the Root Mean Squared Error (RMSE) and the Mean Absolute Error (MAE).
Given a test data set
$(\testInput, \testOutput) = \{(\singleTestInput, \singleTestOutput)\}_{\numTest=1}^{\NumTest}$,
they are calculated as follows,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-scores}
\text{RMSE} &= \sqrt{\frac{1}{\NumTest} \sum_{\numTest=1}^{\NumTest} (\predictedSingleOutput - \singleTestOutput)^2} \\
\text{MAE} &= \frac{1}{\NumTest} \sum_{\numTest=1}^{\NumTest} | \predictedSingleOutput - \singleTestOutput | \\
\text{NLPP} &= \frac{1}{\NumTest} \sum_{\numTest=1}^{\NumTest} - \log (\singleTestOutput \mid \singleTestInput, \mathcal{D}, \expertParams, \gatingParams)
\end{align}
#+END_EXPORT
where $\predictedSingleOutput$ is the models prediction at $\singleTestInput$.
Note that all figures in this section show models that were trained on the full data set, i.e. no test/train split.

*** Evaluation on Motorcycle Data Set
**** intro :ignore:
The Motorcycle data set
(discussed in cite:Silverman1985) contains 133 data points
($\allInput \in \inputDomain \subseteq \R^{133 \times 1}$ and
$\allOutput \in \outputDomain \subseteq \R^{133 \times 1}$)
and input dependent noise.
The data set represents motorcycle impact data -- time (ms) vs acceleration (g).
The data set is represented by the black crosses in cref:fig-y-mcycle-two-experts.


# The model and inference are evaluated by instantiating the model with both two $\ModeInd=2$
# and three $\ModeInd=3$ experts.
To test the performance of our method (MoSVGPE), the model is
instantiated with $\ModeInd=2$ and $\ModeInd=3$ experts.
All experiments on the Motorcycle data set use $\NumInducing=32$ inducing points for
all GPs and are trained for $25,000$ iterations
of Adam citep:kingmaAdam2017, with a learning rate of $0.01$ and a batch size of $\NumData_b=16$.
The results are compared against a Gaussian process (GP) and a sparse variational Gaussian
process (SVGP),
which use Squared Exponential (SE) kernels with automatic relevance determination (ARD) and
a Gaussian likelihood.
# The method is compared to a Sparse Variational Gaussian Process (SVGP) trained with the same
# number of inducing points and training parameters.

cref:tab-mcycle-metrics summarises the results for the three ELBO's
($\tightBound$, $\furtherBound$, $\furtherBoundTwo$)
and compares them to a standard GP regression model
and a SVGP method instantiated with $\NumInducing=16$ and $\NumInducing=32$ inducing points.
Both methods use Gaussian likelihoods and optimise their hyperparameters, noise variances
(and inducing inputs in the SVGP case) using their well known
objectives -- the marginal likelihood and evidence lower bound.

***** Results table :ignore:

#+Name: tab-mcycle-metrics
#+Caption: Results on the Motorcycle data set cite:Silverman1985 with different instantiations of our model (MoSVGPE).
#+Caption: Comparison of the root mean squared error (RMSE) mean absolute error (MAE)
#+Caption: and negative log predictive probability (NLPP) on the test data set.
#+Caption: Results for a Gaussian process (GP) and a sparse variational Gaussian process (SVGP) with
#+Caption: $\NumInducing=16$ and $\NumInducing=32$ inducing points are shown for comparison.
#+Caption: All models were instantiated with Squared Exponential kernels and were
#+Caption: trainind for $25,000$ iterations.
#+Caption: The GP's hyperparamters were optimised using SciPy's citep:2020SciPy-NMeth L-BFGS-B optimiser.
#+Caption: The SVGP and MoSVGPE models were trained with Adam citep:kingmaAdam2017 using a learning rate
#+Caption: of $0.01$ and a minibatch size of $\NumData_b=16$.
#+Caption: The MoSVGPE expertiments used $\NumInducing=32$ inducing points for each expert GP
#+Caption: and each gating function GP.
|-----------------------------------+-------------------+-------------------+-------------------|
|                                   | RMSE              | NLPP              | MAE               |
|-----------------------------------+-------------------+-------------------+-------------------|
| GP                                | $\mathbf{0.4357}$ | $0.9886$          | $0.3242$          |
|-----------------------------------+-------------------+-------------------+-------------------|
| SVGP $(M=16)$                     | $0.4427$          | $0.9762$          | $0.3257$          |
| SVGP $(M=32)$                     | $0.4437$          | $0.9832$          | $0.3271$          |
|-----------------------------------+-------------------+-------------------+-------------------|
| MoSVGPE $(k=2, \tightBound)$      | $0.4442$          | $0.4863$          | $0.3260$          |
| MoSVGPE $(k=2, \furtherBound)$    | $0.4590$          | $0.5073$          | $0.3355}$         |
| MoSVGPE $(k=2, \furtherBoundTwo)$ | $0.4472$          | $0.5271$          | $\mathbf{0.3218}$ |
| MoSVGPE $(k=3, \tightBound)$      | $0.4569$          | $\mathbf{0.2634}$ | $0.3301$          |
| MoSVGPE $(k=3 ,\furtherBound)$    | $0.4866$          | $0.2695$          | $0.3449$          |
| MoSVGPE $(k=3 ,\furtherBoundTwo)$ | $0.4575$          | $0.5467$          | $0.3270$          |

***** After results table :ignore:
The NLPP indicates the probability of the data given the
parameters which are not marginalised, e.g. hyperparameters and inducing inputs.
Following Bayesian model selection, it is known that lower values indicate higher performing models, i.e.
predictive posteriors that more accurately match the distribution of the data.
The predictive posterior is most accurate when MoSVGPE is instantiated with three experts $\ModeInd=3$
and trained using the tight lower bound $\tightBound$.
In both the two and three expert experiments,
the tight lower bound $\tightBound$ achieved better NLPP than both of the further/further^2
lower bounds,  $\furtherBound$ and $\furtherBoundTwo$.
This is expected as it is a tighter bound.
As both of the further/further^2 lower bounds offer improved computational properties,
it is interesting to compare their performance.
The NLPP scores for the further lower bound $\furtherBound$ are almost equal to the tight lower bound $\tightBound$.
In contrast, the NLPP score in the three expert experiment for the further^2 lower bound $\furtherBoundTwo$ is
significantly worse.
This indicates that valuable information is lost in this bound.
This was expected as this bound corresponds to a further likelihood approximation.
This approximation mixes the experts' noise models as opposed to their full SVGP models.


# It is worth noting here that the tight lower bound $\tightBound$ takes
# longer to compute than the further lower bound $\furtherBound$.
# \todo{add something on further bound being better computationally????}

With regards to the accuracy of the predictive means,
the standard GP regression model achieved the best RMSE, followed by the SVGP models and then the
MoSVGPE models.
It is worth noting that all of the RMSE and MAE scores are very similar.
Although adding more experts to the MoSVGPE model appears to learn more accurate predictive posteriors, the
predictive means appear to deteriorate.
This is most likely due to bias at the boundaries between the experts,
resulting from the mixing behaviour arising from our GP-based gating network.
If the gating functions do not have extremely low lengthscales then they will not be able to immediately switch
from one expert to another.
Although this appears to negatively impact performance here, it should be noted that
the GP-based gating network can offer superior generalisation and identifiability.

The two further lower bounds ($\furtherBound$ and $\furtherBoundTwo$),
derived in Section ref:sec-inference, are compared by training each instantiation of
the model using the same model and training parameters.

**** two expert y fig :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_means.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-y-means-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L3/y_means.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-y-means-mcycle-two-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_samples.pdf}
\subcaption{}
\label{fig-y-samples-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L3/y_samples.pdf}
\subcaption{}
\label{fig-y-samples-mcycle-two-experts-further}
\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L1/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-two-experts}
%\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-three-experts}
%\end{minipage}
\caption{Visualisation of the model instantiated with $\ModeInd=2$ experts, after training on the motorcycle data set \citep{Silverman1985} with $\furtherBound$ (left column) and with $\furtherBoundTwo$ (right column).  \subref{fig-y-means-mcycle-two-experts-tight}-\subref{fig-y-means-mcycle-two-experts-further} show the data set (black crosses) and the posterior means associated with the MoSVGPE (black solid line) and a SVGP (red dashed line) for comparison. \subref{fig-y-samples-mcycle-two-experts-tight}-\subref{fig-y-samples-mcycle-two-experts-further} show samples from the MoSVGPE posterior where colour indicates the underlying expert (cyan $K=1$, magenta $\ModeInd=2$) and the red lines show the $\pm 2$ standard deviation error ($95\%$ confidence interval) of the SVGP posterior. All experiments were trained with $25,000$ iterations of Adam \citep{kingmaAdam2017} using a learning rate of $0.01$ and a minibatch size of $\NumData_b=16$. All GPs use Squared Exponential kernels and $M=32$ inducing points.}
\label{fig-y-mcycle-two-experts}
\end{figure}
#+END_EXPORT

# #+caption: \textbf{Motorcycle data} The left plot shows the observations (red crosses) together with the means of our model (black solid line) and a SVGP with noise (blue dashed line). It also shows probabilities assocaited with the gating network for $k=1$ (green line) and $k=2$ (blue line) on the right axis. The right plot shows 100 samples (blue) from the predictive posterior at 200 evenly spaced input locations as well as the $\pm 2$ std error (95% confidence interval) of the SVGP (solid red line). Both models use squared exponential covariance functions for all GPs.

**** two expert latent fig :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{./images/model/mcycle/K=2_L2/experts_f.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-expert-gps-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{./images/model/mcycle/K=2_L3/experts_f.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-expert-gps-mcycle-two-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{./images/model/mcycle/K=2_L2/gating_gps.pdf}
%\subcaption{Gating function posteriors $\predictiveGatingPrior$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-gating-gps-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{./images/model/mcycle/K=2_L3/gating_gps.pdf}
%\subcaption{Gating function posteriors $\predictiveGatingPrior$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-gating-gps-mcycle-two-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{./images/model/mcycle/K=2_L2/gating_mixing_probs.pdf}
%\subcaption{Gating network's predictive mixing probabilities $\predictiveProb$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-mixing-probs-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{./images/model/mcycle/K=2_L3/gating_mixing_probs.pdf}
%\subcaption{Gating network's predictive mixing probabilities $\predictiveProb$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-mixing-probs-mcycle-two-experts-further}
\end{minipage}
\caption{Visualisation of the model's latent variables (instantiated with $\ModeInd=2$ experts), after training on the
Motorcycle data set \citep{Silverman1985}, with $\furtherBound$ (left column)
and with $\furtherBoundTwo$ (right column).
(\subref{fig-expert-gps-mcycle-two-experts-tight}-\subref{fig-expert-gps-mcycle-two-experts-further}) show the GP
posteriors associated with the experts' latent functions $q(\mode{\latentFunc}(\x_*))$,
where the solid lines show the mean and the shaded regions show the $95\%$ confidence intervals, i.e. $\pm2\sigma$.
The gating network's GP posteriors $q(h_{k}(\x_*))$ are shown in
(\subref{fig-gating-gps-mcycle-two-experts-tight}-\subref{fig-gating-gps-mcycle-two-experts-further}) and
their associated mixing probabilities $\approxPredictiveProb$ in
(\subref{fig-mixing-probs-mcycle-two-experts-tight}-\subref{fig-mixing-probs-mcycle-two-experts-further}).}
\label{fig-latent-mcycle-two-experts}
\end{figure}
#+END_EXPORT

**** Two Experts
The two further lower bounds ($\furtherBound$ and $\furtherBoundTwo$) are
compared by instantiating the model with two experts $\ModeInd=2$
and comparing their performance.
The results are shown in cref:fig-y-mcycle-two-experts,fig-latent-mcycle-two-experts,
where cref:fig-y-mcycle-two-experts visualises the predictive posteriors and
cref:fig-latent-mcycle-two-experts visualises the posteriors over the latent variables.
The left column shows results for $\furtherBound$ and the right column shows results for $\furtherBoundTwo$.
This layout is used in
cref:fig-y-mcycle-two-experts,fig-latent-mcycle-two-experts,fig-y-mcycle-three-experts,fig-latent-mcycle-three-experts.

# For each lower bound, cref:fig-y-mcycle-two-experts visualises the predictive mean (top row) and
# predictive density (bottom row) and compares them to a Sparse Variational Gaussian Process (SVGP).
# cref:fig-latent-mcycle-two-experts then visualises the posteriors over the latent variables associated with
# each model.

# for the tight bound $\tightBound$ (ref:fig-y-means-two-experts-tight)
# and the further bound $\furtherBound$ (ref:fig-y-means-two-experts-further) respectively.
cref:fig-y-means-mcycle-two-experts-tight,fig-y-means-mcycle-two-experts-further
compare the posterior means (black solid line) to the SVGP's posterior mean (red dashed line) and
cref:fig-y-samples-mcycle-two-experts-tight,fig-y-samples-mcycle-two-experts-further
compare the posterior densities to the SVGP.
The red lines show plus or minus two standard deviations of the SVGP's posterior variance.
As the MoSVGPE posterior is a Gaussian mixture, it is visualised by drawing samples
from its posterior, i.e. sample a mode indicator variable $\modeVar_*$ and then draw a sample
form the corresponding expert.
The colour of the sample indicates the underlying expert, where
cyan represents expert one $\ModeInd=1$ and magenta represents expert two $\ModeInd=2$.

*Predictive posteriors* Both MoSVGPE results are capable of modelling the non-stationarity
at $x \approx -0.7$ better than the sparse variational Gaussian process (SVGP).
\todo{quantify this with local RMSE?}
At this non-stationary point there are clearly two modes in the MoSVGPE predictive distributions,
indicated by the overlap in samples from each expert
(in cref:fig-y-samples-mcycle-two-experts-tight,fig-y-samples-mcycle-two-experts-further).
It is clear that the SVGP has explained the observations by increasing its single
noise variance term. In contrast, both of the MoSVGPE results have been able to learn
two noise variances and these reflect the noise in the observations much better.
This is indicated by expert one learning a low noise variance and expert two a high noise
variance (similar to the SVGP's noise variance).
\todo{Add values for noise variances}

*Latent variables* More insight into this behaviour can be obtained by considering the latent variables.
Figure ref:fig-latent-mcycle-two-experts shows the posteriors over the latent variables where
cref:fig-expert-gps-mcycle-two-experts-tight,fig-expert-gps-mcycle-two-experts-further
show the GP posteriors over each expert's latent function $q(\mode{\latentFunc}(\x_*))$.
cref:fig-gating-gps-mcycle-two-experts-tight,fig-gating-gps-mcycle-two-experts-further
show the GP posteriors over the latent gating functions $q(\mode{\gatingFunc}(\x_*))$
and cref:fig-mixing-probs-mcycle-two-experts-tight,fig-mixing-probs-mcycle-two-experts-further
show the mixing probabilities associated with the probability mass function over the expert
indicator variable $\modeVar$.
# These figures highlight differences between the two lower bounds,
# in particular, how they represent the uncertainty in the gating network.
# As our variational inference scheme couples the learning of the experts and the gating network
# it is interesting to see their impact on the posteriors over the latent variables.

The lengthscale of the gating network kernel governs how fast the model can shift responsibility from
expert one to expert two.
For both lower bounds
the distribution over the expert indicator variable tends to a uniform distribution (maximum entropy)
at $x \geq 1.5$.
Optimising with both bounds resulted in expert one learning a
long lengthscale to fit the horizontal line from $-2$ to $-1$ and
expert two learning a shorter lengthscale function to fit the wiggly section from $-0.5$ to $1.2$.
The noise variance inferred by expert one is larger for $\furtherBoundTwo$ than for $\furtherBound$.
The uncertainty in the experts' latent functions is also higher for $\furtherBoundTwo$.
This is because $\furtherBoundTwo$ is attempting to fit both experts to the entire data set and only
mixes their noise models.
In contrast, $\furtherBound$ fits each expert only in the regions where the gating network has
assigned it responsibility.

# This is in contrast to the tight lower bound $\tightBound$, whose distribution over the expert indicator variable
# assigns responsibility to expert two and represents the uncertainty in the
# GP posterior over the gating functions, instead of the posterior over the mode indicator variable.

# \todo{to make this point I need to need to train bounds with some missing data and see what happens?}
# The gating network serves as a probabilistic decision boundary that is dependent on both the mean and
# the variance of the GPs over the gating functions $\gatingFunc$.
# As such, the model loses a degree of freedom, w.r.t. interpretability, making it difficult to
# interpret the meaning of $\Pr(\alpha_*=k \mid \mathbf{x}_*, \mathcal{D}, \bm\phi)= 0.5$.
# It could mean that it is confident that the observations were generated by a 50:50 mixture of the two experts.
# Alternatively, the variance of $p(h_*  \mid  \mathbf{x}_*, \mathcal{D}, \bm\phi)$
# may be high indicating that the gating function can not be confident making a prediction at $\mathbf{x}_*$.
# It could also indicate that the model is confident that neither expert explains the data well
# and so the optimisation has set the mixing probability to $0.5$ to prevent either experts
# "fit" degrading.
# For example, if the model needs a third expert.


# This is due to the gating network sharing the responsibility at $x \geq 1.5$ and expert one increasing its
# noise variance to help explain away the data.

**** three expert y fig :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/y_means.pdf}
\subcaption{}
\label{fig-y-means-three-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L3/y_means.pdf}
\subcaption{}
\label{fig-y-means-three-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/y_samples.pdf}
\subcaption{}
\label{fig-y-samples-mcycle-three-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L3/y_samples.pdf}
\subcaption{}
\label{fig-y-samples-mcycle-three-experts-further}
\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L1/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-two-experts}
%\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-three-experts}
%\end{minipage}
\caption{Visualisation of the model instantiated with $\ModeInd=3$ experts, after training on the motorcycle data set \citep{Silverman1985} with $\furtherBound$ (left column) and with $\furtherBoundTwo$ (right column).  \subref{fig-y-means-three-experts-tight}-\subref{fig-y-means-three-experts-further} show the data set (black crosses) and the posterior means associated with the MoSVGPE (black solid line) and a SVGP (red dashed line) for comparison. \subref{fig-y-samples-mcycle-three-experts-tight}-\subref{fig-y-samples-mcycle-three-experts-further} show samples from the MoSVGPE posterior where colour indicates the underlying expert (cyan $K=1$, magenta $\ModeInd=2$, yellow $\ModeInd=3$) and the red lines show the $\pm 2$ standard deviation error ($95\%$ confidence interval) of the SVGP posterior. All experiments were trained with $25,000$ iterations of Adam \citep{kingmaAdam2017} using a learning rate of $0.01$ and a minibatch size of $\NumData_b=16$. All GPs use Squared Exponential kernels and $M=32$ inducing points.}
\label{fig-y-mcycle-three-experts}
\end{figure}
#+END_EXPORT


# #+BEGIN_EXPORT latex
# \begin{figure}[t!]
# \centering
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/two-experts/y_samples.pdf}
# \subcaption{}
# \label{fig-y-samples-mcycle-two-experts}
# \end{minipage}
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/three-experts/y_samples.pdf}
# \subcaption{}
# \label{fig-y-samples-mcycle-three-experts}
# \end{minipage}
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/two-experts/y_samples_density.pdf}
# \subcaption{}
# \label{fig-y-samples-mcycle-two-experts}
# \end{minipage}
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/three-experts/y_samples_density.pdf}
# \subcaption{}
# \label{fig-y-samples-mcycle-three-experts}
# \end{minipage}
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/two-experts/y_means.pdf}
# %\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
# \subcaption{}
# \label{fig-gating-gps-mcycle-two-experts}
# \end{minipage}
# \begin{minipage}[r]{0.49\textwidth}
# \includegraphics[width=\textwidth]{./images/model/mcycle/three-experts/y_means.pdf}
# %\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
# \subcaption{}
# \label{fig-gating-gps-mcycle-three-experts}
# \end{minipage}
# \caption{Visualisation of the model after training on the motorcycle data set with two experts (left column) and  three experts (right column). (a-b) show the GP posteriros associated with the gating funcions, $q(h_{k}(\state_*))$, where the solind lines show the means $\E[h_{k}(\state_*)]$ and the shaded regions show the 95\% confidence interval, i.e. $\pm2\simga$. (c-d) show the predictive mixing probabilities $\predictiveProb$.}
# \label{fig-gating-network-mcycle-subset}
# \end{figure}
# #+END_EXPORT

**** three expert latent fig :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{./images/model/mcycle/K=3_L2/experts_f.pdf}
\subcaption{}
\label{fig-expert-gps-mcycle-three-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{./images/model/mcycle/K=3_L3/experts_f.pdf}
\subcaption{}
\label{fig-expert-gps-mcycle-three-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{./images/model/mcycle/K=3_L2/gating_gps.pdf}
\subcaption{}
\label{fig-gating-gps-mcycle-three-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{./images/model/mcycle/K=3_L3/gating_gps.pdf}
\subcaption{}
\label{fig-gating-gps-mcycle-three-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{./images/model/mcycle/K=3_L2/gating_mixing_probs.pdf}
\subcaption{}
\label{fig-mixing-probs-mcycle-three-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{./images/model/mcycle/K=3_L3/gating_mixing_probs.pdf}
%\subcaption{Gating network's predictive mixing probabilities $\predictiveProb$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-mixing-probs-mcycle-three-experts-further}
\end{minipage}
\caption{Visualisation of the model's latent variables (instantiated with $\ModeInd=3$ experts), after training on the
Motorcycle data set \citep{Silverman1985}, with $\furtherBound$ (left column)
and with $\furtherBoundTwo$ (right column).
(\subref{fig-expert-gps-mcycle-three-experts-tight}-\subref{fig-expert-gps-mcycle-three-experts-further}) show the GP
posteriors associated with the experts' latent functions $q(\mode{\latentFunc}(\state_*))$,
where the solid lines show the mean and the shaded regions show the $95\%$ confidence intervals, i.e. $\pm2\sigma$.
The gating network's GP posteriors $q(h_{k}(\state_*))$ are shown in
(\subref{fig-gating-gps-mcycle-three-experts-tight}-\subref{fig-gating-gps-mcycle-three-experts-further}) and
their associated mixing probabilities $\approxPredictiveProb$ in
(\subref{fig-mixing-probs-mcycle-three-experts-tight}-\subref{fig-mixing-probs-mcycle-three-experts-further}).}
\label{fig-latent-mcycle-three-experts}
\end{figure}
#+END_EXPORT

**** Three Experts
# Given that the two lower bounds lead to different ways for explaining the data
# at $x \geq 1.5$, it is interesting to consider adding a third expert.
The model was then instantiated with three experts $\ModeInd=3$ and trained
following the same procedure as the two exerts experiments,
i.e. same initial model and training parameters.
The results are show in cref:fig-y-mcycle-three-experts,
where the top row visualises the predictive mean and
the bottom row the predictive density, for $\furtherBound$ (left column) and $\furtherBoundTwo$ (right column).
The colour of the samples in cref:fig-y-samples-mcycle-three-experts-tight,fig-y-samples-mcycle-three-experts-further
indicates the underlying expert, where
cyan represents expert one $\modeInd=1$, magenta represents expert two $\modeInd=2$ and yellow represents
expert three $\modeInd=3$.
cref:fig-latent-mcycle-three-experts then visualises the posteriors over the latent variables associated with
each model/bound combination.

From cref:tab-mcycle-metrics, it is clear that the predictive posterior associated with $\furtherBound$
is the most accurate.
As expected, the two lower bounds explain the data completely differently.
Instantiating the model with three experts $\ModeInd=3$ and training with $\furtherBound$,
leads to the third expert fitting to the data at $x \geq 1.5$ and the gating network assigning responsibility to it
in this region.
In contrast, instantiating the model with three experts $\ModeInd=3$ and training with
$\furtherBoundTwo$, results in the gating network "turning off" the third expert at $x \geq 1.5$.
Similar to the two expert case, the distribution over the expert indicator variable at $x \geq 1.5$
tends to a uniform distribution (maximum entropy).

In cref:fig-expert-gps-mcycle-three-experts-tight the third expert's posterior clearly returns to the prior at
$x \geq 1.5$.
This demonstrates that not only is the gating network turning the experts "on" and "off" in different regions
but the model is also exhibiting data assignment behaviour.
That is, each expert appears to only be fitting to the observations in the regions where the gating network
has assigned it responsibility.
In our case, this behaviour is achieved via the inducing variables capturing the joint distribution over the
experts and the set of assignments, i.e. implicitly assigning data points to experts.

# This demonstrates the flexibility of MoSVGPE to assign data points "softly" via the inducing variables.

**** Batch Size vs Number of Inducing Points
# This section provides a brief evaluation on the effect of the number of inducing points and batch size.

*Batch Size*
One of the main benefits of the variational inference scheme presented in this chapter, is that the bound can be
calculated with minibatches of a data set $\dataset$ and used as the objective for stochastic gradient descent.
Decreasing the batch size increases the stochasticity in the bound and leads to convergence in less evaluations of
the ELBO.
This is evident in cref:fig-mcycle-training-curve, which compares the negative ELBO for different
numbers of inducing points and batch sizes.
Further to this, computing the ELBO is less computationally demanding for smaller batch sizes.
However, if the batch size is made too small, then the optimisation can become unstable and prevent the optimiser
from finding a good solution.
The (blue) learning curve for batch size $N_b=32$ in cref:fig-mcycle-training-curve-133 shows an example of this behaviour.
In this case, the learning rate had to be made smaller, leading to slower convergence.
This is shown by the orange learning curve, which has not able to reach the same negative ELBO as the other batch
sizes.
This is most likely due to the lower learning rate.
This interplay between the batch size and learning rate is well known in machine learning.
# Setting the batch size and learning rate is known to be awkward due to their interplay.

*Number of Inducing Points*
Our variational inference scheme models the joint distribution over the data and assignments via the inducing variables
($\expertsInducingOutput$ and $\gatingsInducingOutput$).
In practice, the number of inducing points should be less than the number
of data points $(\NumInducing \ll \NumData)$, to obtain improved computational performance.
The learning curves in cref:fig-mcycle-training-curve visualise the learning process for different numbers
of inducing points $\NumInducing$.
Best performance is obtained when the model is instantiated with $\NumInducing=133$ inducing inputs
i.e. a one-to-one correspondence between inducing inputs and
data inputs, $\expertInducingInput=\gatingInducingInput=\allInput$.
As the number of inducing points decreases the model is still able to recover the same negative ELBO.

\todo{add results for num inducing points leading to worse performance e.g. M=8}

*Evidence LOwer Bounds*
The tight lower bound $\tightBound$ and further lower bound $\furtherBound$ recovered similar results in
all experiments.
This indicates that $\furtherBound$ does not loosen the bound to a point where is loses valuable information.
In contrast, $\furtherBoundTwo$ is not able to recover the same results.
This was expected as $\furtherBoundTwo$ corresponds to a further likelihood approximation, where
the experts' noise models are mixed instead of their full SVGPs.
$\furtherBound$ offers a rich ELBO for optimising MoSVGPE that achieves similar results to $\tightBound$
whilst having lower computational complexity per evaluation.
For this reason, the remainder of this dissertation uses $\furtherBound$ for all experiments.

**** two vs three expert y fig :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_means.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-y-means-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/y_means.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-y-means-mcycle-two-experts-further}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_samples.pdf}
\subcaption{}
\label{fig-y-samples-mcycle-two-experts-tight}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/y_samples.pdf}
\subcaption{}
\label{fig-y-samples-mcycle-two-experts-further}
\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L1/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-two-experts}
%\end{minipage}
%\begin{minipage}[r]{0.49\textwidth}
%\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/y_samples_density.pdf}
%\subcaption{}
%\label{fig-y-samples-mcycle-three-experts}
%\end{minipage}
\caption{Visualisation of the model instantiated with $\ModeInd=2$ experts, after training on the motorcycle data set \citep{Silverman1985} with $\tightBound$ (left column) and with $\furtherBound$ (right column).  \subref{fig-y-means-mcycle-two-experts-tight}-\subref{fig-y-means-mcycle-two-experts-further} show the data set (black crosses) and the posterior means associated with the MoSVGPE (black solid line) and a SVGP (red dashed line) for comparison. \subref{fig-y-samples-mcycle-two-experts-tight}-\subref{fig-y-samples-mcycle-two-experts-further} show samples from the MoSVGPE posterior where colour indicates the underlying expert (cyan $K=1$, magenta $\ModeInd=2$) and the red lines show the $\pm 2$ standard deviation error ($95\%$ confidence interval) of the SVGP posterior. All experiments were trained with $25,000$ iterations of Adam \citep{kingmaAdam2017} using a learning rate of $0.01$ and a minibatch size of $\NumData_b=16$. All GPs use Squared Exponential kernels and $M=32$ inducing points.}
\label{fig-y-mcycle-two-vs-three-experts}
\end{figure}
#+END_EXPORT

# #+caption: \textbf{Motorcycle data} The left plot shows the observations (red crosses) together with the means of our model (black solid line) and a SVGP with noise (blue dashed line). It also shows probabilities assocaited with the gating network for $k=1$ (green line) and $k=2$ (blue line) on the right axis. The right plot shows 100 samples (blue) from the predictive posterior at 200 evenly spaced input locations as well as the $\pm 2$ std error (95% confidence interval) of the SVGP (solid red line). Both models use squared exponential covariance functions for all GPs.

**** two vs three expert latent fig :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/experts_f.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-gating-gps-mcycle-two-experts}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/experts_f.pdf}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-gating-gps-mcycle-three-experts}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/gating_gps.pdf}
%\subcaption{Gating function posteriors $\predictiveGatingPrior$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-gating-gps-mcycle-two-experts}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/gating_gps.pdf}
%\subcaption{Gating function posteriors $\predictiveGatingPrior$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-gating-gps-mcycle-three-experts}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=2_L2/gating_mixing_probs.pdf}
%\subcaption{Gating network's predictive mixing probabilities $\predictiveProb$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-gating-mixing-probs-mcycle-two-experts}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/mcycle/K=3_L2/gating_mixing_probs.pdf}
%\subcaption{Gating network's predictive mixing probabilities $\predictiveProb$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-gating-mixing-probs-mycle-three-experts}
\end{minipage}
\caption{Visualisation of the model after training on the motorcycle data set with two experts (left column) and  three experts (right column). (a-b) show the GP posteriros associated with the gating funcions, $q(h_{k}(\state_*))$, where the solind lines show the means $\E[h_{k}(\state_*)]$ and the shaded regions show the 95\% confidence interval, i.e. $\pm2\simga$. (c-d) show the predictive mixing probabilities $\predictiveProb$.}
\label{fig-gating-network-mcycle-subset}
\end{figure}
#+END_EXPORT

**** training loss fig :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t!]
\centering
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/training_curves/training_loss_num_ind_133.png}
\subcaption{}
\label{fig-mcycle-training-curve-133}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/training_curves/training_loss_num_ind_64.png}
\subcaption{}
\label{fig-mcycle-training-curve-64}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/training_curves/training_loss_num_ind_32.png}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=2$.}
\subcaption{}
\label{fig-mcycle-training-curve-32}
\end{minipage}
\begin{minipage}[r]{0.49\textwidth}
\includegraphics[width=\textwidth]{./images/model/mcycle/training_curves/training_loss_num_ind_16.png}
%\subcaption{Expert's latent function posteriors $\predictiveExpertPrior$ with $\ModeInd=3$.}
\subcaption{}
\label{fig-mcycle-training-curve-16}
\end{minipage}
\caption{Training curves showing the negative ELBO (- $\furtherBound$) vs step when training
MoSVGPE (instantiated with two experts $\ModeInd=2$), on the
Motorcycle data set \citep{Silverman1985} with different numbers of inducing points $M$ and different
batch sizes $\NumData_b$. All experiments used the Adam optimiser \cite{kingmaAdam2017} with a learning rate
of $0.01$ and Squared Exponential kernels for all GPs.}
\label{fig-mcycle-training-curve}
\end{figure}
#+END_EXPORT

**** End :ignore:
\newpage

*** Evaluation on Velocity Controlled Quadcopter label:sec-brl-experiment
**** intro :ignore:
# The goal of this dissertation is to control a DJI Tello quadcopter in an indoor environment
# subject to two modes of operation characterised by turbulence.
In order to verify that MoSVGPE works on real world systems,
it was tested on a real-world quadcopter data set following the illustrative example detailed
in cref:illustrative_example.
The data set was collected at the Bristol Robotics Laboratory using
a velocity controlled DJI Tello quadcopter and a Vicon tracking system.
A high turbulence dynamics mode was induced by placing a desktop fan at the right side of a room.
Figure ref:fig-quadcopter-environment shows a diagram of the environment.
The data set represents samples from a dynamical system with constant controls, i.e.
$\Delta\state_{\timeInd} = \latentFunc(\state_{\timeInd-1};\control_{\timeInd-1}=\control_*)$.
The resulting data set has two-dimensional inputs and two-dimensional outputs, making it easy to visualise
the different components of the model.
#+BEGIN_EXPORT latex
\begin{figure}[h]
\centering
\begin{minipage}[r]{0.55\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/brl-quadcopter-domain-figure.png}
\subcaption{Diagram showing a top down view of the environment (a room in the Bristol Robotics Laboratory).}
\label{fig-quadcopter-environment}
\todo{add better diagram of environment}
\end{minipage}
\begin{minipage}[r]{0.44\columnwidth}
%\includegraphics[width=\textwidth]{./images/quiver_step_20_direction_down.png}
\includegraphics[width=\textwidth]{./images/model/quadcopter/subset-10/dataset_quiver.pdf}
\subcaption{Quiver plot showing the data set of state transitions from 9 trajectories flown 7 times.}
\label{fig-quiver}
\end{minipage}
\caption{Illustration of \subref{fig-quadcopter-environment} the environment (a room in the Bristol Robotics Laboratory) and \subref{fig-quiver} the data set of state transitions. A turbulent dynamics mode is induced by a desk top fan at the right hand side of the room and a subset of the enivornment has not been observed.}
\end{figure}
#+END_EXPORT



*Environment* The environment is assumed to have two dimensions
(the $x$ and $y$ coordinates), which is a realistic assumption,
as altitude control can be achieved with a separate controller.
The state space is then the 2D coordinates $\state = [x, y]$ and the control is simply the velocity
$\control = [\dot{x}, \dot{y}]$.

*Data Collection* The Vicon system provided access to the true position of the quadcopter at all times, which
enabled pre-planned trajectories to be flown, using a simple PID controller on
feedback from the Vicon system.
To simplify data collection,
nine trajectories from $y=2$ to $y=-3$, with different initial $x$ locations,
were used as target trajectories to be tracked by the PID controller.
Each trajectory was repeated 7 times to capture the variability (process noise) in the dynamics.
# To simplify data collection and visualisation, the controls were kept constant during data
# collection, reducing the dynamics to
# ${\Delta \state_\numData= f(\state_{\numData-1}; \control=\control_{\text{fixed}}) + \epsilon}$.

# To simplify data collection and visualisation, the controls were kept constant during data
# collection, reducing the dynamics to
# ${\Delta \state_\numData= f(\state_{\numData-1}; \control=\control_{\text{fixed}}) + \epsilon}$.
# Data from multiple trajectories following the same target trajectory were collected to
# capture the variability (process noise) in the dynamics.

# Nine trajectories (with different starting states but the same target velocity) were used as
# target trajectories
# and repeated
# multiple times
# from multiple trajectories following the same target trajectory were collected to
# capture the variability (process noise) in the dynamics.

*Data Processing* The Vicon stream recorded data at 100Hz, which was then processed to
give a time step $\Delta t$ of $0.1s$.
This reduced the size of the data set and left reasonable lengthscales.
The data set consists of $\NumData=2081$ state transitions.
\todo{update data set size}
Figure ref:fig-quiver is a quiver plot showing the resulting data set.

**** Results
The model was instantiated with two experts, with the goal of each expert learning a separate dynamics mode and the
gating network learning a representation of how the underlying dynamics modes vary over the state space.
The model was trained using the model and training parameters shown in Table ref:tab-params-quadcopter.

# #+NAME: fig-gating-mixing-probs-quadcopter-subset
# #+ATTR_LATEX: :width 1\textwidth :placement [hbt!] :center t
# #+caption: Gating network's predictive mixing probabilties $\predictiveProb$ after training on the quadcopter data set.
# [[file:./images/model/quadcopter/subset/gating_mixing_probs.pdf]]
# #+NAME: fig-gating-gps-quadcopter-subset
# #+ATTR_LATEX: :width 1\textwidth :placement [hbt!] :center t
# #+caption: Gating network's predictive posterior $\predictiveGatingPrior$ after training on the quadcopter data set. Each row corresponds to a single GP posterior, $q(h_{k}(\state_*))$, corresponding to expert $k$. The mean $\E[h_{k}(\state_*)]$ is on the left and the variance $\V[h_{k}(\x_*)]$ is on the right.
# [[file:./images/model/quadcopter/subset/gating_gps.pdf]]

#+NAME: fig-y-mm-quadcopter-subset
#+ATTR_LATEX: :width 1\textwidth :placement [hbt!] :center t
#+caption: Moment matched predictive posterior $p(\Delta \state_* \mid \x_*)$ after training on the quadcopter data set. Each row corresponds to an output dimension $d$ where the left plot shows the moment matched mean $\E[\Delta \state_d]$ and the right plot shows the moment matched variance $\V[\Delta \state_d]$.
[[file:./images/model/quadcopter/subset-10/y_moment_matched.pdf]]

#+NAME: fig-experts-f-quadcopter-subset
#+ATTR_LATEX: :width 1\textwidth :placement [hbt!] :center t
#+caption: Visualisation of the experts' predictive posteriors $\predictiveExpertsPrior$ after training on the quadcopter data set. Each row corresponds to a single GP posterior, $q(f_{kd}(\singleTestInput))$, corresponding to dimension $d$ of expert $\modeInd$. The mean $\E[f_{kd}(\singleTestInput)]$ is on the left and the variance $\V[f_{kd}(\x_*)]$ is on the right. The noise variances learned by Expert 1 and Expert 2 were $\Sigma_1 = \diag\left([0.259,0.386]]\right)$ and $\Sigma_2 = \diag\left([0.869, 1.243]\right)$.
[[file:./images/model/quadcopter/subset-10/experts_f.pdf]]

#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{1.0\textwidth}
\centering
\includegraphics[width=\textwidth]{./images/model/quadcopter/subset-10/gating_mixing_probs.pdf}
\subcaption{Gating network's predictive mixing probabilties $\predictiveProb$.}
\label{fig-gating-mixing-probs-quadcopter-subset}
\end{minipage}
\begin{minipage}[r]{1.0\textwidth}
\includegraphics[width=\textwidth]{./images/model/quadcopter/subset-10/gating_gps.pdf}
\subcaption{Gating network's predictive posterior $\predictiveGatingPrior$ where each row corresponds to a single GP posterior, $q(h_{k}(\singleTestInput))$, corresponding to expert $k$. The left hand plots show the means $\E[h_{k}(\singleTestInput)]$ and the right hand plots show the variances $\V[h_{k}(\singleTestInput)]$.}
\label{fig-gating-gps-quadcopter-subset}
\end{minipage}
\caption{Visualisation of the gating network after training on the quadcopter data set. The plots in a) show the predictive mixing probabilities $\predictiveProb$ for Expert 1 (left) and Expert 2 (right). The plots in b) show the predictive GP posteriors $q(h_{k}(\singleTestInput))$ associated with Expert 1 (top) and Expert 2 (bottom).}
\label{fig-gating-network-quadcopter-subset}
\end{figure}
#+END_EXPORT

# #+NAME: fig-experts-y-quadcopter-subset
# #+ATTR_LATEX: :width 1\textwidth :placement [hbt!] :center t
# #+caption: Experts' predictive posterior over the output $q(\Delta\state_{kd} \mid \state_*)$ after training on the quadcopter data set. Each row corresponds to a single GP posterior, $q(\Delta\state_{kd} \mid \state_*)$, corresponding to dimension $d$ of expert $k$. The mean $\E[\Delta\state_{kd}\mid \state_*]$ is on the left and the variance $\V[\Delta\state_{kd} \mid \x_*]$ is on the right.
# [[file:./images/model/quadcopter/subset/experts_y.pdf]]



At a new input location $\singleTestInput$ the density over the output,
$p(\singleTestOutput \mid \singleTestInput)$,
follows a mixture of $\ModeInd$ Gaussians.
Visualising a mixture of two Gaussians with a two-dimensional input space and a two-dimensional output space
requires the components and mixing probabilities to be visualised separately.
To aid with visualisation, Figure ref:fig-y-mm-quadcopter-subset shows the predictive density
approximated as a unimodal Gaussian density (via moment matching), where each row corresponds to an
output dimension.
The predictive mean is fairly constant over the domain, except for the region in front of the fan, where it is
higher.
This result makes sense as the data set was assumed to be collected with constant controls.
The region with high predictive mean in front of the fan, is modelling the drift arising from the fan blowing the quadrotor in the negative $x$ direction.
The right hand plots of Figure ref:fig-y-mm-quadcopter-subset
show the predictive variance. It is high where there are no training observations,
indicating that the method has successfully represented the model's /epistemic uncertainty/.
It is also high in the region in front of the fan, showing that the model has successfully inferred
the high process noise, associated with the turbulence induced by the fan.
# As the data set was assumed to have constant controls, this result aligns with our knowledge of the environment.
# That is, the dynamics should be constant
# dynamics should be constant

*Experts* Let us now visualise the individual experts and the gating network separately.
Figure ref:fig-experts-f-quadcopter-subset shows the predictive posteriors $q(\latentFunc_{kd}(\singleTestInput))$
associated with each dimension $d$ of each expert $\modeInd$.
The method has successfully learned a factorised representation of the underlying dynamics, where
expert 1 has learned a dynamics mode with low process noise
$\Sigma_1 = \diag\left([0.0063, 0.0259]]\right)$
and expert 2 a mode with high process noise
$\Sigma_2 = \diag\left([0.0874, 0.0432]\right)$.
Expert 2 has also clearly learned the drift induced by the fan, indicated by the dark red region at $y=0$
in the two bottom left plots of Figure ref:fig-experts-f-quadcopter-subset.
It has also learned the control response of the PID controller correcting for the deviation
from the reference trajectory, indicated by the white region below $y=0$.
The control response is an artifact of the data collection process.
It is clear that expert 2 has learned both the drift and process noise terms
associated with the turbulent dynamics mode.

Both experts were initialised with independent inducing inputs, $\expertInducingInput$, providing the model
flexibility to "soft" partition the data set.
That is, each expert has the freedom to set its inducing inputs, $\expertInducingInput$,
to support only a subset of the data set.
The posterior (co)variance associated with each expert represents their /epistemic uncertainty/.
The top right plot in Figure ref:fig-experts-f-quadcopter-subset shows the posterior variance associated with
the $x$ dimension of expert 1.
The posterior variance is high in front of the fan because the gating network has assigned responsibility to the
other expert in this region.
It is also high in the region where the model has had no training observations, as should be
expected.
However, the posterior variance associated with the $y$ dimension of expert 1, is not high in this region.
This is due to the lengthscale of the second output dimension allowing expert 1 to confidently extrapolate.
\todo{is this because of the lengthscale or is it due to gating network}

The bottom right two plots in Figure ref:fig-experts-f-quadcopter-subset show the posterior variance
associated with the $x$ and $y$ dimensions of expert 2.
The posterior variance is high everywhere except for the region in front of the fan.
Again, this is due to the gating network assigning responsibility to the other expert outside of the region in
front of the fan.
It is clear from these results that our likelihood approximation, combined with our gating network
and variational inference, are capable of modelling the assignment of observations to experts via
the inducing points.

# *Gating Network*
# Figure ref:fig-gating-network-quadcopter-subset shows the gating network after training on the
# data set.
# Figure ref:fig-gating-mixing-probs-quadcopter-subset indicates that the model has assigned responsibility
# to expert 2 in front of the fan as its mixing probability
# $\Pr(\singleTestModeVar=1 \mid \singleTestInput)$ is high in front of the fan, i.e. the high-turbulence region.
# The mixing probabilities tend to a uniform distribution
# ${\Pr(\singleTestModeVarK \mid \singleTestInput)=0.5}$
# in the region with no training observations.
# This corresponds to maximum entropy for categorical distributions and is a desirable behaviour.
# However, it is worth noting that the mixing probabilities can tend to a uniform distribution for multiples reasons.
# It could mean that the model is confident that the observations are generated by an equal mixture of the experts
# i.e. the gating function(s) are all equal and their posterior variance(s) are low.
# Alternatively, it could mean that the model is not confident in predicting which expert is responsible at the given
# input location i.e. the variance(s) of the GP posterior(s) associated with the gating function(s) could be high.


# # It is worth noting that the mixing probabilities lose a degree of freedom which makes it difficult to
# # interpret the meaning of $\Pr(\modeVar_*=\modeInd \mid \state_*= 0.5$.
# # It could mean that it is confident that the observations were generated by a 50:50 mixture of the two experts
# # i.e. the gating functions are all equal to zero and the posterior variance is low.
# # Alternatively, the variance of the gating function GP posterior could be high, resulting in the probability tending to
# # maximum entropy.

# Figure ref:fig-gating-gps-quadcopter-subset shows the GP posteriors associated with the gating functions.
# The mean of the gating function associated with expert 1 $\E[h_{1}(\singleTestInput)]$ is
# high in the low-turbulence regions and low in front of the fan, i.e. the high-turbulence mode.
# The mean also tends to zero where the model has had no training observations.
# The posterior variance is also high in this region, indicating that the gating
# network GPs have successfully modelled the /epistemic uncertainty/.
# Exploiting a GP-based gating network has clearly provided a principled approach to distinguishing
# between the different reasons the mixing probabilities may tend to a uniform distribution.



*Gating Network*
Figure ref:fig-gating-network-quadcopter-subset shows the gating network after training on the
data set.
Figure ref:fig-gating-mixing-probs-quadcopter-subset indicates that the model has assigned responsibility
to expert 2 in front of the fan, as its mixing probability
$\Pr(\singleTestModeVar=1 \mid \singleTestInput)$ is high in this region.
Figure ref:fig-gating-gps-quadcopter-subset shows the GP posteriors associated with the gating functions.
The mean of the gating function associated with expert 1 $\E[h_{1}(\singleTestInput)]$ is
high in the low-turbulence regions and low in high-turbulence region in front of the fan.
These results clearly demonstrate that the gating network infers important information regarding how the
system switches between dynamics modes over the input space.
# The mean also tends to zero where the model has had no training observations.

*Identifiability*
These results clearly demonstrate that our gating network is capable of turning a single expert
on in multiple regions of the input space.
This is desirable behaviour as it has enabled only two underlying dynamics modes to be identified.
In contrast, other MoGPE models may have assigned an extra expert to one of the regions modelled by expert one.
In particular, the regions at $y>0$ and $y<-1$ may have been assigned to separate experts.

*Latent Spaces for Control*
The gating network consists of two spaces which are rich with information regarding how the
system switches between dynamics modes.
Firstly, the pmf over the expert indicator variable.
Secondly, the GP posteriors over the gating functions.
It is worth noting that 1) all MoGPE methods obtain a pmf over the expert indicator variable and 2)
this space suffers from interpretability issues.
Consider the meaning of the mixing probabilities tending to a uniform
distribution ${\Pr(\singleTestModeVarK \mid \singleTestInput)=0.5}$.
This corresponds to maximum entropy for a categorical distribution and could mean two different things.
It could mean that,
1) the model has training data in this region, so can confidently predict, but is unsure which expert is responsible,
   - perhaps the observations do not belong to any expert and an extra expert is required,
2) the model does not have training data in this region, so cannot confidently predict which expert is responsible.
# 2) the model has training data in this region, so can confidently predict and is confident that the observations are generated by an equal mixture of the experts,
# It could mean that the model is confident that the observations are generated by an equal mixture of the experts,
# or it could mean that the gating network is not confident making predictions in this region.
This interpretability issue is overcome by our GP-based gating network, as these two cases are modelled differently.
Either the gating function(s) are all equal and their posterior variance(s) are low, implying that the gating network
can predict confidently but is unsure which expert is responsible.
This could mean that an extra expert is responsible for predicting at this location.
Alternatively, the gating functions' posterior variance(s) could be high, implying
that the model is not confident in predicting which expert is responsible at the given input location.
Importantly, the GP posteriors associated with our gating network, not only infer information regarding the
mode switching, but also model the gating network's /epistemic uncertainty/.
These GP posteriors provide convenient latent spaces for control and
are exploited by the trajectory optimisation algorithms presented in the remainder of this dissertation.


# The posterior variance is high where the model has not observed
# the system, indicating that the gating network GPs successfully infer the epistemic uncertainty when using the
# inference scheme in Section ref:sec-inference.
# Formulating the gating network based on GPs has clearly provided a principled approach to distinguishing
# between the different reasons the mixing probabilities may tend to a uniform distribution.

# The predictive posterior is mixture of $\ModeInd$ Gaussian resulting from combining the experts according to
# the gating network.

*** Evaluation on Simulated Quadcopter Data Set :noexport:
#+BEGIN_EXPORT latex
\newcommand{\roll}{\ensuremath{\phi}}
\newcommand{\pitch}{\ensuremath{\theta}}
\newcommand{\yaw}{\ensuremath{\psi}}

\newcommand{\world}{\ensuremath{w}}
\newcommand{\body}{\ensuremath{b}}
\newcommand{\intermediate}{\ensuremath{c}}
\renewcommand{\frame}{\ensuremath{F}}
\newcommand{\worldFrame}{\ensuremath{\frame_\world}}
\newcommand{\bodyFrame}{\ensuremath{\frame_\body}}
\newcommand{\intermediateFrame}{\ensuremath{\frame_\intermediate}}

\newcommand{\inertiaMatrix}{\ensuremath{J}}
\newcommand{\inertiaZ}{\ensuremath{I_{zz}}}
\newcommand{\zw}{\ensuremath{\mathbf{z}_\world}}

\newcommand{\positions}{\ensuremath{\mathbf{h}}}
\newcommand{\velocity}{\ensuremath{v}}
\newcommand{\velocityx}{\ensuremath{\dot{x}}}
\newcommand{\velocityy}{\ensuremath{\dot{y}}}
\newcommand{\Velocity}{\ensuremath{\mathbf{\velocity}}}

\newcommand{\rotationMatrix}{\ensuremath{\mathbf{R}}}
\newcommand{\angularVelocity}{\ensuremath{\bm\omega}}
\newcommand{\angularVelocityMatrix}{\ensuremath{\bm\Omega}}

\renewcommand{\thrust}{\ensuremath{f_{x}}}
\renewcommand{\Thrust}{\ensuremath{\mathbf{f_{x}}}}
\renewcommand{\torque}{\ensuremath{\tau_z}}

\newcommand{\quadrotorDynamics}{\ensuremath{f_{\text{quad}}}}
#+END_EXPORT
The quadrotor frames and Euler angles (roll $\roll$, pitch $\pitch$ and yaw $\yaw$) are shown
in Figure ref:fig-quadcopter-frames.
The subscripts $\world$, $\body$ and $\intermediate$ are used to denote the world
$\worldFrame$, body $\bodyFrame$ and intermediate $\intermediateFrame$ (after yaw angle rotation) frames respectively.
The the 3D nonlinear quadrotor dynamics are described with the quadcopter model from cite:wangSafe2018,zhouVector2014.

The 2D nonlinear quadrotor dynamics are based on the
state vector is given by $\state = [x, y, \velocityx, \velocityy, \yaw]$
where $\positions = [x, y]$ is the Euclidean coordinates relative to the world frame $\worldFrame$,
$\Velocity = [\velocityx, \velocityy]$ is the velocity in the world frame $\worldFrame$ and
$\yaw$ is the yaw angle, i.e. the angle around the $z$ axis.
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\dot{\state} &= \quadrotorDynamics(\state, \control)
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\rotationMatrix_{\body,\world} =
\begin{bmatrix}
\cos{\yaw} & -\sin(\yaw) \\
\sin{\yaw} & \cos(\yaw)
\end{bmatrix}
\end{align}
#+END_EXPORT
where,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\dot{\state}_{1:2} &= \dot{\positions} = \state_{3:4} \\
\dot{\state}_{3:4} &= \dot{\Velocity} = \frac{1}{m} \rotationMatrix \Thrust \\
\dot{\state}_{5} &= \dot{\yaw} = \frac{1}{\inertiaZ} \torque
\end{align}
#+END_EXPORT
where $\Thrust = [\thrust, 0]^T$ is the total thrust force in the quadrotors from the rotors and $\torque$ is the torque on the quadrotor
around the $z$ axis of the world frame $\worldFrame$.
The thrust and torque are realistic controls for a 2D quadrotor system and gives the
control vector \control = [\thrust, \torque].

#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\dot{\Velocity} &= g \zw + \frac{1}{m} \thrust \rotationMatrix \zw \\
\dot{\rotationMatrix} &= \rotationMatrix \angularVelocityMatrix \\
\dot{\angularVelocity} &= \inertiaMatrix^{-1} \torque - \inertiaMatrix^{-1} \angularVelocityMatrix \inertiaMatrix \angularVelocity \\
\dot{\positions} &= \velocity
\end{align}
#+END_EXPORT
where $g=9.81ms^{-2}$ is the acceleration due to gravity, $m$ is the mass, $\zw=[0,0,1]^T$,
$\positions=[x, y, z]^T$ is the position of the quadrotor in the world frame $\worldFrame$,
$\Velocity=[\velocity_x, \velocity_y, \velocity_z]^T$ is the
and velocity of the quadrotor in the world frame $\worldFrame$ respectively.
The angular velocity of the quadrotor in the body frame $\bodyFrame$ is denoted $\angularVelocity=[p, q ,r]^T$
and in tensor form $\angularVelocityMatrix=[[0,-p,q];[r,0,-p];[-q,p,0]]$

$\state=[x, y, z, \velocity_x, \velocity_y, \velocity_z, \roll, \pitch, \yaw, \dot{\roll}, \dot{\pitch}, \dot{\yaw}]$

$\state=[x, y, z, \velocity_x, \velocity_y, \velocity_z, \roll, \pitch, \yaw]^T$
$\state=[\ddot{x}, \ddot{y}, \ddot{z}, \dot{\roll}, \dot{\pitch}, \dot{\yaw}]^T$

\newpage
*** Point Mass 2D Dynamics :noexport:
#+BEGIN_EXPORT latex
\newcommand{\roll}{\ensuremath{\phi}}
\newcommand{\pitch}{\ensuremath{\theta}}
\newcommand{\yaw}{\ensuremath{\psi}}

\newcommand{\world}{\ensuremath{w}}
\newcommand{\body}{\ensuremath{b}}
\newcommand{\intermediate}{\ensuremath{c}}
\renewcommand{\frame}{\ensuremath{F}}
\newcommand{\worldFrame}{\ensuremath{\frame_\world}}
\newcommand{\bodyFrame}{\ensuremath{\frame_\body}}
\newcommand{\intermediateFrame}{\ensuremath{\frame_\intermediate}}

\newcommand{\inertiaMatrix}{\ensuremath{J}}
\newcommand{\inertiaZ}{\ensuremath{I_{zz}}}
\newcommand{\zw}{\ensuremath{\mathbf{z}_\world}}

\newcommand{\positions}{\ensuremath{\mathbf{h}}}
\newcommand{\velocity}{\ensuremath{v}}
\newcommand{\velocityx}{\ensuremath{\dot{x}}}
\newcommand{\velocityy}{\ensuremath{\dot{y}}}
\newcommand{\Velocity}{\ensuremath{\mathbf{\velocity}}}

\newcommand{\rotationMatrix}{\ensuremath{\mathbf{R}}}
\newcommand{\angularVelocity}{\ensuremath{\bm\omega}}
\newcommand{\angularVelocityMatrix}{\ensuremath{\bm\Omega}}

\renewcommand{\thrust}{\ensuremath{f_{x}}}
\renewcommand{\Thrust}{\ensuremath{\mathbf{f_{x}}}}
\renewcommand{\torque}{\ensuremath{\tau_z}}

\newcommand{\quadrotorDynamics}{\ensuremath{f_{\text{quad}}}}
#+END_EXPORT
cite:williamsAdvancing
cite:watsonStochastic2020
cite:watsonAdvancing2021

cite:levineVariational2013


cite:bhardwajDifferentiable2020

cite:mukadamContinuoustime2018

The dynamics of a point mass in 2D can represented with the
state vector $\state = [x, y, \velocityx, \velocityy, \yaw]$,
where $\positions = [x, y]$ denotes the 2D Euclidean coordinates relative to the world frame $\worldFrame$,
$\Velocity = [\velocityx, \velocityy]$ is the velocity in the world frame $\worldFrame$ and
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{tikzpicture}
% body frame
\draw[thick,->] (0,0) -- (1.5,1.5) node[anchor=south] {$x$};
\draw[thick,->] (0,0) -- (1.5,-1.5) node[anchor=north] {$y$};

% world frame
\draw[thick,->] (-3,-3) -- (3,-3) node[anchor=north west] {$x$};
\draw[thick,->] (-3,-3) -- (-3,3) node[anchor=south east] {$y$};
\foreach \x in {-3, -2, -1, 0,1,2,3}
   \draw (\x cm,-3) -- (\x cm,-3) node[anchor=north] {$\x$};
\foreach \y in {-3, -2, -1, 0,1,2,3}
    \draw (-3,\y cm) -- (-3,\y cm) node[anchor=east] {$\y$};
\end{tikzpicture}
\end{figure}
#+END_EXPORT


The point mass can apply a force along its $x$ axis (known as the thrust vector), which is denoted
$\Thrust = [\thrust, 0]^T$.
It can also rotate itself by applying a torque $\torque$ around its $z$ axis.
The resulting control vector for the 2D point mass is given by $\control = [\thrust, \torque]$.
The nonlinear dynamics of the system are given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\dot{\state} &=
\begin{bmatrix}
    \dot{\state}_{1} \\
    \dot{\state}_{2} \\
    \dot{\state}_{3} \\
    \dot{\state}_{4} \\
    \dot{\state}_{5}
\end{bmatrix}
=
\begin{bmatrix}
    \state_{3} \\
    \state_{4} \\
    \frac{1}{m} \thrust \cos{(\yaw)} \\
    \frac{1}{m} \thrust \sin{(\yaw)} \\
    \frac{1}{\inertiaZ} \torque
\end{bmatrix}
\end{align}
#+END_EXPORT
where $m$ is the mass and $\inertiaZ$ is the moment of inertia around the vertical $z$ axis.
Defining the rotation matrix from the body frame $\bodyFrame$ to the world frame $\worldFrame$ as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\rotationMatrix_{\body,\world} =
\begin{bmatrix}
\cos{\yaw} & -\sin(\yaw) \\
\sin{\yaw} & \cos(\yaw)
\end{bmatrix}
\end{align}
#+END_EXPORT
the dynamics can be calculated as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\dot{\state}_{1:2} &= \dot{\positions} = \state_{3:4} \\
\dot{\state}_{3:4} &= \dot{\Velocity} = \frac{1}{m} \rotationMatrix \Thrust \\
\dot{\state}_{5} &= \dot{\yaw} = \frac{1}{\inertiaZ} \torque
\end{align}
#+END_EXPORT

The unknown dynamics can be modelled by placing GP priors on the accelerations ($\dot{\dot{x}}, \dot{\dot{y}}$) and
the angular (yaw) velocity $\dot{\yaw}$,
#+BEGIN_EXPORT latex
\renewcommand{\input}{\ensuremath{\hat{\state}}}
\begin{align} \label{eq-quadrotor-dynamics}
\dot{\state} &=
\begin{bmatrix}
    \dot{\state}_{1} \\
    \dot{\state}_{2} \\
    \dot{\state}_{3} \\
    \dot{\state}_{4} \\
    \dot{\state}_{5}
\end{bmatrix}
=
\begin{bmatrix}
    \state_{3} \\
    \state_{4} \\
    \frac{1}{m} \thrust \cos{(\yaw)} + \mathcal{N} \left( \mu_3(\input),  k_3(\input, \input) \right) \\
    \frac{1}{m} \thrust \sin{(\yaw)} + \mathcal{N} \left( \mu_4(\input),  k_4(\input, \input) \right) \\
    \frac{1}{\inertiaZ} \torque + \mathcal{N} \left( \mu_5(\input),  k_5(\input, \input) \right)
\end{bmatrix}
\end{align}
#+END_EXPORT


\newpage
** Discussion

*Implicit Data Assignment*
It is worth noting that in contrast to other MoGPE models, this model does not directly assign observations to experts.
However, after augmenting each expert with separate inducing points,
the model has the flexibility to loosely /partition/ the data set.
Just as sparse GP methods can be viewed as methods that parameterise a full nonparametric GP,
our approach can be viewed as parameterising the nonparametric Mixture of Gaussian Process Experts.
Conveniently, our parameterisation, in particular the likelihood approximation in cref:eq-likelihood-approximation,
deals with the issue of marginalising exponentially many sets of assignments of observations to experts.
Evident from the results in this chapter, this likelihood approximation appears to retain important information
regarding the assigning of observations to experts, whilst efficiently marginalising the expert indicator variable.
It is also worth noting that the number of inducing points $\NumInducing$ associated with each expert,
could be set by considering the number of data points believed to belong to a particular expert.
Currently, each expert's inducing inputs are initialised by randomly sampling a subset of the data inputs.
Future work could explore different techniques for initialising each expert's inducing inputs.

*Full Bayesian Treatment of Inducing Inputs*
Common practice in sparse GP methods is to jointly optimise the hyperparameters and the inducing inputs.
Optimising only some of the parameters, instead of marginalising all of them, is known as Type-II maximum likelihood.
In Bayesian model selection, it is well known that Type-II maximum likelihood can lead to overfitting
if the number of parameters being optimised is large.
In the case of inducing inputs, there can often be beyond hundreds or thousands that need to be optimised.
Further to this, cite:rossiSparse2021 show that optimising the inducing inputs
relies on being able to optimise both the prior and the posterior, therefore contradicting Bayesian inference.
Our variational inference scheme follows common practice and optimises the inducing inputs
jointly with the hyperparameters.
In some instances, we observe that optimising the inducing inputs leads to them taking values far away from the
training data.
Often this can be avoided by simply sampling the inducing inputs from
the training inputs and fixing them, i.e. not optimising them.
This often leads to better NLPP scores as well.
This observation highlights that a full Bayesian treatment of the inducing inputs is an interesting direction for
future work.
However, specifying priors and performing /efficient/ posterior inference over the inducing inputs
is a challenging problem.

** Conclusion
This chapter has presented a method for learning representations of multimodal dynamical systems using
a Mixture of Gaussian Process Experts method.
Motivated by learning latent spaces for control
and ensuring that the true underlying dynamics modes are identified,
this work formulated a gating network based on input-dependent gating functions.
This gating network can be used to constrain the set of admissible functions through the placement of informative
GP priors on the gating functions.
This aids the inherent identifiability issues associated with mixture models.
As we shall see in cref:chap-traj-opt, the GP posteriors over the gating functions provide convenient
latent spaces for control, as they are rich with information regarding the separation of the underlying dynamics modes.

Further to this, this chapter addresses the issue of marginalising over
every possible set of assignments of observations to experts
-- of which there are $\ModeInd^{\NumData}$ possibilities
-- in the MoGPE marginal likelihood.
It overcomes the issue of assigning observations to experts by augmenting each expert GP
with a set of inducing points.
These inducing points are assumed to be a sufficient statistic for the joint distribution
over every possible set of assignments to experts.
This induces a factorisation over data which
is used to derive three variational lower bounds that provide a nice coupling between the
optimisation of the experts and the gating network, by efficiently marginalising the expert indicator variable for single
data points.
The three different EBLOs are compared on the Motorcycle data set cite:Silverman1985.
The $\furtherBound$ bound provides the best performance as it balances the accuracy offered by the tight bound $\tightBound$,
with the computational improvements offered by further bounding the GPs.
The results demonstrate that our variational inference scheme principally handles uncertainty whilst
providing scalability via stochastic variational inference.
The method is then evaluated on a real-world quadcopter example demonstrating that
it can successfully learn a factorised representation of the underlying dynamics modes.

* Mode Constrained Trajectory Optimisation label:chap-traj-opt
** Maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\nominalStateTraj}{\ensuremath{\stateTraj_*}}
\newcommand{\nominalControlTraj}{\ensuremath{\controlTraj_*}}

\newcommand{\trajectory}{\ensuremath{\bar{\state}}}
\newcommand{\stateControlTraj}{\ensuremath{\bm\tau}}
\newcommand{\jacTraj}{\ensuremath{\bar{\mathbf{J}}}}

\newcommand{\desiredMode}{\ensuremath{k^*}}
\newcommand{\desiredGatingFunction}{\ensuremath{h_{k^*}}}
\newcommand{\desiredDynamicsFunc}{\ensuremath{\mode{\latentFunc}}}
\newcommand{\desiredStateDomain}{\ensuremath{\mode{\stateDomain}}}

\newcommand{\valueFunc}{\ensuremath{V}}

\renewcommand{\controlledPolicyDist}{\ensuremath{q_\policy}}

\renewcommand{\satisfactionProb}{\ensuremath{p_{\modeVar}}}
#+END_EXPORT
# *** Geometry Maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\manifold}{\ensuremath{\mathcal{M}}}
\newcommand{\manifoldFunction}{\ensuremath{h}}
\newcommand{\manifoldDomain}{\ensuremath{\mathcal{Z}}}
\newcommand{\manifoldCodomain}{\ensuremath{\mathcal{X}}}
\newcommand{\ManifoldDim}{\ensuremath{D}}
\newcommand{\manifoldDim}{\ensuremath{d}}
\newcommand{\manifoldInput}{\ensuremath{\mathbf{x}}}

% \newcommand{\jacobian}{\ensuremath{\mathbf{J}_{\mathbf{x}_t}}}
\newcommand{\jacobian}{\ensuremath{\mathbf{J}(\state(t))}}
\newcommand{\metricTensor}{\ensuremath{\mathbf{G}}}

\newcommand{\geodesicFunction}{\ensuremath{f_G}}

%\newcommand{\gatingDomain}{\ensuremath{\hat{\mathcal{X}}}}
%\newcommand{\gatingCodomain}{\ensuremath{\mathcal{A}}}
\newcommand{\gatingDomain}{\ensuremath{\mathcal{Z}}}
\newcommand{\gatingCodomain}{\ensuremath{\mathcal{X}}}

\newcommand{\desiredManifold}{\ensuremath{\mathcal{M}_{k^*}}}
%\newcommand{\desiredMetricTensor}{\ensuremath{\mathbf{G}_{k^*}}}
\newcommand{\desiredMetricTensor}{\ensuremath{\mathbf{G}}}
%\newcommand{\desiredJacobian}{\ensuremath{\mathbf{J}_{k^*}(\state(t))}}
\newcommand{\desiredJacobian}{\ensuremath{\mathbf{J}}}
%\newcommand{\GatingDim}{\ensuremath{D_{x+u}}}
\newcommand{\GatingDim}{\ensuremath{D}}
\newcommand{\gatingDim}{\ensuremath{d}}

% Manfiold kernels
\renewcommand{\manifoldKernelMM}{\ensuremath{\mathbf{K}_{\NumInducing \NumInducing}}}
\newcommand{\jacManifoldKernelsM}{\ensuremath{\partial \mathbf{K}_{* \NumInducing}}}
\newcommand{\jacManifoldKernelMs}{\ensuremath{\partial \mathbf{K}_{\NumInducing *}}}
\newcommand{\hessManifoldKernel}{\ensuremath{\partial^2 \mathbf{K}_{**}}}
\renewcommand{\manifoldKernelNN}{\ensuremath{\mathbf{K}_{\NumData \NumData}}}
\newcommand{\jacManifoldKernelsN}{\ensuremath{\partial \mathbf{K}_{* \NumData}}}
\newcommand{\jacManifoldKernelNs}{\ensuremath{\partial \mathbf{K}_{\NumData *}}}

\newcommand{\manifoldInducingInput}{\ensuremath{\bm\xi}}
\newcommand{\manifoldInducingOutput}{\ensuremath{\mathbf{u}}}
\newcommand{\manifoldInducingVariational}{\ensuremath{q(\mathbf{u})}}
\newcommand{\manifoldInducingOutputMean}{\ensuremath{\mathbf{m}}}
\newcommand{\manifoldInducingOutputCov}{\ensuremath{\mathbf{S}}}
\newcommand{\manifoldMeanFunc}{\ensuremath{\mu}}


%\newcommand{\manifoldFunc}{\ensuremath{\mathbf{h}}}
%\newcommand{\desiredMeanFunc}{\ensuremath{\mu}}
\renewcommand{\muJac}{\ensuremath{\bm\mu_{\mathbf{J}}}}
\renewcommand{\covJac}{\ensuremath{\bm\Sigma_{\mathbf{J}}}}
\renewcommand{\testInput}{\ensuremath{\mathbf{x}_*}}

\newcommand{\stateDiff}{\ensuremath{\Delta \state}}

\renewcommand{\controlCostMatrix}{\ensuremath{\mathbf{R}}}
#+END_EXPORT
** Intro :ignore:
This chapter is concerned with controlling multimodal dynamical systems, whose transition dynamics
are represented with the probabilistic dynamics model from Chapter ref:chap-dynamics,
after training on a historical data set of state transitions $\dataset$.
In particular, it is concerned with /mode constrained/ trajectory optimisation, which attempts to
drive the system from an initial state $\state_0$ -- in a desired dynamics mode -- to a target state $\state_f$,
whilst remaining in the desired dynamics mode.
Given the well-calibrated uncertainty estimates from the model in Chapter ref:chap-dynamics,
this chapter is concerned with *risk-averse* control.
That is, the algorithms should find trajectories that also
avoid entering regions of the transition dynamics that
cannot be predicted confidently i.e. have high /epistemic uncertainty/.
For example, if they have not been observed.
This is because the model cannot be certain whether such a region belongs to the desired dynamics mode
or not.
This Chapter introduces two different approaches to performing /mode constrained/ trajectory optimisation.

# trained on this data set $\dataset$, this chapter exploits the
# well-calibrated uncertainty estimates from the model

# Given that the learned dynamics model may not be fully known

# This chapter considers a *risk-averse setting*
# in regions of the transition dynamics that
# have been observed and can be predicted confidently, i.e. have low /epistemic uncertainty/.

# This chapter is concerned with /mode constrained/ trajectory optimisation in learned dynamics models
# using the model in Chapter ref:chap-dynamics.
# The aim is to find trajectories that remain in a preferred dynamics mode
# where possible, and in regions of the transition dynamics that
# have been observed and can be predicted confidently, i.e. have low /epistemic uncertainty/.
# The main goals of the trajectory optimisation in this Chapter can be summarised as,
# - Goal 1 :: remain in a preferred dynamics mode $k^*$ where possible, label:to-goal-mode
# - Goal 2 :: avoid regions of the learned dynamics with high epistemic uncertainty, i.e. that cannot be predicted confidently (due to limited training observations). label:to-goal-unc


Section ref:sec-problem-statement formally states the problem and then Sections ref:sec-traj-opt-geometric
and ref:sec-traj-opt-inference detail the two trajectory optimisation algorithms.

The work in this chapter is implemented in JAX and is available on GitHub
cite:jax2018github[fn:2:Code accompanying this Chapter can be found @ [[https://github.com/aidanscannell/mogpe][\icon{\faGithub}/aidanscannell/trajectory-optimisation-in-learned-multimodal-dynamical-systems and icon{\faGithub}/aidanscannell/vimpc]].].
\todo{make links work}

** Problem Statement label:sec-problem-statement
This chapter is interested in performing trajectory optimisation in
stochastic, discrete-time, fully-observed, multimodal, nonlinear
systems, with unknown, or partially unknown transition dynamics
$\dynamicsFunc$, learned with the model from Chapter ref:chap-dynamics.
Given a desired dynamics mode, $\desiredMode$, with transition dynamics,
$\desiredDynamicsFunc: \desiredStateDomain \times \controlDomain \rightarrow \desiredStateDomain$,
where $\desiredStateDomain = \{ \state \in \stateDomain \mid \modeVar = \desiredMode \}$,
this work seeks to find the optimal controls over a horizon,
$\TimeInd$, that keeps the dynamics in the desired mode, $\desiredDynamicsFunc$,
and minimises a cost
function, $\costFunc: \stateDomain \times \controlDomain \rightarrow \R$,
whilst moving from an initial state, $\state_{0}$, to a target
state, $\state_g$.
Formally, this problem is given by,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-mode-soc-problem}
\begin{align}
\min_{\controlTraj} &\E\left[ \terminalCostFunc(\state_\TimeInd)
+ \sum_{\timeInd=0}^{\TimeInd-1} \integralCostFunc(\state_\timeInd, \control_\timeInd) \right] \\
\text{s.t.} \quad &\state_{\timeInd+1} = \mode{\latentFunc}(\state_\timeInd, \control_\timeInd) + \epsilon_\modeInd,
\quad \epsilon_\modeInd \sim \mathcal{N}(\mathbf{0}, \Sigma_{\epsilon_\modeInd}) \\
&\modeVar_{\timeInd} = \desiredMode \quad \forall \timeInd \in \mathbb{Z} \cap [1,\TimeInd] \label{eq-mode-soc-problem-constraint}
\end{align}
\end{subequations}
#+END_EXPORT
where the expectation is taken w.r.t. the distribution over state-control
trajectories under policy $\policy$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-trajectory-dist}
%p(\stateTraj, \controlTraj \mid \state_0) =
\controlledPolicyDist(\stateTraj, \controlTraj) =
\prod_{\timeInd=0}^{\TimeInd-1}
p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)
\policy(\control_\timeInd \mid \state_\timeInd).
\end{align}
#+END_EXPORT
For notational conciseness the terminal and integral cost functions are combined into a single cost function,
given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-cost-func}
\costFunc(\state_\timeInd, \control_\timeInd) =
\begin{cases}
\integralCostFunc(\state_\timeInd, \control_\timeInd), &\text{if} \quad \timeInd \in [0, \ldots, \TimeInd-1], \\
\terminalCostFunc(\state_\timeInd), \quad &\text{if} \quad \timeInd=\TimeInd.
\end{cases}
\end{align}
#+END_EXPORT
To simplify notation, the state and control trajectories are denoted
as $\stateTraj = \state_{1:T}$ and $\controlTraj = \control_{0:T-1}$ respectively.

The optimal controls can be derived from Bellman's dynamics programming (DP) perspective, using the value function,
$\valueFunc_\timeInd$, for the expected cost.
For $\timeInd = 0,\ldots, \TimeInd-1$ the value function is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-bellman-equations}
\valueFunc_\timeInd(\state) &= \min_{\control_\timeInd} \E\left[
\integralCostFunc(\state_\timeInd, \control_\timeInd)
+ \valueFunc_{\timeInd+1}(\dynamicsFunc(\state_\timeInd, \control_\timeInd))
\mid \state_0 = \state
\right],
\end{align}
#+END_EXPORT
and the terminal value, $\valueFunc_\TimeInd$, is given by the terminal cost
$\valueFunc_\TimeInd(\state_\TimeInd) = \E \left[ \terminalCostFunc(\state_\TimeInd) \right]$.
Given linear dynamics, quadratic costs and Guassian disturbances, the classic LQG solution
can be derived in closed-form using Eq. ref:eq-bellman-equations.
However, the global value function is usually intractable given arbitrary dynamics, costs, and
uncertainties.
Therefore, solutions to the SOC problem, either approximate the value function, or
directly tackle the constrained optimisation problem, ignoring the temporal structure.

Differential dynamic programming (DDP) citep:jacobsonDifferential1970, iteratively constructs
local Taylor series approximations of the dynamics and cost to locally approximate the
value function. As such, it exploits the temporal structure to obtain a closed-loop controller.
The extension of DDP to stochastic dynamics (SDDP) citep:theodorouStochastic2010, considers
expected costs under Gaussian disturbances.
Iterative linear quadratic Gaussian (iLQG) control citep:todorovGeneralized2005 deploys
a similar approach to DDP but instead
uses a linear approximation of the dynamics --- trading in accuracy for a computational speed-up.
Guided policy search cite:levineGuided2013

\todo{Add details on LQG etc here}

The main goals of the trajectory optimisation in this Chapter can be summarised as,
- Goal 1 :: remain in a preferred dynamics mode $\desiredMode$ where possible, label:to-goal-mode
- Goal 2 :: avoid regions of the learned dynamics with high /epistemic uncertainty/, i.e. that cannot be predicted confidently (due to limited training observations). label:to-goal-unc

# # The standard approach to solving such a problem is to construct an objective function encoding
# # the goals.
# To help provide intuition, first consider an objective function with two terms
# (one to address each of the goals),
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-cost-trajectory}
# J&= \int_{t_0}^{t_f}  g_{\text{mode}}(\mathbf{x}(t), \mathbf{u}(t)) + g_{\text{epistemic}}(\mathbf{x}(t), \mathbf{u}(t)) \text{d}t,
# \end{align}
# #+END_EXPORT
# where the $g_{\text{mode}}$ term favours remaining in dynamics mode $\desiredMode$,
# and the $g_{\text{epistemic}}$ term favours trajectories that avoid regions of the
# dynamics with high epistemic uncertainty.

# Following this approach, an objective function based on probabilistic Riemannian geometry is constructed.
# However, instead of minimising this objective function subject to the systems transition dynamics,
# we exploit the observation detailed in Section ref:sec-riemannian-geometry Eq. ref:eq-length-objective.
# That is, trajectories that minimise our objective function are actually solutions
# to the $2^{\text{nd}}$ order ODE in Eq. ref:eq-2ode;
# an ODE whose solutions are geodesics (i.e. shortest paths) on the Riemannian manifold endowed with the metric
# $\mathbf{G}$.
# With this observation we project the trajectory optimisation onto this ODE
# and deploy a simple objective function.
# Our goals are thus encoded into the Riemannian metric tensor $\mathbf{G}$ that
# appears in Eq ref:eq-2ode.
# Let us now provide an intuitive and thorough walk-through of the approach detailed above.
# \todo{move this paragraph}

** Latent Spaces for Control
*** intro :ignore:
The optimal control problem in cref:eq-mode-soc-problem enforces the mode remaining behaviour via the
equality constraint in cref:eq-mode-soc-problem-constraint.
This constraint has assumed access to the mode indicator variable $\modeVar$ at all time steps, which is
not always possible.
Conveniently, the probabilistic model in cref:chap-dynamics was formulated with latent variables
to represent the mode indicator variable $\modeVar$ and our uncertainty in it.
This chapter now unleashes the power of these latent variables by making decisions under their uncertainty.
Let us consider all of the information inferred by the probabilistic model in cref:chap-dynamics that could be
used to encode our goals.
# It is common to solve such a problem by encoding the goals either in the objective function or via
# inequality constraints.

- *Cost* Minimise length of trajectory on manifold parameterised by the desired mode's mixing probability,
- *Constraints* Enforce the desired mode's probabilities at each time step to be more than some threshold,
- *Inference* Condition optimal trajectory on desired mode's expert indicator variable,

*** Mode Indicator Variable

# *Mode Indicator Posterior*
The mode indicator variable can be used to encode Goal 1 in multiple ways.
The simplest method is to simply maximise the desired mode's mixing probability over
the trajectory, by introducing a cost function,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-cost-trajectory}
\costFunc_{\text{mode}}(\state(t), \control(t)) = - \Pr (\modeVar_{\timeInd} = \desiredMode),
\end{align}
#+END_EXPORT
which favours remaining in dynamics mode $\desiredMode$.
As our model assumes that the mode indicator depends on the state via the gating function,
this probability is calculated as follows,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mode-chance-constraint-integral}
\Pr(\modeVar_{\timeInd} = \desiredMode) =
\int \Pr(\modeVar_{\timeInd} = \desiredMode \mid \GatingFunc(\state_{\timeInd}) )
\int \underbrace{q(\GatingFunc(\state_{\timeInd}) \mid \state_{\timeInd})}_{\text{approx posterior}}
\underbrace{p(\state_{\timeInd})}_{\text{state dist}}
\text{d} \state_{\timeInd}
\text{d} \GatingFunc(\state_{\timeInd})
\end{align}
#+END_EXPORT
where $q(\GatingFunc(\state_{\timeInd}) \mid \state_{\timeInd})$ is the approximate posterior
from cref:eq-predictive-gating. The dependence on the state input $\state_{\timeInd}$ is reintroduced here,
as it becomes a random variable when making multi-step predictions.

*Chance Constraints*
An alternative approach is to encode the goals via constraints.
Due to the stochastic nature of our problem, the equality constraint in cref:eq-mode-soc-problem-constraint
must be reformulated as a chance constraint,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mode-chance-constraint}
\Pr(\modeVar_{\timeInd} = \desiredMode) \geq \satisfactionProb \quad \forall \timeInd.
\end{align}
#+END_EXPORT
This constraint enforces the desired mode's mixing probability to be more than the satisfaction probability
$\satisfactionProb$ at each time step.
# As our model assumes that the mode indicator depends on the state via the gating function,
# this probability is calculated as follows,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-mode-chance-constraint-integral}
# \Pr(\modeVar_{\timeInd} = \desiredMode) = \int
# \Pr(\modeVar_{\timeInd} = \desiredMode \mid \GatingFunc(\state_{\timeInd}) )
# \int \underbrace{q(\GatingFunc(\state_{\timeInd}) \mid \state_{\timeInd})}_{\text{approx posterior}}
# \underbrace{p(\state_{\timeInd})}_{\text{state dist}}
# \text{d} \state_{\timeInd}
# \text{d} \GatingFunc(\state_{\timeInd})
# \end{align}
# #+END_EXPORT
# where $q(\GatingFunc(\state_{\timeInd}) \mid \state_{\timeInd})$ is the approximate posterior
# from cref:eq-predictive-gating. The dependence on the state input $\state_{\timeInd}$ is reintroduced here,
# as it becomes a random variable when making multi-step predictions.

# *Cost Function - Maximum Probability*
# An alternative approach is to encode Goal ref:to-goal-mode into a cost function.
# To help provide intuition, first consider a cost function with a term to address each of the two goals,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-cost-trajectory}
# %J&= \int_{t_0}^{t_f}  g_{\text{mode}}(\state(t), \control(t)) + g_{\text{epistemic}}(\state(t), \control(t)) \text{d}t,
# \costFunc(\state(t), \control(t)) = \costFunc_{\text{mode}}(\state(t), \control(t))
# + \costFunc_{\text{epistemic}}(\state(t), \control(t)),
# \end{align}
# #+END_EXPORT
# where the $\costFunc_{\text{mode}}$ term favours remaining in dynamics mode $\desiredMode$
# and the $\costFunc_{\text{epistemic}}$ term favours trajectories that avoid regions of the
# dynamics with high epistemic uncertainty.

*** Probabilistic Geometry of the Gating Network
**** intro :ignore:
# Now that the relevant concepts from Riemannian geometry have been introduced, the objective function
# for the first trajectory optimisation can be detailed.
# The geometry of the latent gating functions can be used to formulate a cost term
# that favours trajectories remaining in a preferred dynamics mode.
An alternative approach is to consider the geometry of the latent gating functions.
Goal 1 can be encoded as finding length minimising trajectories on the desired mode's gating function.
Intuitively, the length of a trajectory from $\mathbf{x}_0$ to $\mathbf{x}_f$ on the
manifold given by the desired mode's gating function,
increases when it passes over the contours; analogous to climbing a hill.
Given appropriate scaling of the gating function, shortest trajectories between two locations are
those that attempt to follow the contours, i.e. remain in a single mode by not climbing up or down any hills.
Let us quickly recap how to calculate lengths on Riemannian manifolds

**** Lengths in Euclidean Spaces :ignore:
\newline
*Lengths in Euclidean Spaces*
The $l^2$ norm (aka Euclidean norm) provides an intuitive notion for the length of a
vector $\manifoldInput \in \manifoldDomain \subseteq \R^\manifoldDim$
in a Euclidean space. Under the $l^2$ norm, the length of a trajectory
${\trajectory = \{\manifoldInput(t) \in \manifoldDomain \quad \forall t \in [t_0, t_f]\}}$ is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-euclidean-length}
\text{Length}\left(\trajectory\right)
&= \int_{t_0}^{t_f}\left\|\dot{\manifoldInput}(t)\right\|_2 \mathrm{d}t,
\end{align}
#+END_EXPORT
where Newton's notation has been used to denote differentiation with respect to time $t$.
As a norm can be expressed for any space endowed with an inner product, it is possible to
calculate lengths of trajectories on manifolds endowed with an inner product.
# Just as the dot product is the inner product of the Euclidean space, it is possible to
# define inner products for Riemannian spaces (also known as Riemannian manifolds).
# Remembering that a norm can be expressed for any space endowed with an inner product,
# provides us with the tools to for calculating lengths on manifolds.

**** Lengths on Riemannian Manifolds :ignore:
*Lengths on Riemannian Manifolds*
The length of a trajectory $\trajectory$ on a manifold $\manifold$, can be calculated
by mapping it through the function $\manifoldFunction$ and
using Eq. ref:eq-euclidean-length,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-length}
\text{Length}\left(\manifoldFunction(\trajectory)\right)
&= \int_{t_0}^{t_f}\left\|\dot{\manifoldFunction}(\manifoldInput(t))\right\|_2 \mathrm{d}t.
\end{align}
#+END_EXPORT
Applying the chain-rule allows Eq. ref:eq-manifold-length to be expressed in terms of the Jacobian and the
velocity,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-length-chain}
\text{Length}\left(\manifoldFunction(\trajectory)\right)
&= \int_{t_0}^{t_f}\left\|\jacobian \dot{\manifoldInput}(t)\right\|_2 \mathrm{d}t, \\
\jacobian &= \frac{\partial \manifoldFunction}{\partial \manifoldInput(t)} \in \R^{1 \times \ManifoldDim}.
\end{align}
#+END_EXPORT
This implies that the length of a trajectory on the manifold $\manifold$,
can be calculated in the input space $\manifoldDomain$,
using a locally defined norm,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-norm}
\left\|\jacobian \dot{\manifoldInput}(t)\right\|_2
&= \sqrt{ \left( \jacobian \dot{\manifoldInput}(t) \right)^T
\jacobian \dot{\manifoldInput}(t)} \nonumber \\
&= \sqrt{\dot{\mathbf{x}}^T(t) \metricTensor_{\mathbf{x}_t} \dot{\manifoldInput}(t)}
\defeq \left\|\dot{\manifoldInput}(t)\right\|_{\metricTensor_{\manifoldInput_t}},
%= \left\|\dot{\mathbf{x}}(t)\right\|_{\mathbf{G}(\mathbf{x}(t))}.
\end{align}
#+END_EXPORT
where $\metricTensor_{\mathbf{x}_t} = \jacobian^T \jacobian}$
is a symmetric positive definite matrix
(akin to a local Mahalanobis distance measure), known as the natural Riemannian metric.
The length of a trajectory on a manifold $\manifold$, endowed with the metric $\metricTensor$,
can then be calculated as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-length-G}
\text{Length}(\manifoldFunction(\trajectory))
&= \int_{t_0}^{t_f}\left\|\dot{\manifoldInput}(t)\right\|_{\metricTensor_{\mathbf{x}_t}} \mathrm{d}t.
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Riemannian Manifolds}
In this dissertation if suffices to consider manifolds $\manifold$ defined by a mapping,
%#+BEGIN_EXPORT latex
\begin{equation} \label{eq-manifold-function}
\manifoldFunction : \manifoldDomain \rightarrow \manifoldCodomain,
\end{equation}
%#+END_EXPORT
where $\manifoldDomain$ and $\manifoldCodomain$ are open subsets of Euclidean spaces.
The manifold $\manifold$ is given by $\manifold = \manifoldFunction(\manifoldDomain)$ and is said to
be immersed in the ambient space $\manifoldCodomain$.
Riemannian manifolds can intuitively be seen as $d\text{-dimensional}$ curved surfaces with a smoothly
varying positive-definite inner product, governed by the Riemannian metric $\metricTensor$ \citep{carmoRiemannian1992}.
\begin{definition}[Riemannian Metric]
A Riemannian metric $\metricTensor$,
on a manifold $\manifold$, is a smooth function
$\metricTensor : \manifoldDomain \rightarrow \R^{\ManifoldDim \times \ManifoldDim}$
that assigns a symmetric positive definite matrix to any point in $\manifoldDomain$.
%A Riemannian metric $\metricTensor$ on a
%manifold $\manifold$ is a symmetric and positive definite matrix which defines
%a smoothly varying inner product
%$\langle \dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \rangle_{\mathbf{x}} = \dot{\mathbf{x}}_a^T \mathbf{G}(\mathbf{x}) \dot{\mathbf{x}}_b$
%in the tangent space $T_{\mathbf{x}}\manifold$, for each point $\mathbf{x} \in \manifold$ and
%$\dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \in T_{\mathbf{x}}\manifold$.
\end{definition}
Intuitively, the metric forms a local inner product in $\manifoldDomain$ that informs how to measure lengths
on the manifold $\manifold$, locally in $\manifoldDomain$.
Riemannian manifolds locally resemble Euclidean spaces $\R^\manifoldDim$ and have
globally defined differentiable structure.
\end{myquote}
#+END_EXPORT
**** Probabilistic Geometries label:sec-prob-geo
***** intro :ignore:
Length minimising trajectories on the manifold $\desiredManifold=\desiredGatingFunction(\gatingDomain)$
associated with the desired mode's gating function,
$\desiredGatingFunction : \gatingDomain \rightarrow \gatingCodomain$,
encode Goal 1.
That is, trajectories minimising cref:eq-manifold-length,
subject to the metric $\desiredMetricTensor$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-desired-metric-tensor}
\desiredMetricTensor &= \desiredJacobian^T \desiredJacobian, \\
\desiredJacobian &= \frac{\partial \desiredGatingFunction}{\partial \manifoldInput(t)} \in \R^{1 \times \GatingDim},
\end{align}
#+END_EXPORT
will attempt to remain in the desired dynamics mode $\desiredMode$.
This encodes Goal 1 as length minimising trajectories, however, it does address Goal 2.
We now address Goal 2 by observing that the metric tensor is actually stochastic.
For notational conciseness, only a single gating function
$\manifoldFuncion$ is used to denote the desired mode's gating function in this chapter.

***** Extension to Probabilistic Geometries :ignore:
Following cite:tosiMetrics2014 this work formulates a metric tensor that captures the variance in the manifold
via a probability distribution.
First note that as the differential operator is linear, the derivative of a GP is also a GP.
Therefore, the metric tensor $\desiredMetricTensor$ in Eq. ref:eq-desired-metric-tensor is the outer product of
two Normally distributed random variables.
As such, the metric tensor $\desiredMetricTensor$ is also a random variable, following
a non-central Wishart distribution citep:andersonNonCentral1946,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-metric-dist}
  \desiredMetricTensor \sim
  \mathcal{W}_{\GatingDim}\left(P, \covJac, \mathbb{E}\left[\Jac^{T}\right] \mathbb{E}[\Jac]\right),
\end{align}
#+END_EXPORT
where $P$ is the number of degrees of freedom (always one in our case) and
$\E\left[\desiredJacobian\right]$ and $\covJac$ are the mean and covariance matrices
associated with the GP over the Jacobian.
The expected value of the metric tensor in Eq. ref:eq-metric-dist is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-metric}
  \E[\desiredMetricTensor] = \E[\mathbf{J}^T] \E[\mathbf{J}] + \covJac.
\end{align}
#+END_EXPORT
Importantly, this expected metric tensor includes a covariance term $\covJac$,
which implies that lengths on the manifold increase in areas of high covariance.
This is a desirable behaviour because it encourages length minimising trajectories
to avoid regions of the learned dynamics with high epistemic uncertainty,
encoding Goal 2.
The model in Chapter ref:chap-dynamics is built upon sparse GP approximations,
so the Jacobian in Eq. ref:eq-predictive-jacobian-dist must be extended for such approximations.

#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Gaussian Process Jacobain}
As the differential operator is linear,
a function $\manifoldFunction: \gatingDomain \rightarrow  \R$ modelled as a GP,
\begin{align} \label{eq-jacobian-gp-random-var}
\manifoldFunction \sim \mathcal{N}(\mu(\allInput), k(\allInput, \allInput)),
\end{align}
where $\mu$ and $k$ represent the mean and covariance functions,
is jointly Gaussian with its Jacobian
at a new input location $\testInput \in \R^{1 \times \GatingDim}$,
\begin{align} \label{eq-jacobian-random-var}
\testJac = \Jac(\testInput) = \frac{\partial \manifoldFunction}{ \partial \testInput} \in \R^{\GatingDim},
\end{align}
assuming that the mean and covariance functions are differentiable.
As such, the conditional distribution over the Jacobian
$\testJac$ can easily be obtained using the properties of multivariate Normals
and is given by,
\begin{align} \label{eq-predictive-jacobian-dist}
p\left(\testJac | \manifoldFunction(\allInput), \testInput, \allInput \right)
&= \mathcal{N}\left(\testJac \mid \muJac, \covJac \right) \\
\muJac &= \frac{\partial \mu}{\partial \testInput} + \jacManifoldKernelsM \manifoldKernelNN^{-1}
\left(\manifoldFunction(\allInput) - \manifoldMeanFunc(\allInput) \right) \\
\covJac &= \hessManifoldKernel - \jacManifoldKernelsN \manifoldKernelNN^{-1} \jacManifoldKernelNs
\end{align}
where the covariance matrices are given by,
\begin{align} \label{eq}
\manifoldKernelNN &= k\left( \allInput, \allInput \right) \in \R^{\NumData \times \NumData} \\
\jacManifoldKernelsN&= \frac{\partial k\left(\testInput, \allInput\right)}{\partial \testInput} \in \R^{\GatingDim \times \NumData} \\
\hessManifoldKernel &= \frac{\partial^2 k\left(\testInput, \testInput \right)}{\partial \testInput \partial \testInput} \in \R^{\GatingDim \times \GatingDim}.
\end{align}
%Eq. \ref{eq-predictive-jacobian-dist} is a $\StateDim\text{-dimensional}$ multivariate Normal distribution.
\end{myquote}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Sparse Gaussian Process Jacobian}
In the sparse variational  Gaussian process setting, the predictive distribution over the Jacobian given a new
input, is obtained by conditioning on the inducing
variables $\manifoldInducingOutput = \manifoldFunction(\manifoldInducingInput) \in \R^{\NumInducing \times \GatingDim}$,
\begin{equation} \label{eq-}
\manifoldInducingOutput \sim \mathcal{N}(\mu(\manifoldInducingInput),
k(\manifoldInducingInput, \manifoldInducingInput))
\end{equation}
and then marginalising them with respect to their variational posterior,
\begin{equation} \label{eq-}
\manifoldInducingVariational = \mathcal{N}(\manifoldInducingOutput \mid
\manifoldInducingOutputMean, \manifoldInducingOutputCov).
\end{equation}
The distribution over the Jacobian is approximated as follows,
\begin{align}
p(\testJac \mid \testInput, \allOutput, \allInput) &=
\int \int p(\testJac \mid \testInput, \manifoldFunction(\allInput), \manifoldInducingOutput)
p(\manifoldFunction(\allInput), \manifoldInducingOutput \mid \allOutput, \allInput)
\text{d} \manifoldFunction(\allInput) \text{d} \manifoldInducingOutput \nonumber \\
&\approx \int \int p(\testJac \mid \testInput, \manifoldFunction(\allInput), \manifoldInducingOutput)
p(\manifoldFunction(\allInput) \mid \manifoldInducingOutput) \manifoldInducingVariational
\text{d} \manifoldFunction(\allInput) \text{d} \manifoldInducingOutput \nonumber \\
&= \int  p(\testJac \mid \testInput, \manifoldInducingOutput)
\manifoldInducingVariational
\text{d} \manifoldInducingOutput
\coloneqq q(\testJac \mid \testInput)
\end{align}
where the mean and covariance are given by,
\begin{align}
q(\testJac \mid \testInput) &= \mathcal{N}\left(\testJac \mid \muJac, \covJac \right) \\
\muJac &= \jacManifoldKernelsM \manifoldKernelMM^{-1} \left( \manifoldInducingOutputMean - \manifoldMeanFunc(\allInput) \right), \\
\covJac &= \hessManifoldKernel - \jacManifoldKernelsM \manifoldKernelMM^{-1}
\left( \manifoldKernelMM - \manifoldInducingOutputCov \right) \manifoldKernelMM^{-1} \jacManifoldKernelMs \right).
\end{align}
with covariance matrices given by,
\begin{align} \label{eq-}
\manifoldKernelMM &= k\left( \manifoldInducingInput, \manifoldInducingInput \right) \in \R^{\NumInducing \times \NumInducing} \\
\jacManifoldKernelsM&= \frac{\partial k\left(\testInput, \manifoldInducingInput \right)}{\partial \testInput} \in \R^{\GatingDim \times \NumInducing} \\
\end{align}
\end{myquote}
#+END_EXPORT

**** Latent Geometry
Given this latent geometry, Goals 1 and 2 can be encoded into the cost function in cref:eq-mode-soc-problem
as length minimising trajectories using cref:eq-manifold-length-G.
In fact, length minimising trajectories can be found instead by minimising the curve energy
cite:carmoRiemannian1992, given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-trajectory-energy}
\text{Energy}(\manifoldFunction(\trajectory))
= \int \dot{\state}(\timeInd) \metricTensor(\state(\timeInd)) \dot{\state}(\timeInd) \text{d} \timeInd.
\end{align}
#+END_EXPORT
which retains a quadratic form. However, the expected cost for a trajectory,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\E_{\stateTraj, \controlTraj, \jacTraj} \left[ \costFunc(\stateTraj, \controlTraj, \jacTraj) \right]
= \sum_{\timeInd=1}^\TimeInd
\E_{\stateDist}\left[ \stateDiff_\timeInd^T
\E_{\Jac \mid \state_{\timeInd}}\left[ \Jac(\state_\timeInd) \Jac(\state_\timeInd)^T \right]
 \stateDiff_\timeInd \right]
\end{align}
#+END_EXPORT
has no closed-form expression due to the metric's $\metricTensor$ dependence on the state via the Jacobian.


# requires an expectation over the Jacobian $\jacGivenInput$
# as well as the state,
# There is no closed-form expression for this expectation due to the Jacobian's dependence on the state.

** Indirect Control via Latent Geodesics
The first control algorithm presented in this chapter is an indirect trajectory optimisation algorithm that
exploits the fact that
length minimising trajectories, on the manifold endowed with the expected metric from cref:eq-expected-metric,
encodes both of our Goals.
Shortest lengths on a manifold are known as geodesics, so we refer to shortest
trajectories as geodesic trajectories.

*** Geodesics :ignore:
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Geodesics}
Given the method for calculating lengths on Riemannian manifolds in \cref{eq-manifold-length},
it is trivial to define the notion of a shortest trajectory, or geodesic, as a length minimising trajectory.
Formally, a geodesic is defined as follows,
\begin{definition}[Geodesic]
Given two points $\manifoldInput_0, \manifoldInput_f \in
\manifold$, a Geodesic is a length minimising trajectory (curve)
$\trajectory_g$ connecting the points, such that,
\begin{subequations}
\begin{align} \label{eq-geodesic}
  \trajectory_{g} =\arg &\min_{\trajectory} \operatorname{Length}(\trajectory) \\
\text{s.t.} \quad  \manifoldInput(t_0)&=\manifoldInput_{0} \\
  \manifoldInput(t_f)&=\manifoldInput_{f}.
\end{align}
\end{subequations}
\end{definition}
\end{myquote}
#+END_EXPORT
*Geodesic ODE* An important observation from cite:carmoRiemannian1992, is that geodesics
satisfy a continuous-time $2^{\text{nd}}$ order ODE, given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-2ode}
 \ddot{\manifoldInput}(t)
&= \geodesicFunction(t, \manifoldInput, \dot{\manifoldInput}) \nonumber \\
&=-\frac{1}{2} \metricTensor^{-1}(\manifoldInput(t))\left[
\frac{\partial \operatorname{vec}[\metricTensor(\manifoldInput(t))]}{\partial \manifoldInput(t)}
\right]^{T}\left(\dot{\manifoldInput}(t) \otimes \dot{\manifoldInput}(t)\right),
\end{align}
#+END_EXPORT
where $\operatorname{vec}[\metricTensor(\manifoldInput(t)])$ stacks the columns of $\metricTensor(\manifoldInput(t))$
and $\otimes$ denotes the Kronecker product.
The implication of Eqs. ref:eq-geodesic and ref:eq-2ode is that trajectories that are solutions
to the $2^{\text{nd}}$ order ODE in Eq. ref:eq-2ode implicitly minimise the objective,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-length-objective}
J_{\text{geodesic}} &= \min \int_{t_0}^{t_f}\left\|\dot{\manifoldInput}(t)\right\|_{\metricTensor(\mathbf{x}(t))} \mathrm{d}t.
\end{align}
#+END_EXPORT
Given this observation, computing geodesics involves finding a solution to Eq. ref:eq-2ode
with $\manifoldInput(t_0) = \manifoldInput_0$ and $\manifoldInput(t_f) = \manifoldInput_f$.
This is a boundary value problem with a smooth solution so it can be solved using
any BVP solver, e.g. (multiple) shooting and collocation methods.

*** Implicit Trajectory Optimisation
Solving the second-order ODE in cref:eq-2ode with the expected metric from cref:eq-expected-metric,
is equivalent to solving our trajectory optimisation problem subject to the same boundary conditions.
This resembles indirect control methods as it is based on an observation that the
necessary conditions for optimality are encoded via the geodesic ODE.
However, it is worth noting that solutions to the geodesic ODE are not guaranteed to satisfy the
dynamics constraints.

*Differential Flatness*
Since neither $\dot{\mathbf{x}}(t_0)$ nor $\dot{\mathbf{x}}(t_f)$ are known, Eq. ref:eq-2ode cannot
be solved with simple forward or backward integration.
Instead, the problem is transcribed using the approach of differential
flatness cite:milamNew2000,rossPseudospectral2004.
A set of outputs $\mathbf{z}(t)$ are defined such that the
states $\mathbf{x}(t)$ and controls $\mathbf{u}(t)$ can be
expressed in terms of the flat output $\mathbf{z}(t)$ and a finite number of its derivatives,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\mathbf{x}(t) &= A(\mathbf{z}(t), \dot{\mathbf{z}}(t), \ldots) \\
\mathbf{u}(t) &= B(\mathbf{z}(t), \dot{\mathbf{z}}(t), \ldots).
\end{align}
#+END_EXPORT
In the velocity-controlled quadcopter example, the flat output is the
state $\mathbf{z}(t) = \mathbf{x}(t)$
and the control is simply the state derivative
$\mathbf{u}(t) &= \dot{\mathbf{z}}(t)$.
The original trajectory optimisation problem can then be converted to finding $\mathbf{z}(t)$
for $t \in [t_0, t_f]$ subject to the boundary conditions and the geodesic ODE,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-diff-flat-ode}
\ddot{\mathbf{x}}(\mathbf{z}(t), \dot{\mathbf{z}}(t))
&= f_G(t, \mathbf{x}(\mathbf{z}(t), \dot{\mathbf{z}}(t)), \dot{\mathbf{x}}(\mathbf{z}(t), \dot{\mathbf{z}}(t))).
\end{align}
#+END_EXPORT
*Collocation*
Collocation methods are used to transcribe continuous-time trajectory optimisation problems into
nonlinear programs, i.e. constrained parameter optimisation cite:kellyIntroduction2017,fahrooDirect2000  .
The expected metric in cref:eq-expected-metric is substituted into
cref:eq-diff-flat-ode and solved via collocation.
This work implements a simple Hermite-Simpson collocation method that enforces the state
derivative interpolated by the polynomials to equal the geodesic ODE $f_G$
at the midpoints between a set of $I$ collocation points
$\{\mathbf{z}_i\}_{i=1}^I$. This is achieved through the collocation defects,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-defect}
\Delta_{i+\frac{1}{2}} &= \ddot{\mathbf{z}}_{i+\frac{1}{2}} - f_G(t_{i+\frac{1}{2}}, \dot{\mathbf{z}}_{i+\frac{1}{2}},\mathbf{z}_{i+\frac{1}{2}})
\end{align}
#+END_EXPORT
where $\ddot{\mathbf{z}}_{i+\frac{1}{2}}, \dot{\mathbf{z}}_{i+\frac{1}{2}}, \mathbf{z}_{i+\frac{1}{2}}$
are obtained by interpolating between $\mathbf{z}_i$ and $\mathbf{z}_{i+1}$.
cref:eq-defect defines a set of constraints ensuring trajectories are solutions
to the geodesic ODE $f_G$.
The nonlinear program that this work solves is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
%\min_{\mathbf{z}(t), \dot{\mathbf{z}}(t)}& \int_{t_0}^{t_f} \costFunc(\state(\timeInd), \control(\timeInd)) \text{d}t \\
\min_{\mathbf{z}(t), \dot{\mathbf{z}}(t)}& \int_{t_0}^{t_f}
\costFunc(\state(\mathbf{z}(\timeInd), \dot{\mathbf{z}}(\timeInd)),
\control(\mathbf{z}(\timeInd), \dot{\mathbf{z}}(\timeInd))) \text{d}t \\
&\text{s.t. }\text{\cref{eq-defect}}  \\
\mathbf{x}&\left(\mathbf{z}(t_0), \dot{\mathbf{z}}(t_0) \right) = \mathbf{x}_0 \\
\mathbf{x}&(\mathbf{z}(t_f),\dot{\mathbf{z}}(t_f)) = \mathbf{x}_f
\end{align}
#+END_EXPORT
with a quadratic cost function on the controls,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-cost-control}
\costFunc(\stateTraj, \controlTraj)
&= \sum_{\timeInd=0}^{\TimeInd-1} \control_{\timeInd}^T \controlCostMatrix \control_{\timeInd} \nonumber \\
&= \sum_{\timeInd=0}^{\TimeInd-1} \left\| \control_\timeInd \right\|_{\controlCostMatrix},
\end{align}
#+END_EXPORT
where $\controlCostMatrix$ is a user defined, real symmetric positive definite weight matrix.
It is common to find trajectories that minimise the expenditure of control effort,
for example, minimising the total expenditure of fuel, or to minimising the energy dissipated from the system.
This is solved using Sequential Least Squares Programming (SLSQP) in SciPy.

# this work adopts a quadratic cost function on the controls,

# This objective regularises the control trajectory under the L2 norm.
# The meaning of minimum control effort depends upon the specific problem.
# It may refer to minimising the total expenditure of fuel, or to minimising the energy dissipated from the system.
# Regardless of the specific interpretation, it is usually desirable to regularise the controls.

\todo[inline]{Lagrance multipliers to make TO unconstrained? Feels unnecessary to include...}

# *** Results label:sec-traj-opt-results
# #+NAME: fig-geometric-traj-opt-over-svgp
# #+ATTR_LATEX: :width 1.1\textwidth :placement [!t] :center t
# #+caption: Contour plots showing the GP posterior mean (left) and variance (right) over the gating function associated with dynamics mode 1 after training on a subset of the quadcopter data set. The initial and optimised trajectories are overlayed to show the influence of the GP's mean and variance on the trajectory optimisation with different $\lambda$ settings.
# [[file:./images/geometric-traj-opt-over-svgp.pdf]]

*** Evaluation on Velocity Controlled Quadcopter label:sec-traj-opt-results
#+NAME: fig-geometric-traj-opt-over-svgp
#+ATTR_LATEX: :width 1.1\textwidth :placement [!t] :center t
#+caption: Contour plots showing the GP posterior mean (left) and variance (right) over the gating function associated with dynamics mode 1 after training on a subset of the quadcopter data set. The initial and optimised trajectories are overlayed to show the influence of the GP's mean and variance on the trajectory optimisation with different $\lambda$ settings.
[[file:./images/geometric-traj-opt-over-svgp.pdf]]

The trajectory optimisation is tested on the real-world quadcopter control problem from the illustrative example
in cref:fig-problem-statement.
The model from cref:chap-dynamics was instantiated with $\ModeInd=2$ experts and trained on
the data collected from the velocity controlled quadcopter experiment.
The controls were kept constant during data collection so the dynamics reduce to
$\Delta \state_{\timeInd} = \dynamicsFunc(\state_{\timeInd-1})$.
The velocity controls are recovered via differential flatness cite:rossPseudospectral2004.
A subset of the observations were removed from the data set to 1) test the model’s ability to represent
epistemic uncertainty and 2) test the trajectory optimisation's ability to avoid these regions.

cref:fig-geometric-traj-opt-over-prob shows the model's belief (probability) that mode 1 is
responsible for predicting over the domain.
The model has clearly learned two dynamics modes characterised by process noise.
cref:fig-geometric-traj-opt-over-svgp shows the predictive mean (left) and variance (right) of the
gating function $\gatingFunc_1$ associated with dynamics mode 1.
The mean is high where the model believes mode 1 is responsible for predicting, low where it
believes another mode is responsible and zero where it is uncertain.
The variance (right) has also clearly captured information regrading the epistemic uncertainty,
i.e. where there are no observations.

The trajectory optimisation is projected onto learned gating function associated with
the desired mode, shown in cref:fig-geometric-traj-opt-over-svgp.
It seeks to find trajectories between $\mathbf{x}_0$ and $\mathbf{x}_f$
that 1) remain in the non-turbulent mode and 2) avoid regions of the learned
transition dynamics with high epistemic uncertainty.
To aid with user control the metric tensor in Eq. ref:eq-expected-metric is modified with
a weighting parameter $\lambda$ that enables the relevance of the covariance term to be adjusted,
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{0.45\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/geometric-traj-opt-prob-vs-time.pdf}
\subcaption{Mode 1's mixing probability over the trajectories.}
\end{minipage}
\begin{minipage}[r]{0.45\columnwidth}
\includegraphics[width=\textwidth]{./images/geometric-traj-opt-epistemic-vs-time.pdf}
%\label{}
\subcaption{Posterior variance associated with mode 1's gating function over the trajectories.}
\end{minipage}
\caption{\label{fig-metric-vs-time}Comparision of the intial and optimised trajectories' performance (for two settings of $\lambda$) at a) staying in the desired mode and b) avoiding regions of high epistemic uncertainty.}
\end{figure}
#+END_EXPORT




#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{0.49\columnwidth}
\centering
\includegraphics[width=1.1\textwidth]{./images/geometric-traj-opt-over-prob.pdf}
\subcaption{}
\label{fig-geometric-traj-opt-over-prob}
\end{minipage}
\begin{minipage}[r]{0.49\columnwidth}
    \begin{minipage}[r]{0.7\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{./images/geometric-traj-opt-prob-vs-time.pdf}
        %\subcaption{Mode 1's mixing probability over the trajectories.}
        \subcaption{}
        \label{fig-geometric-traj-opt-prob-vs-time}
    \end{minipage}
    \begin{minipage}[r]{0.7\columnwidth}
        \includegraphics[width=\textwidth]{./images/geometric-traj-opt-epistemic-vs-time.pdf}
        \subcaption{}
        \label{fig-geometric-traj-opt-epistemic-vs-time}
        %\subcaption{Posterior variance associated with mode 1's gating function over the trajectories.}
    \end{minipage}
\end{minipage}
\begin{minipage}[r]{1.1\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/geometric-traj-opt-over-svgp.pdf}
\subcaption{}
\label{fig-geometric-traj-opt-over-svgp}
\end{minipage}
\caption{\label{fig-metric-vs-time}Comparision of the intial and optimised trajectories' performance -- for two settings of $\lambda$ -- at 1) staying in the desired mode and 2) avoiding regions of high epistemic uncertainty.
(\subref{eq-geometric-traj-opt-over-prob}) shows the trajectories overlayed on the desired mode's mixing probability
and (\subref{eq-geometric-traj-opt-over-svgp}) shows them overlayed on the desired mode's gating function posterior
mean (left) and posterior variance (right).
(\subref{geometric-traj-opt-prob-vs-time}) and (\subref{geometric-traj-opt-epistemic-vs-time}) show the
probability of remaining in the desired mode and the desired mode's posterior variance
over the trajectories respectively.}
\end{figure}
#+END_EXPORT


#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-metric-weighting}
  \tilde{\mathbf{G}} = \E[\mathbf{J}^T] \E[\mathbf{J}] + \lambda \mathbf{\Sigma}_J.
\end{align}
#+END_EXPORT
Setting $\lambda$ to be small should find trajectories that prioritise staying in the desired mode,
whereas selecting a large $\lambda$ should find trajectories that prioritise avoiding regions
of the dynamics with high epistemic uncertainty.
The trajectory optimisation is tested with two $\lambda$ settings.



The initial (cyan) trajectory in Fig. ref:fig-geometric-traj-opt-over-svgp is initialised with
$10$ collocation points indicated by the crosses.
The trajectories are compared via mode 1's mixing
probability and its gating function's GP variance over the trajectories.
We want to maximise the probability of being in our desired mode
whilst minimising the amount of variance (due to epistemic uncertainty).
The results are shown in Table ref:tab-results.
#+LABEL: tab-results
#+CAPTION: Comparison of performance with different settings of $\lambda$.
#+CAPTION: The performance measures are summed over collocation points.
#+attr_latex: :placement [!t]
|-------------------------+----------------------------------------------------+-----------------------------------------------------|
| Trajectory              | Mixing Probability                                 | Epistemic Uncertainty                               |
|                         | $\sum_{i=1}^{I} \Pr(\alpha_i=1 \mid \mathbf{z}_i)$ | $\sum_{i=1}^{I} \mathbb{V}[h^{(1)}(\mathbf{z}_i)]$  |
|-------------------------+----------------------------------------------------+-----------------------------------------------------|
| Initial                 | $7.480$                                            | $1.345$                                             |
| Optimised $\lambda=20$  | $6.091$                                            | $\mathbf{1.274}$                                    |
| Optimised $\lambda=0.5$ | $\mathbf{8.118}$                                   | $1.437}$                                            |
It is clear from Fig. ref:fig-geometric-traj-opt-over-svgp (left) and Fig. ref:fig-mixing_prob_vs_time
that for $\lambda=0.5$,
trajectories favour remaining in dynamics mode 1 at the cost of entering regions of the learned dynamics
with high epistemic uncertainty (shown in Fig. ref:fig-epistemic_var_vs_time).
For $\lambda=20$, the trajectory has tried to remain in dynamics mode 1 at the
start of the trajectory but then hits the area of high epistemic uncertainty and favours avoiding
this region over remaining in dynamics mode 1.
# These qualitative results are confirmed in Table ref:tab-results.

# Although we have not tested it here, we believe that our approach is theoretically sound and
Although not tested, we believe that our approach is theoretically sound and
can easily be extended to more than two dynamics modes.
However, it is interesting to consider if this is even necessary given our goals.
For example, in the quadcopter experiment, we intentionally instantiated the transition dynamics model
with two dynamics modes, although in reality there could be more.
We engineered our desired dynamics mode to have a noise variance that we deemed operatable.
We then used the other dynamics mode to explain away all of the un-operatable modes.
We think that in most scenarios a similar approach could be followed.


# *** Clearpage :ignore:
# #+begin_latex
# \clearpage
# #+end_latex

*** Discussion
Although this method provides an elegant solution to finding trajectories that satisfy Goals 1 and 2, it
is not without its limitations.
Firstly, this approach does do not necessarily find trajectories that satisfy the dynamics constraints,
as it projects the problem onto the geodesic ODE.
As such, an interesting direction for future work could consider different methods for
incorporating the dynamics constraints into this algorithm.
Secondly, this algorithm exploits differential flatness to obtain the controls from the state trajectory
and geodesic ODE.
Therefore, this approach is only applicable in systems where the state and control can be expressed in terms
of a flat output.

Thirdly, this algorithm does not consider the full stochastic optimal control problem in cref:mode-soc-problem.
In reality, the geodesic ODE in cref:eq-2ode is stochastic because both the state and Jacobian are random
variables.
This algorithm assumes that the goals are encoded via the expected metric in cref:eq-expected-metric
such that the full stochastic problem can be simplified to a deterministic ODE.
In practice, this assumption appears to achieve both of our goals whilst offering an easier problem to solve.

** Control via Variaitional Inference
However, following a mean-field approximation the Jacobian can be assumed to be independent of the state,
resulting in the expected cost being given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\E \left[ J(\stateTraj, \controlTraj) \right] &=
\sum_{\timeInd=1}^\TimeInd
\E_{\stateDist}\left[ \stateDiff_\timeInd^T
\E_{\jacDist}\left[ \Jac(\state_\timeInd) \Jac(\state_\timeInd)^T \right]
 \stateDiff_\timeInd \right] \\
&= \sum_{\timeInd=1}^\TimeInd \stateDiffMean^T (\muJac \muJac^T + \covJac) \stateDiffMean
+ \text{tr}\left(
\left(\muJac \muJac^T + \covJac \right)
\stateDiffCov \right)
\end{align}
#+END_EXPORT
It's a bit hacky but we could assumed the Jacobian distribution is given by marginalising the state
and moment matching,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\mathbf{J}(\state_{\timeInd})) =
%\int \mathbf{J}(\state_{\timeInd}) \stateDist \text{d}\state_{\timeInd}
\int p(\mathbf{J}(\state_{\timeInd}) \mid \state_\timeInd)
\mathcal{N}(\state_{\timeInd} \mid \stateMean_\timeInd, \stateCov_\timeInd)
\text{d}\state_{\timeInd}
\approx \mathcal{N}(\mathbf{J}(\state_{\timeInd}) \mid \muJac, \covJac)
\end{align}
#+END_EXPORT
** Trajectory Optimisation via Latent ODE Collocation label:sec-traj-opt-geometric :noexport:
*** Intro :ignore:
The trajectory optimisation algorithm presented in this Section exploits the latent /geometry/ inferred by the
GP-based gating network from Chapter ref:chap-dynamics.
The geometry and GP posterior covariance associated with the latent gating functions
are used to formulate an objective function that encodes both Goals 1 and 2.
Instead of directly optimising this objective function,
an approach exploiting concepts of Riemannian geometry is used to implicitly optimise it.
More specifically, the trajectory optimisation is projected onto the continuous-time
geodesic ODE, whose solutions implicitly optimise the objective function.

#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Intuition for Geometric Approach} The geometry of the latent gating functions can be used to formulate a cost term
that favours trajectories remaining in a preferred dynamics mode.
The $g_{\text{mode}}$ term in Eq. \ref{eq-cost-trajectory} can be expressed as finding shortest
paths on the desired mode's gating function.
Intuitively, the length of a trajectory from $\mathbf{x}_0$ to $\mathbf{x}_f$ on the
manifold given by the desired mode's gating function,
increases when it passes over the contours; analogous to climbing a hill.
Given appropriate scaling of the gating function, shortest trajectories between two locations are
those that attempt to follow the contours, i.e. remain in a single mode by not climbing up or down any hills.
\end{myquote}
#+END_EXPORT

\todo{state dim or state and control dim??}
Section ref:sec-riemannian-geometry recaps concepts from Riemannian
geometry before detailing how they are extended to probabilistic geometries
and
used to project the trajectory optimisation onto the latent geodesic ODE.
It then details how this latent geodesic ODE is solved using direct collocation.
Section ref:sec-traj-opt-results gives results of the method tested on a real-world
velocity-controlled quadcopter example.

*** Concepts of Riemannian Geometry label:sec-riemannian-geometry
The first trajectory optimisation algorithm presented in this chapter exploits the fact that Goals 1 and 2
are encoded as shortest trajectories (aka geodesic trajectories),
on a latent Riemannian manifold parameterised by the gating network from Chapter ref:chap-dynamics.
The necessary concepts of Riemannian geometry that are needed to understand this section are detailed here.

**** Lengths in Euclidean Spaces :ignore:
\newline
*Lengths in Euclidean Spaces*
The $l^2$ norm (aka Euclidean norm) provides an intuitive notion for the length of a
vector $\manifoldInput \in \manifoldDomain \subseteq \R^\manifoldDim$
in a Euclidean space. Under the $l^2$ norm, the length of a trajectory
${\trajectory = \{\manifoldInput(t) \in \manifoldDomain \quad \forall t \in [t_0, t_f]\}}$ is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-euclidean-length}
\text{Length}\left(\trajectory\right)
&= \int_{t_0}^{t_f}\left\|\dot{\manifoldInput}(t)\right\|_2 \mathrm{d}t,
\end{align}
#+END_EXPORT
where Newton's notation has been used to denote differentiation with respect to time $t$.
As a norm can be expressed for any space endowed with an inner product, it is possible to
calculate lengths of trajectories on manifolds endowed with an inner product.
# Just as the dot product is the inner product of the Euclidean space, it is possible to
# define inner products for Riemannian spaces (also known as Riemannian manifolds).
# Remembering that a norm can be expressed for any space endowed with an inner product,
# provides us with the tools to for calculating lengths on manifolds.

**** Lengths on Riemannian Manifolds :ignore:
# *Lengths on Riemannian Manifolds*
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Riemannian Manifolds}
In this dissertation if suffices to consider manifolds $\manifold$ defined by a mapping
function,
%#+BEGIN_EXPORT latex
\begin{equation} \label{eq-manifold-function}
\manifoldFunction : \manifoldDomain \rightarrow \manifoldCodomain,
\end{equation}
%#+END_EXPORT
where $\manifoldDomain$ and $\manifoldCodomain$ are open subsets of Euclidean spaces.
The manifold $\manifold$ is given by $\manifold = \manifoldFunction(\manifoldDomain)$ and is said to
be immersed in the ambient space $\manifoldCodomain$.
Riemannian manifolds can intuitively be seen as $d\text{-dimensional}$ curved surfaces with a smoothly
varying positive-definite inner product, governed by the Riemannian metric $\metricTensor$ \citep{carmoRiemannian1992}.
\begin{definition}[Riemannian Metric]
A Riemannian metric $\metricTensor$,
on a manifold $\manifold$, is a smooth function
$\metricTensor : \manifoldDomain \rightarrow \R^{\ManifoldDim \times \ManifoldDim}$
that assigns a symmetric positive definite matrix to any point in $\manifoldDomain$.
%A Riemannian metric $\metricTensor$ on a
%manifold $\manifold$ is a symmetric and positive definite matrix which defines
%a smoothly varying inner product
%$\langle \dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \rangle_{\mathbf{x}} = \dot{\mathbf{x}}_a^T \mathbf{G}(\mathbf{x}) \dot{\mathbf{x}}_b$
%in the tangent space $T_{\mathbf{x}}\manifold$, for each point $\mathbf{x} \in \manifold$ and
%$\dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \in T_{\mathbf{x}}\manifold$.
\end{definition}
Intuitively, the metric forms a local inner product in $\manifoldDomain$ that informs how to measure lengths
on the manifold $\manifold$, locally in $\manifoldDomain$.
Riemannian manifolds locally resemble Euclidean spaces $\R^\manifoldDim$ and have
globally defined differentiable structure.
\end{myquote}
#+END_EXPORT
*Lengths on Riemannian Manifolds*
The length of a trajectory $\trajectory$ on a manifold $\manifold$, can be calculated
by mapping it through the function $\manifoldFunction$, and
using Eq. ref:eq-euclidean-length,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-length}
\text{Length}\left(\manifoldFunction(\trajectory)\right)
&= \int_{t_0}^{t_f}\left\|\dot{\manifoldFunction}(\manifoldInput(t))\right\|_2 \mathrm{d}t.
\end{align}
#+END_EXPORT
Applying the chain-rule allows Eq. ref:eq-manifold-length to be expressed in terms of the Jacobian and the
velocity,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-length-chain}
\text{Length}\left(\manifoldFunction(\trajectory)\right)
&= \int_{t_0}^{t_f}\left\|\jacobian \dot{\manifoldInput}(t)\right\|_2 \mathrm{d}t, \\
\jacobian &= \frac{\partial \manifoldFunction}{\partial \manifoldInput(t)} \in \R^{1 \times \ManifoldDim}.
\end{align}
#+END_EXPORT
This implies that the length of a trajectory on the manifold $\manifold$,
can be calculated in the input space $\manifoldDomain$,
using a locally defined norm,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-norm}
\left\|\jacobian \dot{\manifoldInput}(t)\right\|_2
&= \sqrt{ \left( \jacobian \dot{\manifoldInput}(t) \right)^T
\jacobian \dot{\manifoldInput}(t)} \nonumber \\
&= \sqrt{\dot{\mathbf{x}}^T(t) \metricTensor_{\mathbf{x}_t} \dot{\manifoldInput}(t)}
\defeq \left\|\dot{\manifoldInput}(t)\right\|_{\metricTensor_{\manifoldInput_t}},
%= \left\|\dot{\mathbf{x}}(t)\right\|_{\mathbf{G}(\mathbf{x}(t))}.
\end{align}
#+END_EXPORT
where $\metricTensor_{\mathbf{x}_t} = \jacobian^T \jacobian}$
is a symmetric positive definite matrix
(akin to a local Mahalanobis distance measure), known as the natural Riemannian metric.
The length of a trajectory on a manifold $\manifold$, endowed with the metric $\metricTensor$,
can then be calculated as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-manifold-length-G}
\text{Length}(\manifoldFunction(\trajectory))
&= \int_{t_0}^{t_f}\left\|\dot{\manifoldInput}(t)\right\|_{\metricTensor_{\mathbf{x}_t}} \mathrm{d}t.
\end{align}
#+END_EXPORT

**** Geodesics :ignore:
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Geodesics}
Given this method for calculating lengths on Riemannian manifolds,
it is trivial to define the notion of a shortest trajectory, or geodesic, as a length minimising trajectory.
Formally, a geodesic is defined as follows,
\begin{definition}[Geodesic]
Given two points $\manifoldInput_0, \manifoldInput_f \in
\manifold$, a Geodesic is a length minimising trajectory (curve)
$\trajectory_g$ connecting the points, such that,
\begin{align} \label{eq-geodesic}
  \trajectory_{g} =\arg &\min_{\trajectory} \operatorname{Length}(\trajectory) \\
\text{s.t.} \quad  \manifoldInput(t_0)&=\manifoldInput_{0} \\
  \manifoldInput(t_f)&=\manifoldInput_{f}.
\end{align}
\end{definition}
\end{myquote}
#+END_EXPORT
An important observation from cite:carmoRiemannian1992, is that geodesics
satisfy a continuous-time $2^{\text{nd}}$ order ODE, given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-2ode}
 \ddot{\manifoldInput}(t)
&= \geodesicFunction(t, \manifoldInput, \dot{\manifoldInput}) \nonumber \\
&=-\frac{1}{2} \metricTensor^{-1}(\manifoldInput(t))\left[
\frac{\partial \operatorname{vec}[\metricTensor(\manifoldInput(t))]}{\partial \manifoldInput(t)}
\right]^{T}\left(\dot{\manifoldInput}(t) \otimes \dot{\manifoldInput}(t)\right),
\end{align}
#+END_EXPORT
where $\operatorname{vec}[\metricTensor(\manifoldInput(t)])$ stacks the columns of $\metricTensor(\manifoldInput(t))$
and $\otimes$ denotes the Kronecker product.
The implication of Eqs. ref:eq-geodesic and ref:eq-2ode is that trajectories that are solutions
to the $2^{\text{nd}}$ order ODE in Eq. ref:eq-2ode implicitly minimise the objective,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-length-objective}
J_{\text{geodesic}} &= \min \int_{t_0}^{t_f}\left\|\dot{\manifoldInput}(t)\right\|_{\metricTensor(\mathbf{x}(t))} \mathrm{d}t.
\end{align}
#+END_EXPORT
Given this observation, computing geodesics involves finding a solution to Eq. ref:eq-2ode
with $\manifoldInput(t_0) = \manifoldInput_0$ and $\manifoldInput(t_f) = \manifoldInput_f$.
This is a boundary value problem with a smooth solution so it can be solved using
any BVP solver, e.g. (multiple) shooting and collocation methods.

*** Geometric Objective Function
**** old :ignore:
# We will now instantiate Eq. ref:eq-cost-trajectory following two different approaches that use
# information learned by our transition dynamics model.
# The first objective is simple to construct and is intuitively the "go to" objective function for our problem.
# The second objective is more involved and requires the concepts of Riemannian geometry
# from Section ref:sec-riemannian-geometry to be extended to probabilistic geometries.

# **** Simple cost function :ignore:noexport:
# *Simple Cost Function* Remembering that the gating network indicates the probability of being in a particular dynamics mode at a given
# time step, and that the Bayesian formulation of the dynamics model
# lends itself to well-calibrated uncertainty estimates,
# the objective function can be instantiated as,
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-cost-trajectory-learned}
# J &= \int_{t_0}^{t_f} \underbrace{- \Pr(\alpha(t)=k^*)}_{g_{\text{mode}}}
# + \lambda \underbrace{\V_{\text{epistemic}}\left[\latentFunc(\mathbf{x}(t), \mathbf{u}(t))\right]}_{g_{\text{epistemic}}}  \text{d}t,
# \end{align}
# #+END_EXPORT
# where the variance term $\V_{\text{epistemic}}$ is due to the epistemic uncertainty arising from
# learning $f$ from observations and $\lambda$ is a user-tuneable parameter.
# This objective function is fairly intuitive,
# we want to maximise the probability of being in our desired mode and we want to minimise
# the amount of variance (due to epistemic uncertainty) introduced over the trajectory.

# \todo{should probably test this and comment on why we don't use it...}

**** intro :ignore:
Now that the relevant concepts from Riemannian geometry have been introduced, the objective function
for the first trajectory optimisation can be detailed.
Given the desired mode's gating function,
$\desiredGatingFunction : \gatingDomain \rightarrow \gatingCodomain$,
this trajectory optimisation algorithm exploits the fact that length minimising trajectories
on the manifold $\desiredManifold=\desiredGatingFunction(\gatingDomain)$,
endowed with the metric $\desiredMetricTensor$, where,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-desired-metric-tensor}
\desiredMetricTensor &= \desiredJacobian^T \desiredJacobian, \\
\desiredJacobian &= \frac{\partial \desiredGatingFunction}{\partial \manifoldInput(t)} \in \R^{1 \times \gatingDim},
\end{align}
#+END_EXPORT
will attempt to remain in the desired dynamics mode $\desiredMode$ (i.e. encode Goal 1).
Based on this intuition, the following objective function,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-geodesic-objective}
J_{\text{geometric}} = \int_{t_0}^{t_f}\left\|\dot{\manifoldInput}(t)\right\|_{\desiredMetricTensor(\manifoldInput(t))} \mathrm{d}t.
\end{align}
#+END_EXPORT
encodes the $g_{\text{mode}}$ term from Eq. ref:eq-cost-trajectory.
This objective encodes the $g_{\text{mode}}$ term as length minimising trajectories,
however, it has not addressed the $g_{\text{epistemic}}$ term.
Observing that the metric tensor will have a probability distribution induced over
it by the gating function's GP,
the $g_{\text{epistemic}}$ term can be encoded
by extending the metric $\desiredMetricTensor$ to probabilistic geometries.

**** Extension to Probabilistic Geometries label:sec-prob-geo
# Given the Jabobian
# of the desired mode's gating function
# $\jacobian=\frac{\partial \desiredGatingFunction}{\partial \manifoldInput} \in \R^{P \times \StateDim}$
# Each gating function's output is one-dimensional in our case.

Following cite:tosiMetrics2014 this work formulates a metric tensor that captures the variance in the manifold
via a probability distribution.
First note that as the differential operator is linear, the derivative of a GP is also a GP.
Therefore, the metric tensor $\desiredMetricTensor$ in Eq. ref:eq-desired-metric-tensor is the outer product of
two normally distributed random variables.
As such, the metric tensor $\desiredMetricTensor$ is also a random variable, following
a non-central Wishart distribution citep:andersonNonCentral1946,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-metric-dist}
  \desiredMetricTensor \sim
  \mathcal{W}_{\StateDim}\left(P, \covJac, \mathbb{E}\left[\Jac^{T}\right] \mathbb{E}[\Jac]\right),
\end{align}
#+END_EXPORT
where $P$ is the number of degrees of freedom (always one in our case), and
$\E\left[\desiredJacobian\right]$ and $\covJac$ are the mean and covariance matrices
associated with the GP over the Jacobian.
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Gaussian Process Jacobain}
The Jacobian $\testJac = \Jac(\testInput)$ of a function modelled as a GP,
evaluated at a new input, $\testInput$,
is jointly Gaussian with the function evaluated at the training inputs $\allInput$.
The conditional distribution over $\testJac$ can therefore easily be obtained using
the properties of multivariate normal distributions,
\begin{align} \label{eq-predictive-jacobian-dist}
p\left(\testJac | \HDes, \testInput, \allInput \right)
&= \mathcal{N}\left(\testJac \mid \muJac, \covJac \right) \\
\muJac &= \frac{\partial \modeDes{\mu}}{\partial \testInput} + \dK \iKxx \left( \HDes - \modeDes{\bm\mu} \right) \\
\covJac &= \ddK - \dK \iKxx \dK^{T}
\end{align}
%&= \mathcal{N}\left(\testJac \mid \dK \iKxx \left( \HDes - \modeDes{\bm\mu} \right), \ddK - \dK \iKxx \dK^{T} \right) \\
with covariance matrices given by,
\begin{align} \label{eq-joint-jacobian-dist}
\Kxx &= \modeDes{k}\left( \allInput, \allInput \right) \in \R^{\NumData \times \NumData} \\
\dK &= \frac{\partial \modeDes{k}\left(\testInput, \allInput\right)}{\partial \testInput} \in \R^{\StateDim \times \NumData} \\
\ddK &= \frac{\partial^2 \modeDes{k}\left(\testInput, \testInput \right)}{\partial \testInput \partial \testInput} \in \R^{\StateDim \times \StateDim}
\end{align}
where $\modeDes{\mu}$ and $\modeDes{k}$ are the mean and covariance functions
associated with the desired mode's gating function respectively.
Eq. \ref{eq-predictive-jacobian-dist} is a $\StateDim\text{-dimensional}$ multivariate Normal distribution.
\end{myquote}
#+END_EXPORT
The expected value of the metric tensor in Eq. ref:eq-metric-dist is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-metric}
  \E[\desiredMetricTensor] = \E[\mathbf{J}^T] \E[\mathbf{J}] + \covJac.
\end{align}
#+END_EXPORT
Importantly, this expected metric tensor includes a covariance term $\covJac$,
which implies that lengths on the manifold increase in areas of high covariance.
This is a desirable behaviour because it encourages length minimising trajectories
to avoid regions of the learned dynamics with high epistemic uncertainty,
encoding the $g_\text{epistemic}$ cost term in Eq. ref:eq-cost-trajectory.
The model in Chapter ref:chap-dynamics is built upon sparse GP approximations,
so the Jacobian in Eq. ref:eq-predictive-jacobian-dist must be extended for such approximations.

#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Sparse Gaussian Process Jacobian}
In the sparse GP setting, the predictive distribution over the Jacobian given a new
input, can be obtained by conditioning on the inducing variables, $\gatingInducingOutput$,
and then marginalising them with respect to the variational distribution, $\gatingInducingVariational$.
\begin{align}
p(\testJac \mid \testInput, \gatingInducingInput) &=
\int \gatingInducingVariational
p(\testJac \mid \testInput, \gatingInducingOutput, \gatingInducingInput)
\text{d} \gatingInducingOutput
&= \mathcal{N}\left( \testJac \mid \muJac, \covJac \right)
\end{align}

\todo{finish this box}

Sparse approximations are based on hallucinated inducing variables that are samples from the same GP prior,
\begin{equation}
p\left(\uHDes \mid \zHDes \right) &=\mathcal{N}\left(\uHDes \mid \modeDes{\mu}(\zHDes), \modeDes{k}(\zHDes, \zHDes) \right).
\end{equation}
In the sparse approximation setting it is these inducing variables that are jointly
Gaussian with the Jacobian at a new test location,
\begin{align} \label{eq-joint-jacobian-dist-sparse}
\Kzz &= \modeDes{k}\left( \zHDes, \zHDes \right) \in \R^{\NumInd \times \NumInd} \\
\dKz &= \frac{\partial \modeDes{k}\left(\testInput, \zHDes \right)}{\partial \testInput} \in \R^{\StateDim \times \NumInd},
\end{align}

In Section \ref{sec-inference} we adopted a variational approach
and approximated the inducing variable's posterior with a variational distribution,
\begin{equation} \label{eq-}
p\left(\uHDes \mid \zHDes \right) \approx \qDes = \mathcal{N}(\uHDes \mid \mDes, \SDes),
\end{equation}
and treated $\mDes$ and $\SDes$ as variational parameters to learn.
In order to maintain the positive-definiteness of $\SDes$, we actually utilised the Cholesky
decomposition $\SDes = \mathbf{L}\mathbf{L}^T$ and optimised $\mathbf{L}$.
To obtain the predictive distribution over the Jacobian at a new input location $\testInput$
we must condition on the inducing variable $\uHDes$ and then integrate it out with $\qDes$,
which is a GP,
\begin{align} \label{eq-predictive-jacobian-dist-sparse}
\small
q&\left(\testJac \mid \testInput, \zHDes \right)
=\int \qDes p\left(\testJac \mid \testInput, \uHDes, \zHDes \right)
\text{d} \uHDes \\
&= \mathcal{N}\left(\testJac \mid
\dKz \Kzz^{-1} \left( \mDes - \modeDes{\hat{\bm\mu}} \right),
\ddK - \dKz \iKzz \left( \Kzz - \SDes \right) \iKzz \dKz^{T} \right). \nonumber
\normalsize
\end{align}
\todo{assumed that GP jacobian has zero mean}
Note that when $\qDes=\pDes$ Eq. \ref{eq-predictive-jacobian-dist-sparse} reduces to the original marginal
distribution $p(\testJac \mid \testInput)$.
\end{myquote}
#+END_EXPORT



#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Sparse Gaussian Process Jacobian}
Sparse approximations are based on hallucinated inducing variables that are samples from the same GP prior,
\begin{equation}
p\left(\uHDes \mid \zHDes \right) &=\mathcal{N}\left(\uHDes \mid \modeDes{\mu}(\zHDes), \modeDes{k}(\zHDes, \zHDes) \right).
\end{equation}
In the sparse approximation setting it is these inducing variables that are jointly
Gaussian with the Jacobian at a new test location,
\begin{align} \label{eq-joint-jacobian-dist-sparse}
p(\testJac, \uHDes \mid \testInput, \zHDes) &=
 \mathcal{N}\left(
\left[\begin{array}{c}
        \testJac \\
        \uHDes
      \end{array}\right] \mid
\left[\begin{array}{c}
        % {0} \\
        \partial\modeDes{\hat{\bm\mu}} \\
        \modeDes{\hat{\bm\mu}}
      \end{array}\right], \left[\begin{array}{cc}
                              \ddK & \dKz^{T} \\
                              \dKz & \Kzz
                          \end{array}\right]\right) \\
\Kzz &= \modeDes{k}\left( \zHDes, \zHDes \right) \in \R^{\NumInd \times \NumInd} \\
\dKz &= \frac{\partial \modeDes{k}\left(\testInput, \zHDes \right)}{\partial \testInput} \in \R^{\StateDim \times \NumInd},
\end{align}
In Section \ref{sec-inference} we adopted a variational approach
and approximated the inducing variable's posterior with a variational distribution,
\begin{equation} \label{eq-}
p\left(\uHDes \mid \zHDes \right) \approx \qDes = \mathcal{N}(\uHDes \mid \mDes, \SDes),
\end{equation}
and treated $\mDes$ and $\SDes$ as variational parameters to learn.
In order to maintain the positive-definiteness of $\SDes$, we actually utilised the Cholesky
decomposition $\SDes = \mathbf{L}\mathbf{L}^T$ and optimised $\mathbf{L}$.
To obtain the predictive distribution over the Jacobian at a new input location $\testInput$
we must condition on the inducing variable $\uHDes$ and then integrate it out with $\qDes$,
which is a GP,
\begin{align} \label{eq-predictive-jacobian-dist-sparse}
\small
q&\left(\testJac \mid \testInput, \zHDes \right)
=\int \qDes p\left(\testJac \mid \testInput, \uHDes, \zHDes \right)
\text{d} \uHDes \\
&= \mathcal{N}\left(\testJac \mid
\dKz \Kzz^{-1} \left( \mDes - \modeDes{\hat{\bm\mu}} \right),
\ddK - \dKz \iKzz \left( \Kzz - \SDes \right) \iKzz \dKz^{T} \right). \nonumber
\normalsize
\end{align}
\todo{assumed that GP jacobian has zero mean}
Note that when $\qDes=\pDes$ Eq. \ref{eq-predictive-jacobian-dist-sparse} reduces to the original marginal
distribution $p(\testJac \mid \testInput)$.
\end{myquote}
#+END_EXPORT

**** Old :noexport:
The trajectory optimisation is projected onto a continuous-time
ODE whose solutions are geodesics on a probabilistic manifold, induced
by one of the latent gating function GPs.
Solutions to this ODE are trajectories that remain in a single dynamics mode
(where possible) and avoid regions of the dynamics that cannot be predicted confidently.
This latent geodesic ODE is solved using a Hermite-Simpson collocation method cite:kellyIntroduction2017.

The second stage,
this gating function acts as a coordinate map for a latent Riemannian manifold on which
geodesics are solutions to our trajectory optimisation problem.
Geodesics on this manifold satisfy a continuous-time second-order ODE.

A set of collocation constraints are derived that ensure trajectories are solutions to this ODE,
implicitly solving the trajectory optimisation problem.

Motivated by trajectory optimisation, this work adopts (and extends) the well-known
mixture of GP experts (MoGPE) method with a GP-based gating network to learn a time-invariant
transition dynamics model cite:trespMixtures2000a.
The trajectory optimisation exploits the geometric structure
learned by the GP-based gating network along with its well calibrated uncertainty estimates.
The trajectory optimisation is projected onto a continuous-time
ODE whose solutions are geodesics on a probabilistic manifold, induced
by one of the latent gating function GPs.
Solutions to this ODE are trajectories that remain in a single dynamics mode
(where possible) and avoid regions of the dynamics that cannot be predicted confidently.
This latent geodesic ODE is solved using a Hermite-Simpson collocation method cite:kellyIntroduction2017.

*** Implicit Trajectory Optimisation
# The previous section formulated a geometric objective function that encodes Goals 1 and 2.
# This work exploits the fact that solutions to the second-order ODE
# in Eq. ref:eq-2ode implicitly optimise the objective.
Solving the second-order ODE in Eq. ref:eq-2ode
is equivalent to solving the trajectory optimisation in Eq. ref:eq-traj-opt
with the objective function in Eq. ref:eq-geodesic-objective
subject to the same boundary conditions.

*Differential Flatness*
Since neither $\dot{\mathbf{x}}(t_0)$ nor $\dot{\mathbf{x}}(t_f)$ are known, Eq. ref:eq-2ode cannot
be solved with simple forward or backward integration.
Instead, the problem is transcribed using the approach of differential
flatness cite:milamNew2000,rossPseudospectral2004.
A set of outputs $\mathbf{z}(t)$ are defined such that the
states $\mathbf{x}(t)$ and controls $\mathbf{u}(t)$ can be
expressed in terms of the flat output $\mathbf{z}(t)$ and a finite number of its derivatives,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\mathbf{x}(t) &= A(\mathbf{z}(t), \dot{\mathbf{z}}(t), \ldots) \\
\mathbf{u}(t) &= B(\mathbf{z}(t), \dot{\mathbf{z}}(t), \ldots).
\end{align}
#+END_EXPORT
In the velocity-controlled quadcopter example, the flat output is the
state $\mathbf{z}(t) = \mathbf{x}(t)$
and the control is simply the state derivative
$\mathbf{u}(t) &= \dot{\mathbf{z}}(t)$.
The original trajectory optimisation problem can then be converted to finding $\mathbf{z}(t)$
for $t \in [t_0, t_f]$ subject to the boundary conditions and the dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-diff-flat-ode}
\ddot{\mathbf{x}}(\mathbf{z}(t), \dot{\mathbf{z}}(t)) &= f_G(t, \dot{\mathbf{x}}(\mathbf{z}(t), \dot{\mathbf{z}}(t)), \mathbf{x}(\mathbf{z}(t), \dot{\mathbf{z}}(t))).
\end{align}
#+END_EXPORT
*Collocation*
Collocation methods are used to transcribe continuous-time trajectory optimisation problems into
nonlinear programs, i.e. constrained parameter optimisation cite:kellyIntroduction2017,fahrooDirect2000  .
The expected metric in Eq. ref:eq-expected-metric is substituted into
Eq. ref:eq-diff-flat-ode and solved via direct collocation.
This work implements a simple Hermite-Simpson collocation method that enforces the state
derivative interpolated by the polynomials to equal the geodesic ODE $f_G$
at the midpoints between a set of $I$ collocation points
$\{\mathbf{z}_i\}_{i=1}^I$. This is achieved through the collocation defects,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-defect}
\Delta_{i+\frac{1}{2}} &= \ddot{\mathbf{z}}_{{i+\frac{1}{2}} - f_G(t_{{i+\frac{1}{2}}, \dot{\mathbf{z}}_{{i+\frac{1}{2}},\mathbf{z}_{{i+\frac{1}{2}})
\end{align}
#+END_EXPORT
where $\ddot{\mathbf{z}}_{i+\frac{1}{2}}, \dot{\mathbf{z}}_{i+\frac{1}{2}}, {\mathbf{z}}_{i+\frac{1}{2}}$
are obtained by interpolating between $\mathbf{z}_i$ and $\mathbf{z}_{i+1}$.
Eq. ref:eq-defect defines a set of constraints ensuring trajectories are solutions
to the geodesic ODE $f_G$.
The nonlinear program that this work solves is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\min_{\mathbf{z}(t), \dot{\mathbf{z}}(t)}& \int_{t_0}^{t_f} \costFunc(\state(\timeInd), \control(\timeInd)) \text{d}t \\
&\text{s.t. }\text{Eqs. \ref{eq-diff-flat-ode} and \ref{eq-defect}}  \\
\mathbf{x}&\left(\mathbf{z}(t_0), \dot{\mathbf{z}}(t_0) \right) = \mathbf{x}_0 \\
\mathbf{x}&(\mathbf{z}(t_f),\dot{\mathbf{z}}(t_f)) = \mathbf{x}_f
\end{align}
#+END_EXPORT
This is solved using Sequential Least Squares Programming (SLSQP) in SciPy.
\todo[inline]{Lagrance multipliers to make TO unconstrained}

** Trajectory Optimisation as Probabilistic Inference label:sec-traj-opt-inference
*** Maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\startStateDist}{\ensuremath{p(\state_{1})}}
\newcommand{\transitionDist}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \modeVarK)}}
%\newcommand{\trajectoryDist}{\ensuremath{p(\stateControlTraj)}}
\renewcommand{\controlDist}{\ensuremath{\policy(\control_\timeInd \mid \state_\timeInd)}}


\renewcommand{\trajectoryVarDist}{\ensuremath{q(\stateTraj, \controlTraj)}}
\newcommand{\controlTrajVarDist}{\ensuremath{q(\control_{0:\TimeInd} \mid \state_{0:\TimeInd})}}
\newcommand{\controlVarDist}{\ensuremath{q(\control_{\timeInd})}}

\newcommand{\optimalVar}{\ensuremath{\mathcal{O}}}
\newcommand{\monotonicFunc}{\ensuremath{g}}
\newcommand{\temperature}{\ensuremath{\gamma}}

\newcommand{\modeProb}{\ensuremath{\Pr(\modeVar_\timeInd = \modeInd \mid \state_\timeInd, \control_\timeInd)}}
\renewcommand{\optimalProb}{\ensuremath{\Pr(\optimalVar_\timeInd = 1 \mid \state_\timeInd, \control_\timeInd)}}
\newcommand{\optimalDist}{\ensuremath{P(\optimalVar_\timeInd \mid \state_\timeInd, \control_\timeInd)}}

\newcommand{\terminalCostDist}{\ensuremath{\Pr(\optimalVar_\TimeInd=1 \mid \state_\TimeInd)}}

\renewcommand{\marginalLikelihood}{\ensuremath{p(\optimalVar_{0:\TimeInd} = 1, \modeVar_{0:\TimeInd}=\desiredMode \mid \state_0)}}
%\newcommand{\jointDist}{\ensuremath{p(\optimalVar_{1:\TimeInd}=1, \modeVar_{1:\TimeInd}=\modeDes{\modeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})}}
%\newcommand{\jointDist}{\ensuremath{p(\optimalVar_{1:\TimeInd}, \modeVar_{1:\TimeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})}}
\renewcommand{\jointDist}{\ensuremath{p(\optimalVar_{0:\TimeInd}=1, \stateTraj, \controlTraj \mid \state_0})}}

\renewcommand{\trajectoryDist}{\ensuremath{p(\state_{1:\TimeInd}, \control_{0:\TimeInd} \mid \state_0)}}

\renewcommand{\targetState}{\ensuremath{\state_f}}
\renewcommand{\stateCostMatrix}{\ensuremath{\mathbf{Q}}}
\renewcommand{\controlCostMatrix}{\ensuremath{\mathbf{R}}}
\renewcommand{\terminalStateCostMatrix}{\ensuremath{\mathbf{H}}}

\newcommand{\stateMean}{\ensuremath{\bm\mu_{\state_\timeInd}}}
\newcommand{\stateCov}{\ensuremath{\bm\Sigma_{\state_\timeInd}}}
\newcommand{\terminalStateMean}{\ensuremath{\bm\mu_{\state_\TimeInd}}}
\newcommand{\terminalStateCov}{\ensuremath{\bm\Sigma_{\state_\TimeInd}}}
\newcommand{\controlMean}{\ensuremath{\bm\mu_{\control_\timeInd}}}
\newcommand{\controlCov}{\ensuremath{\bm\Sigma_{\control_\timeInd}}}
\newcommand{\stateDiff}{\ensuremath{\Delta \state}}
\newcommand{\stateDiffMean}{\ensuremath{\bm\mu_{\stateDiff_\timeInd}}}
\newcommand{\stateDiffCov}{\ensuremath{\bm\Sigma_{\stateDiff_\timeInd}}}

\newcommand{\objective}{\ensuremath{J_{\text{quadratic}}}}

\renewcommand{\priorPolicy}{\ensuremath{\policy_0}}
\renewcommand{\priorPolicyDist}{\ensuremath{p_{\policy_0}}}
#+END_EXPORT
*** Intro :ignore:
The trajectory optimisation algorithm presented in Section ref:sec-traj-opt-geometric can be viewed as
an indirect method.
\todo{Fact check it as an indirect method?}
The trajectory optimisation algorithm presented in this section is a direct method that considers the
full stochastic nature of the learned dynamics model. In particular, it considers uncertainty propagation
through the dynamics.
Given the SOC problem in Eq. ref:eq-mode-soc-problem, the trajectory optimisation algorithm presented here
builds off of the control-as-inference framework citep:toussaintProbabilistic2006,kappenOptimal2013,
which frames optimal control as probabilistic inference.
Translating optimal control to inference can be viewed as a subset of probabilistic numerics
citep:hennigProbabilistic2015, where statistical methods are used to solve numerical problems,
providing uncertainty quantification, regularisation and faster convergence.
This is achieved by assuming access to a dynamics model and
embedding the cost function into the likelihood.

This section firsts recaps the control-as-inference framework and then details how the framework is extended
to find mode remaining trajectories.

\todo[inline]{implications of approximating expected Riemannian metric? Effect of using mean state for G(x) instead of integrating}

*** Cost Functions as Likelihoods
To formulate optimal control as probabilistic inference
it is first embedded into a graphical model (see Figure ref:fig-basic-control-graphical-model).
The joint probability model (over a trajectory) is augmented with an
additional variable to encode the notion of cost (or reward)
over the trajectory (see Figure ref:fig-augmented-control-graphical-model).
The new variable is a Bernoulli random variable $\optimalVar_\timeInd \in \{0, 1\}$, that indicates if
time step $\timeInd$ is /optimal/ $\optimalVar_\timeInd=1$, or not /optimal/ $\optimalVar_\timeInd=0$.
The likelihood distribution can be formulated by mapping the negative
cost through a monotonically increasing function $\monotonicFunc$,
giving the likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-monotonicOptimalityLikelihood}
\optimalProb &\coloneqq \monotonicFunc( -\costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
\todo{compare different fmons?}
# #+BEGIN_EXPORT latex
# \todo[inline]{add detials on Cross Entropy Method (CEM) cite:chuaDeep2018 and other methods}
# \begin{myquote}
# As shown in \cite{okadaVariational2020}, the choice of monotonic function,
# $\monotonicFunc$,
# leads to the inference algorithm resembling different well-known algorithms.
# For example, selecting $\monotonicFunc$ to be the exponential function,
# \begin{align} \label{eq-}
# \optimalProb &\propto \text{exp}( -\costFunc(\state_\timeInd, \control_\timeInd))
# \end{align}
# leads to the algorithm resembling Model Predictive Path Integral (MPPI)
# \cite{williamsModel2017,williamsInformation2017}, whilst
# selecting $\monotonicFunc$ to as,
# \begin{align} \label{eq-}
# \optimalProb &\propto \mathbb{I}\left[( \costFunc(\state_\timeInd, \control_\timeInd)) \leq \costFunc_{\text{thd}} \right]
# \end{align}
# recovers an algorihtm resembling the Cross Entropy Method (CEM).
# \end{myquote}
# #+END_EXPORT
A common (and convenient) approach is to formulate
the likelihood using an exponential transform of
the cost. This results in a Boltzmann distribution where the
inverse temperature, $\temperature$, is used to scale the cost,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-exponentialOptimalityLikelihood}
\optimalProb &\propto \exp ( -\temperature\costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
The resulting negative log-likelihood,
for a single state-control trajectory \stateTraj, \controlTraj,
is an affine transformation of the cost,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-negative-log-likelihood-cost}
-\log \p(\optimalVar_{0:\TimeInd} \mid \stateTraj, \controlTraj)
&=  \temperature\costFunc(\stateTraj, \controlTraj),
\end{align}
#+END_EXPORT
which preserves convexity.
When the inverse temperature parameter is set to 1, (i.e. $\temperature=1$) the Maximum Likelihood (ML)
trajectory coincides with classical optimal control cite:toussaintRobot2009.

**** Quadratic Cost Functions
This work primarily focuses on quadratic costs, $\costFunc$, which are ubiquitous in control.

*Terminal Control Problems*
To find a trajectory between a start state, $\state_0$, and a target state, $\targetState$, at
time, $\TimeInd$, it is common to minimise the deviation of the final state, $\state_\TimeInd$,
from the desired target state $\targetState$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-terminal-cost-matrix-notation}
J = (\state_{\TimeInd} - \targetState)^T(\state_{\TimeInd} - \targetState).
\end{align}
#+END_EXPORT
This can also be written as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-terminal-cost-norm}
J = || \state_\TimeInd - \targetState ||^2,
\end{align}
#+END_EXPORT
where $|| \state_\TimeInd - \targetState ||^2$ denotes the L2 norm.
To allow greater generality, a user defined, real symmetric positive definite matrix,
$\terminalStateCostMatrix$, can be introduced,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-terminal-cost}
J &= (\state_{\TimeInd} - \targetState)^T \terminalStateCostMatrix (\state_{\TimeInd} - \targetState) \nonumber \\
&= || \state_\TimeInd - \targetState ||^2_{\terminalStateCostMatrix}.
\end{align}
#+END_EXPORT
When $\terminalStateCostMatrix$ is the identity matrix Eq. ref:eq-quadratic-terminal-cost-matrix-notation
is recovered.

*Minimum Control Effort Problems*
It is also common to find trajectories that minimise the expenditure of control effort.
The meaning of minimum control effort depends upon the specific problem.
It may refer to minimising the total expenditure of fuel, or to minimising the energy dissipated from the system.
Regardless of the specific interpretation, it is usually desirable to regularise the controls.
Similar to the terminal cost, it is common to use a quadratic form,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-cost-control}
J &= \sum_{\timeInd=0}^{\TimeInd-1} \control_{\timeInd}^T \controlCostMatrix \control_{\timeInd} \nonumber \\
&= || \control_\timeInd ||_\controlCostMatrix^2,
\end{align}
#+END_EXPORT
where $\controlCostMatrix$ is a user defined, real symmetric positive definite weight matrix.
This objective regularises the control trajectory under the L2 norm.
It is worth noting that the control weight matrix can be a function of time, $\controlCostMatrix(t)$.

#+BEGIN_EXPORT latex
\todo{Add tracking/regulating state cost term? And length of trajectory with $\dot{x}^T\dot{x}$?}
#+END_EXPORT

# *Tracking Problems*
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-quadratic-cost-state}
# J &= \state_{\timeInd}^T \stateCostMatrix \state_{\timeInd}
#   &= || \state_\timeInd ||_\stateCostMatrix^2
# \end{align}
# #+END_EXPORT
# where $\stateCostMatrix$ and $\controlCostMatrix$ are
# user defined, real symmetric positive semi-definite and real symmetric positive definite weight matrices respectively.
# It is worth noting that the state and control weight matrices can be functions of time, i.e.
# $\stateCostMatrix(t)$ and $\controlCostMatrix(t)$.
# This cost function regularises state
This work seeks to control the terminal state and regularise the controls so combines these two objectives as
follows,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadratic-cost-terminal-control}
\objective =
||\state_{\TimeInd} - \targetState ||^2_{\terminalStateCostMatrix}
+ \sum_{\timeInd=0}^{\TimeInd-1}
+ ||\control_{\timeInd}||^2_{\controlCostMatrix}.
\end{align}
#+END_EXPORT
Importantly, given a trajectory where each state and control are normally distributed,
${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$
and
${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$,
the expected cost for a trajectory can be calculated in closed-form,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-quadratic-cost}
%\E_{\stateTraj, \controlTraj} \left[ \costFunc_{\text{quadratic}}(\stateTraj, \controlTraj) \right]
\E_{\stateTraj, \controlTraj} \left[ \objective \right] =
\text{tr}(\terminalStateCostMatrix \terminalStateCov) +
(\terminalStateMean - \targetState)^T \terminalStateCostMatrix (\terminalStateMean - \targetState)
+ \sum_{\timeInd=1}^{\TimeInd-1}
%\text{tr}(\stateCostMatrix \stateCov_\timeInd)
%+ \stateMean_{\timeInd}^T \stateCostMatrix \stateMean_{\timeInd}
\text{tr}(\controlCostMatrix \controlCov )
+ \controlMean^T \controlCostMatrix \controlMean
\end{align}
#+END_EXPORT

It is worth noting that if a cost function is not quadratic then a quadratic approximation
can be obtained with a Taylor expansion.

*** graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
  \begin{subfigure}{.32\textwidth}
  \centering
   \resizebox{0.93\columnwidth}{!}{
 %  \resizebox{0.32\textwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);
    \end{tikzpicture}
    }
  \caption{Graphical model with states and controls.}
\label{fig-basic-control-graphical-model}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
  \centering
   \resizebox{0.93\columnwidth}{!}{
 %  \resizebox{0.32\textwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[obs, above=of u1] (o1) {$\optimalVar_1$};
      \node[obs, right=of o1] (o2) {$\optimalVar_2$};
      \node[obs, right=of o2] (o3) {$\optimalVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);
    \end{tikzpicture}
    }
  \caption{Graphical model with optimality variable.}
\label{fig-augmented-control-graphical-model}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
  \centering
   \resizebox{0.93\textwidth}{!}{
   %\resizebox{0.32\textwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[latent, above=of u1] (o1) {$\optimalVar_1$};
      \node[latent, right=of o1] (o2) {$\optimalVar_2$};
      \node[latent, right=of o2] (o3) {$\optimalVar_3$};

      \node[latent, below=of x1] (a1) {$\modeVar_1$};
      \node[latent, right=of a1] (a2) {$\modeVar_2$};
      \node[latent, right=of a2] (a3) {$\modeVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);

      \draw[->] (x1)--(a1);
      \draw[->] (u1) to [out=-90,in=30] (a1);
      \draw[->] (x2)--(a2);
      \draw[->] (u2) to [out=-90,in=30] (a2);
      \draw[->] (x3)--(a3);
      \draw[->] (u3) to [out=-90,in=30] (a3);
    \end{tikzpicture}
    }
  \caption{Graphical model with optimality and mode indicator variables.}
\label{fig-mode-augmented-control-graphical-model}
\end{subfigure}
\caption{Graphical models of control formulated as inference.}
\label{fig-control-graphical-model}
\end{figure}
#+END_EXPORT

*** Inference of Sequential Latent Variables
\newline
The joint probability for an optimal trajectory (i.e.
for $\optimalVar_\timeInd=1$ for all $\timeInd \in \{1,\ldots,\TimeInd\}$),
can be factorised using its Markovian structure,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-traj-opt-joint-dist}
\jointDist =
\underbrace{\terminalCostDist}_{\text{Terminal Cost}}
\prod_{\timeInd=0}^{\TimeInd-1}
\underbrace{\optimalProb}_{\text{Integral Cost}}
\underbrace{\transitionDist}_{\text{Dynamics}}
\underbrace{\controlDist}_{\text{Controller}}
\end{align}
\normalsize
#+END_EXPORT
where $\controlDist$ denotes the controller or policy.
cite:toussaintRobot2009 highlight that although the ML trajectory coincides with the classical
optimal trajectory, taking expectations over trajectories (i.e. calculating $\log p(\optimalVar_{0:\TimeInd}=1$)
is not equivalent to expected cost minimisation.
cite:rawlikStochastic2013 extend the concepts from  cite:toussaintRobot2009 to show the general relation
to classical SOC.
They introduce a prior policy, $\priorPolicy$, and the distribution over state-control trajectories
under this policy is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-prior-trajectory-dist}
\priorPolicyDist(\stateTraj, \controlTraj) =
p(\stateTraj, \controlTraj \mid \optimalVar_{0:\TimeInd}=1, \state_0) =
Z^{-1}
\controlledPolicyDist(\stateTraj, \controlTraj) \prod_{\timeInd=0}^\TimeInd \exp \left( - \temperature
\integralCostFunc(\state_\timeInd,\control_\timeInd) \right)
\end{align}
#+END_EXPORT
This distribution is conditioned on the optimality variable but generated by a
potentially uniform policy, $\priorPolicy$.
Intuitively $\controlledPolicyDist(\stateTraj, \controlTraj)$, is thought of as the controlled process, which
is not conditioned on optimality and $\priorPolicyDist(\stateTraj, \controlTraj)$,
as the posterior process, conditioned on optimality
but generated by a potentially uniform policy, $\priorPolicy$.
The dual problem is then to find the control policy, $\policy$, where the controlled process,
$\controlledPolicyDist(\stateTraj, \controlTraj)$,
matches the posterior process, $\priorPolicyDist(\stateTraj, \controlTraj)$.
Given $\priorPolicy$ is an arbitrary stochastic policy and $\mathbb{D}$ is the set of deterministic
policies, the problem,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-kl-control}
\policy_* &= \argmin_{\policy \in \mathbb{D}}\text{KL}(\controlledPolicyDist \mid\mid \priorPolicyDist) \nonumber \\
&= \argmin_{\policy \in \mathbb{D}} Z + \temperature
\E_{\controlledPolicyDist(\stateTraj, \controlTraj)}
\left[ \costFunc(\stateTraj, \controlTraj) \right]
+ \E_{\controlledPolicyDist(\stateTraj)} \left[ \text{KL}(\policy \mid\mid \priorPolicy) \right] \nonumber \\
&= \argmin_{\policy \in \mathbb{D}} Z +
\temperature \E_{\controlledPolicyDist(\stateTraj, \controlTraj)}
\left[ \costFunc(\stateTraj, \controlTraj) \right]
- \E_{\controlledPolicyDist(\stateTraj, \controlTraj)} \left[ \log \priorPolicy(\controlTraj \mid \stateTraj) \right]
- \E_{\controlledPolicyDist(\stateTraj)} \left[ H(\policy) \right],
\end{align}
#+END_EXPORT
is equivalent to the SOC problem in Eq. ref:eq-mode-soc-problem, with cost per stage,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-cost-per-stage}
\hat{\integralCostFunc}(\state_\timeInd, \control_\timeInd) = \integralCostFunc(\state_\timeInd, \control_\timeInd)
- \frac{1}{\temperature} \log \priorPolicy(\control_\timeInd \mid \state_\timeInd).
\end{align}
#+END_EXPORT
The problem in Eq. ref:eq-kl-control finds trajectories that balance
minimising expected costs,
$\E_{\controlledPolicyDist(\stateTraj, \controlTraj)} \left[ \costFunc(\stateTraj, \controlTraj) \right]$,
and selecting a policy, $\policy$, that is similar to the prior policy, $\priorPolicy$.
If the prior policy, $\priorPolicy$, is assumed to be uniform then
$\E_{\controlledPolicyDist(\stateTraj, \controlTraj)} \left[ \log \priorPolicy(\controlTraj \mid \stateTraj) \right]$
becomes constant and the optimised policy, $\policy_*$, is a balance of minimising expected costs and
maximising the policy's entropy
$\E_{\controlledPolicyDist(\stateTraj)} \left[ H(\policy) \right]$.

*Maximum Entropy Regularisation* Formulating trajectory optimisation in this way encodes the
maximum causal entropy principle, which is often used to achieve robustness,
in particular for inverse optimal control cite:ziebartModeling2010.
\todo{add something on ME being questionable for stochastic dynamics}
# It is important to note that the maximum entropy framework is problematic for stochastic dynamics.
# In essence, it assumes that the agent is able to control the dynamics as well as the controls in order
# to obtain optimal trajectories.

*Other Approaches* There are multiple approaches to performing inference in this graphical model.
Trading accuracy for computational complexity is often required for real-time control.
In this case, one approach would be to approximate the dynamics with linear or
quadratic approximations, as is done in iLQR/iLQG and DDP respectively.
Given linear dynamics,
the full graphical model in cref:eq-traj-opt-joint-dist can be
computed using approximate Gaussian message passing, for which effective methods exist cite:loeligerFactor2007.
The inference problem can then be solved using the
expectation maximisation algorithm for dynamical system estimation
citep:shumwayAPPROACH1982,ghahramaniLearning1999,schonSystem2011, with input estimation cite:watsonStochastic2021.

However, inference in nonlinear SSMs is

*** Mode Remaining Trajectory Optimisation as Variational Inference
**** maths :ignore:
#+BEGIN_EXPORT latex
%\renewcommand{\optimalProb}{\ensuremath{\Pr(\optimalVar_\timeInd = 1, \modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd)}}
%\renewcommand{\modeProb}{\ensuremath{\Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd)}}
\renewcommand{\modeProb}{\ensuremath{\Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd)}}
\renewcommand{\terminalCostDist}{\ensuremath{\Pr(\optimalVar_\TimeInd=1, \modeVar_\TimeInd=\desiredMode \mid \state_\TimeInd)}}
\renewcommand{\jointDist}{\ensuremath{p(\optimalVar_{0:\TimeInd}=1, \modeVar_{0:\TimeInd}=\desiredMode, \stateTraj, \controlTraj \mid \state_0})}}

\renewcommand{\transitionDist}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \modeVarK)}}

#+END_EXPORT

**** intro :ignore:
# The resulting probabilistic graphical model
# can be solved efficiently using recursive Bayesian inference over time.
Whilst real-time control requires efficient inference algorithms,
"offline" trajectory optimisation can trade in computational cost for greater accuracy.
This work is primarily interested in finding trajectories that attempt to remain in a
desired dynamics mode.
For the sake of simplicity, it considers the "offline" trajectory optimisation setting.
The increased computational time may hinder
its suitability to obtain a closed-loop controller via MPC citep:eduardof.Model2007 .
However, it can be used "offline" to generate reference trajectories for a tracking controller,
or for guided policy search in a model-based RL setting citep:levineGuided2013.
Alternatively, future work could investigate approximate inference methods for efficient state estimation
to aid with real-time control,
e.g. based on linear/quadratic approximations of the dynamics (iLQG/DDP).

*Augmented Graphical Model* In order to find trajectories that remain in a desired
dynamics mode, this work further augments
the graphical model with the mode indicator variable, $\modeVar \in \modeDomain$,
from Eq. ref:eq-multimodal-dynamics-disc.
The resulting graphical model is shown in Figure ref:fig-mode-augmented-control-graphical-model,
where the evidence is that $\optimalVar_{\timeInd}=1$ and $\modeVar_{\timeInd}=\desiredMode$ for
all $\timeInd \in \{0,\ldots,\TimeInd\}$.
The joint probability model is then given by,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-traj-opt-joint-dist-mode}
\jointDist =
&\underbrace{\terminalCostDist}_{\text{Terminal Cost}} \nonumber \\
\prod_{\timeInd=0}^{\TimeInd-1} \bigg[&
\underbrace{\optimalProb}_{\text{Integral Cost}}
\underbrace{\modeProb}_{\text{Mode Remaining Term}} \nonumber \\
&\underbrace{\transitionDist}_{\text{Dynamics}}
\underbrace{\controlDist}_{\text{Controller}} \bigg].
\end{align}
\normalsize
#+END_EXPORT


# Also note that the posterior is conditioned on the initial state $\state_0$.


# Intuitively, the posterior of interest is th
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# p(\state_{1:\TimeInd}, \control_{1:\TimeInd} \mid \optimalVar_{1:\timeInd}=1, \state_1) =
# \trajectoryDist
# \left[
# \prod_{\timeInd=1}^\TimeInd
# p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \optimalVar_{1:\TimeInd})
# \policy(\control_\timeInd \mid \state_\timeInd)
# p(\optimalVar_\timeInd=1 \mid \state_\timeInd, \control_\timeInd)
# \end{align}
# #+END_EXPORT

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# p(\state_{1:\TimeInd}, \control_{1:\TimeInd}\mid \optimalVar_{1:\TimeInd}=1, \state_1)
# =
# \prod_{\timeInd=1}^\TimeInd
# p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)
# p(\optimalVar_\timeInd=1 \mid \state_\timeInd, \control_\timeInd)
# p(\control_\timeInd \mid \state_\timeInd)
# \prod_{\timeInd=1}^\TimeInd
# \text{exp}\left( -\temperature \integralCostFunc(\state_\timeInd, \control_\timeInd) \right)
# \end{align}
# #+END_EXPORT

# In the RL setting the policy is parameterised with parameters, $\theta$, which are optimised and used
# to predict the controls.
# In contrast, the control setting is usually interested in optimising the states and controls directly.

# The object of interest is the distribution over the controls, $\controlTraj$,
# conditioned on the initial state, $\state_0$, all future time steps being optimal and in the
# desired dynamics mode,

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-optimal-control-message-passing}
# p(\controlTraj \mid \state_{0}, \optimalVar_{0:\TimeInd}=1, \modeVar_{0:\TimeInd}=\desiredMode) &=
# \frac{
# p(\optimalVar_{0:\TimeInd}=1 \mid \stateTraj, \controlTraj)
# p(\modeVar_{0:\TimeInd}=\desiredMode \mid \stateTraj, \controlTraj)
# p(\stateTraj, \controlTraj \mid \optimalVar_{0:\TimeInd}=1, \modeVar_{0:\TimeInd}=\desiredMode, \state_0)
# }{}
# \end{align}
# #+END_EXPORT

**** Variational Inference
This work draws on the connection between KL-divergence control citep:rawlikStochastic2013
and structured variational inference.
Whilst the derivation shown here differs from citep:rawlikStochastic2013, the underlying framework and
objective are the same.

# p(\stateTraj, \controlTraj \mid \)$
In variational inference the goal is to approximate a distribution $p(\mathbf{y})$
with another, potentially simpler distribution $q(\mathbf{y})$.
Typically this distribution $q(\mathbf{y})$ is selected to be a product of conditional distributions connected in a
chain or tree, which lends itself to exact inference.
In this work, the goal is to approximate the intractable distribution over optimal trajectories,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\stateTraj, \controlTraj \mid \state_0, \optimalVar_{0:\TimeInd}=1, \modeVar_{0:\TimeInd}=\desiredMode)
= \underbrace{\left[ \prod_{\timeInd=0}^{\TimeInd-1} \transitionDist \policy(\control_\timeInd \mid \state_\timeInd) \right]}_{\text{controlled dynamics}}
\exp \left( \sum_{\timeInd=0}^\TimeInd \costFunc(\state_\timeInd, \control_\timeInd) \right),
\end{align}
#+END_EXPORT
with the variational distribution,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
q(\stateTraj, \controlTraj)
= \prod_{\timeInd=0}^{\TimeInd-1} q(\state_{\timeInd+1} \mid \state_{\timeInd}, \control_\timeInd)
\controlVarDist.
\end{align}
#+END_EXPORT
Setting the variational dynamics $q(\state_{\timeInd+1} \mid \state_{\timeInd}, \control_\timeInd)$,
equal to the learned dynamics, ${\transitionDist}$
simplifies inference and prevents the agent from attempting to change the dynamics.

Variational inference seeks to optimise $q(\stateTraj, \controlTraj \mid \state_0)$
w.r.t. the variational lower bound (aka evidence lower bound or ELBO).
In this setup the evidence is that $\optimalVar_{\timeInd}=1$ and $\modeVar_{\timeInd}=\desiredMode$ for
all $\timeInd \in \{0,\ldots,\TimeInd\}$.
Also note that the posterior is conditioned on the initial state $\state_0$.
Given this, the evidence lower bound is given by,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-}
\text{log} \marginalLikelihood
&= \text{log} \E_{\trajectoryVarDist} \left[ \frac{\jointDist}{\trajectoryVarDist} \right] \nonumber \\
&= \text{log} \E_{\trajectoryVarDist} \left[
\frac{\prod_{\timeInd=0}^{\TimeInd-1} \optimalProb
\Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd)
\controlDist}{\prod_{\timeInd=0}^{\TimeInd-1}  \controlVarDist}
\right] \nonumber \\
&\geq \sum_{\timeInd=0}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
+ \sum_{\timeInd=0}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[
\log \Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd) \right] \nonumber \\
&\quad - \sum_{\timeInd=0}^{\TimeInd}
\text{KL}\left(\controlVarDist \mid\mid \policy(\control_\timeInd \mid \state_\timeInd) \right)  \coloneqq \mathcal{L},
\end{align}
\normalsize
#+END_EXPORT
where for notational conciseness the terminal values have been omitted.
Assuming a uniform prior policy $\policy(\control_\timeInd \mid \state_\timeInd)$ leads to the $\text{KL}$
term reducing to an entropy term and a constant,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-traj-opt-elbo}
\mathcal{L} = &\sum_{\timeInd=0}^{\TimeInd}
\underbrace{\E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]}_{\text{Expected cost}}
+ \sum_{\timeInd=0}^{\TimeInd} \underbrace{\E_{\controlVarDist q(\state_{\timeInd})} \left[
\log \Pr(\modeVar_\timeInd=\desiredMode \mid \state_\timeInd, \control_\timeInd) \right]}_{\text{Mode remainig term}} \nonumber \\
&+ \sum_{\timeInd=0}^{\TimeInd} \underbrace{\E_{\controlVarDist q(\state_{\timeInd})}
\left[\log\policy(\control_{\timeInd} \mid \state_\timeInd)\right]}_{\text{Constant}}
+ \sum_{\timeInd=0}^{\TimeInd} \underbrace{\mathcal{H}(\control_{\timeInd})}_{\text{Entropy}}
\end{align}
\normalsize
#+END_EXPORT
Calculating the objective in Eq. ref:eq-traj-opt-elbo requires simulating the trajectory in the desired
mode's GP dynamics.

*Predictions with Uncertain Inputs*
Consider the problem of predicting the next state  $\state_{\timeInd+1}$
given multivariate Normally distributed states,
${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$,
and controls,
${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$,
where $\desiredDynamicsFunc \sim \mathcal{GP}$.
This prediction problem corresponds to calculating the following integral,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-state-unc-prop}
p(\state_{\timeInd+1}) = \int \int
p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd) p(\control_\timeInd) p(\state_\timeInd)
\text{d}\state_{\timeInd} \text{d}\control_{\timeInd}
\end{align}
#+END_EXPORT
Approximate closed-form solutions exist for propagating uncertain inputs through GP models
cite:girardGaussian2003,kussGaussian2006,quinonero-candelaPropagation2003.
\todo{correct girard citation}
This work exploits the moment-matching approximation
implemented in the =uncertain_conditional= from GPflow cite:GPflow2017.
For more details on the approximation see Section 7.2.1 of cite:kussGaussian2006.
\todo{Add moment matching figure?}
This work propagates model uncertainty forwards by cascading such one-step predictions.


#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-traj-opt-elbo}
\mathcal{N}(\state_1 \mid M_1, V_1) \coloneq \mathcal{N}(\state_1)
\end{align}
\normalsize
#+END_EXPORT
The objective in Eq. ref:eq-traj-opt-elbo is calculated as follows:
1. Rollout the given control trajectory in the GP dynamics,
   - Propagate the state and control uncertainty via moment matching.
2. Rollout the given control trajectory in the GP dynamics,


# #+BEGIN_EXPORT latex
# \begin{algorithm}
# $i\gets 10$\;
# \for{$t \in [0, \ldots, \TimeInd$}
# {
# $\dynamicsFunc$
#     $i\gets i-1$\;
# }{
#     \If{$i\leq 3$}
#     {
#         $i\gets i+2$\;
#     }
# }
# \end{algorithm}
# #+END_EXPORT

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \text{log} \marginalLikelihood
# &= \text{log} \E_{\trajectoryVarDist} \left[ \frac{\jointDist}{\trajectoryVarDist} \right] \\
# &= \text{log} \E_{\trajectoryVarDist} \left[
# \frac{p(\optimalVar_\TimeInd \mid \state_{\TimeInd})
# \prod_{\timeInd=0}^{\TimeInd-1} \optimalProb \cancel{\transitionDist} \controlDist}{\prod_{\timeInd=0}^{\TimeInd-1} \cancel{\transitionDist} \controlVarDist}
# \right] \\
# &\geq \E_{\trajectoryVarDist} \left[
# \sum_{\timeInd=0}^{\TimeInd-1} \text{log} \optimalProb - \sum_{\timeInd=0}^{\TimeInd-1} \text{log}
# \frac{\controlVarDist}{\controlDist} \right] \\
# &= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
# + \sum_{\timeInd=1}^{\TimeInd} \underbrace{- \E_{\controlVarDist} \left[ \text{log} \controlVarDist \right]}_{\text{Entropy}} \\
# &= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
# + \sum_{\timeInd=1}^{\TimeInd} \mathcal{H}(\control_{\timeInd})
# \end{align}
# #+END_EXPORT

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \text{log} \marginalLikelihood &\geq
#  \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb
# - \text{log}\controlVarDist\right] \\
# &= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \text{log} \optimalProb \right]
# - \E_{\controlVarDist q(\state_{\timdInd})} \left[ \text{log}\controlVarDist \right] \\
# &= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
# + \sum_{\timeInd=1}^{\TimeInd} \underbrace{- \E_{\controlVarDist} \left[ \text{log} \controlVarDist \right]}_{\text{Entropy}} \\
# &= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
# + \sum_{\timeInd=1}^{\TimeInd} \mathcal{H}(\control_{\timeInd})
# \end{align}
# #+END_EXPORT

**** Approximate Inference for Dynamics Predictions
#+BEGIN_EXPORT latex
\renewcommand{\singleInput}{\ensuremath{\state_\timeInd, \control_\timeInd}}
\renewcommand{\singleInput}{\ensuremath{\hat{\state}_\timeInd}}
\renewcommand{\singleOutput}{\ensuremath{\Delta\state_{\timeInd+1}}}

\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput)}}
\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput)}}
%\newcommand{\expertInducingVariational}{\ensuremath{q(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\expertInducingVariational}{\ensuremath{q(\expertInducingOutput)}}
\newcommand{\expertsInducingVariational}{\ensuremath{q(\expertsInducingOutput)}}
\newcommand{\expertsVariational}{\ensuremath{q(\LatentFunc_\numData)}}
%\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \expertInducingOutput)}}
%\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))}}
\renewcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \singleInput, \expertInducingOutput)}}

\renewcommand{\expertVariational}{\ensuremath{q(\mode{\latentFunc}(\singleInput) \mid \singleInput)}}

\newcommand{\singleInputMean}{\ensuremath{\hat{\bm\mu}}}
\newcommand{\singleInputCov}{\ensuremath{\hat{\bm\Sigma}}}

%\newpage
#+END_EXPORT
The aformentioned trajectory optimisation technique relies on the ability to simulate the controlled
system using the learned dynamics model.
Recursively estimating the state of a nonlinear dynamical system is a common problem.
Exact Bayesian solutions in closed-form can only be obtained for a few special cases.
For example, the Kalman filter for linear Gaussian systems citep:kalmanNew1960 is exact.
In the nonlinear case, approximate methods are required to obtain efficient closed-form solutions.
\todo{cite Extended Kalman filter (EKF), Unscented Kalman filter (UKF) etc?}

Constructing approximate closed-form solutions based on the model in Chapter ref:chap-dynamics
is especially difficult, due to the exponential growth in the number of Gaussian components.
\marginpar{multimodal exponential growth}
Consider approximating the dynamics modes to be independent over a trajectory of length, $N$,
and recursively propagating each component through both modes.
This approximation would lead to the distribution over the final state consisting of
$\ModeInd^N$ Gaussian components.

# After the initial time step the state distribution will be a mixture of $\ModeInd$ modes.
# Assuming that the $\ModeInd$ components are independent and that each of the dynamics GPs are independent,
# propagating the components would result in a
# mode would result in the next state distribution containing $\ModInd^2$ modes.

# This is due to the explosion in the number of Gaussian components that would need to be modelled.

Luckily, this work is interested in remaining in a single dynamics mode $\desiredMode$,
which simplifies the problem.
Assuming that the controlled system remains in this desired dynamics mode,
the state trajectory can be simulated using only a single dynamics function,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\state_{\timeInd+1} &= \mode{\latentFunc}(\state_\timeInd, \control_\timeInd) + \state_\timeInd +  \mode{\epsilon}.
\end{align}
#+END_EXPORT
The model in Chapter ref:chap-dynamics represents each mode's dynamics function as a sparse variational GP,
giving the transition density as,
#+BEGIN_EXPORT latex
\begin{subequations}
\label{eq-sparse-gp-dynamics}
\begin{align}
p(\Delta\state_{\timeInd+1} \mid \singleInput, \modeVarK) &=
\E_{\expertVariational} \left[\singleExpertLikelihood \right] \label{eq-sparse-gp-dynamics-0} \\
\expertVariational &=  \E_{ \expertInducingVariational} \left[  \singleLatentExpertGivenInducing \right] \label{eq-sparse-gp-dynamics-1} \\
\expertInducingVariational &= \mathcal{N}\left( \expertInducingOutput \mid \mode{\mathbf{m}}, \mode{\mathbf{S}} \right)
\label{eq-sparse-gp-dynamics-2}
\end{align}
\end{subequations}
#+END_EXPORT
where the functional form of \expertVariational is given by,
#+BEGIN_EXPORT latex
\begin{align}
\expertVariational &=
\mathcal{N} \left( \mode{\latentFunc}(\singleInput) \mid
\mode{\mathbf{A}} \mode{\mathbf{m}},
\expertKernel(\singleInput, \singleInput)
+ (\mode{\mathbf{S}} - \expertKernel(\expertInducingInput,\expertInducingInput))
\mode{\mathbf{A}}^T
\right) \\
\mode{\mathbf{A}} &=
\expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}.
\end{align}
#+END_EXPORT
Remember that the variational posterior $\expertVariational$ is an approximation,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\expertVariational &\approx p(\mode{\latentFunc}(\singleInput) \mid \singleInput, \allInput, \allOutput),
\end{align}
#+END_EXPORT
that captures the joint distribution in the data through the inducing variables $\expertInducingOutput$.
Given a deterministic state-control input, $\singleInput$, Eq. ref:eq-sparse-gp-dynamics can be used
to calculate the density over the next state $\state_{\timeInd+1}$.
However, as both the state and control at a given time step could also be Gaussian distributed,
${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$,
and
${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$,
the joint distribution of the state and control, $p(\singleInput)$, is also Gaussian distributed,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\singleInput) = p(\state_{\timeInd}, \control_\timeInd) = p(\state_{\timeInd}) p(\control_\timeInd).
\end{align}
#+END_EXPORT

***** Predictions with Uncertain Inputs
Consider the problem of predicting the next state  $\state_{\timeInd+1}$
given multivariate Normally distributed states,
${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$,
and controls,
${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$,
where $\desiredDynamicsFunc \sim \mathcal{GP}$.
This prediction problem corresponds to calculating the following integral,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-state-unc-prop}
p(\state_{\timeInd+1}) = \int \int
p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd) p(\control_\timeInd) p(\state_\timeInd)
\text{d}\state_{\timeInd} \text{d}\control_{\timeInd}
\end{align}
#+END_EXPORT
Approximate closed-form solutions exist for propagating uncertain inputs through GP models
cite:girardGaussian2003,kussGaussian2006,quinonero-candelaPropagation2003.
\todo{correct girard citation}
This work exploits the moment-matching approximation
implemented in the =uncertain_conditional= from GPflow cite:GPflow2017.
For more details on the approximation see Section 7.2.1 of cite:kussGaussian2006.
\todo{Add moment matching figure?}
This work propagates model uncertainty forwards by cascading such one-step predictions.

# The transition density $\transitionDist$ can be obtained
# by recursive moment-matching
# This is the case when using a GP model to simulate more than a single time step.

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# q(\desiredDynamicsFunc(\singleInput) ) = \int  \underbrace{q(\desiredDynamicsFunc(\singleInput) \mid \singleInput)}_{\text{GP predictive dist}}
# p(\singleInput) \text{d}\singleInput.
# \end{align}
# #+END_EXPORT



# Importantly, given a trajectory where each state and control are normally distributed,
# ${\state_{\timeInd} \sim \mathcal{N}(\stateMean, \stateCov)}$
# and
# ${\control_{\timeInd} \sim \mathcal{N}(\controlMean, \controlCov)}$,
# the expected cost for a trajectory can be calculated in closed-form,

*** old :noexport:
#+BEGIN_EXPORT latex
\begin{align} \label{eq-optimal-control-message-passing}
p(\controlTraj \mid \state_{0}, \optimalVar_{0:\TimeInd}=1, \modelVar_{0:\TimeInd}=\desiredMode) &=
\frac{p(\optimalVar_{1:\TimeInd}=1, \control_{1:\TimeInd} \mid \state_\timeInd)}{
p(\optimalVar_{1:\TimeInd}=1 \mid \state_\timeInd)
} \\
&=
\frac{p(\optimalVar_{0}=1 \mid \state_0, \control_0)
\policy(\control_0 \mid \state_0)
\prod_{i=\timeInd+1}^{\TimeInd} \int p(\optimalVar_{i}=1 \mid \state_i, \control_i)
p(\state_i \mid \state_{i-1}, \control_{i-1})
\policy(\control_{i} \mid \state_i)
\text{d}\state_{i}}{
p(\optimalVar_{\TimeInd}=1 \mid \state_\TimeInd, \control_\TimeInd)
\prod_\timeInd^{\TimeInd-1}
\int p(\optimalVar_{\timeInd}=1 \mid \state_\timeInd, \control_\timeInd)
\transitionDist
\policy(\control_\timeInd \mid \state_\timeInd)
\text{d}\state_{\timeInd+1} \text{d}\control_{\timeInd+1}}. \\
&=
\frac{ \int \prod_{\timeInd=1}^{\TimeInd} p(\optimalVar_{\timeInd}=1 \mid \state_\timeInd, \control_\timeInd)
\transitionDist
\policy(\control_{\timeInd} \mid \state_\timeInd)
\text{d}\state_{\timeInd:\TimeInd}}{
\int \prod_{\timeInd=1}^{\TimeInd}
p(\optimalVar_{\timeInd}=1 \mid \state_\timeInd, \control_\timeInd)
\transitionDist
\policy(\control_\timeInd\mid \state_\timeInd)
\text{d}\state_{\timeInd+1:\TimeInd} \text{d}\control_{\timeInd:\TimeInd}}.
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{align} \label{eq-optimal-control-message-passing}
p(\control_{\timeInd} \mid \state_{\timeInd}, \optimalVar_{\timeInd:\TimeInd}=1) =
\frac{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)
\controlDist p(\state_\timeInd)}{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd)
p(\state_\timeInd)}.
\end{align}
#+END_EXPORT
This distribution represents the optimal policy and can be obtained using a standard sum-product inference
algorithm. See cite:levineReinforcement2018 for more details.
Intuitively, $p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)$
denotes the probability that a trajectory is optimal from $\timeInd$ to $\TimeInd$ given it begins in state
$\state_\timeInd$ with control $\control_\timeInd$.
Similarly $p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd)$
denotes the probability that the trajectory is optimal from $\timeInd$ to $\TimeInd$ given it begins in state
$\state_\timeInd$.
The state only message, $p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd)$, can be recovered
from the state-control message,
$p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)$,
by simply marginalising the control,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd)
&=
\int p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd)
p(\control_{\timeInd} \mid \state_{\timeInd})
\text{d}\control_{\timeInd}.
\end{align}
#+END_EXPORT
Computing the state-control distribution,
$p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)$,
requires a recursive backwards message passing algorithm
starting from the last time step $\timeInd=\TimeInd$ and passing information backwards in time to $\timeInd=1$.
The terminal term $p(\optimalVar_\TimeInd=1 \mid \state_\TimeInd, \control_\TimeInd)$ can be calculated
from the "cost" likelihood (Eq. ref:eq-monotonicOptimalityLikelihood) and the proceeding time steps
can be obtained via recursive message passing,
#+BEGIN_EXPORT latex
%\small
\begin{align*} \label{eq-}
p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd)
&=
\prod_{\timeInd}^{\TimeInd-1}
\int \int
\optimalProb p(\state_{\timeInd+1} \mid \state_{\timeInd}, \control_{\timeInd}) p(\control_{\timeInd+1} \mid \state_{\timeInd+1})
\text{d}\state_{\timeInd+1} \text{d}\control_{\timeInd+1}
\end{align*}
%\normalsize
#+END_EXPORT
Given these recursive messages the optimal policy
$p(\control_{\timeInd} \mid \state_{\timeInd}, \optimalVar_{\timeInd:\TimeInd}=1)$
can be calculated using Eq. ref:eq-optimal-control-message-passing.

This inference mirrors trajectory optimisation as it also has a forward pass that simulates the
current controller and a backward pass that updates the controller to improve the trajectory.
However, what does this inference procedure actually optimise?

*Maximum Entropy Regularisation* Formulating trajectory optimisation in this way encodes the
maximum causal entropy principle, which is often used to achieve robustness,
in particular for inverse optimal control cite:ziebartModeling2010.
It is important to note that the maximum entropy framework is problematic for stochastic dynamics.
In essence, it assumes that the agent is able to control the dynamics as well as the controls in order
to obtain optimal trajectories.

This inference procedure is obtained by marginalising the full trajectory distribution
and conditioning the policy on $\state_\timeInd$ at each time step.
The full trajectory distribution is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\stateControlTraj) = p(\state_1 \mid \optimalVar_{1:\TimeInd}) \prod_{\timeInd=1}^\TimeInd
p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \optimalVar_{1:\TimeInd})
p(\control_\timeInd \mid \state_\timeInd, \optimalVar_{1:\TimeInd})
\end{align}
#+END_EXPORT

In the case of stochastic dynamics the optimised trajectory distribution is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\stateControlTraj) = p(\state_1 \mid \optimalVar_{1:\TimeInd}) \prod_{\timeInd=1}^\TimeInd
p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \optimalVar_{1:\TimeInd})
p(\control_\timeInd \mid \state_\timeInd, \optimalVar_{1:\TimeInd})
\end{align}
#+END_EXPORT


#+BEGIN_EXPORT latex
\todo[inline]{Considering process noise in the dynamics separately from the epistemic (parametric) uncertainty,
\begin{align} \label{eq-}
p(\optimalVar_{1:\TimeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})
= \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \optimalDist
p(\state_{\timeInd+1} \mid \dynamicsFunc(\state_\timeInd)) p(\dynamicsFunc(\state_\timeInd) \mid \state_\timeInd, \control_\timeInd)
\controlDist
\end{align}
\begin{align} \label{eq-}
\optimalProb &= \monotonicFunc( -\costFunc(\state_\timeInd, \dynamicsFunc(\state_{\timeInd-1}), \control_\timeInd))
\end{align}
}
#+END_EXPORT

*** Joint Probability Model :ignore:noexport:
\newline
The joint probability for an optimal trajectory, i.e.
for $\optimalVar_\timeInd=1$ and $\modeVarK$ for all $\timeInd \in \{1,\ldots,\TimeInd\}$, is given by,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-}
\jointDist =
& \startStateDist \underset{\text{Terminal Cost}}{\terminalCostDist} \\
&\prod_{\timeInd=1}^{\TimeInd-1}
\underset{\text{Cost}}{\optimalProb}
\underset{\text{Mode Constraint}}{\modeProb}
\underset{\text{Dynamics}}{\transitionDist}
\underset{\text{Controller}}{\controlDist}
\end{align}
\normalsize
#+END_EXPORT
where $\controlDist$ denotes the policy.
In the case of RL the policy is parameterised with parameters, $\theta$, but in control we are usually interested in
optimising the states and controls directly.
The object of interest in this model is the distribution over the control, $\control_\timeInd$,
conditioned on the state, $\state_\timeInd$, and all future time steps being optimal,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\control_{\timeInd} \mid \state_{\timeInd}, \optimalVar_{\timeInd:\TimeInd}=1, \modeVar_{\timeInd:\TimeInd}=\modeInd) =
\frac{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd)
p(\modeVar_{\timeInd:\TimeInd}=\modeInd \mid \state_\timeInd, \control_\timeInd)
\controlDist p(\state_\timeInd)}{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd)
p(\modeVar_{\timeInd:\TimeInd}=\modeInd \mid \state_\timeInd)
p(\state_\timeInd)}.
\end{align}
#+END_EXPORT
The distributions $p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd)$,
$p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd)$,
$p(\modeVar_{\timeInd:\TimeInd}=\modeInd \mid \state_\timeInd, \control_\timeInd)$ and
$p(\modeVar_{\timeInd:\TimeInd}=\modeInd \mid \state_\timeInd)$
can be obtained via recursive message passing,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd) =
\prod_{\timeInd+1}^{\TimeInd-1}
\int \int \optimalProb p(\state_\timeInd \mid \state_{\timeInd-1}, \control_{\timeInd-1}) \controlDist \text{d}\state_{\timeInd} \text{d}\control_{\timeInd}.
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\todo[inline]{Considering process noise in the dynamics separately from the epistemic (parametric) uncertainty,
\begin{align} \label{eq-}
p(\optimalVar_{1:\TimeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})
= \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \optimalDist
p(\state_{\timeInd+1} \mid \dynamicsFunc(\state_\timeInd)) p(\dynamicsFunc(\state_\timeInd) \mid \state_\timeInd, \control_\timeInd)
\controlDist
\end{align}
\begin{align} \label{eq-}
\optimalProb &= \monotonicFunc( -\costFunc(\state_\timeInd, \dynamicsFunc(\state_{\timeInd-1}), \control_\timeInd)) \\
\end{align}
}
#+END_EXPORT

*** graphical model :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[h!]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[latent, above=of u1] (o1) {$\optimalVar_1$};
      \node[latent, right=of o1] (o2) {$\optimalVar_2$};
      \node[latent, right=of o2] (o3) {$\optimalVar_3$};

      \node[latent, below=of x1] (a1) {$\modeVar_1$};
      \node[latent, right=of a1] (a2) {$\modeVar_2$};
      \node[latent, right=of a2] (a3) {$\modeVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);

      \draw[->] (x1)--(a1);
      \draw[->] (u1) to [out=-90,in=30] (a1);
      \draw[->] (x2)--(a2);
      \draw[->] (u2) to [out=-90,in=30] (a2);
      \draw[->] (x3)--(a3);
      \draw[->] (u3) to [out=-90,in=30] (a3);
    \end{tikzpicture}
    %}
\caption{Graphical model with optimality and mode indicator variables.}
\label{fig-constrained-control-graphical-model}
\end{figure}
#+END_EXPORT

*** graphical model :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[obs, above=of u1] (o1) {$\optimalVar_1$};
      \node[obs, right=of o1] (o2) {$\optimalVar_2$};
      \node[obs, right=of o2] (o3) {$\optimalVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);
    \end{tikzpicture}
    %}
  \caption{Graphical model of control formulated as inference.}
\label{fig-control-graphical-model}
\end{figure}
#+END_EXPORT

** Results label:sec-traj-opt-results
The algorithms in this chapter are evaluated and compared in a simulated environment.

The simulation environment models a velocity controlled quadcopter in an environment with spatially
varying turbulence.

*** Velocity Controlled Quadcopter Simulator
These figures are slices with controls equal zero
#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/baseline-traj-opt/trajectories_over_desired_prob.pdf}
\subcaption{}
\label{eq-traj-opt-over-prob-baseline}
\end{minipage}
\begin{minipage}[r]{1.2\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/baseline-traj-opt/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{eq-traj-opt-over-svgp-baseline}
\end{minipage}
\caption{\label{fig-metric-vs-time}(\subref{eq-traj-opt-over-prob-baseline}) shows the trajectories overlayed on the desired mode's mixing probability
and (\subref{eq-traj-opt-over-svgp-baseline}) shows them overlayed on the desired mode's gating function posterior
mean (left) and posterior variance (right).}
\end{figure}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/conditioning-energy/trajectories_over_desired_prob.pdf}
\subcaption{}
\label{eq-geometric-traj-opt-over-prob}
\end{minipage}
\begin{minipage}[r]{0.99\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/conditioning-energy/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{eq-geometric-traj-opt-over-svgp}
\end{minipage}
\caption{\label{fig-metric-vs-time}(\subref{eq-geometric-traj-opt-over-prob}) shows the trajectories overlayed on the desired mode's mixing probability
and (\subref{eq-geometric-traj-opt-over-svgp}) shows them overlayed on the desired mode's gating function posterior
mean (left) and posterior variance (right).}
\end{figure}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{figure}
\centering
\begin{minipage}[r]{0.49\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/conditioning-energy/trajectories_over_desired_prob.pdf}
\subcaption{}
\label{eq-geometric-traj-opt-over-prob}
\end{minipage}
\begin{minipage}[r]{0.49\columnwidth}
    \begin{minipage}[r]{0.7\columnwidth}
        \centering
        %\includegraphics[width=\textwidth]{./images/geometric-traj-opt-prob-vs-time.pdf}
        \includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/conditioning-energy/mode_probs_vs_time.pdf}
        %\subcaption{Mode 1's mixing probability over the trajectories.}
        \subcaption{}
        \label{geometric-traj-opt-prob-vs-time}
    \end{minipage}
    \begin{minipage}[r]{0.7\columnwidth}
        %\includegraphics[width=\textwidth]{./images/geometric-traj-opt-epistemic-vs-time.pdf}
        \includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/conditioning-energy/variance_vs_time.pdf}
        \subcaption{}
        \label{geometric-traj-opt-epistemic-vs-time}
        %\subcaption{Posterior variance associated with mode 1's gating function over the trajectories.}
    \end{minipage}
\end{minipage}
\begin{minipage}[r]{0.99\columnwidth}
\centering
\includegraphics[width=\textwidth]{./images/traj-opt/mode-remaining/conditioning-energy/trajectories_over_desired_gating_gp.pdf}
\subcaption{}
\label{eq-geometric-traj-opt-over-svgp}
\end{minipage}
\caption{\label{fig-metric-vs-time}Comparision of the intial and optimised trajectories' performance -- for two settings of $\lambda$ -- at 1) staying in the desired mode and 2) avoiding regions of high epistemic uncertainty.
(\subref{eq-geometric-traj-opt-over-prob}) shows the trajectories overlayed on the desired mode's mixing probability
and (\subref{eq-geometric-traj-opt-over-svgp}) shows them overlayed on the desired mode's gating function posterior
mean (left) and posterior variance (right).
(\subref{geometric-traj-opt-prob-vs-time}) and (\subref{geometric-traj-opt-epistemic-vs-time}) show the
probability of remaining in the desired mode and the desired mode's posterior variance
over the trajectories respectively.}
\end{figure}
#+END_EXPORT

** Optimal Control as Probabilistic Inference :noexport:
*** maths :ignore:
#+BEGIN_EXPORT latex

\newcommand{\startStateDist}{\ensuremath{p(\state_{1})}}
\newcommand{\transitionDist}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd)}}
\newcommand{\stateControlTraj}{\ensuremath{\bm\tau}}
\newcommand{\trajectoryDist}{\ensuremath{p(\stateControlTraj)}}
\newcommand{\controlDist}{\ensuremath{p(\control_\timeInd \mid \state_\timeInd)}}

\newcommand{\optimalVar}{\ensuremath{\mathcal{O}}}
%\newcommand{\dynamicsFunc}{\ensuremath{f_{k_*}}}
\newcommand{\dynamicsFunc}{\ensuremath{f}}
\newcommand{\costFunc}{\ensuremath{c}}
\newcommand{\monotonicFunc}{\ensuremath{g}}

\newcommand{\optimalProb}{\ensuremath{\Pr(\optimalVar_\timeInd = 1 \mid \state_\timeInd, \control_\timeInd)}}
\newcommand{\optimalDist}{\ensuremath{P(\optimalVar_\timeInd \mid \state_\timeInd, \control_\timeInd)}}

\newcommand{\marginalLikelihood}{\ensuremath{p(\optimalVar_{1:\TimeInd} = 1)}}
\newcommand{\jointDist}{\ensuremath{p(\optimalVar_{1:\TimeInd}=1, \state_{1:\TimeInd}, \control_{1:\TimeInd})}}

\newcommand{\temperatureParam}{\ensuremath{\lambda}}
#+END_EXPORT
*** lit review :ignore:
cite:toussaintRobot2009 formulate trajectory optimisation (and stochastic optimal control) as
approximate inference and derive a local approximation
where the maximum likelihood solution returns the deterministic optimal trajectory, and the local Gaussian belief
approximation is equivalent to the LQG perturbation model around the optimal trajectory.
cite:levineReinforcement2018 followed a similar approach formulating RL as probabilistic inference
and showing how different approximate inference algorithms instantiated different popular RL algorithms.

this is from cite:watsonStochastic2021
Control-as-inference techniques use a belief in optimality
to perform optimal control [1], a binary random variable
O ∈ {0, 1}, for which 1 indicates optimality. A convenient
approach for this likelihood is an exponential transform of
the cost [48]. The resulting Boltzmann distribution introduces
inverse ‘temperature’ α to scale the cost,
p(O=1|x,u) ∝ exp(−αC(x,u)). (10)
As a result, the negative log-likelihood is an affine transform
of the cost, which preserves convexity. In this work, we
consider when the distribution in (10) is the multivariate
Normal distribution, for which C is a Mahalanobis distance
from a target state z
∗
is space z ∈ R
dz via transform h,


*** intro :ignore:
Optimal control as input estimation

To formulate the optimal control problem as probabilistic inference we follow cite:levineReinforcement2018
and embed the control problem into a graphical model (see Figure ref:fig-basic-control-graphical-model).
The joint probability model (over a trajectory) is augmented with an
additional variable to encode the notion of cost (or reward)
over the trajectory (see Figure ref:fig-augmented-control-graphical-model).
The new variable is a Bernoulli random variable $\optimalVar_\timeInd \in \{0, 1\}$, that indicates if
time step $\timeInd$ is /optimal/ $\optimalVar_\timeInd=1$, or not /optimal/ $\optimalVar_\timeInd=0$.
The probability $\Pr(\optimalVar_\timeInd = 1 \mid \state_\timeInd, \control_\timeInd)$ of this
Bernoulli distribution can be formulated by mapping the negative cost through a monotonically increasing
function $\monotonicFunc$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-boltzmann-dist}
\optimalProb &\propto \monotonicFunc( -\temperatureParam \costFunc(\state_\timeInd, \control_\timeInd)).
\end{align}
#+END_EXPORT
We will return to the choice of $\monotonicFunc$ in Section \todo{insert ref to section}
as it leads to our algorithm resembling different well-known algorithms.


*** graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}[b]
  \centering
  \begin{subfigure}{.45\textwidth}
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);
    \end{tikzpicture}
    %}
  \caption{Graphical model with states and controls.}
\label{fig-basic-control-graphical-model}
\end{subfigure}
\begin{subfigure}{.45\textwidth}
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[obs, above=of u1] (o1) {$\optimalVar_1$};
      \node[obs, right=of o1] (o2) {$\optimalVar_2$};
      \node[obs, right=of o2] (o3) {$\optimalVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);
    \end{tikzpicture}
    %}
  \caption{Graphical model with optimality variable.}
\label{fig-augmented-control-graphical-model}
\end{subfigure}
\caption{Graphical models of control formulated as inference.}
\label{fig-control-graphical-model}
\end{figure}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{figure}[h!]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[latent, above=of u1] (o1) {$\optimalVar_1$};
      \node[latent, right=of o1] (o2) {$\optimalVar_2$};
      \node[latent, right=of o2] (o3) {$\optimalVar_3$};

      \node[latent, below=of x1] (a1) {$\modeVar_1$};
      \node[latent, right=of a1] (a2) {$\modeVar_2$};
      \node[latent, right=of a2] (a3) {$\modeVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);

      \draw[->] (x1)--(a1);
      \draw[->] (u1) to [out=-90,in=30] (a1);
      \draw[->] (x2)--(a2);
      \draw[->] (u2) to [out=-90,in=30] (a2);
      \draw[->] (x3)--(a3);
      \draw[->] (u3) to [out=-90,in=30] (a3);
    \end{tikzpicture}
    %}
\caption{Graphical model with optimality and mode indicator variables.}
\label{fig-constrained-control-graphical-model}
\end{figure}
#+END_EXPORT
*** model :ignore:
\newline
*Joint Probability Model* The marginal likelihood of this augmented model for an optimal trajectory i.e.
for $\optimalVar_\timeInd=1$ for all $\timeInd \in \{1,\ldots,\TimeInd\}$, is obtained by
marginalising the states and controls over the trajectory,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\marginalLikelihood &=
\int \int \jointDist \text{d} \state_{1:\TimeInd} \text{d} \control_{1:\TimeInd} \\
&= \int \int \startStateDist \prod_{\timeInd=1}^{\TimeInd} \optimalProb \transitionDist \controlDist
\text{d} \state_{1:\TimeInd} \text{d} \control_{1:\TimeInd}
\end{align}
#+END_EXPORT
where $\controlDist$ denotes the policy.
In the case of RL the policy is parameterised with parameters $\theta$ but in control we are usually interested in
optimising the states and controls directly.
The object of interest in this model is the distribution over the control $\control_\timeInd$
conditioned on the state $\state_\timeInd$ and all future time steps being optimal,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\control_{\timeInd} \mid \state_{\timeInd}, \optimalVar_{\timeInd:\TimeInd}=1) =
\frac{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd, \control_\timeInd) \controlDist p(\state_\timeInd)}{p(\optimalVar_{\timeInd:\TimeInd}=1 \mid \state_\timeInd) p(\state_\timeInd)}.
\end{align}
#+END_EXPORT
The distributions $p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd)$
and $p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd)$
can be obtained via recursive message passing,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\optimalVar_{\timeInd:\TimeInd} \mid \state_\timeInd, \control_\timeInd) =
\prod_{\timeInd+1}^{\TimeInd-1}
\int \int \optimalProb p(\state_\timeInd \mid \state_{\timeInd-1}, \control_{\timeInd-1}) \controlDist \text{d}\state_{\timeInd} \text{d}\control_{\timeInd}.
\end{align}
#+END_EXPORT
#+BEGIN_EXPORT latex
\todo[inline]{Considering process noise in the dynamics separately from the epistemic (parametric) uncertainty,
Considering process noise in the dynamics separately from the epistemic (parametric) uncertainty,
\begin{align} \label{eq-}
p(\optimalVar_{1:\TimeInd}, \state_{1:\TimeInd}, \control_{1:\TimeInd})
= \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \optimalDist
p(\state_{\timeInd+1} \mid \dynamicsFunc(\state_\timeInd)) p(\dynamicsFunc(\state_\timeInd) \mid \state_\timeInd, \control_\timeInd)
\controlDist
\end{align}
\begin{align} \label{eq-}
\optimalProb &= \monotonicFunc( -\costFunc(\state_\timeInd, \dynamicsFunc(\state_{\timeInd-1}), \control_\timeInd)) \\
\end{align}
}
#+END_EXPORT

*** Variational Inference
In variational inference the distribution over the latent variables
$p(\state_{1:\TimeInd}, \control_{1:\TimeInd})$
is approximated with another, potentially simpler
(e.g. factorised) distribution $q(\state_{1:\TimeInd}, \control_{1:\TimeInd})$.
In this case, the aim is to approximate the distribution over the optimal policy,
$p(\control_{1:\TimeInd} \mid \state_{1:\TimeInd}, \optimalVar_{1:\TimeInd})$.
#+BEGIN_EXPORT latex
\newcommand{\trajectoryVarDist}{\ensuremath{q(\state_{1:\TimeInd}, \control_{1:\TimeInd})}}
\newcommand{\controlTrajVarDist}{\ensuremath{q(\control_{1:\TimeInd} \mid \state_{1:\TimeInd})}}
\newcommand{\controlVarDist}{\ensuremath{q(\control_{\timeInd})}}
\begin{align} \label{eq-}
\text{log} \marginalLikelihood &= \text{log} \int \int \jointDist \text{d} \state_{1:\TimeInd} \text{d} \control_{1:\TimeInd} \\
&= \text{log} \int \int \jointDist \frac{\trajectoryVarDist}{\trajectoryVarDist}
\text{d} \state_{1:\TimeInd} \text{d} \control_{1:\TimeInd} \\
&= \text{log} \E_\trajectoryVarDist \left[ \frac{\jointDist}{\trajectoryVarDist} \right] \\
&= \text{log} \E_\trajectoryVarDist \left[
\frac{\cancel{\startStateDist} \prod_{\timeInd=1}^{\TimeInd} \optimalProb \cancel{\transitionDist} \controlDist}{\cancel{\startStateDist} \prod_{\timeInd=1}^\TimeInd \cancel{\transitionDist} \controlVarDist}
\right] \\
&\geq \E_\trajectoryVarDist \left[
\sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb - \sum_{\timeInd=1}^\TimeInd \text{log}
\frac{\controlVarDist}{\controlDist} \right] \\
&= \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb \right]
- \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^\TimeInd \text{log}\frac{\controlVarDist}{\controlDist}\right] \\
%- \sum_{\timeInd=1}^\TimeInd \text{KL}\left(\controlVarDist \mid \mid \controlDist \right)
&= \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb
- \text{log}\controlVarDist\right] \\
&= \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \costFunc(\state_\timeInd, \control_\timeInd)
- \text{log}\controlVarDist\right]
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\text{log} \marginalLikelihood &\geq
 \E_\trajectoryVarDist \left[ \sum_{\timeInd=1}^{\TimeInd} \text{log} \optimalProb
- \text{log}\controlVarDist\right] \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \text{log} \optimalProb \right]
- \E_{\controlVarDist q(\state_{\timdInd})} \left[ \text{log}\controlVarDist \right] \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
- \sum_{\timeInd=1}^{\TimeInd} \underbrace{\E_{\controlVarDist} \left[ \text{log} \controlVarDist \right]}_{\text{-Entropy}} \\
&= \sum_{\timeInd=1}^{\TimeInd} \E_{\controlVarDist q(\state_{\timeInd})} \left[ \costFunc(\state_\timeInd, \control_\timeInd) \right]
+ \sum_{\timeInd=1}^{\TimeInd} \mathcal{H}(\control_{\timeInd})
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\trajectoryVarDist = \startStateDist \prod_{\timeInd=1}^{\TimeInd-1} \transitionDist \controlVarDist
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{myquote}
As shown in \cite{okadaVariational2020}, the choice of $\monotonicFunc$ leads to the optimisation algorithm
resembling different well-known algorithms.
For example, selecting $\monotonicFunc$ to be the exponential function,
\begin{align} \label{eq-}
\optimalProb &= \text{exp}( -\costFunc(\state_\timeInd, \control_\timeInd))
\end{align}
leads to the algorithm resembling Model Predictive Path Integral (MPPI)
\cite{williamsModel2017,williamsInformation2017}.
\end{myquote}
\todo{add detials on Cross Entropy Method (CEM) cite:chuaDeep2018 and other methods}
#+END_EXPORT

*** graphical model :ignore:noexport:
#+BEGIN_EXPORT latex
\begin{figure}[t]
  \centering
%   \resizebox{0.8\columnwidth}{!}{
    \begin{tikzpicture}[
      pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
      post/.style={->,shorten >=0.4pt,>=stealth',semithick}
      ]
      \node[obs] (x1) {$\state_1$};
      \node[latent, right=of x1] (x2) {$\state_2$};
      \node[latent, right=of x2] (x3) {$\state_3$};
      \node[latent, right=of x3] (x4) {$\state_4$};

      \node[latent, above=of x1, xshift=0.6cm] (u1) {$\control_1$};
      \node[latent, right=of u1] (u2) {$\control_2$};
      \node[latent, right=of u2] (u3) {$\control_3$};

      \node[obs, above=of u1] (o1) {$\optimalVar_1$};
      \node[obs, right=of o1] (o2) {$\optimalVar_2$};
      \node[obs, right=of o2] (o3) {$\optimalVar_3$};

      \draw[post] (x1)--(x2);
      \draw[post] (x2)--(x3);
      \draw[post] (x3)--(x4);

      \draw[post] (x1)--(u1);
      \draw[post] (x2)--(u2);
      \draw[post] (x3)--(u3);

      \draw[post] (u1)--(x2);
      \draw[post] (u2)--(x3);
      \draw[post] (u3)--(x4);

      \draw[->] (u1)--(o1);
      \draw[->] (x1) to [out=100,in=240] (o1);
      \draw[->] (u2)--(o2);
      \draw[->] (x2) to [out=100,in=240] (o2);
      \draw[->] (u3)--(o3);
      \draw[->] (x3) to [out=100,in=240] (o3);
    \end{tikzpicture}
    %}
  \caption{Graphical model of control formulated as inference.}
\label{fig-control-graphical-model}
\end{figure}
#+END_EXPORT

** Trajectory Optimisation label:sec-traj-opt :noexport:
This work seeks to solve the trajectory optimisation in Eq. ref:eq-objective, i.e.
find trajectories from $\mathbf{x}_0$ to $\mathbf{x}_f$ that
minimise the cost in Eq. ref:eq-cost-trajectory.
This section details our approach that
projects the trajectory optimisation onto one of the gating function's GPs,
implicitly minimising the cost function in Eq. ref:eq-cost-trajectory.

The length of a trajectory from $\mathbf{x}_0$ to $\mathbf{x}_f$ on the
manifold given by the mean (Fig. ref:fig-svgp_2d_traj left)
increases when it passes over the contours - analogous to climbing a hill.
Given appropriate scaling of the mean,
shortest trajectories (geodesics) between two locations will be those that attempt to follow contours
i.e. remain in a single mode.
Geodesics are solutions to a continuous-time second-order ODE onto which the trajectory
optimisation is projected.
This section now recaps concepts of Riemannian geometry before extending them to probabilistic geometries
and detailing how this latent ODE is solved using direct collocation.

*** A Geometric Cost Function
The cost function in Eq. ref:eq-cost-trajectory is difficult to pose, but
the $g_{\text{mode}}$ term can be expressed as finding shortest
paths on the desired mode's gating function.
This formulation is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-geodesic-objective}
\min \int_{t_0}^{t_f}\left\|\dot{\mathbf{x}}(t)\right\|_{\mathbf{G}(\mathbf{x}(t))} \mathrm{d}t
\end{align}
#+END_EXPORT
where $\mathbf{G}(\mathbf{x}(t))$ is the metric tensor at $\mathbf{x}(t)$ for a Riemannian manifold
encoding the desired cost, i.e. the desired mode's gating function.
Intuitively Riemannian manifolds are smoothly curved spaces with
an inner product.
Formally, they are smooth manifolds equipped with a Riemannian metric cite:carmoRiemannian1992,
#+BEGIN_EXPORT latex
\begin{definition}[Riemannian Metric]
A Riemannian metric $\mathbf{G}$ on a
manifold $\mathcal{M}$ is a symmetric and positive definite matrix which defines
a smoothly varying inner product
$\langle \dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \rangle_{\mathbf{x}} = \dot{\mathbf{x}}_a^T \mathbf{G}(\mathbf{x}) \dot{\mathbf{x}}_b$
in the tangent space $T_{\mathbf{x}}\mathcal{M}$, for each point $\mathbf{x} \in \mathcal{M}$ and
$\dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \in T_{\mathbf{x}}\mathcal{M}$.
\end{definition}
#+END_EXPORT


# The gating function defining the coordinate map of the Riemannian manifold is modelled as a
# sparse variational Gaussian process and is therefore probabilistic.
# This work extends the probabilistic metric tensor by cite:Tosi2014 to sparse
# variational GPs.
The Riemannian manifold in this work is actually probabilistic because its coordinate
map is modelled as a sparse variational GP.
This work follows cite:tosiMetrics2014 and uses a metric tensor that captures the variance in the manifold
by means of a probability distribution.
In particular, the expected value of this metric tensor contains a covariance term which
leads to lengths on the manifold increasing in areas of high covariance.
This is a desirable behaviour because it encourages trajectories to
avoid regions of the learned dynamics with high epistemic uncertainty -
implementing the $g_{\text{epistemic}}$ cost term in Eq. ref:eq-cost-trajectory.

Motivated by obtaining a probability distribution over the metric tensor, they
introduce the following Riemannian metric,
#+BEGIN_EXPORT latex
\begin{align}
  \langle \dot{\mathbf{x}}_a, \dot{\mathbf{x}}_b \rangle_{\mathbf{x}} = \dot{\mathbf{x}}_a^T \mathbf{J}^T \mathbf{J} \dot{\mathbf{x}}_b =
  \dot{\mathbf{x}}_a^T \mathbf{G}(\mathbf{x}) \dot{\mathbf{x}}_b
\end{align}
#+END_EXPORT
where $\mathbf{J}=\frac{\partial h}{\partial \mathbf{x}}$ denotes the Jacobian of $h$.
As the differential operator is linear, the derivative of a GP is also a GP.
For a sparse GP
the Jacobian $\mathbf{J}_*$ of the function evaluated at a new input $\mathbf{x}_*$
is jointly Gaussian with the function's associated inducing variables.
For the $k^{\text{th}}$ gating function it is given by,
#+BEGIN_EXPORT latex
\small
\begin{align*} \label{eq-joint-jacobian-dist}
p(\mathbf{J}^{(k)}_*, \hat{\mathbf{h}}^{(k)} &\mid \mathbf{x}_*, \bm\xi_h^{(k)}) = \\
& \mathcal{N}\left(
\left[\begin{array}{c}
        \mathbf{J}^{(k)}_* \\
        \hat{\mathbf{h}}^{(k)}
      \end{array}\right] \mid
\left[\begin{array}{c}
        \bm{0} \\
        \bm{\mu}_h^{(k)}
      \end{array}\right], \left[\begin{array}{cc}
                              \partial^2\mathbf{K}_{**}^{(k)} & \partial\mathbf{K}_{*h}^{(k)} \\
                            \partial\mathbf{K}_{h*}^{(k)} & \mathbf{K}_{hh}^{(k)}
                          \end{array}\right]\right),
\end{align*}
\normalsize
#+END_EXPORT
where $\K_{hh}^{(k)} = k^{(k)}\left(\bm\xi_h^{(k)}, \bm\xi_h^{(k)}\right) \in \R^{M \times M}$ is the
$k^{\text{th}}$ gating function's covariance function evaluated between its inducing inputs,
$\partial\K_{*h}^{(k)} = \frac{\partial k^{(k)}\left(\mathbf{x}_*, \bm\xi_h^{(k)}\right)}{\partial \mathbf{x}_*} \in \R^{D \times M}$ is
its partial derivative w.r.t its first input (the new input $\mathbf{x}_*$), and
$\partial^2\K_{**}^{(k)} = \frac{\partial^2 k^{(k)}\left(\mathbf{x}_*, \mathbf{x}_*\right)}{\partial \mathbf{x}_* \partial \mathbf{x}_*} \in \R^{D \times D}$
is its derivative w.r.t both inputs (which are both the new input $\mathbf{x}_*$).
Remembering that the inducing variables are actually probabilistic and modelled as a Gaussian
$q\left(\hat{\mathbf{h}}^{(k)}\right) = \mathcal{N}\left(\hat{\mathbf{h}}^{(k)} \mid \mathbf{m}_h^{(k)}, \mathbf{S}_h^{(k)}\right)$,
the predictive distribution of the Jacobian given a new input $\mathbf{x}_*$
is obtained by marginalising the inducing variables,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-predictive-jacobian-dist}
p\left(\mathbf{J}^{(k)}_* | \mathbf{x}_*, \bm\xi_h^{(k)}\right)
&=\int q\left(\hat{\mathbf{h}}^{(k)}\right) p\left(\mathbf{J}^{(k)}_* \mid \mathbf{x}_*, \hat{\mathbf{h}}^{(k)}, \bm\xi_h^{(k)}\right) \text{d} \hat{\mathbf{h}}^{(k)} \\
&= \mathcal{N}\left(\mathbf{J}^{(k)}_* \mid \bm\mu_J^{(k)}, \mathbf{\Sigma}_{J}^{(k)}\right). \numberthis
\end{align*}
#+END_EXPORT
This induces a non-central Wishart distribution over the metric tensor $\mathbf{G}$,
#+BEGIN_EXPORT latex
\begin{align}
  \mathbf{G}=\mathcal{W}_{D+F}\left(p, \boldsymbol{\Sigma}_{J}, \mathbb{E}\left[\mathbf{J}^{\top}\right] \mathbb{E}[\mathbf{J}]\right),
\end{align}
#+END_EXPORT
where $p$ is the number of degrees of freedom (always one in our case).
The expected metric tensor is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-metric}
  \E[\mathbf{G}] = \E[\mathbf{J}^T] \E[\mathbf{J}] + \mathbf{\Sigma}_J.
\end{align}
#+END_EXPORT
This expected metric tensor includes a covariance term $\mathbf{\Sigma}_J$ which implies that the
metric is larger when the covariance in the mapping is higher.
As a result, trajectories minimising Eq. ref:eq-geodesic-objective endowed with this
metric will attempt to avoid regions of the transition dynamics with high epistemic uncertainty.

# where,
# #+BEGIN_EXPORT latex
# \small
# \begin{align*} \label{eq-predictive-jacobian-mean-var}
# \bm\mu_J^{(k)} &= \partial\mathbf{K}_{*h}^{(k)} \left( \mathbf{K}_{hh}^{(k)}\right)^{-1} \mathbf{m}_h^{(k)}, \\
# \mathbf{\Sigma}_J^{(k)} &= \partial^2{\K_{**}^{(k)}} -
# \partial{\K_{*h}^{(k)}} \left(\mathbf{K}_{hh}^{(k)}\right)^{-1} \partial{\K_{h*}^{(k)}} \\
# &+ \partial{\K_{*h}^{(k)}} \left(\mathbf{K}_{hh}^{(k)}\right)^{-1} \mathbf{S}_h^{(k)} \left(\mathbf{K}_{hh}^{(k)}\right)^{-1} \partial{\K_{h*}^{(k)}}.
# \end{align*}
# \normalsize
# #+END_EXPORT

*** Latent Geodesic ODE Collocation
#+BEGIN_EXPORT latex
\begin{figure}[!t]
\centering
\includegraphics[width=0.8\columnwidth]{images/mixing_prob_vs_time.pdf}
\caption{Comparision of the intial and optimised trajectories' performance at staying in the desired mode. The plot shows mode 1's mixing probability over the trajectories for two settings of $\lambda$.}
\label{fig-mixing_prob_vs_time}
\end{figure}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{figure}[!t]
\centering
\includegraphics[width=0.8\columnwidth]{images/epistemic_var_traj.pdf}
\caption{Comparision of the trajectories' performance at avoiding
regions of the learned transition dynamics with high epistemic uncertainty for two settings of $\lambda$.
The plot shows the GP's posterior variance associated with mode 1's gating function over the
trajectories.}
\label{fig-epistemic_var_vs_time}
\end{figure}
#+END_EXPORT
Trajectories minimising Eq. ref:eq-geodesic-objective are geodesics on $\mathcal{M}$
and must satisfy the continuous-time second-order ODE cite:carmoRiemannian1992,
#+BEGIN_EXPORT latex
\small
\begin{align*} \label{eq-2ode}
 \ddot{\mathbf{x}}(t)
&= f_G(t, \dot{\mathbf{x}}, \mathbf{x}) \\
&=-\frac{1}{2} \mathbf{G}^{-1}(\mathbf{x}(t))\left[\frac{\partial \operatorname{vec}[\mathbf{G}(\mathbf{x}(t))]}{\partial \mathbf{x}(t)}\right]^{T}\left(\dot{\mathbf{x}}(t) \otimes \dot{\mathbf{x}}(t)\right), \numberthis
\end{align*}
\normalsize
#+END_EXPORT
where $\operatorname{vec}[\mathbf{G}(\mathbf{x}(t)])$ stacks the columns of $\mathbf{G}(\mathbf{x}(t))$
and $\otimes$ denotes the Kronecker product.
Performing  trajectory optimisation in Eq. ref:eq-objective with the cost function in Eq. ref:eq-cost-trajectory
is equivalent to solving the ODE in Eq. ref:eq-2ode subject to the same boundary conditions.
However, since neither $\dot{\mathbf{x}}(t_0)$ nor $\dot{\mathbf{x}}(t_f)$ are known, it cannot
be solved with simple forward or backward integration.
Instead, the problem is transcribed using the approach of differential
flatness cite:milamNew2000c,rossPseudospectral2004.
A set of outputs $\mathbf{z}(t)$ are defined such that the
states $\mathbf{x}(t)$ and controls $\mathbf{u}(t)$ can be
expressed in terms of the flat output $\mathbf{z}(t)$ and a finite number of its derivatives,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\mathbf{x}(t) &= A(\mathbf{z}(t), \dot{\mathbf{z}}(t), \ldots) \\
\mathbf{u}(t) &= B(\mathbf{z}(t), \dot{\mathbf{z}}(t), \ldots).
\end{align}
#+END_EXPORT
In the velocity-controlled quadcopter example, the flat output is the
state $\mathbf{z}(t) = \mathbf{x}(t)$
and the control is simply the state derivative
$\mathbf{u}(t) &= \dot{\mathbf{z}}(t)$.

The original trajectory optimisation problem can then be converted to finding $\mathbf{z}(t)$
for $t \in [t_0, t_f]$ subject to the boundary conditions and the dynamics,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-diff-flat-ode}
\ddot{\mathbf{x}}(\mathbf{z}(t), \dot{\mathbf{z}}(t)) &= f_G(t, \dot{\mathbf{x}}(\mathbf{z}(t), \dot{\mathbf{z}}(t)), \mathbf{x}(\mathbf{z}(t), \dot{\mathbf{z}}(t))).
\end{align}
#+END_EXPORT
Collocation methods are used to transcribe continuous-time trajectory optimisation problems into
nonlinear programs, i.e. constrained parameter optimisation cite:kellyIntroduction2017,fahrooDirect2000  .
The expected metric in Eq. ref:eq-expected-metric is substituted into
Eq. ref:eq-diff-flat-ode and solved via direct collocation.
This work implements a simple Hermite-Simpson collocation method
that enforces the state derivative predicted by the polynomials to
equal the geodesic ODE $f_G$ at a set of $I$ collocation points $\{\mathbf{z}_{i,c}\}_{i=1}^I$.
This is achieved through the collocation defects,
#+BEGIN_EXPORT latex
\begin{align*} \label{eq-defect}
\Delta_i &= \ddot{\mathbf{z}}_{i,c} - f_G(t_{i,c}, \dot{\mathbf{z}}_{i,c},\mathbf{z}_{i,c}) \numberthis
\end{align*}
#+END_EXPORT
where $\ddot{\mathbf{z}}_{i,c}$ is the $2^{\text{nd}}$
derivative w.r.t time at the $i^{\text{th}}$ collocation point predicted by the polynomials.
Eq. ref:eq-defect defines a set of contraints ensuring trajectories are
solutions to the geodesic ODE $f_G$.
The nonlinear program that this work solves uses a dummy cost and is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\min_{\mathbf{z}(t), \dot{\mathbf{z}}(t)}& \int_{t_0}^{t_f} 1 \text{d}t \\
&\text{s.t. }\text{Eqs. \ref{eq-diff-flat-ode} and \ref{eq-defect}}  \\
\mathbf{x}&\left(\mathbf{z}(t_0), \dot{\mathbf{z}}(t_0) \right) = \mathbf{x}_0 \\
\mathbf{x}&(\mathbf{z}(t_f),\dot{\mathbf{z}}(t_f)) = \mathbf{x}_f
%c&(\mathbf{x}(\mathbf{z}(t), \dot{\mathbf{z}}(t)), \mathbf{u}(\mathbf{z}(t), \dot{\mathbf{z}}(t)) \leq 0 \quad \forall t
\end{align}
#+END_EXPORT
This is solved using Sequential Least Squares Programming (SLSQP) in SciPy.
# Solutions to this nonlinear program are trajectories that
# attempt to remain in a single dynamics mode and, importantly, also
# avoid areas of the learned transition dynamics with high epistemic uncertainty.

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-}
# \min_{\mathbf{x}(t), \dot{\mathbf{x}}(t)}& \int_{t_0}^{t_f} 1 \text{d}t \\
# \text{s.t. }&\text{Eqs. \ref{eq-diff-flat-ode} and \ref{eq-defect}}  \\
# \mathbf{x}&\left(\mathbf{z}(t_0), \dot{\mathbf{z}}(t_0), \ddot{\mathbf{z}}(t_0) \right) = \mathbf{x}_0 \\
# \mathbf{x}&(\mathbf{z}(t_f),\dot{\mathbf{z}}(t_f), \ddot{\mathbf{z}}(t_f)) = \mathbf{x}_f \\
# c&(\mathbf{x}(t), \dot{\mathbf{x}}(t)) \leq 0 \quad \forall t
# \end{align}
# #+END_EXPORT


# ** Direct Collocation


# This work implements a simple Hermite-Simpson collocation method
# that enforces the state derivative predicted by the polynomials to
# equal the geodesic ODE $f_G$ at a set of collocation points.
# This is achieved via the collocation defects,
# #+BEGIN_EXPORT latex
# \begin{align*} \label{eq-defect}
# \Delta_i &= \dot{\mathbf{z}}_{i,c} - y(\mathbf{z}_{i,c}) \numberthis
# \end{align*}
# #+END_EXPORT
# which define a set of contraints ensuring trajectories are solutions to the geodesic ODE $f_G$.

# The nonlinear program that this work solves is given by Eq. ref:eq-objective
# with $g(\mathbf{x}(t), \mathbf{u}(t)) = 1$ and the collocation constraints in Eq. ref:eq-defect.


*** Concepts of Riemannian Geometry :noexport:
Let us now introduce the necessary concepts for finding shortest paths (geodesics) on Riemannian
manifolds.
This section considers continuous-time inputs denoted as $\mathbf{z}(t) = \mathbf{x}(t)$.
Intuitively Riemannian manifolds are smoothly curved spaces with
an inner product.
Formally they are smooth manifolds equipped with a Riemannian metric cite:carmoRiemannian1992,
#+BEGIN_EXPORT latex
\begin{definition}[Riemannian Metric]
A Riemannian metric $\mathbf{G}$ on a
manifold $\mathcal{M}$ is a symmetric and positive definite matrix which defines
a smoothly varying inner product
$\langle \dot{\mathbf{z}}_a, \dot{\mathbf{z}}_b \rangle_{\mathbf{z}} = \dot{\mathbf{z}}_a^T \mathbf{G}(\mathbf{z}) \dot{\mathbf{z}}_b$
in the tangent space $T_{\mathbf{z}}\mathcal{M}$, for each point $\mathbf{z} \in \mathcal{M}$ and
$\dot{\mathbf{z}}_a, \dot{\mathbf{z}}_b \in T_{\mathbf{z}}\mathcal{M}$.
\end{definition}
#+END_EXPORT
Riemannian manifolds are often represented as charts; a parameter space for the
curved surface. An example of a chart is the spherical coordinate system that is
used to describe a sphere.
The chart is often a flat space and the curvature of the manifold arises
through smooth changes in the metric.
Measurements on the surface can thus be computed in the chart locally and
integrated to give global measures.
On a Riemannian manifold $\mathcal{M}$ the length of a trajectory (curve) $\bar{\mathbf{z}}$
is given by the norm of the tangent vector along the trajectory,
#+BEGIN_EXPORT latex
\small
\begin{align*} \label{eq-length}
\text{Length}(\bar{\mathbf{z}}) &=\int_{t_0}^{t_f}\left\|\dot{\mathbf{z}}(t)\right\|_{\mathbf{G}(\mathbf{z}(t))} \mathrm{d}t
=\int_{t_0}^{t_f}\sqrt{\dot{\mathbf{z}}(t)^T \mathbf{G}(\mathbf{z}(t)) \dot{\mathbf{z}}(t) } \mathrm{d} t,
\end{align*}
\normalsize
% \begin{align} \label{eq-length}
% \text { Length }(\bar{\mathbf{x}}) &=\int_{t_0}^{t_f}\left\|\dot{\mathbf{x}}(t)\right\|_{\mathbf{G}(\mathbf{x}(t))} \mathrm{d} t \\
% &=\int_{t_0}^{t_f}\sqrt{\dot{\mathbf{x}}(t)^T \mathbf{G}(\mathbf{x}(t)) \dot{\mathbf{x}}(t) } \mathrm{d} t,
% \end{align}
#+END_EXPORT
where $\mathbf{G}(\mathbf{z}(t))$ is the metric tensor at $\mathbf{z}(t)$.
With this method for calculating lengths on manifolds the concept of a geodesic can be formally defined.
#+BEGIN_EXPORT latex
\begin{definition}[Geodesic]
Given two points $\mathbf{z}_0, \mathbf{z}_f \in
\mathcal{M}$, a Geodesic is a length minimising trajectory (curve)
$\bar{\mathbf{z}}_g$ connecting the points such that,
\begin{align}
  \bar{\mathbf{z}}_{g}=\arg \min_{\bar{\mathbf{z}}} \operatorname{Length}(\bar{\mathbf{z}}), \quad \bar{\mathbf{z}}(t_0)=\mathbf{z}_{0}, \bar{\mathbf{z}}(t_f)=\mathbf{z}_{f}.
\end{align}
\end{definition}
#+END_EXPORT
Geodesics satisfy a continuous-time $2^{\text{nd}}$ order ODE cite:carmoRiemannian1992,
#+BEGIN_EXPORT latex
\small
\begin{align} \label{eq-2ode}
 \ddot{\mathbf{z}}(t)
&= y(t, \mathbf{z}, \dot{\mathbf{z}}) \\
&=-\frac{1}{2} \mathbf{G}^{-1}(\mathbf{x}(t))\left[\frac{\partial \operatorname{vec}[\mathbf{G}(\mathbf{z}(t))]}{\partial \mathbf{z}(t)}\right]^{T}\left(\dot{\mathbf{z}}(t) \otimes \dot{\mathbf{z}}(t)\right),
\end{align}
\normalsize
#+END_EXPORT
where $\operatorname{vec}[\mathbf{G}(\mathbf{z}(t)])$ stacks the columns of $\mathbf{G}(\mathbf{z}(t))$
and $\otimes$ denotes the Kronecker product.
# Computing geodesics involves finding a solution to Eq. ref:eq-2ode
# with $\mathbf{x}(t_0) = \mathbf{x}_1$ and $\mathbf{x}(t_f) = \mathbf{x}_2$.
# This is a boundary value problem with a smooth solution so it can be solved
# using any direct trajectory optimisation framework and can therefore
# incorporate state and action constraints.
# #+BEGIN_EXPORT latex
# \todo[inline]{Hmm, not sure this is true, read up - It only involves discretizing the geodesic curve and not
# the feature space and as this is always 1-dimensional the approach scales
# to higher dimensional feature spaces.}
# #+END_EXPORT


# We can formulate this as an initial value problem and use techniques such as the
# shooting method to determine the correct inintial velocity and from this the
# geodesic path.
# This is advantageous as it only involves discretizing the geodesic curve and not
# the the feature space. This is always 1-dimensional and thus the approach scales
# to highgher dimensional feature spaces.

** Comparison and Discussion

- Algorithm 2 can remain in a set of modes whereas Algorithm 1 can only remain in a single mode. \todo{could Algorithm 1 be modified to remain in a set of modes? Transforming gating functions?}
- Algorithm 2 more naturally handles chance constraints as it calculates
  the mode probabilities.
- Trajectories from Algorithm 2 will satisfy the dynamics whereas trajectories from Algorithm 1 may not.
  This is because the state and control trajectories obtained from
  Algorithm 1 are solutions to the geodesic ODE and not the dynamics.
- How to add constraints to Algorithm 1 e.g. for smoothness?
- Algorithm 2 doesn't decouple mean and variance and allow a $\lambda$ parameter.
- Algorithm 2 considers uncertainty in dynamics. Algorithm 1 approximates geodesic SDE to be deterministic.

The control methods required control regularisation to prevent the controls leading to state transitions that
"jump" over the undesired mode.
That is, trajectories whose discrete time steps appear to satisfy the constraints/cost but the actual continuos time
trajectory passes through the undesired mode.

Methods for learning continuous time controls that can be used to interpolate cost over trajectory
to get higher resolution.
For example a Gaussian process over controls instead of Gaussian.
Or stochastic collocation.
** Conclusion
This chapter has presented a method for performing trajectory optimisation in
multimodal dynamical systems with the transition dynamics modelled as a
Mixtures of Gaussian Process Experts method.
The trajectory optimisation is projected onto a probabilistic Riemannian
manifold parameterised by the gating network of the MoGPE model.
Given a start and end state, the trajectory optimisation can be tuned to find trajectories that either
prioritise remaining in a desired dynamics mode or prioritise avoiding regions of the learned transition
dynamics model with high epistemic uncertainty.

* Trajectory Optimisation in Desired Modes as Probabilistic Inference :noexport:
** Intro :ignore:noexport:
#+BEGIN_EXPORT latex
\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{\MakeUppercase{\timeInd}}}
\newcommand{\stateTraj}{\ensuremath{\bar{\state}}}
\newcommand{\controlTraj}{\ensuremath{\bar{\control}}}
\newcommand{\nominalStateTraj}{\ensuremath{\stateTraj_*}}
\newcommand{\nominalControlTraj}{\ensuremath{\controlTraj_*}}

\newcommand{\utraj}{\ensuremath{\bar{\control}}}
\newcommand{\policies}{\ensuremath{\Pi}}
\newcommand{\policy}{\ensuremath{\pi_{\theta}}}
#+END_EXPORT
** Differential Flatness of 3D Quadrotor Dynamics :noexport:
#+BEGIN_EXPORT latex
\newcommand{\roll}{\ensuremath{\phi}}
\newcommand{\pitch}{\ensuremath{\theta}}
\newcommand{\yaw}{\ensuremath{\psi}}

\newcommand{\world}{\ensuremath{w}}
\newcommand{\body}{\ensuremath{b}}
\newcommand{\intermediate}{\ensuremath{c}}
\renewcommand{\frame}{\ensuremath{F}}
\newcommand{\worldFrame}{\ensuremath{\frame_\world}}
\newcommand{\bodyFrame}{\ensuremath{\frame_\body}}
\newcommand{\intermediateFrame}{\ensuremath{\frame_\intermediate}}

\newcommand{\inertiaMatrix}{\ensuremath{J}}
\newcommand{\zw}{\ensuremath{\mathbf{z}_\world}}

\newcommand{\positions}{\ensuremath{\mathbf{h}}}
\newcommand{\velocity}{\ensuremath{v}}
\newcommand{\Velocity}{\ensuremath{\mathbf{\velocity}}}

\newcommand{\rotationMatrix}{\ensuremath{\mathbf{R}}}
\newcommand{\angularVelocity}{\ensuremath{\bm\omega}}
\newcommand{\angularVelocityMatrix}{\ensuremath{\bm\Omega}}

\newcommand{\thrust}{\ensuremath{f_z}}
\newcommand{\torque}{\ensuremath{\tau}}
#+END_EXPORT
The quadrotor frames and Euler angles (roll $\roll$, pitch $\pitch$ and yaw $\yaw$) are shown
in Figure ref:fig-quadcopter-frames.
The subscripts $\world$, $\body$ and $\intermediate$ are used to denote the world
$\worldFrame$, body $\bodyFrame$ and intermediate $\intermediateFrame$ (after yaw angle rotation) frames respectively.
The the 3D nonlinear quadrotor dynamics are described with the quadcopter model from cite:wangSafe2018,zhouVector2014.
#+BEGIN_EXPORT latex
\begin{align} \label{eq-quadrotor-dynamics}
\dot{\Velocity} &= g \zw + \frac{1}{m} \thrust \rotationMatrix \zw \\
\dot{\rotationMatrix} &= \rotationMatrix \angularVelocityMatrix \\
\dot{\angularVelocity} &= \inertiaMatrix^{-1} \torque - \inertiaMatrix^{-1} \angularVelocityMatrix \inertiaMatrix \angularVelocity \\
\dot{\positions} &= \velocity
\end{align}
#+END_EXPORT
where $g=9.81ms^{-2}$ is the acceleration due to gravity, $m$ is the mass, $\zw=[0,0,1]^T$,
$\positions=[x, y, z]^T$ is the position of the quadrotor in the world frame $\worldFrame$,
$\Velocity=[\velocity_x, \velocity_y, \velocity_z]^T$ is the
and velocity of the quadrotor in the world frame $\worldFrame$ respectively.
The angular velocity of the quadrotor in the body frame $\bodyFrame$ is denoted $\angularVelocity=[p, q ,r]^T$
and in tensor form $\angularVelocityMatrix=[[0,-p,q];[r,0,-p];[-q,p,0]]$

$\state=[x, y, z, \velocity_x, \velocity_y, \velocity_z, \roll, \pitch, \yaw, \dot{\roll}, \dot{\pitch}, \dot{\yaw}]$

$\state=[x, y, z, \velocity_x, \velocity_y, \velocity_z, \roll, \pitch, \yaw]^T$
$\state=[\ddot{x}, \ddot{y}, \ddot{z}, \dot{\roll}, \dot{\pitch}, \dot{\yaw}]^T$

\newpage
** Safety :noexport:
#+BEGIN_EXPORT latex
\newcommand{\controlCost}{\ensuremath{\mathbf{R}}}
\newcommand{\stateCost}{\ensuremath{\mathbf{Q}}}

\newcommand{\stateDynamics}{\ensuremath{\mathbf{A}}}
\newcommand{\controlDynamics}{\ensuremath{\mathbf{B}}}

\newcommand{\stateTrajDist}{\ensuremath{p(\stateTraj)}}
\newcommand{\stateDist}{\ensuremath{p(\state_{\timeInd})}}
\newcommand{\stateMean}{\ensuremath{\bm\mu}}
\newcommand{\stateCov}{\ensuremath{\bm\Sigma}}
\newcommand{\stateDiff}{\ensuremath{\Delta \state}}
\newcommand{\stateDiffMean}{\ensuremath{\bm\mu_{\stateDiff_\timeInd}}}
\newcommand{\stateDiffCov}{\ensuremath{\bm\Sigma_{\stateDiff_\timeInd}}}

\newcommand{\metricTensor}{\ensuremath{\mathbf{G}}}
\newcommand{\jac}{\ensuremath{\mathbf{J}}}
\newcommand{\jacGivenInput}{\ensuremath{p(\jac(\state_\timeInd) \mid \state_\timeInd)}}
\newcommand{\jacDist}{\ensuremath{p(\jac(\state_\timeInd))}}
\newcommand{\jacMean}{\ensuremath{\bm\mu_\jac}}
\newcommand{\jacCov}{\ensuremath{\bm\Sigma_\jac}}

\newcommand{\safeSet}{\ensuremath{\mathcal{C}}}
\newcommand{\StateDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\ControlDomain}{\ensuremath{\mathcal{U}}}

\newcommand{\constraintFunc}{\ensuremath{c}}
\newcommand{\barrierFunc}{\ensuremath{h}}

\newcommand{\gatingThreshold}{\ensuremath{\delta_\gatingFunc}}
#+END_EXPORT
Intuitively, the notion of safety in dynamical systems implies that /bad/ things do not happen.
However, what exactly are /bad/ things, i.e. what is safety?

*Stability* of an equilibrium point has been considered a property of a safety cite:berkenkampSafe2019.
Stability is a property of infinitely long trajectories.
A closed-loop system is said to be stable, if the state remains within some norm-ball around its equilibrium point for
all time steps, and is asymptotically stable if it eventually converges.
# For examples, stability can be considered  safety can be  Lyapunov functions

*Invariance* can also be see as a safety property.

*** Lyapunov Stability
\todo{add section on lyapunov stability}


*** Constraint Satisfaction
Alongside stability, it is common to require constraints on the states $\state$ and controls $\control$
of a controlled system.
For example, an autonomous system may wish to remain in a subset of it's state space where it knows its dynamics model
is valid.
The system may also be subject to constraints on the controls due to physical limitations, e.g.
how quickly a quadcopter can accelerate and turn.
Constraints on this type of safety can be encoded via inequality constraints on the states $\state$
and controls $\control$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
&\constraintFunc_\state(\state_\timeInd) \geq 0, \forall \timeInd \geq 0, \\
&\constraintFunc_\control(\control_\timeInd) \geq 0, \forall \timeInd \geq 0.
\end{align}
#+END_EXPORT
The feasible regions of these constraints can be written as sets,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\StateDomain =&\{ \state \in \R^\StateDim \mid \constraintFunc_\state(\state) \geq 0 \}, \\
\ControlDomain =&\{ \control \in \R^\ControlDim \mid \constraintFunc_\control(\control) \geq 0 \},
\end{align}
#+END_EXPORT
so the constraints can alternatively be written as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\state_\timeInd \in \StateDomain, \control_\timeInd \in \ControlDomain, \forall t \geq 0.
\end{align}
#+END_EXPORT
For a parametric policy $\policy$, the control constraints can be encoded directly into the policy by
parameterising the policy so that its range is restricted to $\ControlDomain$,
i.e. $\policy(\state_\timeInd) \in \ControlDomain$, for all $\state \in \R^\StateDomain$.
The state constraints can be enforced by ensuring that the set $\StateDomain$ is forward invariant.
#+BEGIN_EXPORT latex
\begin{definition}
Given a dynamical system $\dot{\state}(t) = f(\state(t))$ with states $\state(t) \in \StateDomain, \forall t$, then
a set $\safeSet$ is \textbf{forward invariant}, iff, for every possible initial state in the set $\state(0) \in \safeSet$,
all future states remain in the set, i.e. $\state(t) \in \safeSet, \forall t$.
The system is safe with respect to the set $\safeSet$ if the set is \textbf{forward invariant}.
\end{definition}
#+END_EXPORT
There are multiple approaches to enforcing state constraints via invariant sets.
Namely, Lyapanov functions \todo{cite lyapunov}
and control barrier functions cite:amesControl2019.
Lyapanov functions are more restrictive than control barrier functions as they provide stability guarantees
which are not a necessary condition to render $\StateDomain$ forward invariant.

*** Control Barrier Functions
In cite:nagumoUber1942 the authors derived the necessary and sufficient conditions for
set invariance in dynamical systems.
See cite:abrahamManifolds1988 for a modern proof.

*Barrier Certificates* were introduced in cite:prajnaBarrier2006,prajnaSafety2004 to formally guarantee safety
in nonlinear and hybrid systems.
The term /barrier/ is motivated by the optimisation literature where cost functions are augmented with
barrier functions to avoid undesirable regions.
#+BEGIN_EXPORT latex
\begin{theorem}[Nagumo's Theorem]
Given a dynamical system $\dot{\state} = f(\state)$ with $\state \in \R^\StateDim$ and a safe set $\safeSet$, that is the
superlevel set of a smooth function $\barrierFunc : \R^\StateDim \rightarrow \R$,
i.e. $\safeSet = \{ \state \in \R^\StateDim \mid \barrierFunc(\state) \geq 0 \}$ and
$\frac{\partial \barrierFunc}{\partial \state}(\state) \neq 0, \forall \state \in \R^D$
then Nagumo's Theorem gives the necessary and sufficient conditions for set invariance,
\begin{equation} \label{eq-}
\dot{\barrierFunc}(\state) \geq 0 \quad \forall \state \in \partial \safeSet \quad \implies \safeSet \quad \text{is invariant}
\end{equation}
where $\partial\safeSet$ denotes the boundary of the safe set, i.e. $\partial\safeSet = \{\state \in \R^\StateDim \mid \barrierFunc(\state)=  0 \}$.
\end{theorem}
#+END_EXPORT
These conditions have been rediscovered multiple times, in particular
cite:brezisCharacterization1970 and cite:bonyPrincipe1969.


*Control Barrier Certificates*
Safe sets based on control Lyapunov functions (CLF) can be overly restrictive as they render every
sub-level set

Compared with Lyapunov sublevel set based safe regions, control barrier certificates offer a more permissive
notion of safety.

In cite:wangSafe2018 they formulate safety through control barrier certificates.
#+BEGIN_EXPORT latex
\begin{equation} \label{eq-}
\exists \control \quad \text{s.t.} \quad \dot{\barrierFunc}(\state, \control) \geq - \alpha(\barrierFunc(\state))
\implies \safeSet \quad \text{is invariant}
\end{equation}
#+END_EXPORT
where $\alpha$ is an (extended) class $\kappa$ function.


*** Stochastic Safety
*Chance Constraints* The stochastic MPC formulation provides a principled approach to handling chance constraints
by imposing a maximum probability of violation.
The chance constrained, stochastic optimal control problem is then given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-stochastic-mpc}
\min_{\policies} &\E \left[ l_\TimeInd(\state_\TimeInd, \control_\TimeInd) + \sum_{\timeInd=1}^{\TimeInd-1} l_\timeInd(\state_\timeInd, \control_\timeInd) \right] \\
\text{s.t.} \quad &\state_{\timeInd+1} = f(\state_\timeInd, \control_\timeInd) \\
&\control_\timeInd = \pi_\timeInd (\state_\timeInd) \\
&\Pr \{ \state_\timeInd \in \StateDomain_{\timeInd+1} \} \geq p_{\state} \\
&\Pr \{ \control_\timeInd \in \ControlDomain_{\timeInd+1} \} \geq p_{\control}
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{definition}[Probabilistic i-step Reachable Set] \label{def-prs}
A set $\mathcal{R}_\timeInd$ is said to be a probabilistic reachable set ($i$-step PRS) of probability level $p$ if,
\begin{equation}
\Pr(e_\timeInd \in \mathcal{R}_\timeInd \mid e_0=0) \geq p.
\end{equation}
\end{definition}
#+END_EXPORT

$\StateDomain_{\timeInd} = \{ \state_\timeInd \in \StateDomain \mid \gatingFunc(\state_t, \control_t) > 0 \}$

linear state feedback controllers,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-policy}
\pi_\timeInd = \FeedbackGain_\timeInd (\mu^x_i - \state_\timeInd) + \mu^x_i
\end{align}
#+END_EXPORT

*** Latent Constraints
The gating network contains important information regarding our constraints.
In particular, when aiming to remain in a single desired dynamics mode the corresponding gating function
should remain high.
Equivalence of the gating functions in the deterministic setting implies a uniform distribution over
the mode indicator variable.
Given a threshold $\gatingThreshold \geq 0$ the safe set associated with the gating function safety constraints
can be written as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\mathcal{X} &= \{ \state \in \R^\StateDim \mid \gatingFunc(\state) \geq \gatingThreshold \}.
\end{align}
#+END_EXPORT
Given that the gating function is modelled as a Gaussian process
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\Pr(\gatingFunc(\state, \control) \geq \gatingThreshold \mid \state, \control) =
1 - \Phi\left( \frac{\gatingThreshold - \E\left[\gatingFunc(\state, \control) \right]}{\sqrt{\V\left[\gatingFunc(\state, \control) \right]}} \right)
\geq p_\state
\end{align}
#+END_EXPORT

*** old
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\mathcal{X} &= \{ \state \in \R^\StateDim \mid \gatingFunc(\state) > 0 \} \\
\mathcal{X} &= \{ \state \in \R^\StateDim \mid \E\left[ \gatingFunc(\state) > 0 \right] \} \\
\mathcal{X} &= \{ \state \in \R^\StateDim \mid \Pr\left[ \modeVar = k \right] \} \\
\mathcal{X} &= \{ \state \in \R^\StateDim \mid \V\left[ \gatingFunc(\state) < 0.1 \right] \}
\end{align}
#+END_EXPORT
Use RKHS to get something like -
#+BEGIN_EXPORT latex
\begin{lemma}
Assume that $h$ has bounded RKHS norm $|| h' ||_k \leq B$ and that measurements are corrupted by
$\sigma$-sub-Gaussian noise. If $\beta_n^{1/2} = B + 4 \sigma \sqrt{I(y_n ; h) + 1 + \text{ln}(1/\delta)}$, then for all
$a \in \mathcal{A}$ and $n>0$ it holds jointly with probability at least $1-\delta$ that
$|h'(a) - \mu_n(a)| \leq \beta_n^{1/2} \simga_n(a)$.
\end{lemma}
#+END_EXPORT
#+BEGIN_EXPORT latex
\begin{theorem}
For any $\delta \in (0,1)$, we have with probability at least $1 - \delta$ that $\gatingFunc(\state) > 0$.
\end{theorem}
#+END_EXPORT



#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\mathcal{X} &= \{ \state \in \R^\StateDim \mid \gatingFunc(\state) > 0 \}
\end{align}
#+END_EXPORT

\newpage
** Probabilistic Geometries for Quadratic Costs
*Linear Dynamics*
Approximate dynamics to be linear around a nominal trajectory
$(\nominalStateTraj, \nominalControlTraj)$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\state_{\timeInd+1} &= \dynamicsFunc(\state_{\timeInd}, \control_{\timeInd}) \\
&= \stateDynamics \state_{\timeInd} + \controlDynamics \control_{\timeInd}
\end{align}
#+END_EXPORT
where,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\stateDynamics = \frac{\partial \dynamicsFunc}{\partial \state} \quad
\controlDynamics = \frac{\partial \dynamicsFunc}{\partial \control}
\end{align}
#+END_EXPORT
Given that the mean and covariance functions are twice differentiable
then $\stateDynamics$ and $\controlDynamics$ are also GPs.

*Quadratic Cost*
A popular cost function is the quadratic cost given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
J(\traj, \utraj) = \sum_{\timeInd=1}^\TimeInd
\state_{\timeInd}^T
\stateCost
\state_{\timeInd}
+
\control_{\timeInd}^T
\controlCost
\control_{\timeInd}
\end{align}
#+END_EXPORT
where $\stateCost$ and $\controlCost$ are
user defined, positive semi-definite and positive
definite weight matrices respectively.
If the cost function is not quadratic then a quadratic approximation would be obtained with a Taylor Expansion.
Importantly, given a state trajectory where each state is
normally distributed
$\state_{\timeInd} \sim \mathcal{N}(\stateMean_{\timeInd}, \stateCov_{\timeInd})$
the expected cost for a trajectory can be calculated in closed-form,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\E_{\stateTrajDist} \left[ J(\traj, \utraj) \right]
= \sum_{\timeInd=1}^\TimeInd
\text{tr}(\stateCost \stateCov_\timeInd)
+
\stateMean_{\timeInd}^T
\stateCost
\stateMean_{\timeInd}
+
\control_{\timeInd}^T
\controlCost
\control_{\timeInd}
\end{align}
#+END_EXPORT
*Riemannian Cost Function*
Consider defining the state weight matrix $\stateCost$
as the Riemannian metric tensor $\metricTensor$ and replacing the state
\state_{\timeInd} with the state difference
(\state_{\timeInd} - \state_{\timeInd - 1}) to get the cost,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
J(\traj, \utraj) =& \sum_{\timeInd=1}^\TimeInd
(\state_{\timeInd} - \state_{\timeInd - 1})^T
\metricTensor(\state_{\timeInd})
(\state_{\timeInd} - \state_{\timeInd - 1})
+ \control_{\timeInd}^T \controlCost \control_{\timeInd} \\
=& \sum_{\timeInd=1}^\TimeInd
\stateDiff_{\timeInd}^T
\metricTensor(\state_{\timeInd})
\stateDiff_{\timeInd}
+ \control_{\timeInd}^T \controlCost \control_{\timeInd}.
\end{align}
#+END_EXPORT
This cost function is calculating the length
of the trajectory on the manifold $\manifold$ endowed with
the metric $\metricTensor$.
Note that
$(\state_{\timeInd} - \state_{\timeInd - 1})^T \metricTensor(\state_{\timeInd}) (\state_{\timeInd} - \state_{\timeInd - 1})$
is the curve energy but Soren says that minimising this is equivalent
to minimising the curve length.

Following from the sum and linear transformation rules of normally
distributed random variable the state difference
$\stateDiff_\timeInd =\state_{\timeInd} - \state_{\timeInd - 1}$
is also
normally distributed,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\stateDiff_\timeInd
\sim \mathcal{N}(\stateDiffMean, \stateDiffCov),
\end{align}
#+END_EXPORT
where
$\stateDiffMean=\stateMean_{\timeInd} - \stateMean_{\timeInd-1}$
and $\stateDiffCov = \stateCov_{\timeInd} + \stateCov_{\timeInd-1}$.
The expected cost for a trajectory requires an expectation over the Jacobian $\jacGivenInput$
as well as the state,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\E_{p(\state_\timeInd, \state_{\timeInd-1}, \jac(\state_\timeInd))} \left[ J(\stateTraj, \controlTraj) \right]
= \sum_{\timeInd=1}^\TimeInd
\E_{\stateDist}\left[ \stateDiff_\timeInd^T
\E_{\jacGivenInput}\left[ \jac(\state_\timeInd) \jac(\state_\timeInd)^T \right]
 \stateDiff_\timeInd \right]
\end{align}
#+END_EXPORT
There is no closed-form expression for this expectation due to the Jacobian's dependence on the state.
However, following a mean-field approximation  the Jacobain can be assumed to be independent of the state,
resulting in the expected cost being given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
\E \left[ J(\stateTraj, \controlTraj) \right] &=
\sum_{\timeInd=1}^\TimeInd
\E_{\stateDist}\left[ \stateDiff_\timeInd^T
\E_{\jacDist}\left[ \jac(\state_\timeInd) \jac(\state_\timeInd)^T \right]
 \stateDiff_\timeInd \right] \\
&= \sum_{\timeInd=1}^\TimeInd \stateDiffMean^T (\jacMean \jacMean^T + \jacCov) \stateDiffMean
+ \text{tr}\left(
\left(\jacMean \jacMean^T + \jacCov\right)
\stateDiffCov \right)
\end{align}
#+END_EXPORT
It's a bit hacky but we could assumed the Jacobian distribution is given by marginalising the state
and moment matching,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-}
p(\mathbf{J}(\state_{\timeInd})) =
%\int \mathbf{J}(\state_{\timeInd}) \stateDist \text{d}\state_{\timeInd}
\int p(\mathbf{J}(\state_{\timeInd}) \mid \state_\timeInd)
\mathcal{N}(\state_{\timeInd} \mid \stateMean_\timeInd, \stateCov_\timeInd)
\text{d}\state_{\timeInd}
\approx \mathcal{N}(\mathbf{J}(\state_{\timeInd}) \mid \jacMean, \jacCov)
\end{align}
#+END_EXPORT

** Results
** Conclusion
* Active Learning in Multimodal Dynamical Systems label:chap-active-learning
#+begin_export latex
\epigraph{Real knowledge is to know the extent of one’s ignorance.”}{\textit{Confucius (Philosopher, 551–479BC).}}
#+end_export
** Intro :ignore:
#+BEGIN_EXPORT latex
\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{\MakeUppercase{\timeInd}}}
\newcommand{\stateTraj}{\ensuremath{\bar{\state}}}
\newcommand{\controlTraj}{\ensuremath{\bar{\control}}}
\newcommand{\nominalStateTraj}{\ensuremath{\stateTraj_*}}
\newcommand{\nominalControlTraj}{\ensuremath{\controlTraj_*}}

\newcommand{\utraj}{\ensuremath{\bar{\control}}}
\newcommand{\policies}{\ensuremath{\Pi}}
\newcommand{\policy}{\ensuremath{\pi_{\theta}}}


\renewcommand{\dataset}{\ensuremath{\mathcal{D}}}
\renewcommand{\params}{\ensuremath{\bm\theta}}
\renewcommand{\input}{\ensuremath{\hat{\state}}}
\renewcommand{\output}{\ensuremath{\modeVar}}
\renewcommand{\inputDomain}{\ensuremath{\controlDomain}}
\renewcommand{\outputDomain}{\ensuremath{\mathcal{A}}}

\newcommand{\outputGivenInputParams}{\ensuremath{p(\output \mid \input, \params)}}
\newcommand{\outputGivenInputData}{\ensuremath{p(\output \mid \input, \mathcal{D})}}

\newcommand{\gatingFunc}{\ensuremath{g}}
#+END_EXPORT
This chapter is concerned with exploration in mulitmodal dynamical systems
where the transition dynamics are /not fully known a priori/.
In particular, it is interested in exploring a single desired dynamics
mode whilst avoiding entering any of the other modes.
This is a challenging problem in systems where both the dynamical modes and
how they switch over the state-control space are /not known a priori/.
This is because the agent must observe regions outside of the desired dynamics mode
in order to know that a particular region does not belong to the desired mode.
Based on this logic, this chapter utilises the model from Chapter ref:chap-dynamics
to design an information theoretic trajectory optimisation algorithm
that

The trajectory optimiastion algorithm in this chapter is only concerned with
the aforementioned exploration and is not intended to run fast enough for
real-time MPC.
Although not explored here, the algorithm could be used for guided policy
search in a model-based reinforcement learning setting
cite:levineGuided2013,levineVariational2013,okadaVariational2020.
Alternatively, one could explore approximations enabling the approach to
work for real time MPC e.g. linearising the dynamics and/or a second order
Taylor expansion of the cost function.
\todo{should this be cost of value function??}

To the best of our knowledge, there is no previous work addressing exploration of
a single dynamics mode in multimodal dynamical systems.
cite:schreiterSafe2015 use a GP classifier to identify safe and unsafe regions
when learning GP dynamics models in an active learning setting.
However, they assume that they can directly observe whether a particular
data point from the environment belongs to either the safe or unsafe regions.
In contrast, this chapter is concerned with scenarios where the mode cannot be directly observed from the
environment, but instead, is inferred by a probabilistic dynamics model.

Utilising mutual information for active learning has been well motivated by, for example,
cite:krauseNearOptimal2008.
They highlight that mutual information may lead to a more accurate model than differential entropy.
It has also been shown that minimising the mutual information  is the same as minimising the expected posterior
uncertainty (conditional entropy) in the model cite:ertinMaximum2003.

** Bayesian Information Theoretic Active Learning
Following cite:houlsbyBayesian2011 this section introduces Bayesian
information theoretic active learning.
The Bayesian framework assumes that there are some latent parameters $\params$,
that control the dependence between the inputs $\input \in \inputDomain$
and the outputs $p(\output \mid \input, \params)$.
Given observations of the system
$\mathcal{D} = \{(\input_\timeInd, \output_\timeInd)\}_{\timeInd=1}^\TimeInd$,
it is assumed that the posterior over the parameters given the data
$p(\params \mid \dataset)$
has been inferred.
The goal of information theoretic active learning is to reduce the number of
possible hypothesis as fast as possible, i.e. minimise the uncertainty associated
with the parameters using Shannon's entropy cite:coverElements2006.
#+BEGIN_EXPORT latex
\newcommand{\crv}{\ensuremath{X}}
\newcommand{\density}{\ensuremath{p(\crv)}}
\begin{myquote}
\textbf{Differential Entropy}
Let $\crv$ be a continuos random variable, with a probability density
$\density$,
whose support is a set $\mathcal{X}$.
The differential entropy $H(\crv)$ is then defined as,
\begin{align} \label{eq-differential-entropy}
H(\crv) = - \int_{\mathcal{X}} \density \text{log} \density \text{d} \crv.
\end{align}
\end{myquote}
#+END_EXPORT
Data points $\dataset^*$ should thus be selected according to,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-posterior-entropy}
\text{arg}\underset{\dataset^*}{\min}
H(\params \mid \dataset^*)
= \text{arg}\underset{\dataset^*}{\min}
- \int p(\params \mid \dataset^*) \text{log} p(\params \mid \dataset^*) \text{d} \params.
\end{align}
#+END_EXPORT
In general, solving this problem is NP-hard,
so it is common to approximate it with a myopic greedy approximation.
cite:krauseNearOptimal2008,dasguptaAnalysis2005 show that a myopic policy can perform near-optimally.
The objective is then to seek the input $\input$ that maximises the decrease in
expected posterior entropy,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-expected-decrease-entropy}
\text{arg}\underset{\control}{\max}
H(\params \mid \dataset)
- \E_{\output \sim \outputGivenInputData}
\left[ H(\params \mid \output, \input, \dataset) \right].
\end{align}
#+END_EXPORT
# The expectation requires the unseen output $\output$ and many works have
An important insight is that minimising Eq. ref:eq-expected-decrease-entropy
is equivalent to minimising the conditional mutual information between
the output and the parameters $I(\output, \params \mid \input, \dataset$.
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Mutual Information}
Given two sets of random variables, $\mathbf{X}$ and $\mathbf{F}$, with joint density $p(\mathbf{X}, \mathbf{F})$ the
mutual information \cite{coverElements2006} is given by,
\begin{align} \label{eq-mutual-information}
I(\mathbf{X};\mathbf{F}) = \int p(\mathbf{X},\mathbf{F}) \text{log}\frac{p(\mathbf{X},\mathbf{F})}{p(\mathbf{X})p(\mathbf{F})}
\text{d}\mathb{X} \text{d}\mathb{F},
\end{align}
#+END_EXPORT
and its well known relationship to differential entropy $H(\cdot)$ is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information-entropy}
I(\mathbf{X};\mathbf{F}) =  H(\mathbf{X}) - H(\mathbf{X} \mid \mathbf{F}).
\end{align}
\end{myquote}
#+END_EXPORT
An equivalent objective function can therefore be formulated in the output $\output$
space as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-output-space-entropy}
\text{arg}\underset{\input}{\max}
H(\output \mid \input, \dataset)
- \E_{\params \sim p(\params \mid\dataset)} \left[ H(\output \mid \input, \params) \right].
\end{align}
#+END_EXPORT
The entropy terms in Eq. ref:eq-output-space-entropy are now calculated
in a (usually) low dimensional output space.
In the binary classification setting, Eq. ref:eq-output-space-entropy simply
involves calculating the entropy of Bernoulli random variables.
#+BEGIN_EXPORT latex
\begin{myquote}
\textbf{Entropy of Bernoulli Variable}
Consider a Bernoulli random varialbe $\output$ with probability $p$.
The entropy of such a Bernoulli random variable is given by,
\begin{align} \label{eq-entropy-bernoulli}
H(\alpha) &= h_{\text{Bern}}(p) \\
h_{\text{Bern}}(p) &= -p \text{log}p - (1-p) \text{log}(1-p),
\end{align}
\end{myquote}
#+END_EXPORT
Intuitively, the first term in Eq. ref:eq-output-space-entropy
($H(\output \mid \input, \dataset)$) seeks to
find the input $\input$ where the model is marginally most uncertain about
its corresponding output $\output$, whilst the
second term
$(- \E_{\params \sim p(\params \mid\dataset)} \left[ H(\output \mid \input, \params) \right])$
prefers inputs whose parameter settings are confident.
As highlighted by cite:houlsbyBayesian2011, this can be interpreted as finding the
input $\input$ for which the parameters under the posterior, disagree about
the output $\output$ the most.

*** Active Learning for GP Classification
cite:houlsbyBayesian2011 formulate Bayesian active learning in the GP
binary classification model,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-binary-gp-classification}
\gatingFunc &\sim \mathcal{GP}(\mu(\cdot), k(\cdot,\cdot)) \\
\output \mid \gatingFunc, \input &\sim \text{Bernoulli}(\Phi(\gatingFunc(\input)))
\end{align}
#+END_EXPORT
where $\Phi$ represents the Gaussian CDF and $\Phi(\gatingFunc(\input))$ is
the probability of the (Bernoulli) output variable
$\alpha$ cite:nickischApproximations2008.
In this context,  Eq. ref:eq-output-space-entropy can be re-written as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-output-space-entropy-gp-classification}
\text{arg}\underset{\input}{\max}
H(\output \mid \input, \dataset)
- \E_{\gatingFunc \sim p(\gatingFunc \mid\dataset)} \left[ H(\output \mid \input, \gatingFunc) \right].
\end{align}
#+END_EXPORT
Their approach introduces approximations to the two terms in
Eq. ref:eq-output-space-entropy-gp-classification.
They assume that the posterior over the latent function $\gatingFunc$
is approximated to be Gaussian,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-classification-posterior}
\gatingFunc \mid \input, \mathcal{D} &\sim \mathcal{N}(\gatingFunc(\input) \mid \mu_{\input,\dataset}, \sigma^2_{\input,\dataset}).
%\mu_{\input,\dataset} &= k(\input, \allInput) k(\allInput, \allInput)^{-1} \bm\alpha \\
%\sigma^2_{\input,\dataset}) &= k(\input, \input)
%- k(\input, \allInput) k(\allInput, \allInput)^{-1} k(\allInput, \input)
\end{align}
#+END_EXPORT
The $\overset{1}{\approx}$ symbol will be used to indicate when such an approximation
has been exploited.
The first term in Eq. ref:eq-output-space-entropy-gp-classification is approximated as follows,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-entropy--data}
H(\output \mid \input, \dataset) &\overset{1}{\approx}
h_{\text{Bern}} \left(\int \Phi(\gatingFunc(\input)) \mathcal{N}(\gatingFunc(\input) \mid \mu_{\input,\dataset}, \sigma^2_{\input,\dataset}) \text{d} \gatingFunc(\input) \right) \\
&= h_{\text{Bern}} \left( \Phi \left( \frac{\mu_{\input,\dataset}}{\sqrt{\sigma^2_{\input,\dataset} + 1}} \right) \right).
\end{align}
#+END_EXPORT
The second term is approximated as follows,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-entropy-expected-term}
\E_{\gatingFunc \sim p(\gatingFunc \mid \dataset)}
\left[H(\output \mid \input, \gatingFunc) \right]
&\overset{1}{\approx} \int h_{\text{Bern}}(\Phi(\gatingFunc(\input)))
\mathcal{N}(\gatingFunc(\input) \mid \mu_{\input,\dataset}, \sigma^2_{\input,\dataset})
\text{d} \gatingFunc(\input) \\
&\overset{2}{\approx}
\int \text{exp}\left( -\frac{\gatingFunc(\input)^2}{\pi \text{ln}2} \right)
\mathcal{N}(\gatingFunc(\input) \mid \mu_{\input,\dataset}, \sigma^2_{\input,\dataset})
\text{d} \gatingFunc(\input) \\
&= \frac{C}{\sqrt{\sigma_{\input,\dataset}^2 + C^2}}
\text{exp}\left( -\frac{\mu_{\input,\dataset}^2}{2\left( \sigma_{\input,\dataset}^2 + C^2 \right)} \right) \label{eq-entropy-expected-term-last}
\end{align}
#+END_EXPORT
where $C = \sqrt{\frac{\pi \text{ln}2}{2}}$.
This approximation exploits a Taylor expansion of $\text{ln} h_{\text{Bern}}(\Phi(\gatingFunc(\input)))$
(see supplementary material of cite:houlsbyBayesian2011)
allowing it to be represented up to $\mathcal{O}(\gatingFunc(\input)^4)$ by a squared
exponential curve ($\text{exp}(-\gatingFunc(\input)^2/\pi\text{ln}2)$).
This approximation is referred to as $\overset{2}{\approx}$.
A simple Gaussian convolution then gives the closed form expression in Eq.
ref:eq-entropy-expected-term-last.
The objective in Eq. ref:eq-output-space-entropy-gp-classification can then be approximated as,
#+BEGIN_EXPORT latex
\newcommand{\approxEntropy}{\ensuremath{\hat{H}}}
\begin{align} \label{eq-approximate-output-space-entropy-obj}
\text{arg}\max_{\input}
\approxEntropy(\output)
\end{align}
#+END_EXPORT
where,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-approximate-output-space-entropy}
\approxEntropy(\output)
\coloneqq
h_{\text{Bern}} \left( \Phi \left( \frac{\mu_{\input,\dataset}}{\sqrt{\sigma^2_{\input,\dataset} + 1}} \right) \right)
+ \frac{C}{\sqrt{\sigma_{\input,\dataset}^2 + C^2}}
\text{exp}\left( -\frac{\mu_{\input,\dataset}^2}{2\left( \sigma_{\input,\dataset}^2 + C^2 \right)} \right)
\end{align}
#+END_EXPORT
The objective is smooth and differentiable for most practically relevant kernels of $\input$, so
gradient-based optimisation can be used to find the maximally informative $\input$.


# #+BEGIN_EXPORT latex
# \begin{myquote}
# \textbf{Entropy of Bernoulli Variable}
# Consider the probit case, where the value of the output $\output$,
# given latent gating function $\gatingFunc(\input)$, takes a Bernoulli
# distribution with probability $\Phi(\gatingFunc(\input))$,
# where $\Phi$ represents the Gaussian CDF.
# The entropy of such a Bernoulli random variable is given by,
# \begin{align} \label{eq-entropy-bernoulli}
# H(\alpha \mid \control, \gatingFunc) &= H_{\mathcal{Bern}}(\Phi(\gatingFunc(\control))) \\
# H_{\mathcal{Bern}}(p) &= -p \text{log}p - (1-p) \text{log}(1-p),
# \end{align}
# where the probability $\Phi(\gatingFunc(\input))$ has been denoted $p$.
# \end{myquote}
# #+END_EXPORT

** Bayesian Information Theoretic Exploration Strategy
#+BEGIN_EXPORT latex
\newcommand{\posterior}{\ensuremath{p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \control_{\timeInd})}}
#+END_EXPORT

# The /static/ active learning problem addressed previously is fundamentally different
# to the problem in dynamical systems.
# In the /static/ problem, it is possible to query any point in the input domain.
# As such, it is open to a clean information-theoretic treatment.
# In the /dynamic/ problem, the system must be steered through the
# unknown dynamics $\dynamics$ by a sequence of controls $\controlTraj$.
In dynamical systems an arbitrary state $\state$ cannot be sampled, so instead, the dynamics must be steered
to $\state$ through the unknown dynamics $\dynamicsFunc$ through a sequence of controls $\control$.
Thus, there is information gain along the trajectory which must also be
considered.
As highlighted by cite:buisson-fenetActively2020, information gain in dynamical systems is therefore
fundamentally different to the /static/ problem addressed by cite:krauseNearOptimal2008
and cite:houlsbyBayesian2011.
The goal here is to pick the most informative control trajectory $\controlTraj$ whilst observing $\stateTraj$.

Recent work has addressed active learning in (unimodal) GP dynamics models
citep:buisson-fenetActively2020 and (unimodal)
GP state-space models citep:caponeLocalized2020.
cite:schreiterSafe2015 consider safe exploration for active learning where they distinguish safe and unsafe regions
with a binary GP classifier, which is learned separately to the dynamics model. Their exploration strategy considers
the differential entropy of the posterior GP associated with the dynamics model and they use the GP classifier to
define a set of constraints.
In contrast, this chapter exploits the coupled learning of the dynamics modes and the gating network (i.e. the classifier)
and projects the exploration strategy onto the posterior GP(s) associated with the gating network.
The coupled learning provides well-calibrated uncertainty estimates in the gating network, making it a
good choice for information-based exploration.

In order to calculate the approximate entropy objective in the output space $\approxEntropy(\output_{\timeInd})$
(Eq. ref:eq-approximate-output-space-entropy-obj)
at a given time step $\timeInd$, the posterior $\posterior$ over the gating function is required.
After the first time step this distribution is calculated as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-posterior-0}
p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \control_{\timeInd})&=
p(\state_0)
\int p(\gatingFunc(\state_{1}, \control_{1}) \mid \state_{0}, \control_{0})
p(\state_{1} \mid \state_{0}, \control_{0}) p(\control_{0}) \text{d}\state_0
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \control_{\timeInd})&=
p(\state_0) \prod_{\timeInd=1}^{\timeInd}
\int p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \state_{\timeInd}, \control_{\timeInd})
p(\state_{\timeInd} \mid \state_{\timeInd-1}, \control_{\timeInd-1}) p(\control_{\timeInd}) \text{d}\state_\timeInd
\end{align}
#+END_EXPORT


#+BEGIN_EXPORT latex
\newcommand{\gatingOutput}{\ensuremath{h(\singleInput)}}
\newcommand{\gatingTransition}{\ensuremath{p(\gatingOutput \mid \state_\timeInd, \control_\timeInd)}}
\begin{align} \label{eq-mutual-information}
\underset{\stateTraj \in \stateDomain^\TimeInd, \controlTraj \in \controlDomain^\TimeInd}{\text{argmax}} \approxEntropy(\stateTraj, \controlTraj) &=
\underset{\state \in \stateDomain, \control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd \approxEntropy_{\timeInd}(\state_{\timeInd}, \control_{\timeInd})
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-gp-classification-posterior}
\gatingFunc \mid \input, \mathcal{D} &\sim \mathcal{N}(\gatingFunc(\input) \mid \mu_{\input,\dataset}, \sigma^2_{\input,\dataset}).
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\newcommand{\gatingOutput}{\ensuremath{h(\singleInput)}}
\newcommand{\gatingTransition}{\ensuremath{p(\gatingOutput \mid \state_\timeInd, \control_\timeInd)}}
\begin{align} \label{eq-mutual-information}
\underset{\stateTraj \in \stateDomain^\TimeInd, \controlTraj \in \controlDomain^\TimeInd}{\text{argmax}} H(\stateTraj, \controlTraj) &=
\underset{\state \in \stateDomain, \control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd H_{\timeInd}(\state_{\timeInd}, \control_{\timeInd}) \\
&= \underset{\state \in \stateDomain, \control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd - \int \gatingTransition
\text{log} \gatingTransition \text{d} \gatingOutput
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
\underset{\control \in \controlDomain}{\text{argmax}} H(\stateTraj, \controlTraj) &=
\underset{\control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd H_{\timeInd}(\state_{\timeInd}, \control_{\timeInd}) \\
&= \underset{\control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd - \int \gatingTransition
\text{log} \gatingTransition \text{d} \gatingOutput \\
\end{align}
#+END_EXPORT


The exploration strategy introduced in this chapter builds on the approximate
entropy objective in Eq. ref:eq-approximate-output-space-entropy.


is a selective
sampling approach based on the differential entropy of the GP posterior
over the desired modes gating function.
The
Considering a full discriminative model,
discover the dependence of the mode indicator variable $\modeVar \in \modeDomain$
on the state-control input $\input \in \inputDomain$





prediction with uncertain input


We consider a fully discriminative model where the goal of active learning is to discover the dependence of some variable y ∈ Y on an input variable x ∈ X . The key idea in active learning is that the learner chooses the input queries xi ∈ X and observes the system’s response yi, rather than passively receiving
(xiyi) pairs.




#+BEGIN_EXPORT latex
\begin{align} \label{eq--entropy-model}
\E_{\gatingFunc \sim p(\gatingFunc \mid \mathcal{D})} \left[ H(\alpha \mid \control, \gatingFunc) \right]
&\approx \int h \left( \Phi(\gatingFunc(\state, \control)) \mathcal{N}(\gatingFunc(\state, \control) \mid \mu_\gatingFunc, \sigma^2_\gatingFunc) \text{d} \gatingFunc(\state, \control)
\end{align}
#+END_EXPORT

** Mode Constraints
In order to ensure that the exploration strategy remains in the desired dynamics
mode this section introduces additional information to describe the discriminative
function when it gets close to the decision boundary  w

** The Algorithm
This section introduces the entropy based active learning framework extended by

** Content
An approximated mutual information exploration criterion is used

entropy

#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
\underset{\control \in \controlDomain}{\text{argmax}} H(\gatingFunc(\stateTraj, \controlTraj) \mid \controlTraj) &=
\underset{\control \in \controlDomain}{\text{argmax}} \sum_{\timeInd=1}^\TimeInd H_{\timeInd}(\gatingFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \control_\timeInd)
\end{align}
#+END_EXPORT
where
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mutual-information}
p(\gatingFunc(\state_{1:\TimeInd}, \control_{1:\TimeInd}) \mid \control_{1:\TimeInd})&=
p(\state_0) \prod_{\timeInd=1}^{\TimeInd-1}
\int p(\gatingFunc(\state_{\timeInd}, \control_{\timeInd} \mid \state_{\timeInd}, \control_{\timeInd})
p(\state_{\timeInd} \mid \state_{\timeInd-1}, \control_{\timeInd-1}) p(\control_{\timeInd}) \text{d}\state_\timeInd
\end{align}
#+END_EXPORT

* Applications
** 2D Point Mass Simulation
** 3D Quadcopter Simulation
** 2D Quadcopter Real-World
* Conclusion
* Appendix
** Multivariate Normals
** Derivatives of a Gaussian Process
** Results
*** Velocity Controlled Quadcopter Experiments
Table ref:tab-params-quadcopter contains the optimisation settings and initial values for the optimisable parameters
that were used to train the model on the velocity controlled quadcopter data set in  Section ref:sec-brl-experiment.

*Experts* Both expert's GP priors were initialised with constant mean functions (with
a learnable parameter $c_{\modeInd}$) and separate independent squared exponential
kernels (with Automatic Relevance Determination) on each output dimension.
Table ref:tab-params-quadcopter shows a single set of kernel parameters $\sigma_f, l$ for each expert, as
the kernel associated with each output dimension was initialised with the same initial values.
Each experts likelihood was initialised with diagonal covariance matrices $\Sigma_{\epsilon_{\modeInd}}$.

*Gating Network* The gating network GP was initialised with a zero mean function, and a squared exponential kernel
with ARD.

#+begin_table
#+LATEX: \caption{Optimiser settings and initial values for optimisable parameters before training on the DJI Tello quadcopter data set.}
#+LATEX: \label{tab-params-quadcopter}
#+LATEX: \resizebox{\textwidth}{!}{
#+ATTR_LATEX: :center nil
|                 | Description                | Symbol                    | Value                                                                                   |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Num data                   | $\NumData$                | $1129$                                                                                  |
|                 | Batch size                 | $\NumData_b$              | $64$                                                                                    |
| Optimiser       | Num epochs                 | N/A                       | $6000$                                                                                  |
|                 | Num samples                | $S$                       | $1$                                                                                     |
|                 | Learning rate              | N/A                       | $0.01$                                                                                  |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Constant mean function     | $c_1$                     | $[0,0]$                                                                                 |
|                 | Kernel variance            | $\sigma_f$                | $0.1$                                                                                   |
|                 | Kernel lengthscales        | $l$                       | $[10, 10]$                                                                              |
|                 | Likelihood variance        | $\Sigma_{\epsilon_1}$     | $\diag([0.0011, 0.0011])$                                                               |
| Expert 1        | Num inducing points        | $\NumInducing$            | $100$                                                                                   |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Constant mean function     | $c_2$                     | $[0,0]$                                                                                 |
|                 | Kernel variance            | $\sigma_f$                | $20$                                                                                    |
|                 | Kernel lengthscales        | $l$                       | $[0.5, 0.5]$                                                                            |
|                 | Likelihood variance        | $\Sigma_{\epsilon_2}$     | $\diag([1.9,1.9])$                                                                      |
| Expert 2        | Num inducing points        | $\NumInducing$            | $100$                                                                                   |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel variance            | $\sigma_f$                | $0.6$                                                                                   |
|                 | Kernel lengthscales        | $l$                       | $[1.5, 1.5]$                                                                            |
| Gating function | Num inducing points        | $\NumInducing$            | $100$                                                                                   |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | $2 \times$ ones plus Gaussian noise                                                     |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
#+LATEX: }
#+end_table

*** Motorcycle Experiments

**** Two Experts :ignore:
\newline
*Model Training*
Table ref:tab-params-quadcopter contains the initial values for all of the trainable parameters in the model.

#+begin_table
#+LATEX: \caption{Initial parameter settings before training on motorcycle data set with two experts.}
#+LATEX: \label{tab-params-quadcopter}
#+LATEX: \resizebox{\textwidth}{!}{
#+ATTR_LATEX: :center nil
|                 | Description                | Symbol                    | Value                                                                                   |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Num data                   | $\NumData$                | $133$                                                                                   |
|                 | Batch size                 | $\NumData_b$              | $30$                                                                                    |
| Optimiser       | Epochs                     | N/A                       | $8000$                                                                                  |
|                 | Num samples                | $S$                       | $1$                                                                                     |
|                 | Learning rate              | N/A                       | $0.001$                                                                                 |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $0.1$                                                                                   |
|                 | Kernel Lengthscales        | $l$                       | $10$                                                                                    |
|                 | Likelihood Variance        | $\sigma_n$                | $0.0011$                                                                                |
| Expert 1        | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $20$                                                                                    |
|                 | Kernel Lengthscales        | $l$                       | $0.5$                                                                                   |
|                 | Likelihood Variance        | $\sigma_n$                | $0.9$                                                                                   |
| Expert 2        | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $3$                                                                                     |
|                 | Kernel Lengthscales        | $l$                       | $0.5$                                                                                   |
| Gating Function | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | $10 \times$ ones plus Gaussian noise                                                    |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
#+LATEX: }
#+end_table

**** Three Experts :ignore:
\newline
*Model Training*
Table ref:tab-params-quadcopter contains the initial values for all of the trainable parameters in the model.

#+begin_table
#+LATEX: \caption{Initial parameter settings before training on motorcycle data set with two experts.}
#+LATEX: \label{tab-params-quadcopter}
#+LATEX: \resizebox{\textwidth}{!}{
#+ATTR_LATEX: :center nil
|                 | Description                | Symbol                    | Value                                                                                   |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Num data                   | $\NumData$                | $133$                                                                                   |
|                 | Batch size                 | $\NumData_b$              | $30$                                                                                    |
| Optimiser       | Epochs                     | N/A                       | $8000$                                                                                  |
|                 | Num samples                | $S$                       | $1$                                                                                     |
|                 | Learning rate              | N/A                       | $0.001$                                                                                 |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $0.1$                                                                                   |
|                 | Kernel Lengthscales        | $l$                       | $10$                                                                                    |
|                 | Likelihood Variance        | $\sigma_n$                | $0.0011$                                                                                |
| Expert 1        | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $20$                                                                                    |
|                 | Kernel Lengthscales        | $l$                       | $0.5$                                                                                   |
|                 | Likelihood Variance        | $\sigma_n$                | $0.9$                                                                                   |
| Expert 2        | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | ones plus Gaussian noise                                                                |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
|                 | Kernel Variance            | $\sigma_f$                | $3$                                                                                     |
|                 | Kernel Lengthscales        | $l$                       | $0.5$                                                                                   |
| Gating Function | Num inducing points        | $\NumInducing$            | $30$                                                                                    |
|                 | Inducing inputs            | $\expertInducingInput$    | $\expertInducingInput \subseteq \allInput$ with $\#\expertInducingInput = \NumInducing$ |
|                 | Inducing variable mean     | $\mode{\hat{\mathbf{m}}}$ | zeros plus Gaussian noise                                                               |
|                 | Inducing variable Cholesky | $\mode{\hat{\mathbf{S}}}$ | $10 \times$ ones plus Gaussian noise                                                    |
|-----------------+----------------------------+---------------------------+-----------------------------------------------------------------------------------------|
#+LATEX: }
#+end_table

* Back Matter :ignore:
** Bibliography :ignore:

#+BEGIN_EXPORT latex
% \begingroup
% \sloppy
% \setstretch{1}
% \setlength\bibitemsep{3pt}
\printbibliography
% \endgroup
#+END_EXPORT

* citations to include

cite:schneiderExploiting1996
